
@inproceedings{rigoll_high_1998,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {High performance real-time gesture recognition using {Hidden} {Markov} {Models}},
	isbn = {978-3-540-69782-4},
	doi = {10.1007/BFb0052990},
	abstract = {An advanced real-time system for gesture recognition is presented, which is able to recognize complex dynamic gestures, such as ”hand waving”, ”spin”, ”pointing”, and ”head moving”. The recognition is based on global motion features, extracted from each difference image of the image sequence. The system uses Hidden Markov Models (HMMs) as statistical classifier. These HMMs are trained on a database of 24 isolated gestures, performed by 14 different people. With the use of global motion features, a recognition rate of 92.9\% is achieved for a person and background independent recognition.},
	language = {en},
	booktitle = {Gesture and {Sign} {Language} in {Human}-{Computer} {Interaction}},
	publisher = {Springer},
	author = {Rigoll, Gerhard and Kosmala, Andreas and Eickeler, Stefan},
	editor = {Wachsmuth, Ipke and Fröhlich, Martin},
	year = {1998},
	note = {27 citations (Crossref) [2023-07-11]
122 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, tech:rgb, from:cite.bib, classes:24, participants:14},
	pages = {69--80},
	annote = {[TLDR] An advanced real-time system for gesture recognition is presented, which is able to recognize complex dynamic gestures, such as hand waving, spin, pointing, and head moving, based on global motion features extracted from each difference image of the image sequence.},
	file = {Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf:/Users/brk/Zotero/storage/BBCJSZZM/Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf:application/pdf},
}

@article{vasconez_hand_2022,
	title = {Hand {Gesture} {Recognition} {Using} {EMG}-{IMU} {Signals} and {Deep} {Q}-{Networks}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/24/9613},
	doi = {10.3390/s22249613},
	abstract = {Hand gesture recognition systems (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) have been studied for different applications in recent years. Most commonly, cutting-edge HGR methods are based on supervised machine learning methods. However, the potential benefits of reinforcement learning (RL) techniques have shown that these techniques could be a viable option for classifying EMGs. Methods based on RL have several advantages such as promising classification performance and online learning from experience. In this work, we developed an HGR system made up of the following stages: pre-processing, feature extraction, classification, and post-processing. For the classification stage, we built an RL-based agent capable of learning to classify and recognize eleven hand gestures—five static and six dynamic—using a deep Q-network (DQN) algorithm based on EMG and IMU information. The proposed system uses a feed-forward artificial neural network (ANN) for the representation of the agent policy. We carried out the same experiments with two different types of sensors to compare their performance, which are the Myo armband sensor and the G-force sensor. We performed experiments using training, validation, and test set distributions, and the results were evaluated for user-specific HGR models. The final accuracy results demonstrated that the best model was able to reach up to 97.50\%±1.13\% and 88.15\%±2.84\% for the classification and recognition, respectively, with regard to static gestures, and 98.95\%±0.62\% and 90.47\%±4.57\% for the classification and recognition, respectively, with regard to dynamic gestures with the Myo armband sensor. The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.},
	language = {en},
	number = {24},
	urldate = {2023-03-07},
	journal = {Sensors},
	author = {Vásconez, Juan Pablo and Barona López, Lorena Isabel and Valdivieso Caraguay, Ángel Leonardo and Benalcázar, Marco E.},
	month = dec,
	year = {2022},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg},
	pages = {9613},
	annote = {[TLDR] The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/AJ2WMLPJ/Vásconez et al. - 2022 - Hand Gesture Recognition Using EMG-IMU Signals and.pdf:application/pdf},
}

@article{galka_inertial_2016,
	title = {Inertial {Motion} {Sensing} {Glove} for {Sign} {Language} {Gesture} {Acquisition} and {Recognition}},
	volume = {16},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {http://ieeexplore.ieee.org/document/7497574/},
	doi = {10.1109/JSEN.2016.2583542},
	abstract = {The most popular systems for automatic sign language recognition are based on vision. They are user-friendly, but very sensitive to changes in regard to recording conditions. This paper presents a description of the construction of a more robust system-an accelerometer glove-as well as its application in the recognition of sign language gestures. The basic data regarding inertial motion sensors and the design of the gesture acquisition system as well as project proposals are presented. The evaluation of the solution presents the results of the gesture recognition attempt by using a selected set of sign language gestures with a described method based on Hidden Markov Model (HMM) and parallel HMM approaches. The proposed usage of parallel HMM for sensor-fusion modeling reduced the equal error rate by more than 60\%, while preserving 99.75\% recognition accuracy.},
	number = {16},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Galka, Jakub and Masior, Mariusz and Zaborski, Mateusz and Barczewska, Katarzyna},
	month = aug,
	year = {2016},
	note = {81 citations (Crossref) [2023-07-11]
92 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:hmm, app:sign-language, read-priority-1, from:cite.bib},
	pages = {6310--6316},
	annote = {[TLDR] The construction of a more robust system-an accelerometer glove-as well as its application in the recognition of sign language gestures with a described method based on Hidden Markov Model (HMM) and parallel HMM approaches are presented.},
	file = {Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf:/Users/brk/Zotero/storage/AI4LXML7/Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf:application/pdf},
}

@inproceedings{saliba_compact_2004,
	title = {A {Compact} {Glove} {Input} {Device} to {Measure} {Human} {Hand}, {Wrist} and {Forearm} {Joint} {Positions} for {Teleoperation} {Applications}},
	url = {https://www.semanticscholar.org/paper/A-Compact-Glove-Input-Device-to-Measure-Human-Hand%2C-Saliba-Farrugia/f99389f732525d8a5603da09b2d3de257b763e43},
	abstract = {In this work, we have developed a new glove input device that is able to measure the angular joint positions of two fingers and of the thumb on the human hand, as well as the pitch position of the wrist and the roll position of the radio-ulnar joint of the human forearm. The glove has various new features, including the measurement of forearm roll position, that are not found in other glove input devices described in the literature. The glove contains a number of flexible, plastic bands whose displacement, during joint rotation, is measured using linear potentiometers. The new glove is light, compact, easy to wear and use, robust, and inexpensive, and is intended for use in teleoperation applications in conjunction with a remotely located robot hand/wrist. Another property is that it can be easily adjusted to fit a wide range of human hand sizes. Preliminary testing of the glove has shown that it can achieve an accuracy in position measurement that compares well to that of a number of commercially-produced gloves that are presently in use. Keywords—glove input device; whole hand input device; teleoperation; human joint position sensing.},
	urldate = {2023-03-07},
	author = {Saliba, M. and Farrugia, F. and Giordmaina, A.},
	year = {2004},
	keywords = {model:none},
	file = {Full Text PDF:/Users/brk/Zotero/storage/446F9VJ8/Saliba et al. - 2004 - A Compact Glove Input Device to Measure Human Hand.pdf:application/pdf},
}

@article{alsaedi_efficient_2020,
	title = {An efficient hand gestures recognition system},
	volume = {745},
	issn = {1757-8981, 1757-899X},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/745/1/012045},
	doi = {10.1088/1757-899X/745/1/012045},
	abstract = {Abstract
            Talking about gestures make us return to the historical beginning of human communication because there is no language completely free of gestures. People cannot communicate without gestures. Any action or movement without gestures is free of real feelings and cannot express the thoughts. The purpose of any hand gesture recognition system is to recognize the hand gesture and used it to transfer a certain meaning or for computer control or/and a device. This paper introduced an efficient system to recognize hand gestures in real-time. Generally, the system is divided into five phases, first to image acquisition, second to pre-processing the image, third for detection and segmentation of the hand region, fourth to features extraction and fifth to count the numbers of fingers for gesture recognition. The system has been coded by Python language, PyAutoGUI library, OS Module of Python and the Open CV library.},
	number = {1},
	urldate = {2023-03-07},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {AlSaedi, Ahmed Kadem Hamed and AlAsadi, Abbas H. Hassin},
	month = feb,
	year = {2020},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, classes:10-29},
	pages = {012045},
	annote = {[TLDR] An efficient system to recognize hand gestures in real-time is introduced and has been coded by Python language, PyAutoGUI library, OS Module of Python and the Open CV library.},
	annote = {[TLDR] An efficient system to recognize hand gestures in real-time is introduced and has been coded by Python language, PyAutoGUI library, OS Module of Python and the Open CV library.},
	file = {Full Text:/Users/brk/Zotero/storage/2NTXV8NA/AlSaedi and AlAsadi - 2020 - An efficient hand gestures recognition system.pdf:application/pdf},
}

@inproceedings{zimmerman_hand_1987,
	address = {Toronto, Ontario, Canada},
	title = {A hand gesture interface device},
	isbn = {978-0-89791-213-6},
	url = {http://portal.acm.org/citation.cfm?doid=29933.275628},
	doi = {10.1145/29933.275628},
	abstract = {This paper reports on the development of a hand to machine interface device that provides real-time gesture, position and orientation information. The key element is a glove and the device as a whole incorporates a collection of technologies. Analog flex sensors on the glove measure finger bending. Hand position and orientation are measured either by ultrasonics, providing five degrees of freedom, or magnetic flux sensors, which provide six degrees of freedom. Piezoceramic benders provide the wearer of the glove with tactile feedback. These sensors are mounted on the light-weight glove and connected to the driving hardware via a small cable.
Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the {SIGCHI}/{GI} conference on {Human} factors in computing systems and graphics interface  - {CHI} '87},
	publisher = {ACM Press},
	author = {Zimmerman, Thomas G. and Lanier, Jaron and Blanchard, Chuck and Bryson, Steve and Harvill, Young},
	year = {1987},
	note = {501 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, model:none},
	pages = {189--192},
	annote = {[TLDR] Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
	file = {Full Text:/Users/brk/Zotero/storage/CTAKLA8C/Zimmerman et al. - 1987 - A hand gesture interface device.pdf:application/pdf},
}

@article{xu_zhang_framework_2011,
	title = {A {Framework} for {Hand} {Gesture} {Recognition} {Based} on {Accelerometer} and {EMG} {Sensors}},
	volume = {41},
	issn = {1083-4427, 1558-2426},
	url = {http://ieeexplore.ieee.org/document/5735233/},
	doi = {10.1109/TSMCA.2011.2116004},
	abstract = {This paper presents a framework for hand gesture recognition based on the information fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors. In our framework, the start and end points of meaningful gesture segments are detected automatically by the intensity of the EMG signals. A decision tree and multistream hidden Markov models are utilized as decision-level fusion to get the final results. For sign language recognition (SLR), experimental results on the classification of 72 Chinese Sign Language (CSL) words demonstrate the complementary functionality of the ACC and EMG sensors and the effectiveness of our framework. Additionally, the recognition of 40 CSL sentences is implemented to evaluate our framework for continuous SLR. For gesture-based control, a real-time interactive system is built as a virtual Rubik's cube game using 18 kinds of hand gestures as control commands. While ten subjects play the game, the performance is also examined in user-specific and user-independent classification. Our proposed framework facilitates intelligent and natural control in gesture-based interaction.},
	number = {6},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {{Xu Zhang} and {Xiang Chen} and {Yun Li} and Lantz, V. and {Kongqiao Wang} and {Jihai Yang}},
	month = nov,
	year = {2011},
	note = {518 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:hmm, tech:emg, model:decision-tree, classes:72, app:sign-language, classes:50-99},
	pages = {1064--1076},
	annote = {[TLDR] A framework for hand gesture recognition based on the information fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors that facilitates intelligent and natural control in gesture-based interaction.},
}

@article{wu_wearable_2016,
	title = {A {Wearable} {System} for {Recognizing} {American} {Sign} {Language} in {Real}-{Time} {Using} {IMU} and {Surface} {EMG} {Sensors}},
	volume = {20},
	issn = {2168-2194, 2168-2208},
	url = {http://ieeexplore.ieee.org/document/7552525/},
	doi = {10.1109/JBHI.2016.2598302},
	abstract = {A sign language recognition system translates signs performed by deaf individuals into text/speech in real time. Inertial measurement unit and surface electromyography (sEMG) are both useful modalities to detect hand/arm gestures. They are able to capture signs and the fusion of these two complementary sensor modalities will enhance system performance. In this paper, a wearable system for recognizing American Sign Language (ASL) in real time is proposed, fusing information from an inertial sensor and sEMG sensors. An information gain-based feature selection scheme is used to select the best subset of features from a broad range of well-established features. Four popular classification algorithms are evaluated for 80 commonly used ASL signs on four subjects. The experimental results show 96.16\% and 85.24\% average accuracies for intra-subject and intra-subject cross session evaluation, respectively, with the selected feature subset and a support vector machine classifier. The significance of adding sEMG for ASL recognition is explored and the best channel of sEMG is highlighted.},
	number = {5},
	urldate = {2023-03-07},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Wu, Jian and Sun, Lu and Jafari, Roozbeh},
	month = sep,
	year = {2016},
	note = {166 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:svm, tech:emg, classes:80, model:dt, model:knn, model:nb, impressive, app:sign-language, classes:50-99},
	pages = {1281--1290},
	annote = {[TLDR] A wearable system for recognizing ASL in real time is proposed, fusing information from an inertial sensor and sEMG sensors, and an information gain-based feature selection scheme is used to select the best subset of features from a broad range of well-established features.},
	file = {Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf:/Users/brk/Zotero/storage/2J63995L/Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf:application/pdf},
}

@article{lee_smart_2018,
	title = {Smart {Wearable} {Hand} {Device} for {Sign} {Language} {Interpretation} {System} {With} {Sensors} {Fusion}},
	volume = {18},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {http://ieeexplore.ieee.org/document/8126796/},
	doi = {10.1109/JSEN.2017.2779466},
	abstract = {Gesturing is an instinctive way of communicating to present a specific meaning or intent. Therefore, research into sign language interpretation using gestures has been explored progressively during recent decades to serve as an auxiliary tool for deaf and mute people to blend into society without barriers. In this paper, a smart sign language interpretation system using a wearable hand device is proposed to meet this purpose. This wearable system utilizes five flex-sensors, two pressure sensors, and a three-axis inertial motion sensor to distinguish the characters in the American sign language alphabet. The entire system mainly consists of three modules: 1) a wearable device with a sensor module; 2) a processing module; and 3) a display unit mobile application module. Sensor data are collected and analyzed using a built-in embedded support vector machine classifier. Subsequently, the recognized alphabet is further transmitted to a mobile device through Bluetooth low energy wireless communication. An Android-based mobile application was developed with a text-to-speech function that converts the received textinto audible voice output. Experiment results indicate that a true sign language recognition accuracy rate of 65.7\% can be achieved on average in the first version without pressure sensors. A second version of the proposed wearable system with the fusion of pressure sensors on the middle finger increased the recognition accuracy rate dramatically to 98.2\%. The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Lee, Boon Giin and Lee, Su Min},
	month = feb,
	year = {2018},
	note = {102 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1224--1232},
	annote = {[TLDR] The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
	annote = {[TLDR] The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
	file = {Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf:/Users/brk/Zotero/storage/CQT634W2/Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf:application/pdf},
}

@article{kudrinko_wearable_2021,
	title = {Wearable {Sensor}-{Based} {Sign} {Language} {Recognition}: {A} {Comprehensive} {Review}},
	volume = {14},
	issn = {1937-3333, 1941-1189},
	shorttitle = {Wearable {Sensor}-{Based} {Sign} {Language} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9178440/},
	doi = {10.1109/RBME.2020.3019769},
	abstract = {Sign language is used as a primary form of communication by many people who are Deaf, deafened, hard of hearing, and non-verbal. Communication barriers exist for members of these populations during daily interactions with those who are unable to understand or use sign language. Advancements in technology and machine learning techniques have led to the development of innovative approaches for gesture recognition. This literature review focuses on analyzing studies that use wearable sensor-based systems to classify sign language gestures. A review of 72 studies from 1991 to 2019 was performed to identify trends, best practices, and common challenges. Attributes including sign language variation, sensor configuration, classification method, study design, and performance metrics were analyzed and compared. Results from this literature review could aid in the development of user-centred and robust wearable sensor-based systems for sign language recognition.},
	urldate = {2023-03-07},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Kudrinko, Karly and Flavin, Emile and Zhu, Xiaodan and Li, Qingguo},
	year = {2021},
	note = {37 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, type:survey},
	pages = {82--97},
	annote = {[TLDR] Analysis of studies that use wearable sensor-based systems to classify sign language gestures finds trends, best practices, and common challenges could aid in the development of user-centred and robust wearable sensors for sign language recognition.},
}

@article{mardiyanto_development_2017,
	title = {Development of hand gesture recognition sensor based on accelerometer and gyroscope for controlling arm of underwater remotely operated robot},
	url = {http://ieeexplore.ieee.org/document/8124104/},
	doi = {10.1109/ISITIA.2017.8124104},
	abstract = {Hand Gesture Recognition sensor based on accelerometer and gyroscope is a sensor for capturing the positions of operator hand while controlling underwater remotely operated vehicle equipped with an arm. The proposed system has an advantage in its convenience by means of no training or exercise needed for operator before using it. The key issue here is how beginner operator could use easily the underwater remotely operated robot arm without any specific training. The conventional one uses a joystick for controlling the underwater system and it is inconvenience for beginner user as well as less precision. The proposed system consists of two main part: (1) ground station and (2) underwater remotely operated robot arm. This paper proposes the development of hand gesture recognition sensor used by operator at the ground station for controlling robot arm at the underwater robot. The proposed sensor uses accelerometers and gyroscopes installed in elbow, forearm, and wrist. These devices measure 3D position of each joints for constructing 3D position of hand. We design sensor's casing for its convenience of use by using CAD software. Each sensor is connected by Arduino Nano microcontroller having compact circuit and embedded it into sensor's casing. The sensors are connected to a microcontroller acting as master connected to microcontroller slave (sensor part). These sensors value are converted to 3D position by using forward kinematic. The forward kinematic values are sent to the underwater robot by using a wire utilizing Pulse Position Signal. Then, it converted again to servo's movement by using inverse kinematic. The result is operator able to control the underwater remotely robot arm by utilizing hand gesture directly. The last, operator could control the robot gripper based on flex sensor installed in operator's fingers. The accuracy of the sensor has been tested under laboratory condition, it has 98\% of accuracy.},
	urldate = {2023-03-07},
	journal = {2017 International Seminar on Intelligent Technology and Its Applications (ISITIA)},
	author = {Mardiyanto, Ronny and Utomo, Mochamad Fajar Rinaldi and Purwanto, Djoko and Suryoatmojo, Heri},
	month = aug,
	year = {2017},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 International Seminar on Intelligent Technology and its Applications (ISITIA)
ISBN: 9781538627082
Place: Surabaya
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:flex, app:robot-control, arduino-nano},
	pages = {329--333},
	annote = {[TLDR] The development of hand gesture recognition sensor used by operator at the ground station for controlling robot arm at the underwater robot and operator able to control the underwater remotely robot arm by utilizing hand gesture directly is proposed.},
}

@article{chu_sensor-based_2021,
	title = {A {Sensor}-{Based} {Hand} {Gesture} {Recognition} {System} for {Japanese} {Sign} {Language}},
	url = {https://ieeexplore.ieee.org/document/9391981/},
	doi = {10.1109/LifeTech52111.2021.9391981},
	abstract = {In this paper, we propose a sensor-based data acquisition glove for Japanese Sign Language (JSL) hand gesture recognition. Five flex sensors, an Inertial Measurement Unit (IMU), and three Force Sensing Resistors (FSRs) are used to detect the bending degree of fingers and hand movement information. The detected data are transmitted to the computer by an Arduino Micro. The average accuracy of the hand gesture recognition for a single subject, using the Support Vector Machine (SVM) based and the Dynamic Time Wrapping (DTW) based algorithm are 96.9\% and 94.5\%, respectively. Our proposed system also achieves an average recognition accuracy of about 82.5\% for the cross-recognition among three subjects. The experimental results indicate that our proposed system has great potential for JSL hand gesture recognition.},
	urldate = {2023-03-07},
	journal = {2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)},
	author = {Chu, Xianzhi and Liu, Jiang and Shimamoto, Shigeru},
	month = mar,
	year = {2021},
	note = {8 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)
ISBN: 9781665418751
Place: Nara, Japan
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {311--312},
	annote = {[TLDR] A sensor-based data acquisition glove for Japanese Sign Language (JSL) hand gesture recognition using five flex sensors, an Inertial Measurement Unit, and three Force Sensing Resistors to detect the bending degree of fingers and hand movement information.},
}

@inproceedings{xie_accelerometer_2014,
	title = {Accelerometer {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Gesture-Recognition-Xie/c8bb38e46b7d97cc1a4ccd4a9d6df9717c1c3ff7},
	abstract = {Our goal is to make gesture-based input for smartphones and smartwatches accurate and feasible to use. With a custom Android application to record accelerometer data for 5 gestures, we developed a highly accurate SVM classifier using only 1 training example per class. Our novel Dynamic-Threshold Truncation algorithm during preprocessing improved accuracy on 1 training example per class by 14\% and the addition of axis-wise Discrete Fourier Transform coefficient features improved accuracy on 1 training example per class by 5\%. With 5 gesture classes, 1 training example for each class, and 30 test examples for each class, our classifier achieves 96\% accuracy. With 5 training examples per class, the classifier achieves 98\% accuracy, which is greater than the 10-example accuracy of other efforts using HMM’s[1, 2]. This makes it feasible for a real-time implementation of accelerometer-based gesture recognition to identify user-defined gestures with high accuracy while requiring little training effort from the user.},
	urldate = {2023-03-07},
	author = {Xie, Michael},
	year = {2014},
	keywords = {tech:accelerometer, classes:5, model:svm, classes:{\textless}10},
	annote = {[TLDR] A highly accurate SVM classifier using only 1 training example per class is developed that makes it feasible for a real-time implementation of accelerometer-based gesture recognition to identify user-defined gestures with high accuracy while requiring little training effort from the user.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/EC67HB44/Xie - 2014 - Accelerometer Gesture Recognition.pdf:application/pdf},
}

@article{patil_marathi_2022,
	title = {Marathi {Sign} {Language} {Hand} {Gesture} {Recognition} {Using} {Accelerometer} and {3D} {Printed} {Gloves}},
	url = {https://ieeexplore.ieee.org/document/10008290/},
	doi = {10.1109/CICN56167.2022.10008290},
	abstract = {Due to the communication abilities impairment, deaf \& dumb peoples are more or less isolated from mainstream societal activities. NGOs, social workers \& Governments in India are putting efforts with multiple initiatives to increase the involvement of these peoples in the mainstream activities happening in the rest of world. This paper proposes to establish an alternative method to streamline the communications among normal \& speech impaired persons by using smart hand glove system equipped with 3D accelerometer modules at the fingertips. The proposed system is delivering the measurement results in very proficient way. The experiments were carried out over 15 persons using the sign gestures, providing good co-relation between the Posed gestures versus system mapped gestures.},
	urldate = {2023-03-07},
	journal = {2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)},
	author = {Patil, Sachin and Joshi, Sarang and Kulkarni, Hrushikesh B. and Hagawane, Pradnesh and Shinde, Pradnya},
	month = dec,
	year = {2022},
	note = {0 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)
ISBN: 9781665487719
Place: Al-Khobar, Saudi Arabia
Publisher: IEEE},
	keywords = {tech:accelerometer, app:sign-language, app:marathi-sl},
	pages = {72--77},
	annote = {[TLDR] The proposed smart hand glove system equipped with 3D accelerometer modules at the fingertips is delivering the measurement results in very proficient way, providing good co-relation between the Posed gestures versus system mapped gestures.},
}

@article{alzubaidi_novel_2023,
	title = {A {Novel} {Assistive} {Glove} to {Convert} {Arabic} {Sign} {Language} into {Speech}},
	volume = {22},
	issn = {2375-4699, 2375-4702},
	url = {https://dl.acm.org/doi/10.1145/3545113},
	doi = {10.1145/3545113},
	abstract = {People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98\%. A usability study was then conducted to determine the usefulness of the assistive device in real-time.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	author = {Alzubaidi, Mohammad A. and Otoom, Mwaffaq and Abu Rwaq, Areen M.},
	month = mar,
	year = {2023},
	note = {3 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1--16},
	annote = {[TLDR] The research described in this paper is aimed at providing an assistive device to help people with speech disorders communicate with others, by translating their gestures into a spoken voice that others can understand.},
}

@article{ahmed_real-time_2021,
	title = {Real-time sign language framework based on wearable device: analysis of {MSL}, {DataGlove}, and gesture recognition},
	volume = {25},
	issn = {1432-7643, 1433-7479},
	shorttitle = {Real-time sign language framework based on wearable device},
	url = {https://link.springer.com/10.1007/s00500-021-05855-6},
	doi = {10.1007/s00500-021-05855-6},
	abstract = {Researchers have been inspired to use technology to enable people with hearing and speech impairment to communicate and engage with others around them. Sensory approach to recognition facilitates real-time and accurate recognition of signs. Thus, this study proposes a Malaysian Sign Language (MSL) recognition framework. The framework consists of three sub-modules for the recognition of static isolated signs based on data collected from a DataGlove. The first module focuses on the characteristics of signs, yielding sign recognition system requirements. The second module describes the different steps required to develop a wearable sign-capture device. The third module discusses the real-time SL recognition approach, which uses a template-matching algorithm to recognize acquired data. The final design of the DataGlove with 65 data channel fulfils the requirement identified from an analysis of MSL. The DataGlove is able to record data for all of the signs (both dynamic and static) of MSL due to the wide range of captured hand features. As a result, the recognition engine can accurately recognize complex signs.},
	language = {en},
	number = {16},
	urldate = {2023-03-07},
	journal = {Soft Computing},
	author = {Ahmed, M. A. and Zaidan, B. B. and Zaidan, A. A. and Alamoodi, A. H. and Albahri, O. S. and Al-Qaysi, Z. T. and Albahri, A. S. and Salih, Mahmood M.},
	month = aug,
	year = {2021},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {11101--11122},
	annote = {[TLDR] A Malaysian Sign Language (MSL) recognition framework is proposed, which consists of three sub-modules for the recognition of static isolated signs based on data collected from a DataGlove, and which uses a template-matching algorithm to recognize acquired data.},
}

@article{mummadi_real-time_2018,
	title = {Real-{Time} and {Embedded} {Detection} of {Hand} {Gestures} with an {IMU}-{Based} {Glove}},
	volume = {5},
	issn = {2227-9709},
	url = {http://www.mdpi.com/2227-9709/5/2/28},
	doi = {10.3390/informatics5020028},
	abstract = {This article focuses on the use of data gloves for human-computer interaction concepts, where external sensors cannot always fully observe the user’s hand. A good concept hereby allows to intuitively switch the interaction context on demand by using different hand gestures. The recognition of various, possibly complex hand gestures, however, introduces unintentional overhead to the system. Consequently, we present a data glove prototype comprising a glove-embedded gesture classifier utilizing data from Inertial Measurement Units (IMUs) in the fingertips. In an extensive set of experiments with 57 participants, our system was tested with 22 hand gestures, all taken from the French Sign Language (LSF) alphabet. Results show that our system is capable of detecting the LSF alphabet with a mean accuracy score of 92\% and an F1 score of 91\%, using complementary filter with a gyroscope-to-accelerometer ratio of 93\%. Our approach has also been compared to the local fusion algorithm on an IMU motion sensor, showing faster settling times and less delays after gesture changes. Real-time performance of the recognition is shown to occur within 63 milliseconds, allowing fluent use of the gestures via Bluetooth-connected systems.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {Informatics},
	author = {Mummadi, Chaithanya and Leo, Frederic and Verma, Keshav and Kasireddy, Shivaji and Scholl, Philipp and Kempfle, Jochen and Laerhoven, Kristof},
	month = jun,
	year = {2018},
	note = {46 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, app:sign-language, classes:10-29, app:french-sl},
	pages = {28},
	annote = {[TLDR] A data glove prototype comprising a glove-embedded gesture classifier utilizing data from Inertial Measurement Units (IMUs) in the fingertips, allowing fluent use of the gestures via Bluetooth-connected systems.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/VFSAQLD7/Mummadi et al. - 2018 - Real-Time and Embedded Detection of Hand Gestures .pdf:application/pdf},
}

@article{lee_deep_2020,
	title = {Deep {Learning} {Based} {Real}-{Time} {Recognition} of {Dynamic} {Finger} {Gestures} {Using} a {Data} {Glove}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9264164/},
	doi = {10.1109/ACCESS.2020.3039401},
	abstract = {In this article, a real-time dynamic finger gesture recognition using a soft sensor embedded data glove is presented, which measures the metacarpophalangeal (MCP) and proximal interphalangeal (PIP) joint angles of five fingers. In the gesture recognition field, a challenging problem is that of separating meaningful dynamic gestures from a continuous data stream. Unconscious hand motions or sudden tremors, which can easily lead to segmentation ambiguity, makes this problem difficult. Furthermore, the hand shapes and speeds of users differ when performing the same dynamic gesture, and even those made by one user often vary. To solve the problem of separating meaningful dynamic gestures, we propose a deep learning-based gesture spotting algorithm that detects the start/end of a gesture sequence in a continuous data stream. The gesture spotting algorithm takes window data and estimates a scalar value named gesture progress sequence (GPS). GPS is a quantity that represents gesture progress. Moreover, to solve the gesture variation problem, we propose a sequence simplification algorithm and a deep learning-based gesture recognition algorithm. The proposed three algorithms (gesture spotting algorithm, sequence simplification algorithm, and gesture recognition algorithm) are unified into the real-time gesture recognition system and the system was tested with 11 dynamic finger gestures in real-time. The proposed system took only 6 ms to estimate a GPS and no more than 12 ms to recognize the completed gesture in real-time.},
	urldate = {2023-03-07},
	journal = {IEEE Access},
	author = {Lee, Minhyuk and Bae, Joonbum},
	year = {2020},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, movement:dynamic, classes:10-29},
	pages = {219923--219933},
	annote = {[TLDR] A real-time dynamic finger gesture recognition using a soft sensor embedded data glove is presented, which measures the metacarpophalangeal and proximal interphalangeal joint angles of five fingers and proposes a deep learning-based gesture spotting algorithm that detects the start/end of a gesture sequence in a continuous data stream.},
	file = {Full Text:/Users/brk/Zotero/storage/NRM9LBZL/Lee and Bae - 2020 - Deep Learning Based Real-Time Recognition of Dynam.pdf:application/pdf},
}

@article{makaussov_low-cost_2020,
	title = {A {Low}-{Cost}, {IMU}-{Based} {Real}-{Time} {On} {Device} {Gesture} {Recognition} {Glove}},
	url = {https://ieeexplore.ieee.org/document/9283231/},
	doi = {10.1109/SMC42975.2020.9283231},
	abstract = {This paper evaluates the possibility of performing fine gesture recognition including finger movements on a low-tech device. In particular, we present a solution with a recognition model that is small enough to fit in the memory of a low- tech device and describe related difficulties associated with this approach. Several different Machine Learning techniques are employed and their individual advantages and drawbacks are explored for the task at hand. Our results indicate an average of 95\% accuracy during real-time testing for an eight class decoding task with a custom Recurrent Neural Network approach, that runs on the low-tech device, namely an Arduino Nano 33 BLE. The novelty and strength of this research lies in the fact that we are able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures. The recognition process is conducted on the low-tech device and as a result this solution has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.},
	urldate = {2023-03-07},
	journal = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Makaussov, Oleg and Krassavin, Mikhail and Zhabinets, Maxim and Fazli, Siamac},
	month = oct,
	year = {2020},
	note = {3 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
ISBN: 9781728185262
Place: Toronto, ON, Canada
Publisher: IEEE},
	keywords = {tech:accelerometer, read-priority-1, model:rnn, arduino-nano-ble, classes:{\textless}10},
	pages = {3346--3351},
	annote = {[TLDR] This research is able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures, and has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.},
}

@article{qaroush_smart_2021,
	title = {Smart, comfortable wearable system for recognizing {Arabic} {Sign} {Language} in real-time using {IMUs} and features-based fusion},
	volume = {184},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421008629},
	doi = {10.1016/j.eswa.2021.115448},
	abstract = {Semantic Scholar extracted view of "Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion" by Aziz Qaroush et al.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Expert Systems with Applications},
	author = {Qaroush, Aziz and Yassin, Sara and Al-Nubani, Ali and Alqam, Ameer},
	month = dec,
	year = {2021},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {115448},
}

@article{qi_multi-sensor_2021,
	title = {Multi-{Sensor} {Guided} {Hand} {Gesture} {Recognition} for a {Teleoperated} {Robot} {Using} a {Recurrent} {Neural} {Network}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9457168/},
	doi = {10.1109/LRA.2021.3089999},
	abstract = {Touch-free guided hand gesture recognition for human-robot interactions plays an increasingly significant role in teleoperated surgical robot systems. Indeed, despite depth cameras provide more practical information for recognition accuracy enhancement, the instability and computational burden of depth data represent a tricky problem. In this letter, we propose a novel multi-sensor guided hand gesture recognition system for surgical robot teleoperation. A multi-sensor data fusion model is designed for performing interference in the presence of occlusions. A multilayer Recurrent Neural Network (RNN) consisting of a Long Short-Term Memory (LSTM) module and a dropout layer (LSTM-RNN) is proposed for multiple hand gestures classification. Detected hand gestures are used to perform a set of human-robot collaboration tasks on a surgical robot platform. Classification performance and prediction time is compared among the LSTM-RNN model and several traditional Machine Learning (ML) algorithms, such as k-Nearest Neighbor (k-NN) and Support Vector Machines (SVM). Results show that the proposed LSTM-RNN classifier is able to achieve a higher recognition rate and faster inference speed. In addition, the present adaptive data fusion system shows a strong anti-interference capability for hand gesture recognition in real-time.},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Qi, Wen and Ovur, Salih Ertug and Li, Zhijun and Marzullo, Aldo and Song, Rong},
	month = jul,
	year = {2021},
	note = {62 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:svm, model:knn, model:rnn, model:lstm, app:robot-control, app:surgery},
	pages = {6039--6045},
	annote = {[TLDR] Results show that the proposed LSTM-RNN classifier is able to achieve a higher recognition rate and faster inference speed, and shows a strong anti-interference capability for hand gesture recognition in real-time.},
	file = {Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf:/Users/brk/Zotero/storage/8MHHF5ZG/Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf:application/pdf},
}

@article{liu_dynamic_2021,
	title = {Dynamic {Gesture} {Recognition} {Algorithm} {Based} on {3D} {Convolutional} {Neural} {Network}},
	volume = {2021},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2021/4828102/},
	doi = {10.1155/2021/4828102},
	abstract = {Gesture recognition is one of the important ways of human-computer interaction, which is mainly detected by visual technology. The temporal and spatial features are extracted by convolution of the video containing gesture. However, compared with the convolution calculation of a single image, multiframe image of dynamic gestures has more computation, more complex feature extraction, and more network parameters, which affects the recognition efficiency and real-time performance of the model. To solve above problems, a dynamic gesture recognition model based on CBAM-C3D is proposed. Key frame extraction technology, multimodal joint training, and network optimization with BN layer are used for making the network performance better. The experiments show that the recognition accuracy of the proposed 3D convolutional neural network combined with attention mechanism reaches 72.4\% on EgoGesture dataset, which is improved greatly compared with the current main dynamic gesture recognition methods, and the effectiveness of the proposed algorithm is verified.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Computational Intelligence and Neuroscience},
	author = {Liu, Yuting and Jiang, Du and Duan, Haojie and Sun, Ying and Li, Gongfa and Tao, Bo and Yun, Juntong and Liu, Ying and Chen, Baojia},
	editor = {Ahmed, Syed Hassan},
	month = aug,
	year = {2021},
	note = {40 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb},
	pages = {1--12},
	annote = {[TLDR] A dynamic gesture recognition model based on CBAM-C3D is proposed, and the recognition accuracy of the proposed 3D convolutional neural network combined with attention mechanism reaches 72.4\% on EgoGesture dataset, which is improved greatly compared with the current main dynamic gestures recognition methods.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ELPL8EWG/Liu et al. - 2021 - Dynamic Gesture Recognition Algorithm Based on 3D .pdf:application/pdf},
}

@article{mujahid_real-time_2021,
	title = {Real-{Time} {Hand} {Gesture} {Recognition} {Based} on {Deep} {Learning} {YOLOv3} {Model}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/9/4164},
	doi = {10.3390/app11094164},
	abstract = {Using gestures can help people with certain disabilities in communicating with other people. This paper proposes a lightweight model based on YOLO (You Only Look Once) v3 and DarkNet-53 convolutional neural networks for gesture recognition without additional preprocessing, image filtering, and enhancement of images. The proposed model achieved high accuracy even in a complex environment, and it successfully detected gestures even in low-resolution picture mode. The proposed model was evaluated on a labeled dataset of hand gestures in both Pascal VOC and YOLO format. We achieved better results by extracting features from the hand and recognized hand gestures of our proposed YOLOv3 based model with accuracy, precision, recall, and an F-1 score of 97.68, 94.88, 98.66, and 96.70\%, respectively. Further, we compared our model with Single Shot Detector (SSD) and Visual Geometry Group (VGG16), which achieved an accuracy between 82 and 85\%. The trained model can be used for real-time detection, both for static hand images and dynamic gestures recorded on a video.},
	language = {en},
	number = {9},
	urldate = {2023-03-07},
	journal = {Applied Sciences},
	author = {Mujahid, Abdullah and Awan, Mazhar Javed and Yasin, Awais and Mohammed, Mazin Abed and Damaševičius, Robertas and Maskeliūnas, Rytis and Abdulkareem, Karrar Hameed},
	month = may,
	year = {2021},
	note = {85 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb, movement:static, movement:dynamic},
	pages = {4164},
	annote = {[TLDR] A lightweight model based on YOLO (You Only Look Once) v3 and DarkNet-53 convolutional neural networks for gesture recognition without additional preprocessing, image filtering, and enhancement of images is proposed.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/VWFYIVN9/Mujahid et al. - 2021 - Real-Time Hand Gesture Recognition Based on Deep L.pdf:application/pdf},
}

@article{wen_machine_2020,
	title = {Machine {Learning} {Glove} {Using} {Self}‐{Powered} {Conductive} {Superhydrophobic} {Triboelectric} {Textile} for {Gesture} {Recognition} in {VR}/{AR} {Applications}},
	volume = {7},
	issn = {2198-3844, 2198-3844},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202000261},
	doi = {10.1002/advs.202000261},
	abstract = {The rapid progress of Internet of things (IoT) technology raises an imperative demand on human machine interfaces (HMIs) which provide a critical linkage between human and machines. Using a glove as an intuitive and low‐cost HMI can expediently track the motions of human fingers, resulting in a straightforward communication media of human–machine interactions. When combining several triboelectric textile sensors and proper machine learning technique, it has great potential to realize complex gesture recognition with the minimalist‐designed glove for the comprehensive control in both real and virtual space. However, humidity or sweat may negatively affect the triboelectric output as well as the textile itself. Hence, in this work, a facile carbon nanotubes/thermoplastic elastomer (CNTs/TPE) coating approach is investigated in detail to achieve superhydrophobicity of the triboelectric textile for performance improvement. With great energy harvesting and human motion sensing capabilities, the glove using the superhydrophobic textile realizes a low‐cost and self‐powered interface for gesture recognition. By leveraging machine learning technology, various gesture recognition tasks are done in real time by using gestures to achieve highly accurate virtual reality/augmented reality (VR/AR) controls including gun shooting, baseball pitching, and flower arrangement, with minimized effect from sweat during operation.},
	language = {en},
	number = {14},
	urldate = {2023-03-07},
	journal = {Advanced Science},
	author = {Wen, Feng and Sun, Zhongda and He, Tianyiyi and Shi, Qiongfeng and Zhu, Minglu and Zhang, Zixuan and Li, Lianhui and Zhang, Ting and Lee, Chengkuo},
	month = jul,
	year = {2020},
	note = {189 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, model:cnn, classes:9, classes:{\textless}10},
	pages = {2000261},
	annote = {[TLDR] A facile carbon nanotubes/thermoplastic elastomer (CNTs/TPE) coating approach is investigated in detail to achieve superhydrophobicity of the triboelectric textile for performance improvement and realizes a low‐cost and self‐powered interface for gesture recognition.},
	file = {Full Text:/Users/brk/Zotero/storage/ETVGGB62/Wen et al. - 2020 - Machine Learning Glove Using Self‐Powered Conducti.pdf:application/pdf},
}

@article{zhang_widar30_2021,
	title = {Widar3.0: {Zero}-{Effort} {Cross}-{Domain} {Gesture} {Recognition} with {Wi}-{Fi}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Widar3.0},
	url = {https://ieeexplore.ieee.org/document/9516988/},
	doi = {10.1109/TPAMI.2021.3105387},
	abstract = {With the development of signal processing technology, the ubiquitous Wi-Fi devices open an unprecedented opportunity to solve the challenging human gesture recognition problem by learning motion representations from wireless signals. Wi-Fi-based gesture recognition systems, although yield good performance on specific data domains, are still practically difficult to be used without explicit adaptation efforts to new domains. Various pioneering approaches have been proposed to resolve this contradiction but extra training efforts are still necessary for either data collection or model re-training when new data domains appear. To advance cross-domain recognition and achieve fully zero-effort recognition, we propose Widar3.0, a Wi-Fi-based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all general model that requires only one-time training but can adapt to different data domains. Experiments on various domain factors (i.e. environments, locations, and orientations of persons) demonstrate the accuracy of 92.7\% for in-domain recognition and 82.6\%-92.4\% for cross-domain recognition without model re-training, outperforming the state-of-the-art solutions.},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Yi and Zheng, Yue and Qian, Kun and Zhang, Guidong and Liu, Yunhao and Wu, Chenshu and Yang, Zheng},
	year = {2021},
	note = {256 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:wifi, model:cnn, model:rnn-gru, classes:6, classes:10, classes:10-29, classes:{\textless}10},
	pages = {1--1},
	annote = {[TLDR] The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains, and develops a one-fits-all general model that requires only one-time training but can adapt to different data domains.},
	file = {Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf:/Users/brk/Zotero/storage/9FJSTD3F/Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf:application/pdf},
}

@article{moin_wearable_2020,
	title = {A wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition},
	volume = {4},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-020-00510-8},
	doi = {10.1038/s41928-020-00510-8},
	abstract = {Wearable devices that monitor muscle activity based on surface electromyography could be of use in the development of hand gesture recognition applications. Such devices typically use machine-learning models, either locally or externally, for gesture classification. However, most devices with local processing cannot offer training and updating of the machine-learning model during use, resulting in suboptimal performance under practical conditions. Here we report a wearable surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities. Our system implements a neuro-inspired hyperdimensional computing algorithm locally for real-time gesture classification, as well as model training and updating under variable conditions such as different arm positions and sensor replacement. The system can classify 13 hand gestures with 97.12\% accuracy for two participants when training with a single trial per gesture. A high accuracy (92.87\%) is preserved on expanding to 21 gestures, and accuracy is recovered by 9.5\% by implementing model updates in response to varying conditions, without additional computation on an external device. A surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities can classify human gestures in real time and with high accuracy.},
	language = {en},
	number = {1},
	urldate = {2023-03-07},
	journal = {Nature Electronics},
	author = {Moin, Ali and Zhou, Andy and Rahimi, Abbas and Menon, Alisha and Benatti, Simone and Alexandrov, George and Tamakloe, Senam and Ting, Jonathan and Yamamoto, Natasha and Khan, Yasser and Burghardt, Fred and Benini, Luca and Arias, Ana C. and Rabaey, Jan M.},
	month = dec,
	year = {2020},
	note = {222 citations (Crossref) [2023-07-02]
0 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, classes:10-29},
	pages = {54--63},
	annote = {[TLDR] A wearable surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities can classify human gestures in real time and with high accuracy.},
	file = {Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf:/Users/brk/Zotero/storage/NMEFHCRT/Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf:application/pdf},
}

@article{wong_multi-features_2021,
	title = {Multi-{Features} {Capacitive} {Hand} {Gesture} {Recognition} {Sensor}: {A} {Machine} {Learning} {Approach}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	shorttitle = {Multi-{Features} {Capacitive} {Hand} {Gesture} {Recognition} {Sensor}},
	url = {https://ieeexplore.ieee.org/document/9314169/},
	doi = {10.1109/JSEN.2021.3049273},
	abstract = {Gesture recognition technology enables machines to understand human gestures. The technology is considered as a key enabler for gaming and virtual reality applications. In this paper, we propose an effective, low-cost capacitive sensor device to recognize hand gestures. In particular, we designed a prototype of a wearable capacitive sensor unit to capture the capacitance values from the electrodes placed on finger phalanges. The sensor captures finger capacitance values. Each gesture has specific finger capacitance values. We applied a running median filter to the output of the sensor and extracted 15 features for gesture classification training and testing tasks. Subsequently, various analyses were performed to provide more insights into the sensing data. We applied and compared two machine learning algorithms: Error Correction Output Code Support Vector Machines (ECOC-SVM) and \$\{K\}\$ -Nearest Neighbour (KNN) classifiers. The training and testing recognition rates were observed for both intra-participant and inter-participant data sets. Further, we introduced a feature compression approach derived from correlation analysis to reduce the complexity of the machine learning algorithms. Using cross validation, we achieved a classification rate of approximately 99\% for intra-participant data. We achieved a lower recognition rate of 97\% (average cross validation testing) for compressed feature data set using both machine learning approaches. For the inter-participant data, the recognition rate was 99\% (normalized feature data) using KNN and 97\% using ECOC-SVM. The research findings show that our recognition system is competitive and has an immense potential for further study.},
	number = {6},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Wong, W. K. and Juwono, Filbert H. and Khoo, Brendan Teng Thiam},
	month = mar,
	year = {2021},
	note = {35 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:svm, model:knn, tech:capacitive},
	pages = {8441--8450},
	annote = {[TLDR] This paper designs a prototype of a wearable capacitive sensor unit to capture the capacitance values from the electrodes placed on finger phalanges and introduces a feature compression approach derived from correlation analysis to reduce the complexity of the machine learning algorithms.},
}

@inproceedings{klingmann_accelerometer-based_2009,
	title = {Accelerometer-{Based} {Gesture} {Recognition} with the {iPhone}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Gesture-Recognition-with-the-Klingmann/cbe0d68e8e9ec8d6b3d4129366214e98437c896b},
	abstract = {The growing number of small sensors built into consumer electronic devices, such as mobile phones, allow experiments with alternative interaction methods in favour of more physical, intuitive and pervasive human computer interaction. This paper examines hand gestures as an alternative or supplementary input modality for mobile devices. The iPhone is chosen as sensing and processing device. Based on its built-in accelerometer, hand movements are detected and classified into previously trained gestures. A software library for accelerometer-based gesture recognition and a demonstration iPhone application have been developed. The system allows the training and recognition of free-from hand gestures. Discrete hidden Markov models form the core part of the gesture recognition apparatus. Five test gestures have been defined and used to evaluate the performance of the application. The evaluation shows that with 10 training repetitions, an average recognition rate of over 90 percent can be achieved.},
	urldate = {2023-03-07},
	author = {Klingmann, Marco},
	year = {2009},
	note = {22 Citations},
	keywords = {tech:accelerometer, model:hmm, classes:{\textless}10},
	annote = {[TLDR] A software library for accelerometer-based gesture recognition and a demonstration iPhone application have been developed that allows the training and recognition of free-from hand gestures.},
}

@inproceedings{zhang_gesture_2009,
	address = {Berlin, Heidelberg},
	title = {Gesture {Recognition} with a 3-{D} {Accelerometer}},
	volume = {5585},
	isbn = {978-3-642-02829-8 978-3-642-02830-4},
	url = {http://link.springer.com/10.1007/978-3-642-02830-4_4},
	doi = {10.1007/978-3-642-02830-4_4},
	abstract = {Gesture-based interaction, as a natural way for human-computer interaction, has a wide range of applications in ubiquitous computing environment. This paper presents an acceleration-based gesture recognition approach, called FDSVM ( Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer. With FDSVM, firstly, the acceleration data of a gesture is collected and represented by a frame-based descriptor, to extract the discriminative information. Then a SVM-based multi-class gesture classifier is built for recognition in the nonlinear gesture feature space. Extensive experimental results on a data set with 3360 gesture samples of 12 gestures over weeks demonstrate that the proposed FDSVM approach significantly outperforms other four methods: DTW, Naive Bayes, C4.5 and HMM. In the user-dependent case, FDSVM achieves the recognition rate of 99.38\% for the 4 direction gestures and 95.21\% for all the 12 gestures. In the user-independent case, it obtains the recognition rate of 98.93\% for 4 gestures and 89.29\% for 12 gestures. Compared to other accelerometer-based gesture recognition approaches reported in literature FDSVM gives the best resulrs for both user-dependent and user-independent cases.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Wu, Jiahui and Pan, Gang and Zhang, Daqing and Qi, Guande and Li, Shijian},
	editor = {Zhang, Daqing and Portmann, Marius and Tan, Ah-Hwee and Indulska, Jadwiga},
	year = {2009},
	doi = {10.1007/978-3-642-02830-4_4},
	note = {133 citations (Crossref) [2023-07-11]
240 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Ubiquitous Intelligence and Computing
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:svm, model:hmm, classes:12, model:nb, model:dtw, read-priority-1, from:cite.bib, model:c4.5},
	pages = {25--38},
	annote = {@wu2009 presented the Frame-based Descriptor and Multi-Class SVM (FDSVM) as an accelerometer-based gesture recognition model. They trained FDSVM on 12 gestures in order to prove that it outperforms approaches based on HMMs, Naïve Bayes, Dynamic Time Warping (see @senin2008 for a review), and C4.5 [@quinlan1992]. The Nintendo Wii remote was employed as the device by which sensor data was collected.

},
	annote = {[TLDR] An acceleration-based gesture recognition approach, called FDSVM ( Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer and gives the best resulrs for both user-dependent and user-independent cases.},
	file = {Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf:/Users/brk/Zotero/storage/UFY9XY4F/Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf:application/pdf},
}

@inproceedings{prekopcsak_accelerometer_2008,
	title = {Accelerometer {Based} {Real}-{Time} {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Real-Time-Gesture-Recognition-Prekopcs%C3%A1k/4e0b17d35696db6aa1cd0ea3565b47ff317e3ad3},
	abstract = {Gesture is a natural expression form for humans, but its recognition is a similarly hard problem as speech recognition. In this paper, I present a real-time hand gesture recognition system, which identifies relevant parts in the continous sensor data stream, and classifies them to the most probable gesture. Instead of the usual button-based segmentation, I have created an automatic segmentation method, which makes the interface more natural. The results showed that the two different classifiers reach 97.4\% and 96\% accuracy on a personalized gesture set, and these results can be improved for certain gesture sets with the combination of the two algorithms. Furthermore, the system has great performance and low response time, so the user experience is much better than with previous gesture recognizers.},
	urldate = {2023-03-07},
	author = {Prekopcsák, Zoltán},
	year = {2008},
	keywords = {model:svm, model:hmm, classes:10, classes:10-29},
	annote = {[TLDR] This paper presents a real-time hand gesture recognition system, which identifies relevant parts in the continous sensor data stream, and classifies them to the most probable gesture, and creates an automatic segmentation method, which makes the interface more natural.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/Q44WBJN9/Prekopcsák - 2008 - Accelerometer Based Real-Time Gesture Recognition.pdf:application/pdf},
}

@inproceedings{hutchison_accelerometer_2005,
	address = {Berlin, Heidelberg},
	title = {Accelerometer {Based} {Gesture} {Recognition} {Using} {Continuous} {HMMs}},
	volume = {3522},
	isbn = {978-3-540-26153-7 978-3-540-32237-5},
	url = {http://link.springer.com/10.1007/11492429_77},
	doi = {10.1007/11492429_77},
	abstract = {This paper presents a gesture recognition system based on continuous hidden Markov models. Gestures here are hand movements which are recorded by a 3D accelerometer embedded in a handheld device. In addition to standard hidden Markov model classifier, the recognition system has a preprocessing step which removes the effect of device orientation from the data. The performance of the recognizer is evaluated in both user dependent and user independent cases. The effects of sample resolution and sampling rate are studied in the user dependent case.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Pylvänäinen, Timo},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Marques, Jorge S. and Pérez de la Blanca, Nicolás and Pina, Pedro},
	year = {2005},
	doi = {10.1007/11492429_77},
	note = {150 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Pattern Recognition and Image Analysis
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:hmm},
	pages = {639--646},
	annote = {[TLDR] A gesture recognition system based on continuous hidden Markov models which removes the effect of device orientation from the data and is evaluated in both user dependent and user independent cases.},
}

@article{sethu_janaki_real_2013,
	title = {Real time recognition of {3D} gestures in mobile devices},
	url = {http://ieeexplore.ieee.org/document/6745463/},
	doi = {10.1109/RAICS.2013.6745463},
	abstract = {Gesture-based user interaction is increasingly relevant today as the use of personal computing devices becomes widespread. Smartphones have several inbuilt sensors like accelerometer, orientation sensor and gyroscope, which are able to provide data on motion of the device in 3D space. This paper proposes a mechanism for real-time recognition of 3D gestures using sensors in mobile devices. 3D gestures are space-drawn gestures computed from 3-axial accelerometer readings. The algorithms discussed in this paper include single value decomposition, dynamic time warping and Mahalanobis distance.},
	urldate = {2023-03-07},
	journal = {2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},
	author = {Sethu Janaki, V M and Babu, Satish and Sreekanth, S S},
	month = dec,
	year = {2013},
	note = {8 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)
ISBN: 9781479921782 9781479921775
Place: Trivandrum, India
Publisher: IEEE},
	keywords = {tech:accelerometer, model:knn, model:dtw, model:svd},
	pages = {149--152},
	annote = {[TLDR] A mechanism for real-time recognition of 3D gestures using sensors in mobile devices using single value decomposition, dynamic time warping and Mahalanobis distance is proposed.},
}

@article{kong_gesture_2009,
	title = {Gesture recognition model based on {3D} accelerations},
	url = {https://ieeexplore.ieee.org/document/5228524/},
	doi = {10.1109/ICCSE.2009.5228524},
	abstract = {This paper presents a recognition model on gesture interaction, gesture recognition based on accererations, which are collected by 3D accelerometer. Unlike most other vision-based gesture recognition, our method resorting to accelerations of the gesture in three directions. Gesture is divided into small units, and each unit is modeled by an HMM, the HMM classifies the gesture unit after being well trained, the gesture identified when all of the corresponding gesture units recognized successfully, then the instruction will be triggered, and the Human-Computer Interaction achieved. Through experiments, the method proved to be effective. This recognition model can be wildly used in the field of mobile computing and remote control, and people could use computer more friendly and naturally.},
	urldate = {2023-03-07},
	journal = {2009 4th International Conference on Computer Science \& Education},
	author = {Kong, Jun-qi and Wang, Hui and Zhang, Guang-quan},
	month = jul,
	year = {2009},
	note = {17 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2009 4th International Conference on Computer Science \&amp; Education (ICCSE 2009)
ISBN: 9781424435203
Place: Nanning
Publisher: IEEE},
	keywords = {tech:accelerometer, model:hmm},
	pages = {66--70},
	annote = {[TLDR] A recognition model on gesture interaction, gesture recognition based on accererations, which are collected by 3D accelerometer, can be wildly used in the field of mobile computing and remote control, and people could use computer more friendly and naturally.},
}

@inproceedings{hutchison_comparative_2014,
	address = {Cham},
	title = {A {Comparative} {Study} of {User} {Dependent} and {Independent} {Accelerometer}-{Based} {Gesture} {Recognition} {Algorithms}},
	volume = {8530},
	isbn = {978-3-319-07787-1 978-3-319-07788-8},
	url = {http://link.springer.com/10.1007/978-3-319-07788-8_12},
	doi = {10.1007/978-3-319-07788-8_12},
	abstract = {In this paper, we introduce an evaluation of accelerometer-based gesture recognition algorithms in user dependent and independent cases. Gesture recognition has many algorithms and this evaluation includes Hidden Markov Models, Support Vector Machine, K-nearest neighbor, Artificial Neural Net-work and Dynamic Time Warping. Recognition results are based on acceleration data collected from 12 users. We evaluated the algorithms based on the recognition accuracy related to different number of gestures from two datasets. Evaluation results show that the best accuracy for 8 and 18 gestures is achieved with dynamic time warping and K-nearest neighbor algorithms.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer International Publishing},
	author = {Hamdy Ali, Aya and Atia, Ayman and Sami, Mostafa},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Streitz, Norbert and Markopoulos, Panos},
	year = {2014},
	doi = {10.1007/978-3-319-07788-8_12},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Distributed, Ambient, and Pervasive Interactions
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:svm, model:hmm, model:knn, model:dtw, model:ffnn, classes:10-29, classes:{\textless}10},
	pages = {119--129},
	annote = {[TLDR] Evaluation of accelerometer-based gesture recognition algorithms in user dependent and independent cases shows that the best accuracy for 8 and 18 gestures is achieved with dynamic time warping and K-nearest neighbor algorithms.},
	file = {Full Text:/Users/brk/Zotero/storage/CXIM8DZI/Hamdy Ali et al. - 2014 - A Comparative Study of User Dependent and Independ.pdf:application/pdf},
}

@inproceedings{tiwary_single_2009,
	address = {New Delhi},
	title = {A {Single} {Accelerometer} based {Wireless} {Embedded} {System} for {Predefined} {Dynamic} {Gesture} {Recognition}},
	isbn = {978-81-8489-404-2 978-81-8489-203-1},
	url = {http://link.springer.com/10.1007/978-81-8489-203-1_18},
	doi = {10.1007/978-81-8489-203-1_18},
	abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction. A complete embedded system which facilitates the data acquisition, analysis, recognition, and the transmission wirelessly, of human dynamic gestures to a computer, is described. An intuitive algorithm for processing the accelerometer data was implemented and tested. This method permits all the analysis to be done by the embedded system processor. The system is capable of recognizing gestures involving a combination of straight line motions in three dimensions. These gestures are then used to control a host computer which executes tasks based on the gesture received. A sample application showing how the gestures can be mapped to the English alphabet is also shown.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer India},
	author = {Parsani, Rahul and Singh, Karandeep},
	editor = {Tiwary, U. S. and Siddiqui, Tanveer J. and Radhakrishna, M. and Tiwari, M. D.},
	year = {2009},
	doi = {10.1007/978-81-8489-203-1_18},
	note = {4 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Proceedings of the First International Conference on Intelligent Human Computer Interaction},
	keywords = {tech:accelerometer},
	pages = {195--201},
	annote = {[TLDR] A complete embedded system which facilitates the data acquisition, analysis, recognition, and the transmission wirelessly, of human dynamic gestures to a computer, is described.},
}

@inproceedings{wang_user-independent_2013,
	title = {User-independent accelerometer-based gesture recognition for mobile devices},
	volume = {1},
	url = {https://revistas.usal.es/index.php/2255-2863/article/view/ADCAIJ20121311125},
	doi = {10.14201/ADCAIJ20121311125},
	abstract = {Many mobile devices embed nowadays inertial sensors. This enables new forms of human-computer interaction through the use of gestures (movements performed with the mobile device) as a way of communication. This paper presents an accelerometer-based gesture recognition system for mobile devices which is able to recognize a collection of 10 different hand gestures. The system was conceived to be light and to operate in a user-independent manner in real time. The recognition system was implemented in a smart phone and evaluated through a collection of user tests, which showed a recognition accuracy similar to other state-of-the art techniques and a lower computational complexity. The system was also used to build a human-robot interface that enables controlling a wheeled robot with the gestures made with the mobile phone},
	urldate = {2023-03-07},
	booktitle = {{ADCAIJ}: {Advances} in {Distributed} {Computing} and {Artificial} {Intelligence} {Journal}},
	author = {Wang, Xian and Tarrío, Paula and Bernardos, Ana María and Metola, Eduardo and Casar, José Ramón},
	month = jul,
	year = {2013},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 2255-2863
Issue: 3
Journal Abbreviation: ADCAIJ},
	keywords = {tech:accelerometer, classes:10-29, app:robot-control},
	pages = {11--25},
	annote = {[TLDR] An accelerometer-based gesture recognition system for mobile devices which is able to recognize a collection of 10 different hand gestures which was conceived to be light and to operate in a user-independent manner in real time.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2YFHA534/Wang et al. - 2013 - User-independent accelerometer-based gesture recog.pdf:application/pdf},
}

@article{akl_accelerometer-based_2010,
	title = {Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, \&\#x00026; compressive sensing},
	url = {http://ieeexplore.ieee.org/document/5495895/},
	doi = {10.1109/ICASSP.2010.5495895},
	abstract = {We propose a gesture recognition system based primarily on a single 3-axis accelerometer. The system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition. A dictionary of 18 gestures is defined and a database of over 3,700 repetitions is created from 7 users. Our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition, to the best of our knowledge. The proposed system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
	urldate = {2023-03-07},
	journal = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
	author = {Akl, Ahmad and Valaee, Shahrokh},
	year = {2010},
	note = {142 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2010 IEEE International Conference on Acoustics, Speech and Signal Processing
ISBN: 9781424442959
Place: Dallas, TX, USA
Publisher: IEEE},
	keywords = {tech:accelerometer, classes:10-29, participants:7},
	pages = {2270--2273},
	annote = {[TLDR] A gesture recognition system based primarily on a single 3-axis accelerometer that achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
	file = {Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf:/Users/brk/Zotero/storage/85QXVKVJ/Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf:application/pdf},
}

@article{zinnen_new_2008,
	title = {A new approach to enable gesture recognition in continuous data streams},
	url = {http://ieeexplore.ieee.org/document/4911581/},
	doi = {10.1109/ISWC.2008.4911581},
	abstract = {Gesture recognition has great potential for mobile and wearable computing. Most papers in this area focus on classifying different gestures, but do not evaluate the distinctiveness of gestures in continuous recordings of gestures in daily life. This paper presents a new approach for the important and challenging problem of gesture recognition in continuous data streams. We use turning points of arm movements to identify segments of interest in the continuous data stream. The recognition algorithm considers both the direction of movements between turning points and the shape of the turning points for classification. Using the new method, seven gestures of different complexity are evaluated against a realistic background class of daily gestures in five different scenarios.},
	urldate = {2023-03-07},
	journal = {2008 12th IEEE International Symposium on Wearable Computers},
	author = {Zinnen, Andreas and Schiele, Bernt},
	year = {2008},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2008 12th IEEE International Symposium on Wearable Computers
ISBN: 9781424426379
Place: Pittaburgh, PA, USA
Publisher: IEEE},
	keywords = {classes:{\textless}10},
	pages = {33--40},
	annote = {[TLDR] A new approach for the important and challenging problem of gesture recognition in continuous data streams is presented, which uses turning points of arm movements to identify segments of interest in the continuous data stream.},
}

@inproceedings{hutchison_continuous_2010,
	address = {Berlin, Heidelberg},
	title = {Continuous {Realtime} {Gesture} {Following} and {Recognition}},
	volume = {5934},
	isbn = {978-3-642-12552-2 978-3-642-12553-9},
	url = {http://link.springer.com/10.1007/978-3-642-12553-9_7},
	doi = {10.1007/978-3-642-12553-9_7},
	abstract = {We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Bevilacqua, Frédéric and Zamborlin, Bruno and Sypniewski, Anthony and Schnell, Norbert and Guédy, Fabrice and Rasamimanana, Nicolas},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kopp, Stefan and Wachsmuth, Ipke},
	year = {2010},
	doi = {10.1007/978-3-642-12553-9_7},
	note = {207 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Gesture in Embodied Communication and Human-Computer Interaction
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm},
	pages = {73--84},
	annote = {[TLDR] A HMM based system for real-time gesture analysis that relies on a detailed modeling of multidimensional temporal curves allowing for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.},
	file = {Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf:/Users/brk/Zotero/storage/9YT2MAGT/Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf:application/pdf},
}

@inproceedings{schlomer_gesture_2008,
	address = {Bonn Germany},
	title = {Gesture recognition with a {Wii} controller},
	isbn = {978-1-60558-004-3},
	url = {https://dl.acm.org/doi/10.1145/1347390.1347395},
	doi = {10.1145/1347390.1347395},
	abstract = {In many applications today user interaction is moving away from mouse and pens and is becoming pervasive and much more physical and tangible. New emerging interaction technologies allow developing and experimenting with new interaction methods on the long way to providing intuitive human computer interaction. In this paper, we aim at recognizing gestures to interact with an application and present the design and evaluation of our sensor-based gesture recognition. As input device we employ the Wii-controller (Wiimote) which recently gained much attention world wide. We use the Wiimote's acceleration sensor independent of the gaming console for gesture recognition. The system allows the training of arbitrary gestures by users which can then be recalled for interacting with systems like photo browsing on a home TV. The developed library exploits Wii-sensor data and employs a hidden Markov model for training and recognizing user-chosen gestures. Our evaluation shows that we can already recognize gestures with a small number of training samples. In addition to the gesture recognition we also present our experiences with the Wii-controller and the implementation of the gesture recognition. The system forms the basis for our ongoing work on multimodal intuitive media browsing and are available to other researchers in the field.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 2nd international conference on {Tangible} and embedded interaction},
	publisher = {ACM},
	author = {Schlömer, Thomas and Poppinga, Benjamin and Henze, Niels and Boll, Susanne},
	month = feb,
	year = {2008},
	note = {560 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, classes:5, model:hmm, hardware:wii, classes:{\textless}10},
	pages = {11--14},
	annote = {[TLDR] The design and evaluation of the sensor-based gesture recognition system is presented, which allows the training of arbitrary gestures by users which can then be recalled for interacting with systems like photo browsing on a home TV.},
	file = {Submitted Version:/Users/brk/Zotero/storage/QUVGMHED/Schlömer et al. - 2008 - Gesture recognition with a Wii controller.pdf:application/pdf},
}

@article{rajko_real-time_2007,
	title = {Real-time {Gesture} {Recognition} with {Minimal} {Training} {Requirements} and {On}-line {Learning}},
	url = {http://ieeexplore.ieee.org/document/4270328/},
	doi = {10.1109/CVPR.2007.383330},
	abstract = {In this paper, we introduce the semantic network model (SNM), a generalization of the hidden Markov model (HMM) that uses factorization of state transition probabilities to reduce training requirements, increase the efficiency of gesture recognition and on-line learning, and allow more precision in gesture modeling. We demonstrate the advantages both formally and experimentally, using examples such as full-body multimodal gesture recognition via optical motion capture and a pressure sensitive floor, as well as mouse/pen gesture recognition. Our results show that our algorithm performs much better than the traditional approach in situations where training samples are limited and/or the precision of the gesture model is high.},
	urldate = {2023-03-07},
	journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
	author = {Rajko, Stjepan and Qian, Gang and Ingalls, Todd and James, Jodi},
	month = jun,
	year = {2007},
	note = {69 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2007 IEEE Conference on Computer Vision and Pattern Recognition
ISBN: 9781424411795 9781424411801
Place: Minneapolis, MN, USA
Publisher: IEEE},
	keywords = {model:hmm},
	pages = {1--8},
	annote = {[TLDR] The semantic network model (SNM), a generalization of the hidden Markov model (HMM) that uses factorization of state transition probabilities to reduce training requirements, increase the efficiency of gesture recognition and on-line learning, and allow more precision in gesture modeling is introduced.},
	file = {Rajko et al. - 2007 - Real-time Gesture Recognition with Minimal Trainin.pdf:/Users/brk/Zotero/storage/7PDEU72Y/Rajko et al. - 2007 - Real-time Gesture Recognition with Minimal Trainin.pdf:application/pdf},
}

@article{ren_robust_2011,
	title = {Robust hand gesture recognition with kinect sensor},
	url = {https://dl.acm.org/doi/10.1145/2072298.2072443},
	doi = {10.1145/2072298.2072443},
	abstract = {Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Proceedings of the 19th ACM international conference on Multimedia},
	author = {Ren, Zhou and Meng, Jingjing and Yuan, Junsong and Zhang, Zhengyou},
	month = nov,
	year = {2011},
	note = {309 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: MM '11: ACM Multimedia Conference
ISBN: 9781450306164
Place: Scottsdale Arizona USA
Publisher: ACM},
	keywords = {hardware:kinect, classes:14, model:cnn, tech:rgbd, classes:10-29},
	pages = {759--760},
	annote = {[TLDR] This work presents a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions, and applies a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) forHand gesture recognition.},
	file = {Ren et al. - 2011 - Robust hand gesture recognition with kinect sensor.pdf:/Users/brk/Zotero/storage/2NSG3HQQ/Ren et al. - 2011 - Robust hand gesture recognition with kinect sensor.pdf:application/pdf},
}

@inproceedings{mantyjarvi_enabling_2004,
	address = {College Park Maryland USA},
	title = {Enabling fast and effortless customisation in accelerometer based gesture interaction},
	isbn = {978-1-58113-981-5},
	url = {https://dl.acm.org/doi/10.1145/1052380.1052385},
	doi = {10.1145/1052380.1052385},
	abstract = {Accelerometer based gesture control is proposed as a complementary interaction modality for handheld devices. Predetermined gesture commands or freely trainable by the user can be used for controlling functions also in other devices. To support versatility of gesture commands in various types of personal device applications gestures should be customisable, easy and quick to train. In this paper we experiment with a procedure for training/recognizing customised accelerometer based gestures with minimum amount of user effort in training. Discrete Hidden Markov Models (HMM) are applied. Recognition results are presented for an external device, a DVD player gesture commands. A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training. For a set of eight gestures, each trained with two original gestures and with two Gaussian noise-distorted duplicates, the average recognition accuracy was 97\%, and with two original gestures and with four noise-distorted duplicates, the average recognition accuracy was 98\%, cross-validated from a total data set of 240 gestures. Use of procedure facilitates quick and effortless customisation in accelerometer based interaction.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 3rd international conference on {Mobile} and ubiquitous multimedia},
	publisher = {ACM},
	author = {Mäntyjärvi, Jani and Kela, Juha and Korpipää, Panu and Kallio, Sanna},
	month = oct,
	year = {2004},
	note = {78 citations (Crossref) [2023-07-11]
165 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:hmm, classes:8, classes:{\textless}10, from:cite.bib},
	pages = {25--31},
	annote = {@mantyjarvi2004 developed a HMM which used linear acceleration in three axes to classify eight gestures. This work was built upon by @kela2006 who utilised the Sensing, Operating, and Activating Peripheral Box (SoapBox) developed by @tuulari2002 to collect motion data which was classified into gestures by a HMM. The resulting classifications were used to interact with a custom designed suite of software which was tailored to make use of the more intuitive gesture interface. The number of gestures that their system could recognise was not reported.

},
	annote = {[TLDR] A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training, facilitating quick and effortless customisation in accelerometer based interaction.},
	file = {Mäntyjärvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf:/Users/brk/Zotero/storage/9GG99UUB/Mäntyjärvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf:application/pdf},
}

@article{mantyjarvi_gesture_2005,
	title = {Gesture {Interaction} for {Small} {Handheld} {Devices} to {Support} {Multimedia} {Applications}},
	url = {https://www.semanticscholar.org/paper/Gesture-Interaction-for-Small-Handheld-Devices-to-M%C3%A4ntyj%C3%A4rvi-Kallio/4945fca3ffa1fe292e0f61ffc4fd9cc51e9c86f8},
	abstract = {Accelerometer-based gesture control is proposed as a complementary interaction modality for small handheld devices to enable a variety of multimedia applications. The motivation for experimenting with gesture interaction is justified by the personal and public domain prototype applications developed. The challenges related to developing user-dependent and independent gesture control are presented. In this article, we experiment with methods for user-dependent gesture recognition with a low number of training repetitions, and for feasible user-independent gesture recognition from a moderately large set of gestures. The user-dependent gesture recognition performance of the continuous Hidden Markov Model (HMM) is better when compared to discrete HMM with three gesture repetitions in a training set. With continuous HMM, a recognition accuracy level of 95\% is obtained with or without tilt normalization, while for discrete HMM a best recognition accuracy of 90\% is obtained. The user-independent gesture recognition performance with continuous HMM of 89\% is considerably better compared to tests with discrete HMM, when both are obtained with cross-validation from 2,520 gestures. An important result is that the effect of using tilt normalization notably increases the user-independent gesture recognition performance by 10- 15\% depending on the method used. The chosen methods show great potential for gesture-based interaction in multimedia applications.},
	urldate = {2023-03-07},
	journal = {J. Mobile Multimedia},
	author = {Mäntyjärvi, Jani and Kallio, S. and Korpipää, Panu and Kela, J. and Plomp, J.},
	month = jun,
	year = {2005},
	note = {19 Citations},
	keywords = {tech:accelerometer, model:hmm},
	annote = {[TLDR] An important result is that the effect of using tilt normalization notably increases the user-independent gesture recognition performance by 10- 15\% depending on the method used, which shows great potential for gesture-based interaction in multimedia applications.},
}

@article{marasovic_motion-based_2015,
	title = {Motion-{Based} {Gesture} {Recognition} {Algorithms} for {Robot} {Manipulation}},
	volume = {12},
	issn = {1729-8814, 1729-8814},
	url = {http://journals.sagepub.com/doi/10.5772/60077},
	doi = {10.5772/60077},
	abstract = {The prevailing trend of integrating inertial sensors in consumer electronics devices has inspired research on new forms of human-computer interaction utilizing hand gestures, which may be set-up on mobile devices themselves. At present, motion gesture recognition is intensely studied, with various recognition techniques being employed and tested. This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings. The study takes two of the most popular and arguably the best recognition methods currently in use - dynamic time warping and hidden Markov model - and sets them against a relatively novel approach founded on distance metric learning. The three selected algorithms are evaluated in terms of their overall performance, accuracy, training time, execution time and storage efficacy. The optimal algorithm is further implemented in a prototype user application, aimed to serve as an interface for controlling the motion of a toy robot via gestures made with a smartphone.},
	language = {en},
	number = {5},
	urldate = {2023-03-07},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Marasović, Tea and Papić, Vladan and Marasović, Jadranka},
	month = may,
	year = {2015},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, model:dtw},
	pages = {51},
	annote = {[TLDR] This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings, and takes two of the most popular and arguably the best recognition methods currently in use and sets them against a relatively novel approach founded on distance metric learning.},
	annote = {[TLDR] This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings, and takes two of the most popular and arguably the best recognition methods currently in use and sets them against a relatively novel approach founded on distance metric learning.},
	file = {Full Text:/Users/brk/Zotero/storage/H6IC9DRX/Marasović et al. - 2015 - Motion-Based Gesture Recognition Algorithms for Ro.pdf:application/pdf},
}

@article{zhang_hand_2009,
	title = {Hand gesture recognition and virtual game control based on {3D} accelerometer and {EMG} sensors},
	url = {https://dl.acm.org/doi/10.1145/1502650.1502708},
	doi = {10.1145/1502650.1502708},
	abstract = {This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7\% in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Proceedings of the 14th international conference on Intelligent user interfaces},
	author = {Zhang, Xu and Chen, Xiang and Wang, Wen-hui and Yang, Ji-hai and Lantz, Vuokko and Wang, Kong-qiao},
	month = feb,
	year = {2009},
	note = {229 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: IUI09: 14th International Conference on Intelligent User Interfaces
ISBN: 9781605581682
Place: Sanibel Island Florida USA
Publisher: ACM},
	keywords = {model:hmm, tech:rgb, classes:8, model:nearest-conflicting-neighbors, classes:{\textless}10},
	pages = {401--406},
	annote = {Cameras track the user’s two hands, and then finds the smallest convex hull that surrounds each hand. This hull is then classified using a funky algorithm I’d never heard of.

},
	annote = {[TLDR] A novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) sensors to realize user-friendly interaction between human and computers is described.},
	annote = {[TLDR] A novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) sensors to realize user-friendly interaction between human and computers is described.},
	annote = {[TLDR] A novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) sensors to realize user-friendly interaction between human and computers is described.},
	annote = {[TLDR] A novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) sensors to realize user-friendly interaction between human and computers is described.},
	file = {Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf:/Users/brk/Zotero/storage/4QW723JU/Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf:application/pdf},
}

@article{jiang_development_2016,
	title = {Development of a real-time hand gesture recognition wristband based on {sEMG} and {IMU} sensing},
	url = {http://ieeexplore.ieee.org/document/7866498/},
	doi = {10.1109/ROBIO.2016.7866498},
	abstract = {Human computer interaction is becoming more integrated in daily life with the proliferation of mobile devices and virtual reality technology. Hand gesture recognition is a potentially promising mechanism to facilitate human computer interaction, however wrist-mounted surface electromyography (sEMG) hand gesture classification is particularly challenging given the relatively small sEMG signals as compared to traditional forearm-based sEMG sensing. This paper introduces the development of a wristband for detecting eight air gestures and four surface gestures at two different force levels through sEMG and inertial measurement unit (IMU) sensor fusion. To validate the wrist-worn device, ten healthy subjects performed hand gesture recognition experiments resulting in a total average recognition rate of 92.6\% for air gestures and 88.8\% for surface gestures. This paper demonstrates the potential of wrist-worn devices for accurate hand gesture recognition applications.},
	urldate = {2023-03-07},
	journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
	author = {Jiang, Shuo and Lv, Bo and Sheng, Xinjun and Zhang, Chao and Wang, Haitao and Shull, Peter B.},
	month = dec,
	year = {2016},
	note = {16 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)
ISBN: 9781509043644
Place: Qingdao, China
Publisher: IEEE},
	keywords = {tech:emg, classes:10-29},
	pages = {1256--1261},
	annote = {[TLDR] The development of a wristband for detecting eight air gestures and four surface gestures at two different force levels through sEMG and inertial measurement unit (IMU) sensor fusion.},
}

@article{li_hand_2018,
	title = {Hand {Gesture} {Recognition} and {Real}-time {Game} {Control} {Based} on {A} {Wearable} {Band} with 6-axis {Sensors}},
	url = {https://ieeexplore.ieee.org/document/8489743/},
	doi = {10.1109/IJCNN.2018.8489743},
	abstract = {Human-computer interaction introduces critical open door with the proceeds with improvement of wearable gadgets. Gesture recognition through smart devices is becoming a popular research direction. This paper proposes a hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband. We utilize three-axis accelerator and gyroscope sensors embedded in smart band to collect hand motion information and use Kinect camera capture video information for manual segmentation during the training model phase. A continuous gesture segmentation algorithm based on sliding window and DTW algorithm is developed to detect meaningful gestures in the real-time game control stage. In addition, an android game named Fly Birds which is controlled by gesture recognition result is presented to simulate real-time human-computer interaction. Then, we classify the data in the window using common classifiers. Finally, our experimental results show that, we can accurately identify the designed gestures during the stage of static gesture recognition, and we also achieve a perfect interactive effect in the process of dynamic real-time game control. The experiment outcomes will advance the ascent of human-PC cooperation in view of hand gesture recognition and related applications will rise in vast numbers.},
	urldate = {2023-03-07},
	journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
	author = {Li, Yande and Wang, Taiqian and khan, Aamir and Li, Lian and Li, Caihong and Yang, Yi and Liu, Li},
	month = jul,
	year = {2018},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2018 International Joint Conference on Neural Networks (IJCNN)
ISBN: 9781509060146
Place: Rio de Janeiro
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:rgbd, model:dtw, app:gaming},
	pages = {1--6},
	annote = {[TLDR] A hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband and can accurately identify the designed gestures during the stage of static gesture recognition is proposed.},
}

@article{akl_novel_2011,
	title = {A {Novel} {Accelerometer}-{Based} {Gesture} {Recognition} {System}},
	volume = {59},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5993550/},
	doi = {10.1109/TSP.2011.2165707},
	abstract = {In this paper, we address the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an ℓ1-minimization problem. The gesture recognition system operates primarily on data from a single 3-axis accelerometer and comprises two main stages: a training stage and a testing stage. For training, the system employs dynamic time warping as well as affinity propagation to create exemplars for each gesture while for testing, the system projects all candidate traces and also the unknown trace onto the same lower dimensional subspace for recognition. A dictionary of 18 gestures is defined and a database of over 3700 traces is created from seven subjects on which the system is tested and evaluated. To the best of our knowledge, our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition. The system achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
	number = {12},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Signal Processing},
	author = {Akl, Ahmad and Feng, Chen and Valaee, Shahrokh},
	month = dec,
	year = {2011},
	note = {100 citations (Crossref) [2023-07-11]
170 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, msc, model:dtw, classes:18, read-priority-1, classes:10-29, participants:7, from:cite.bib, claims-to-be-first},
	pages = {6197--6205},
	annote = {@akl2011 used a single three-axis accelerometer and dynamic time warping to classify 18 gestures comprised of single hand motions. These gestures were simple single handed pattens traced out in the air, such as circles, lines, and triangles.

},
	annote = {[TLDR] This paper addresses the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an ℓ1-minimization problem and achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
	file = {Full Text:/Users/brk/Zotero/storage/SAQUJ2PY/Akl et al. - 2011 - A Novel Accelerometer-Based Gesture Recognition Sy.pdf:application/pdf},
}

@article{zhao_real-time_2017,
	title = {Real-time head gesture recognition on head-mounted displays using cascaded hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/8122975/},
	doi = {10.1109/SMC.2017.8122975},
	abstract = {Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.},
	urldate = {2023-03-07},
	journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Zhao, Jingbo and Allison, Robert S.},
	month = oct,
	year = {2017},
	note = {11 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 IEEE International Conference on Systems, Man and Cybernetics (SMC)
ISBN: 9781538616451
Place: Banff, AB
Publisher: IEEE},
	keywords = {model:hmm, tech:rgb, classes:9, classes:{\textless}10, hardware:oculus-rift},
	pages = {2361--2366},
	annote = {[TLDR] The results show that the proposed approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models is effective in recognizing head gestures and can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.},
	file = {Submitted Version:/Users/brk/Zotero/storage/JUTLYHW4/Zhao and Allison - 2017 - Real-time head gesture recognition on head-mounted.pdf:application/pdf},
}

@inproceedings{hutchison_australian_2013,
	address = {Berlin, Heidelberg},
	title = {Australian {Sign} {Language} {Recognition} {Using} {Moment} {Invariants}},
	volume = {7996},
	isbn = {978-3-642-39481-2 978-3-642-39482-9},
	url = {http://link.springer.com/10.1007/978-3-642-39482-9_59},
	doi = {10.1007/978-3-642-39482-9_59},
	abstract = {Human Computer Interaction is geared towards seamless human machine integration without the need for LCDs, Keyboards or Gloves. Systems have already been developed to react to limited hand gestures especially in gaming and in consumer electronics control. Yet, it is a monumental task in bridging the well-developed sign languages in different parts of the world with a machine to interpret the meaning. One reason is the sheer extent of the vocabulary used in sign language and the sequence of gestures needed to communicate different words and phrases. Auslan the Australian Sign Language is comprised of numbers, finger spelling for words used in common practice and a medical dictionary. There are 7415 words listed in Auslan website. This research article tries to implement recognition of numerals using a computer using the static hand gesture recognition system developed for consumer electronics control at the University of Wollongong in Australia. The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures. The system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Huang, De-Shuang and Jo, Kang-Hyun and Zhou, Yong-Quan and Han, Kyungsook},
	year = {2013},
	doi = {10.1007/978-3-642-39482-9_59},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Intelligent Computing Theories and Technology
Series Title: Lecture Notes in Computer Science},
	keywords = {app:sign-language},
	pages = {509--514},
	annote = {[TLDR] The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures and the system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
}

@inproceedings{kadous_machine_1996,
	title = {Machine {Recognition} of {Auslan} {Signs} {Using} {PowerGloves}: {Towards} {Large}-{Lexicon} {Recognition} of {Sign} {Lan}},
	shorttitle = {Machine {Recognition} of {Auslan} {Signs} {Using} {PowerGloves}},
	url = {https://www.semanticscholar.org/paper/Machine-Recognition-of-Auslan-Signs-Using-Towards-Kadous/1ed3e20bbcf12c60e766189987074ac2935d7140},
	abstract = {Instrumented gloves use a variety of sensors to provide information about the user's hand. They can be used for recognition of gestures; especially well-deened gesture sets such as sign languages. However, recognising gestures is a diicult task, due to intrapersonal and inter-personal variations in performing them. One approach to solving this problem is to use machine learning. In this case, samples of 95 discrete Australian Sign Language (Auslan) signs were collected using a Power-Glove. Two machine learning techniques were applied \{\vphantom{\}} instance-based learning (IBL) and decision-tree learning \{\vphantom{\}} to the data after some simple features were extracted. Accuracy of approximately 80 per cent was achieved using IBL, despite the severe limitations of the glove.},
	urldate = {2023-03-07},
	author = {Kadous, M. W.},
	year = {1996},
	note = {115 Citations},
	keywords = {model:decision-tree, app:sign-language, app:australian-sl, classes:50-100, model:instance-based-learning},
	annote = {[TLDR] In this case, samples of 95 discrete Australian Sign Language (Auslan) signs were collected using a Power-Glove, and instance-based learning (IBL) and decision-tree learning \{\vphantom{\}} to the data after some simple features were extracted.},
}

@article{mohandes_automation_2004,
	title = {Automation of the arabic sign language recognition},
	url = {http://ieeexplore.ieee.org/document/1307840/},
	doi = {10.1109/ICTTA.2004.1307840},
	abstract = {This paper introduces a system to recognize the Arabic sign language using an instrumented glove and a machine learning method. Interfaces in sign language systems can be categorized as direct-device or vision-based. The direct-device approach uses measurement devices that are in direct contact with the hand such as instrumented gloves, flexion sensors, styli and position-tracking devices. On the other hand, the vision-based approach captures the movement of the singer's hand using a camera that is sometimes aided by making the signer wear a glove that has painted areas indicating the positions of the fingers or knuckles. The proposed system basically consists of a PowerGlove that is connected through the serial port to a workstation running the support vector machine algorithm. Obtained results are promising even though a simple and cheap glove with limited sensors was utilized.},
	urldate = {2023-03-07},
	journal = {Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.},
	author = {Mohandes, M. and A-Buraiky, S. and Halawani, T. and Al-Baiyat, S.},
	year = {2004},
	note = {41 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.
ISBN: 9780780384828
Place: Damascus, Syria
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {479--480},
	annote = {[TLDR] A system to recognize the Arabic sign language using an instrumented glove and a machine learning method and results are promising even though a simple and cheap glove with limited sensors was utilized.},
}

@inproceedings{moeslund_sign_2011,
	address = {London},
	title = {Sign {Language} {Recognition}},
	isbn = {978-0-85729-996-3 978-0-85729-997-0},
	url = {https://link.springer.com/10.1007/978-0-85729-997-0_27},
	doi = {10.1007/978-0-85729-997-0_27},
	abstract = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a precis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer London},
	author = {Cooper, Helen and Holt, Brian and Bowden, Richard},
	editor = {Moeslund, Thomas B. and Hilton, Adrian and Krüger, Volker and Sigal, Leonid},
	year = {2011},
	doi = {10.1007/978-0-85729-997-0_27},
	note = {293 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Visual Analysis of Humans},
	keywords = {app:sign-language},
	pages = {539--562},
	annote = {[TLDR] This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a precis of sign linguistics and their impact on the field.},
}

@article{nath_hand_2019,
	title = {Hand {Gesture} {Recognition}: {A} {Survey}},
	volume = {511},
	shorttitle = {Hand {Gesture} {Recognition}},
	url = {http://link.springer.com/10.1007/978-981-13-0776-8_33},
	doi = {10.1007/978-981-13-0776-8_33},
	abstract = {A human–computer interaction is generally limited to taking input from the user using handheld devices like keyboard, mouse, or scanners. With the advancement in computers, the user interaction approaches have also advanced. Direct use of hands as an input device is an attractive method for providing natural Human–Computer Interaction. It is also helpful for people who use sign language. The chapter aims to study the existing methods for Hand Gesture Recognition and provide a comparative analysis of the same. The entire process of hand gesture recognition is divided into three phases: hand detection, hand tracking, and recognition. The chapter includes a review of the different methods used for the hand gesture recognition. The recognition phase is classified based on the way the input is received as glove based or vision based. For recognition, various methods like Feature extraction, Hidden Markov Model (HMM), Principal Component Analysis (PCA) are compared along with the reported accuracy.},
	language = {en},
	urldate = {2023-03-07},
	author = {Anwar, Shamama and Sinha, Subham Kumar and Vivek, Snehanshu and Ashank, Vishal},
	editor = {Nath, Vijay and Mandal, Jyotsna Kumar},
	year = {2019},
	doi = {10.1007/978-981-13-0776-8_33},
	note = {25 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Nanoelectronics, Circuits and Communication Systems
ISBN: 9789811307751 9789811307768
Place: Singapore
Publisher: Springer Singapore},
	keywords = {type:survey},
	pages = {365--371},
	annote = {[TLDR] The chapter aims to study the existing methods for Hand Gesture recognition and provide a comparative analysis of the same and includes a review of the different methods used for the hand gesture recognition.},
}

@article{bansal_gesture_2016,
	title = {Gesture {Recognition}: {A} {Survey}},
	volume = {139},
	issn = {09758887},
	shorttitle = {Gesture {Recognition}},
	url = {http://www.ijcaonline.org/research/volume139/number2/bansal-2016-ijca-909103.pdf},
	doi = {10.5120/ijca2016909103},
	abstract = {With increasing use of computers in our daily lives, lately there has been a rapid increase in the efforts to develop a better human computer interaction interface. The need of easy to use and advance types of human-computer interaction with natural interfaces is more than ever. In the present framework, the UI (User Interface) of a computer allows user to interact with electronic devices with graphical icons and visual indicators, which is still inconvenient and not suitable for working in virtual environments. An interface which allow user to communicate through gestures is the next step in the direction of advance human computer interface. In the present paper author explore different aspects of gesture recognition techniques.},
	number = {2},
	urldate = {2023-03-07},
	journal = {International Journal of Computer Applications},
	author = {Bansal, Bharti},
	month = apr,
	year = {2016},
	note = {650 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {8--10},
	annote = {[TLDR] In the present paper author explore different aspects of gesture recognition techniques, which are the next step in the direction of advance human computer interface.},
	file = {Full Text:/Users/brk/Zotero/storage/DXTI6K39/Bansal - 2016 - Gesture Recognition A Survey.pdf:application/pdf},
}

@article{mitra_gesture_2007,
	title = {Gesture {Recognition}: {A} {Survey}},
	volume = {37},
	issn = {1094-6977},
	shorttitle = {Gesture {Recognition}},
	url = {http://ieeexplore.ieee.org/document/4154947/},
	doi = {10.1109/TSMCC.2007.893280},
	abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
	author = {Mitra, Sushmita and Acharya, Tinku},
	month = may,
	year = {2007},
	note = {1146 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, type:survey},
	pages = {311--324},
	annote = {This paper has a large section dedicated to face recognition and facial gesture recognition.

“soft computing [4]” (Mitra and Acharya, 2007, p. 1)
“[4] S. Mitra and T. Acharya, Data Mining: Multimedia, Soft Computing, and Bioinformatics. New York: Wiley, 2003.” (Mitra and Acharya, 2007, p. 13) Soft computing?

“Most of the problems have been addressed based on statistical modeling, such as PCA, HMMs [3], [7], [8], Kalman filtering [9], more advanced particle filtering [10], [11] and condensation algorithms [12]–[14]” (Mitra and Acharya, 2007, p. 2)


There is no PCA reference ):




The condensation algorithms, particle filters, kalman filters, references only describe what they are, nothing about how they’re used for gesture recognition


“FSM has been effectively employed in modeling human gestures [15]–[18].” (Mitra and Acharya, 2007, p. 2)
“Connectionist approaches [22], involving multilayer perceptron (MLP), timedelay neural network (TDNN), and radial basis function network (RBFN), have been utilized in gesture recognition as well” (Mitra and Acharya, 2007, p. 2)


This reference is just to explain what NNs are, and doesn’t describe how they’re used for gesture recognition ):


“Particle-filtering-based tracking and its applications in gesture recognition systems became popular very recently [10]–[14].” (Mitra and Acharya, 2007, p. 3) None of these references seem to actually use the described methods for gesture tracking



},
	annote = {[TLDR] A survey on gesture recognition with particular emphasis on hand gestures and facial expressions is provided, and applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail.},
	annote = {[TLDR] A survey on gesture recognition with particular emphasis on hand gestures and facial expressions is provided, and applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail.},
	file = {Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf:/Users/brk/Zotero/storage/32EDNKX8/Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf:application/pdf},
}

@article{khan_survey_2012,
	title = {Survey on {Gesture} {Recognition} for {Hand} {Image} {Postures}},
	volume = {5},
	issn = {1913-8997, 1913-8989},
	url = {http://www.ccsenet.org/journal/index.php/cis/article/view/16598},
	doi = {10.5539/cis.v5n3p110},
	abstract = {One of the attractive methods for providing natural human-computer interaction is the use of the hand as an input device rather than the cumbersome devices such as keyboards and mice, which need the user to be located in a specific location to use these devices. Since human hand is an articulated object, it is an open issue to discuss. The most important thing in hand gesture recognition system is the input features, and the selection of good features representation. This paper presents a review study on the hand postures and gesture recognition methods, which is considered to be a challenging problem in the human-computer interaction context and promising as well. Many applications and techniques were discussed here with the explanation of system recognition framework and its main phases.},
	number = {3},
	urldate = {2023-03-07},
	journal = {Computer and Information Science},
	author = {Khan, Rafiqul Zaman and Ibraheem, Noor Adnan},
	month = apr,
	year = {2012},
	note = {94 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {p110},
	annote = {[TLDR] This paper presents a review study on the hand postures and gesture recognition methods, which is considered to be a challenging problem in the human-computer interaction context and promising as well.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/J2T7FEZP/Khan and Ibraheem - 2012 - Survey on Gesture Recognition for Hand Image Postu.pdf:application/pdf},
}

@article{bloit_short-time_2008,
	title = {Short-time {Viterbi} for online {HMM} decoding: {Evaluation} on a real-time phone recognition task},
	issn = {1520-6149},
	shorttitle = {Short-time {Viterbi} for online {HMM} decoding},
	url = {http://ieeexplore.ieee.org/document/4518061/},
	doi = {10.1109/ICASSP.2008.4518061},
	abstract = {In this paper we present the implementation of an online HMM decoding process. The algorithm derives an online version of the Viterbi algorithm on successive variable length windows, iteratively storing portions of the optimal state path. We explicit the relation between the hidden layer's topology and the applicability and performance prediction of the algorithm. We evaluate the validity and performance of the algorithm on a phone recognition task on a database of continuous speech from a native French speaker. We specifically study the latency-accuracy performance of the algorithm.},
	urldate = {2023-03-07},
	journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
	author = {Bloit, Julien and Rodet, Xavier},
	month = mar,
	year = {2008},
	note = {52 citations (Semantic Scholar/DOI) [2023-03-07]
Conference Name: ICASSP 2008 - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
ISBN: 9781424414833 9781424414840
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {background},
	pages = {2121--2124},
	annote = {[TLDR] This paper presents the implementation of an online HMM decoding process that derives an online version of the Viterbi algorithm on successive variable length windows, iteratively storing portions of the optimal state path.},
	file = {Submitted Version:/Users/brk/Zotero/storage/BTJHRA6M/Bloit and Rodet - 2008 - Short-time Viterbi for online HMM decoding Evalua.pdf:application/pdf},
}

@article{wynants_three_2019,
	title = {Three myths about risk thresholds for prediction models},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1425-3},
	doi = {10.1186/s12916-019-1425-3},
	abstract = {Clinical prediction models are useful in estimating a patient’s risk of having a certain disease or experiencing an event in the future based on their current characteristics. Defining an appropriate risk threshold to recommend intervention is a key challenge in bringing a risk prediction model to clinical application; such risk thresholds are often defined in an ad hoc way. This is problematic because tacitly assumed costs of false positive and false negative classifications may not be clinically sensible. For example, when choosing the risk threshold that maximizes the proportion of patients correctly classified, false positives and false negatives are assumed equally costly. Furthermore, small to moderate sample sizes may lead to unstable optimal thresholds, which requires a particularly cautious interpretation of results.},
	number = {1},
	urldate = {2023-03-24},
	journal = {BMC Medicine},
	author = {Wynants, Laure and van Smeden, Maarten and McLernon, David J. and Timmerman, Dirk and Steyerberg, Ewout W. and Van Calster, Ben and {on behalf of the Topic Group ‘Evaluating diagnostic tests and prediction models’ of the STRATOS initiative}},
	month = oct,
	year = {2019},
	note = {68 citations (Semantic Scholar/DOI) [2023-03-24]},
	keywords = {background},
	pages = {192},
	file = {Full Text PDF:/Users/brk/Zotero/storage/R6RTYDFE/Wynants et al. - 2019 - Three myths about risk thresholds for prediction m.pdf:application/pdf;Snapshot:/Users/brk/Zotero/storage/V9FAH7AE/s12916-019-1425-3.html:text/html},
}

@article{berrar_caveats_2012,
	title = {Caveats and pitfalls of {ROC} analysis in clinical microarray research (and how to avoid them)},
	volume = {13},
	issn = {1467-5463},
	url = {https://doi.org/10.1093/bib/bbr008},
	doi = {10.1093/bib/bbr008},
	abstract = {The receiver operating characteristic (ROC) has emerged as the gold standard for assessing and comparing the performance of classifiers in a wide range of disciplines including the life sciences. ROC curves are frequently summarized in a single scalar, the area under the curve (AUC). This article discusses the caveats and pitfalls of ROC analysis in clinical microarray research, particularly in relation to (i) the interpretation of AUC (especially a value close to 0.5); (ii) model comparisons based on AUC; (iii) the differences between ranking and classification; (iv) effects due to multiple hypotheses testing; (v) the importance of confidence intervals for AUC; and (vi) the choice of the appropriate performance metric. With a discussion of illustrative examples and concrete real-world studies, this article highlights critical misconceptions that can profoundly impact the conclusions about the observed performance.},
	number = {1},
	urldate = {2023-03-24},
	journal = {Briefings in Bioinformatics},
	author = {Berrar, Daniel and Flach, Peter},
	month = jan,
	year = {2012},
	note = {91 citations (Semantic Scholar/DOI) [2023-03-24]},
	keywords = {background},
	pages = {83--97},
	file = {Full Text PDF:/Users/brk/Zotero/storage/RLN68QM9/Berrar and Flach - 2012 - Caveats and pitfalls of ROC analysis in clinical m.pdf:application/pdf},
}

@inproceedings{flach_precision-recall-gain_2015,
	title = {Precision-{Recall}-{Gain} {Curves}: {PR} {Analysis} {Done} {Right}},
	volume = {28},
	shorttitle = {Precision-{Recall}-{Gain} {Curves}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
	urldate = {2023-03-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Flach, Peter and Kull, Meelis},
	year = {2015},
	keywords = {background},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5B3MNYUA/Flach and Kull - 2015 - Precision-Recall-Gain Curves PR Analysis Done Rig.pdf:application/pdf},
}

@article{page_cumulative_1961,
	title = {Cumulative {Sum} {Charts}},
	volume = {3},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1961.10489922},
	doi = {10.1080/00401706.1961.10489922},
	abstract = {This paper, presented orally to the Gordon Research Conference on Statistics in Chemistry in July 1960, traces the development of process inspection schemes from the original methods of Shewhart to the new charts using cumulative sums, and surveys the present practice in the operation of schemes based on cumulative sums. In spitc of the completely different appearance of the visual records kept for Shewhart and cumulative sum charts, a continuous sequence of development from the one type of scheme to the other can be established. The criteria by which a particular process inspection scheme is chosen are also developed and the practical choice of schemes is described.},
	number = {1},
	urldate = {2023-05-08},
	journal = {Technometrics},
	author = {Page, E. S.},
	month = feb,
	year = {1961},
	note = {250 citations (Semantic Scholar/DOI) [2023-05-08]
Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1961.10489922},
	keywords = {background, model:cusum},
	pages = {1--9},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to the {UCR} {Time} {Series} {Classification}/{Clustering} {Page}},
	url = {https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/},
	urldate = {2023-05-13},
	keywords = {type:dataset, dataset:ucr-time-series-classification},
	file = {Welcome to the UCR Time Series Classification/Clustering Page:/Users/brk/Zotero/storage/VML93JCD/time_series_data_2018.html:text/html},
}

@article{bagnall_great_2017,
	title = {The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {31},
	issn = {1573-756X},
	shorttitle = {The great time series classification bake off},
	url = {https://doi.org/10.1007/s10618-016-0483-9},
	doi = {10.1007/s10618-016-0483-9},
	abstract = {In the last 5 years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.},
	language = {en},
	number = {3},
	urldate = {2023-05-13},
	journal = {Data Mining and Knowledge Discovery},
	author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
	month = may,
	year = {2017},
	note = {5 citations (Semantic Scholar/DOI) [2023-05-13]},
	keywords = {background, type:survey},
	pages = {606--660},
	file = {Full Text PDF:/Users/brk/Zotero/storage/SRFG34ZY/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf:application/pdf},
}

@misc{wang_time_2016,
	title = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}: {A} {Strong} {Baseline}},
	shorttitle = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.06455},
	doi = {10.48550/arXiv.1611.06455},
	abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
	urldate = {2023-05-13},
	publisher = {arXiv},
	author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
	month = dec,
	year = {2016},
	note = {1024 citations (Semantic Scholar/arXiv) [2023-05-13]
arXiv:1611.06455 [cs, stat]},
	keywords = {background, time-series},
	file = {arXiv Fulltext PDF:/Users/brk/Zotero/storage/F8CWCQYK/Wang et al. - 2016 - Time Series Classification from Scratch with Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/brk/Zotero/storage/NUUI7HZK/1611.html:text/html},
}

@misc{norvig_how_2007,
	title = {How to {Write} a {Spelling} {Corrector}},
	url = {https://norvig.com/spell-correct.html},
	urldate = {2023-05-25},
	author = {Norvig, Peter},
	year = {2007},
	keywords = {background, autocorrect},
	file = {How to Write a Spelling Corrector:/Users/brk/Zotero/storage/ERETGY5C/spell-correct.html:text/html},
}

@article{page_continuous_1954,
	title = {{CONTINUOUS} {INSPECTION} {SCHEMES}},
	volume = {41},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/41.1-2.100},
	doi = {10.1093/biomet/41.1-2.100},
	number = {1-2},
	urldate = {2023-05-30},
	journal = {Biometrika},
	author = {PAGE, E. S.},
	month = jun,
	year = {1954},
	note = {4816 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background, model:cusum},
	pages = {100--115},
	file = {CONTINUOUS INSPECTION SCHEMES | Biometrika | Oxford Academic:/Users/brk/Zotero/storage/2CCAB6JV/456627.html:text/html;Snapshot:/Users/brk/Zotero/storage/886PZALL/456627.html:text/html},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
	language = {en},
	number = {5},
	urldate = {2023-05-30},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	note = {9990 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background, model:ffnn, universal-approximators},
	pages = {359--366},
}

@article{neal_pattern_2007,
	title = {Pattern {Recognition} and {Machine} {Learning}},
	volume = {49},
	issn = {0040-1706, 1537-2723},
	url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2007.s518},
	doi = {10.1198/tech.2007.s518},
	abstract = {the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.},
	language = {en},
	number = {3},
	urldate = {2023-05-30},
	journal = {Technometrics},
	author = {Neal, Radford M},
	month = aug,
	year = {2007},
	note = {9985 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background},
	pages = {366--366},
	annote = {[TLDR] This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion and will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments.},
}

@article{white_principles_1963,
	title = {Principles of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	volume = {76},
	issn = {00029556},
	shorttitle = {Principles of {Neurodynamics}},
	url = {https://www.jstor.org/stable/1419730?origin=crossref},
	doi = {10.2307/1419730},
	abstract = {Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.},
	number = {4},
	urldate = {2023-06-07},
	journal = {The American Journal of Psychology},
	author = {White, B. W. and Rosenblatt, Frank},
	month = dec,
	year = {1963},
	note = {2231 citations (Semantic Scholar/DOI) [2023-06-07]},
	keywords = {background, perceptrons},
	pages = {705},
	annote = {[TLDR] The background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons are reviewed, and some of the notation to be used in later sections are presented.},
	annote = {[TLDR] The background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons are reviewed, and some of the notation to be used in later sections are presented.},
}

@article{mcculloch_logical_1990,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {52},
	issn = {0092-8240, 1522-9602},
	url = {http://link.springer.com/10.1007/BF02459570},
	doi = {10.1007/BF02459570},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {1-2},
	urldate = {2023-06-07},
	journal = {Bulletin of Mathematical Biology},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = jan,
	year = {1990},
	note = {129 Citations},
	keywords = {background},
	pages = {99--115},
	annote = {[TLDR] It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time.},
	annote = {[TLDR] It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time.},
}

@article{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	language = {en},
	urldate = {2023-06-07},
	author = {Nielsen, Michael A.},
	year = {2015},
	note = {Publisher: Determination Press},
	keywords = {background},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-06-07},
	journal = {CoRR},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	keywords = {background},
	annote = {[TLDR] This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/SB7A4J49/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2023-06-08},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {9997 citations (Semantic Scholar/DOI) [2023-06-08]},
	keywords = {background, backpropogation},
	pages = {533--536},
	annote = {[TLDR] Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain.},
}

@article{baum_statistical_1966,
	title = {Statistical {Inference} for {Probabilistic} {Functions} of {Finite} {State} {Markov} {Chains}},
	volume = {37},
	url = {https://doi.org/10.1214/aoms/1177699147},
	doi = {10.1214/aoms/1177699147},
	number = {6},
	journal = {The Annals of Mathematical Statistics},
	author = {Baum, Leonard E. and Petrie, Ted},
	year = {1966},
	note = {2933 citations (Semantic Scholar/DOI) [2023-06-19]
Publisher: Institute of Mathematical Statistics},
	keywords = {model:hmm, background, model:finite-state-markov-chains},
	pages = {1554 -- 1563},
	file = {Full Text:/Users/brk/Zotero/storage/VMY4KUTL/Baum and Petrie - 1966 - Statistical Inference for Probabilistic Functions .pdf:application/pdf},
}

@article{wilson_parametric_1999,
	title = {Parametric hidden {Markov} models for gesture recognition},
	volume = {21},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/790429/},
	doi = {10.1109/34.790429},
	abstract = {A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction.},
	number = {9},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wilson, A.D. and Bobick, A.F.},
	month = sep,
	year = {1999},
	note = {675 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm},
	pages = {884--900},
	annote = {[TLDR] The approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states by forming an expectation-maximization (EM) method for training the parametric HMM.},
}

@article{starner_real-time_1995,
	title = {Real-time {American} {Sign} {Language} recognition from video using hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/477012/},
	doi = {10.1109/ISCV.1995.477012},
	abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
	urldate = {2023-06-22},
	journal = {Proceedings of International Symposium on Computer Vision - ISCV},
	author = {Starner, T. and Pentland, A.},
	year = {1995},
	note = {994 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: International Symposium on Computer Vision - ISCV
ISBN: 9780818671906
Place: Coral Gables, FL, USA
Publisher: IEEE Comput. Soc. Press},
	keywords = {model:hmm, tech:rgb, app:sign-language, app:american-sl, type:seminal, author:starner},
	pages = {265--270},
	annote = {[TLDR] A real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5LD7H7UI/Starner - 1995 - Visual Recognition of American Sign Language Using.pdf:application/pdf},
}

@article{starner_real-time_1998,
	title = {Real-time {American} sign language recognition using desk and wearable computer based video},
	volume = {20},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/735811/},
	doi = {10.1109/34.735811},
	abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
	number = {12},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Starner, T. and Weaver, J. and Pentland, A.},
	month = dec,
	year = {1998},
	note = {1394 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, type:seminal, author:starner},
	pages = {1371--1375},
	annote = {[TLDR] Two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands are presented.},
	file = {Starner et al. - 1998 - Real-time American sign language recognition using.pdf:/Users/brk/Zotero/storage/JITL4ER8/Starner et al. - 1998 - Real-time American sign language recognition using.pdf:application/pdf},
}

@inproceedings{shen_using_2019,
	address = {Cham},
	title = {Using {3D} {Convolutional} {Neural} {Networks} to {Learn} {Spatiotemporal} {Features} for {Automatic} {Surgical} {Gesture} {Recognition} in {Video}},
	volume = {11768},
	isbn = {978-3-030-32253-3 978-3-030-32254-0},
	url = {http://link.springer.com/10.1007/978-3-030-32254-0_52},
	doi = {10.1007/978-3-030-32254-0_52},
	abstract = {Automatically recognizing surgical gestures is a crucial step towards a thorough understanding of surgical skill. Possible areas of application include automatic skill assessment, intra-operative monitoring of critical surgical steps, and semi-automation of surgical tasks. Solutions that rely only on the laparoscopic video and do not require additional sensor hardware are especially attractive as they can be implemented at low cost in many scenarios. However, surgical gesture recognition based only on video is a challenging problem that requires effective means to extract both visual and temporal information from the video. Previous approaches mainly rely on frame-wise feature extractors, either handcrafted or learned, which fail to capture the dynamics in surgical video. To address this issue, we propose to use a 3D Convolutional Neural Network (CNN) to learn spatiotemporal features from consecutive video frames. We evaluate our approach on recordings of robot-assisted suturing on a bench-top model, which are taken from the publicly available JIGSAWS dataset. Our approach achieves high frame-wise surgical gesture recognition accuracies of more than 84\%, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately. For the first time, these results demonstrate the benefit of spatiotemporal CNNs for video-based surgical gesture recognition.},
	language = {en},
	urldate = {2023-06-22},
	publisher = {Springer International Publishing},
	author = {Funke, Isabel and Bodenstedt, Sebastian and Oehme, Florian and Von Bechtolsheim, Felix and Weitz, Jürgen and Speidel, Stefanie},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	doi = {10.1007/978-3-030-32254-0_52},
	note = {61 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Medical Image Computing and Computer Assisted Intervention – MICCAI 2019
Series Title: Lecture Notes in Computer Science},
	keywords = {model:cnn, tech:rgb},
	pages = {467--475},
	annote = {[TLDR] This work proposes to use a 3D Convolutional Neural Network to learn spatiotemporal features from consecutive video frames to achieve high frame-wise surgical gesture recognition accuracies, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2P6JU4JY/Funke et al. - 2019 - Using 3D Convolutional Neural Networks to Learn Sp.pdf:application/pdf},
}

@article{ur_rehman_dynamic_2022,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Using} {3D}-{CNN} and {LSTM} {Networks}},
	volume = {70},
	issn = {1546-2226},
	url = {https://www.techscience.com/cmc/v70n3/44942},
	doi = {10.32604/cmc.2022.019586},
	abstract = {: Recognition of dynamic hand gestures in real-time is a difficult task because the system can never know when or from where the gesture starts and ends in a video stream. Many researchers have been working on vision-based gesture recognition due to its various applications. This paper proposes a deep learning architecture based on the combination of a 3D Convolutional Neural Network (3D-CNN) and a Long Short-Term Memory (LSTM) network. The proposed architecture extracts spatial-temporal information from video sequences input while avoiding extensive computation. The 3D-CNN is used for the extraction of spectral and spatial features which are then given to the LSTM network through which classification is carried out. The proposed model is a light-weight architecture with only 3.7 million training parameters. The model has been evaluated on 15 classes from the 20BN-jester dataset available publicly. The model was trained on 2000 video-clips per class which were separated into 80\% training and 20\% validation sets. An accuracy of 99\% and 97\% was achieved on training and testing data, respectively. We further show that the combination of 3D-CNN with LSTM gives superior results as compared to MobileNetv2 + LSTM.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Computers, Materials \& Continua},
	author = {Ur Rehman, Muneeb and Ahmed, Fawad and Attique Khan, Muhammad and Tariq, Usman and Abdulaziz Alfouzan, Faisal and M. Alzahrani, Nouf and Ahmad, Jawad},
	year = {2022},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb, movement:dynamic, model:lstm, dataset:20bn-jester},
	pages = {4675--4690},
	annote = {[TLDR] A deep learning architecture based on the combination of a 3D Convolutional Neural Network (3D-CNN) and a Long Short-Term Memory (LSTM) network that extracts spatial-temporal information from video sequences input while avoiding extensive computation is proposed.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/7NE5RC22/Ur Rehman et al. - 2022 - Dynamic Hand Gesture Recognition Using 3D-CNN and .pdf:application/pdf},
}

@article{wu_deep_2016,
	title = {Deep {Dynamic} {Neural} {Networks} for {Multimodal} {Gesture} {Segmentation} and {Recognition}},
	volume = {38},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/7423804/},
	doi = {10.1109/TPAMI.2016.2537340},
	abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
	number = {8},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wu, Di and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
	month = aug,
	year = {2016},
	note = {373 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, model:cnn, tech:rgbd, model:ffnn},
	pages = {1583--1597},
	annote = {[TLDR] A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations.},
	file = {Accepted Version:/Users/brk/Zotero/storage/UNZIHXE3/Wu et al. - 2016 - Deep Dynamic Neural Networks for Multimodal Gestur.pdf:application/pdf},
}

@article{elbadawy_arabic_2017,
	title = {Arabic sign language recognition with {3D} convolutional neural networks},
	url = {http://ieeexplore.ieee.org/document/8260028/},
	doi = {10.1109/INTELCIS.2017.8260028},
	abstract = {Sign Language recognition is very important for communication purposes between Hearing Impaired (HI) people and hearing ones. Arabic Sign Language Recognition field became widespread because of its difficult nature and numerous details. Most researchers employed different input sensors, features extractors, and classifiers on static and dynamic data. These different ways were customized and employed in our previous work in the Arabic Sign Language Recognition field. In this paper, features extractor with deep behavior was used to deal with the minor details of Arabic Sign Language. 3D Convolutional Neural Network (CNN) was used to recognize 25 gestures from Arabic sign language dictionary. The recognition system was fed with data from depth maps. The system achieved 98\% accuracy for observed data and 85\% average accuracy for new data. The results could be improved as more data from more different signers are included.},
	urldate = {2023-06-22},
	journal = {2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)},
	author = {ElBadawy, Menna and Elons, A. S. and Shedeed, Howida A. and Tolba, M. F.},
	month = dec,
	year = {2017},
	note = {43 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)
ISBN: 9781538608210
Place: Cairo
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {66--71},
	annote = {[TLDR] 3D Convolutional Neural Network (CNN) was used to recognize 25 gestures from Arabic sign language dictionary and achieved 98\% accuracy for observed data and 85\% average accuracy for new data.},
}

@article{liang_3d_2018,
	title = {{3D} {Convolutional} {Neural} {Networks} for {Dynamic} {Sign} {Language} {Recognition}},
	volume = {61},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article/61/11/1724/4995616},
	doi = {10.1093/comjnl/bxy049},
	abstract = {Automatic dynamic sign language recognition is even more challenging than gesture recognition due to the fact that the vocabularies are large and signs are context dependent. Previous works in this direction tend to build classifiers based on complex hand-crafted features computed from the raw inputs. As a type of deep learning model, convolutional neural networks (CNNs) have significantly advanced the accuracy of human gesture classification. However, such methods are currently used to treat video frames as 2D images and recognize gestures at the individual frame level. In this paper, we present a data driven system in which 3D-CNNs are applied to extract spatial and temporal features from video streams, and the motion information is captured by noting the variation in depth between each pair of consecutive frames. To further boost the performance, multi-modal of video streams, including infrared, contour and skeleton are used as input for the architecture and the prediction results estimated from the different sub-networks were fused together. In order to validate our method, we introduce a new challenging multi-modal dynamic sign language dataset captured with Kinect sensors. We evaluate the proposed approach on the collected dataset and achieve superior performance. Moreover, our method achieves a mean Jaccard Index score of 0.836 on the ChaLearn Looking at People Gesture datasets.},
	language = {en},
	number = {11},
	urldate = {2023-06-22},
	journal = {The Computer Journal},
	author = {Liang, Zhi-jie and Liao, Sheng-bin and Hu, Bing-zhang},
	editor = {Manolopoulos, Yannis},
	month = nov,
	year = {2018},
	note = {47 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1724--1736},
	annote = {[TLDR] This paper presents a data driven system in which 3D-CNNs are applied to extract spatial and temporal features from video streams, and the motion information is captured by noting the variation in depth between each pair of consecutive frames.},
}

@article{bhagat_indian_2019,
	title = {Indian {Sign} {Language} {Gesture} {Recognition} using {Image} {Processing} and {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8945850/},
	doi = {10.1109/DICTA47822.2019.8945850},
	abstract = {Speech impaired people use hand based gestures to communicate. Unfortunately, the vast majority of the people are not aware of the semantics of these gestures. In a attempt to bridge the same, we propose a real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera. Given that there is no one to one mapping between the pixels of the depth and the RGB camera, we used computer vision techniques like 3D contruction and affine transformation. After achieving one to one mapping, segmentation of the hand gestures was done from the background noise. Convolutional Neural Networks (CNNs) were utilised for training 36 static gestures relating to Indian Sign Language (ISL) alphabets and numbers. The model achieved an accuracy of 98.81\% on training using 45,000 RGB images and 45,000 depth images. Further Convolutional LSTMs were used for training 10 ISL dynamic word gestures and an accuracy of 99.08\% was obtained by training 1080 videos. The model showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures. The model also showed competitive adaptability to American Sign Language (ASL) gestures when the ISL models weights were transfer learned to ASL and it resulted in giving 97.71\% accuracy.},
	urldate = {2023-06-22},
	journal = {2019 Digital Image Computing: Techniques and Applications (DICTA)},
	author = {Bhagat, Neel Kamal and Vishnusai, Y. and Rathna, G. N.},
	month = dec,
	year = {2019},
	note = {26 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 Digital Image Computing: Techniques and Applications (DICTA)
ISBN: 9781728138572
Place: Perth, Australia
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {1--8},
	annote = {[TLDR] A real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera, which showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures.},
}

@article{sharma_asl-3dcnn_2021,
	title = {{ASL}-{3DCNN}: {American} sign language recognition technique using 3-{D} convolutional neural networks},
	volume = {80},
	issn = {1380-7501, 1573-7721},
	shorttitle = {{ASL}-{3DCNN}},
	url = {https://link.springer.com/10.1007/s11042-021-10768-5},
	doi = {10.1007/s11042-021-10768-5},
	abstract = {The communication between a person from the impaired community with a person who does not understand sign language could be a tedious task. Sign language is the art of conveying messages using hand gestures. Recognition of dynamic hand gestures in American Sign Language (ASL) became a very important challenge that is still unresolved. In order to resolve the challenges of dynamic ASL recognition, a more advanced successor of the Convolutional Neural Networks (CNNs) called 3-D CNNs is employed, which can recognize the patterns in volumetric data like videos. The CNN is trained for classification of 100 words on Boston ASL (Lexicon Video Dataset) LVD dataset with more than 3300 English words signed by 6 different signers. 70\% of the dataset is used for Training while the remaining 30\% dataset is used for testing the model. The proposed work outperforms the existing state-of-art models in terms of precision (3.7\%), recall (4.3\%), and f-measure (3.9\%). The computing time (0.19 seconds per frame) of the proposed work shows that the proposal may be used in real-time applications.},
	language = {en},
	number = {17},
	urldate = {2023-06-22},
	journal = {Multimedia Tools and Applications},
	author = {Sharma, Shikhar and Kumar, Krishan},
	month = jul,
	year = {2021},
	note = {44 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {26319--26331},
	annote = {[TLDR] A more advanced successor of the Convolutional Neural Networks (CNNs) called 3-D CNNs is employed, which can recognize the patterns in volumetric data like videos like videos, which outperforms the existing state-of-art models in terms of precision, recall, and f-measure.},
}

@article{cheng_survey_2016,
	title = {Survey on {3D} {Hand} {Gesture} {Recognition}},
	volume = {26},
	issn = {1051-8215, 1558-2205},
	url = {http://ieeexplore.ieee.org/document/7208833/},
	doi = {10.1109/TCSVT.2015.2469551},
	abstract = {Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners.},
	number = {9},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Cheng, Hong and Yang, Lu and Liu, Zicheng},
	month = sep,
	year = {2016},
	note = {265 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {1659--1673},
	annote = {[TLDR] This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors, and reviews the commercial depth sensors and public data sets that are widely used in this field.},
}

@article{harshith_survey_2010,
	title = {Survey on {Various} {Gesture} {Recognition} {Techniques} for {Interfacing} {Machines} {Based} on {Ambient} {Intelligence}},
	volume = {1},
	issn = {09763252},
	url = {http://www.airccse.org/journal/ijcses/papers/1110ijcses03.pdf},
	doi = {10.5121/ijcses.2010.1203},
	abstract = {Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions. Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods, appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.},
	number = {2},
	urldate = {2023-06-22},
	journal = {International Journal of Computer Science \& Engineering Survey},
	author = {Harshith, C and Shastry, Karthik.R. and Ravindran, Manoj and Srikanth, M.V.V.N.S and Lakshmikhanth, Naveen},
	month = nov,
	year = {2010},
	note = {53 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {31--42},
	annote = {[TLDR] The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/UDQVJPM7/Harshith et al. - 2010 - Survey on Various Gesture Recognition Techniques f.pdf:application/pdf},
}

@article{cai_rgb-d_2017,
	title = {{RGB}-{D} datasets using microsoft kinect or similar sensors: a survey},
	volume = {76},
	issn = {1380-7501, 1573-7721},
	shorttitle = {{RGB}-{D} datasets using microsoft kinect or similar sensors},
	url = {http://link.springer.com/10.1007/s11042-016-3374-6},
	doi = {10.1007/s11042-016-3374-6},
	abstract = {RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems. It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color, illumination, rotation angle and scale. With the invention of the low-cost Microsoft Kinect sensor, which was initially used for gaming and later became a popular device for computer vision, high quality RGB-D data can be acquired easily. In recent years, more and more RGB-D image/video datasets dedicated to various applications have become available, which are of great importance to benchmark the state-of-the-art. In this paper, we systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation. We provide the insights into the characteristics of each important dataset, and compare the popularity and the difficulty of those datasets. Overall, the main goal of this survey is to give a comprehensive description about the available RGB-D datasets and thus to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Multimedia Tools and Applications},
	author = {Cai, Ziyun and Han, Jungong and Liu, Li and Shao, Ling},
	month = feb,
	year = {2017},
	note = {115 Citations},
	keywords = {hardware:kinect, tech:rgbd, read-priority-5, type:survey},
	pages = {4313--4355},
	annote = {[TLDR] This paper systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
	annote = {[TLDR] This paper systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
	file = {Full Text:/Users/brk/Zotero/storage/Q3HIZYJF/Cai et al. - 2017 - RGB-D datasets using microsoft kinect or similar s.pdf:application/pdf},
}

@article{al-qaness_wiger_2016,
	title = {{WiGeR}: {WiFi}-{Based} {Gesture} {Recognition} {System}},
	volume = {5},
	issn = {2220-9964},
	shorttitle = {{WiGeR}},
	url = {http://www.mdpi.com/2220-9964/5/6/92},
	doi = {10.3390/ijgi5060092},
	abstract = {Recently, researchers around the world have been striving to develop and modernize human–computer interaction systems by exploiting advances in modern communication systems. The priority in this field involves exploiting radio signals so human–computer interaction will require neither special devices nor vision-based technology. In this context, hand gesture recognition is one of the most important issues in human–computer interfaces. In this paper, we present a novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions. We extract CSI from any common WiFi router and then filter out the noise to obtain the CSI fluctuation trends generated by hand motions. We design a novel and agile segmentation and windowing algorithm based on wavelet analysis and short-time energy to reveal the specific pattern associated with each hand gesture and detect duration of the hand motion. Furthermore, we design a fast dynamic time warping algorithm to classify our system’s proposed hand gestures. We implement and test our system through experiments involving various scenarios. The results show that WiGeR can classify gestures with high accuracy, even in scenarios where the signal passes through multiple walls.},
	language = {en},
	number = {6},
	urldate = {2023-06-22},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Al-qaness, Mohammed and Li, Fangmin},
	month = jun,
	year = {2016},
	note = {84 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:wifi, model:dtw, movement:static, movement:dynamic, classes:10-29},
	pages = {92},
	annote = {[TLDR] A novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions to classify gestures with high accuracy.},
	annote = {[TLDR] A novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions to classify gestures with high accuracy.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/DV7B98FA/Al-qaness and Li - 2016 - WiGeR WiFi-Based Gesture Recognition System.pdf:application/pdf},
}

@article{he_wig_2015,
	title = {{WiG}: {WiFi}-{Based} {Gesture} {Recognition} {System}},
	shorttitle = {{WiG}},
	url = {http://ieeexplore.ieee.org/document/7288485/},
	doi = {10.1109/ICCCN.2015.7288485},
	abstract = {Most recently, gesture recognition has increasingly attracted intense academic and industrial interest due to its various applications in daily life, such as home automation, mobile games. Present approaches for gesture recognition, mainly including vision-based, sensor-based and RF-based, all have certain limitations which hinder their practical use in some scenarios. For example, the vision-based approaches fail to work well in poor light conditions and the sensor-based ones require users to wear devices. To address these, we propose WiG in this paper, a device-free gesture recognition system based solely on Commercial Off-The-Shelf (COTS) WiFi infrastructures and devices. Compared with existing Radio Frequency (RF)-based systems, WiG stands out for its systematic simplicity, extremely low cost and high practicability. We implemented WiG in indoor environment and conducted experiments to evaluate its performance in two typical scenarios. The results demonstrate that WiG can achieve an average recognition accuracy of 92\% in line-of-sight scenario and average accuracy of 88\% in the none-line-of sight scenario.},
	urldate = {2023-06-22},
	journal = {2015 24th International Conference on Computer Communication and Networks (ICCCN)},
	author = {He, Wenfeng and Wu, Kaishun and Zou, Yongpan and Ming, Zhong},
	month = aug,
	year = {2015},
	note = {132 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 24th International Conference on Computer Communication and Networks (ICCCN)
ISBN: 9781479999644
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {tech:wifi},
	pages = {1--7},
	annote = {[TLDR] WiG is proposed, a device-free gesture recognition system based solely on Commercial Off-The-Shelf (COTS) WiFi infrastructures and devices that stands out for its systematic simplicity, extremely low cost and high practicability.},
}

@article{hakim_dynamic_2019,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Using} {3DCNN} and {LSTM} with {FSM} {Context}-{Aware} {Model}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/24/5429},
	doi = {10.3390/s19245429},
	abstract = {With the recent growth of Smart TV technology, the demand for unique and beneficial applications motivates the study of a unique gesture-based system for a smart TV-like environment. Combining movie recommendation, social media platform, call a friend application, weather updates, chatting app, and tourism platform into a single system regulated by natural-like gesture controller is proposed to allow the ease of use and natural interaction. Gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment. Dataset of a sequence of RGB and depth images were collected, preprocessed, and trained in the proposed deep learning architecture. Combination of three-dimensional Convolutional Neural Network (3DCNN) followed by Long Short-Term Memory (LSTM) model was used to extract the spatio-temporal features. At the end of the classification, Finite State Machine (FSM) communicates the model to control the class decision results based on application context. The result suggested the combination data of depth and RGB to hold 97.8\% of accuracy rate on eight selected gestures, while the FSM has improved the recognition rate from 89\% to 91\% in a real-time performance.},
	language = {en},
	number = {24},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {Hakim, Noorkholis Luthfil and Shih, Timothy K. and Kasthuri Arachchi, Sandeli Priyanwada and Aditya, Wisnu and Chen, Yi-Cheng and Lin, Chih-Yang},
	month = dec,
	year = {2019},
	note = {32 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgbd, movement:static, movement:dynamic, model:lstm, classes:10-29, model:fsm},
	pages = {5429},
	annote = {[TLDR] G gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment, and the Finite State Machine (FSM) has improved the recognition rate from 89\% to 91\% in a real-time performance.},
	annote = {[TLDR] G gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment, and the Finite State Machine (FSM) has improved the recognition rate from 89\% to 91\% in a real-time performance.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/W6XRMHDD/Hakim et al. - 2019 - Dynamic Hand Gesture Recognition Using 3DCNN and L.pdf:application/pdf},
}

@article{zhao_multi-feature_2016,
	title = {Multi-feature gesture recognition based on {Kinect}},
	url = {http://ieeexplore.ieee.org/document/7574856/},
	doi = {10.1109/CYBER.2016.7574856},
	abstract = {Human Computer Interaction (HCI) has been a popular research area during the last few years. Compared with the tradition HCI methods such as using a keyboard or mouse, people prefer to have their tasks done in a more natural way. As an essential form of non-verbal communication in daily life, gesture is a good choice to turn the ideas into reality. Although various recognition methods are proposed to solve the problem, these methods are time-tensed, space-tensed or miscellaneous. This paper introduced a new method to recognize the hand gesture correctly and efficiently. The recognition is done through two phases: the skeleton phase concerning capturing and processing skeleton feature of the hand gesture, and the hand phase focusing on extracting hand contour feature of the hand gesture. Experimental results confirm an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
	urldate = {2023-06-22},
	journal = {2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
	author = {Zhao, Yue and Liu, Yunda and Dong, Min and Bi, Sheng},
	month = jun,
	year = {2016},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)
ISBN: 9781509027330
Place: Chengdu, China
Publisher: IEEE},
	keywords = {hardware:kinect, tech:rgbd},
	pages = {392--396},
	annote = {[TLDR] A new method to recognize the hand gesture correctly and efficiently is introduced, with an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
	annote = {[TLDR] A new method to recognize the hand gesture correctly and efficiently is introduced, with an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
}

@article{koch_recurrent_2019,
	title = {A {Recurrent} {Neural} {Network} for {Hand} {Gesture} {Recognition} based on {Accelerometer} {Data}},
	url = {https://ieeexplore.ieee.org/document/8856844/},
	doi = {10.1109/EMBC.2019.8856844},
	abstract = {For many applications, hand gesture recognition systems that rely on biosignal data exclusively are mandatory. Usually, theses systems have to be affordable, reliable as well as mobile. The hand is moved due to muscle contractions that cause motions of the forearm skin. Theses motions can be captured with cheap and reliable accelerometers placed around the forearm. Since accelerometers can also be integrated into mobile systems easily, the possibility of a robust hand gesture recognition based on accelerometer signals is evaluated in this work. For this, a neural network architecture consisting of two different kinds of recurrent neural network (RNN) cells is proposed. Experiments on three databases reveal that this relatively small network outperforms by far state-of-the-art hand gesture recognition approaches that rely on multi-modal data. The combination of accelerometer data and an RNN forms a robust hand gesture classification system, i.e., the performance of the network does not vary a lot between subjects and it is outstanding for amputees. Furthermore, the proposed network uses only 5 ms short windows to classify the hand gestures. Consequently, this approach allows for a quick, and potentially delay-free hand gesture detection.},
	urldate = {2023-06-22},
	journal = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	author = {Koch, Philipp and Dreier, Mark and Maass, Marco and Bohme, Martina and Phan, Huy and Mertins, Alfred},
	month = jul,
	year = {2019},
	note = {16 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 41st Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC)
ISBN: 9781538613115
Place: Berlin, Germany
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:emg, model:rnn},
	pages = {5088--5091},
	annote = {[TLDR] A neural network architecture consisting of two different kinds of recurrent neural network cells is proposed that outperforms by far state-of-the-art hand gesture recognition approaches that rely on multi-modal data and forms a robust hand gesture classification system.},
	file = {Accepted Version:/Users/brk/Zotero/storage/ZF224WN7/Koch et al. - 2019 - A Recurrent Neural Network for Hand Gesture Recogn.pdf:application/pdf},
}

@article{zhang_real-time_2019,
	title = {Real-{Time} {Surface} {EMG} {Pattern} {Recognition} for {Hand} {Gestures} {Based} on an {Artificial} {Neural} {Network}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/14/3170},
	doi = {10.3390/s19143170},
	abstract = {In recent years, surface electromyography (sEMG) signals have been increasingly used in pattern recognition and rehabilitation. In this paper, a real-time hand gesture recognition model using sEMG is proposed. We use an armband to acquire sEMG signals and apply a sliding window approach to segment the data in extracting features. A feedforward artificial neural network (ANN) is founded and trained by the training dataset. A test method is used in which the gesture will be recognized when recognized label times reach the threshold of activation times by the ANN classifier. In the experiment, we collected real sEMG data from twelve subjects and used a set of five gestures from each subject to evaluate our model, with an average recognition rate of 98.7\% and an average response time of 227.76 ms, which is only one-third of the gesture time. Therefore, the pattern recognition system might be able to recognize a gesture before the gesture is completed.},
	language = {en},
	number = {14},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {{Zhang} and {Yang} and {Qian} and {Zhang}},
	month = jul,
	year = {2019},
	note = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, model:ffnn, classes:{\textless}10},
	pages = {3170},
	annote = {[TLDR] A real-time hand gesture recognition model using sEMG is proposed that might be able to recognize a gesture before the gesture is completed, and a feedforward artificial neural network (ANN) is founded and trained by the training dataset.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/NV2YLHJB/Zhang et al. - 2019 - Real-Time Surface EMG Pattern Recognition for Hand.pdf:application/pdf},
}

@article{wan_explore_2016,
	title = {Explore {Efficient} {Local} {Features} from {RGB}-{D} {Data} for {One}-{Shot} {Learning} {Gesture} {Recognition}},
	volume = {38},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/7368923/},
	doi = {10.1109/TPAMI.2015.2513479},
	abstract = {Availability of handy RGB-D sensors has brought about a surge of gesture recognition research and applications. Among various approaches, one shot learning approach is advantageous because it requires minimum amount of data. Here, we provide a thorough review about one-shot learning gesture recognition from RGB-D data and propose a novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK). In the review, we analyze the challenges that we are facing, and point out some future research directions which may enlighten researchers in this field. The proposed MFSK feature is robust and invariant to scale, rotation and partial occlusions. To alleviate the insufficiency of one shot training samples, we augment the training samples by artificially synthesizing versions of various temporal scales, which is beneficial for coping with gestures performed at varying speed. We evaluate the proposed method on the Chalearn gesture dataset (CGD). The results show that our approach outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets. When applied to the RGB-D datasets that are not one-shot (e.g., the Cornell Activity Dataset-60 and MSR Daily Activity 3D dataset), the proposed feature also produces very promising results under leave-one-out cross validation or one-shot learning.},
	number = {8},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wan, Jun and Guo, Guodong and Li, Stan Z.},
	month = aug,
	year = {2016},
	note = {93 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgbd, app:robot-control, app:surgery, app:medical, dataset:chalearn-gesture, dataset:cornell-activity-60, dataset:msr-daily-activity-3d},
	pages = {1626--1639},
	annote = {[TLDR] A novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK) is proposed, which outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets.},
}

@article{mohammed_deep_2019,
	title = {A {Deep} {Learning}-{Based} {End}-to-{End} {Composite} {System} for {Hand} {Detection} and {Gesture} {Recognition}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/23/5282},
	doi = {10.3390/s19235282},
	abstract = {Recent research on hand detection and gesture recognition has attracted increasing interest due to its broad range of potential applications, such as human-computer interaction, sign language recognition, hand action analysis, driver hand behavior monitoring, and virtual reality. In recent years, several approaches have been proposed with the aim of developing a robust algorithm which functions in complex and cluttered environments. Although several researchers have addressed this challenging problem, a robust system is still elusive. Therefore, we propose a deep learning-based architecture to jointly detect and classify hand gestures. In the proposed architecture, the whole image is passed through a one-stage dense object detector to extract hand regions, which, in turn, pass through a lightweight convolutional neural network (CNN) for hand gesture recognition. To evaluate our approach, we conducted extensive experiments on four publicly available datasets for hand detection, including the Oxford, 5-signers, EgoHands, and Indian classical dance (ICD) datasets, along with two hand gesture datasets with different gesture vocabularies for hand gesture recognition, namely, the LaRED and TinyHands datasets. Here, experimental results demonstrate that the proposed architecture is efficient and robust. In addition, it outperforms other approaches in both the hand detection and gesture classification tasks.},
	language = {en},
	number = {23},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {Mohammed, Adam Ahmed Qaid and Lv, Jiancheng and Islam, Md. Sajjatul},
	month = nov,
	year = {2019},
	note = {34 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb},
	pages = {5282},
	annote = {[TLDR] This work proposes a deep learning-based architecture to jointly detect and classify hand gestures and demonstrates that the proposed architecture is efficient and robust, and outperforms other approaches in both the hand detection and gesture classification tasks.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/V5WL63KJ/Mohammed et al. - 2019 - A Deep Learning-Based End-to-End Composite System .pdf:application/pdf},
}

@article{kundu_hand_2018,
	title = {Hand {Gesture} {Recognition} {Based} {Omnidirectional} {Wheelchair} {Control} {Using} {IMU} and {EMG} {Sensors}},
	volume = {91},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-017-0725-0},
	doi = {10.1007/s10846-017-0725-0},
	abstract = {This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors. Seven common gestures are recognized and classified using shape based feature extraction and Dendogram Support Vector Machine (DSVM) classifier. The dynamic gestures are mapped to the omnidirectional motion commands to navigate the wheelchair. A single IMU is used to measure the wrist tilt angle and acceleration in three axis. EMG signals are extracted from two forearm muscles namely Extensor Carpi Radialis and Flexor Carpi Radialis and processed to provide Root Mean Square (RMS) signal. Initiation and termination of dynamic activities are based on autonomous identification of static to dynamic or dynamic to static transition by setting static thresholds on processed IMU and myoelectric sensor data. Classification involves recognizing the activity pattern based on periodic shape of trajectories of the triaxial wrist tilt angle and EMG-RMS from the two selected muscles. Second order Polynomial coefficients extracted from the sensor trajectory templates during specific dynamic activity cycles are used as features to classify dynamic activities. Classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested by five healthy subjects. Classification accuracy of 94\% was achieved by DSVM classifier on ‘k’ fold cross validation data of 5 users. Classification accuracy while operating the wheelchair was 90.5\%.},
	language = {en},
	number = {3-4},
	urldate = {2023-06-22},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis},
	month = sep,
	year = {2018},
	note = {70 Citations},
	keywords = {tech:accelerometer, model:svm, tech:emg},
	pages = {529--541},
	annote = {[TLDR] This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors and classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested.},
	annote = {[TLDR] This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors and classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested.},
}

@article{ma_hand_2017,
	title = {Hand gesture recognition with convolutional neural networks for the multimodal {UAV} control},
	url = {http://ieeexplore.ieee.org/document/8101666/},
	doi = {10.1109/RED-UAS.2017.8101666},
	abstract = {We introduce a robust wearable sensor suite fusing arm motion and hand gesture recognition for operator control of UAVs. The sensor suite fuses mechanomyography (MMG) and an inertial measurement unit (IMU) to capture multi-modal (arm movement and hand gesture) command signals simultaneously. The IMU produces world referenced orientation and acceleration data while concomitant MMG tracks muscle activation through surface vibration. The use of surface muscle vibration for gesture recognition removes the need for electrical contact with the skin, which has impeded other forms of muscle measurement for gesture recognition in the field. This investigation presents hardware design, inertial recognition of arm movement, and the detailed structure of a convolutional neural network (CNN) system used for real-time hand gesture recognition based on MMG signals. The system achieved 94\% accuracy for five gestures with simple calibration for each user, thereby providing an intuitive gesture based UAV control system. To our knowledge this is the first wearable system enabling multimodal control of UAVs through intuitive gestures that does not require electrical skin contact. Future work involves testing the system with larger UAV swarms.},
	urldate = {2023-06-22},
	journal = {2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)},
	author = {Ma, Yuntao and Liu, Yuxuan and Jin, Ruiyang and Yuan, Xingyang and Sekha, Raza and Wilson, Samuel and Vaidyanathan, Ravi},
	month = oct,
	year = {2017},
	note = {35 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)
ISBN: 9781538609392
Place: Linkoping
Publisher: IEEE},
	keywords = {model:cnn, app:robot-control, classes:{\textless}10, tech:mechanomyography, tech:imu},
	pages = {198--203},
	annote = {[TLDR] This investigation presents hardware design, inertial recognition of arm movement, and the detailed structure of a convolutional neural network system used for real-time hand gesture recognition based on MMG signals, which achieved 94\% accuracy for five gestures with simple calibration for each user.},
}

@article{aly_survey_2005,
	title = {Survey on multiclass classification methods},
	volume = {19},
	number = {1-9},
	journal = {Neural Netw},
	author = {Aly, Mohamed},
	year = {2005},
	note = {Publisher: Citeseer},
	keywords = {background},
	pages = {2},
}

@inproceedings{suarez_hand_2012,
	title = {Hand gesture recognition with depth images: {A} review},
	shorttitle = {Hand gesture recognition with depth images},
	doi = {10.1109/ROMAN.2012.6343787},
	abstract = {This paper presents a literature review on the use of depth for hand tracking and gesture recognition. The survey examines 37 papers describing depth-based gesture recognition systems in terms of (1) the hand localization and gesture classification methods developed and used, (2) the applications where gesture recognition has been tested, and (3) the effects of the low-cost Kinect and OpenNI software libraries on gesture recognition research. The survey is organized around a novel model of the hand gesture recognition process. In the reviewed literature, 13 methods were found for hand localization and 11 were found for gesture classification. 24 of the papers included real-world applications to test a gesture recognition system, but only 8 application categories were found (and three applications accounted for 18 of the papers). The papers that use the Kinect and the OpenNI libraries for hand tracking tend to focus more on applications than on localization and classification methods, and show that the OpenNI hand tracking method is good enough for the applications tested thus far. However, the limitations of the Kinect and other depth sensors for gesture recognition have yet to be tested in challenging applications and environments.},
	booktitle = {2012 {IEEE} {RO}-{MAN}: {The} 21st {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Suarez, Jesus and Murphy, Robin R.},
	month = sep,
	year = {2012},
	note = {364 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1944-9437},
	keywords = {tech:rgb, tech:rgbd, type:survey},
	pages = {411--417},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/EYLG8BUK/6343787.html:text/html},
}

@article{oudah_hand_2020,
	title = {Hand {Gesture} {Recognition} {Based} on {Computer} {Vision}: {A} {Review} of {Techniques}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Hand {Gesture} {Recognition} {Based} on {Computer} {Vision}},
	url = {https://www.mdpi.com/2313-433X/6/8/73},
	doi = {10.3390/jimaging6080073},
	abstract = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human–computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Journal of Imaging},
	author = {Oudah, Munir and Al-Naji, Ali and Chahl, Javaan},
	month = aug,
	year = {2020},
	note = {178 citations (Semantic Scholar/DOI) [2023-07-02]
Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {tech:rgb, type:survey},
	pages = {73},
	file = {Full Text PDF:/Users/brk/Zotero/storage/WREJRICA/Oudah et al. - 2020 - Hand Gesture Recognition Based on Computer Vision.pdf:application/pdf},
}

@inproceedings{fang_real-time_2007,
	title = {A {Real}-{Time} {Hand} {Gesture} {Recognition} {Method}},
	doi = {10.1109/ICME.2007.4284820},
	abstract = {Compared with the traditional interaction approaches, such as keyboard, mouse, pen, etc, vision based hand interaction is more natural and efficient. In this paper, we proposed a robust real-time hand gesture recognition method. In our method, firstly, a specific gesture is required to trigger the hand detection followed by tracking; then hand is segmented using motion and color cues; finally, in order to break the limitation of aspect ratio encountered in most of learning based hand gesture methods, the scale-space feature detection is integrated into gesture recognition. Applying the proposed method to navigation of image browsing, experimental results show that our method achieves satisfactory performance.},
	booktitle = {2007 {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Fang, Yikai and Wang, Kongqiao and Cheng, Jian and Lu, Hanqing},
	month = jul,
	year = {2007},
	note = {227 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1945-788X},
	keywords = {tech:rgb, tech:rgbd},
	pages = {995--998},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/4T92MUB7/4284820.html:text/html},
}

@inproceedings{liu_hand_2004,
	title = {Hand gesture recognition using depth data},
	doi = {10.1109/AFGR.2004.1301587},
	abstract = {A method is presented for recognizing hand gestures by using a sequence of real-time depth image data acquired by an active sensing hardware. Hand posture and motion information extracted from a video is represented in a gesture space which consists of a number of aspects including hand shape, location and motion information. In this space, it is shown to be possible to recognize many types of gestures. Experimental results are shown to validate our approach and characteristics of our approach are discussed.},
	booktitle = {Sixth {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition}, 2004. {Proceedings}.},
	author = {Liu, Xia and Fujimura, K.},
	month = may,
	year = {2004},
	note = {243 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, tech:rgbd},
	pages = {529--534},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/R7WNTKII/1301587.html:text/html},
}

@article{chen_hand_2003,
	title = {Hand gesture recognition using a real-time tracking method and hidden {Markov} models},
	volume = {21},
	issn = {0262-8856},
	url = {https://www.sciencedirect.com/science/article/pii/S0262885603000702},
	doi = {10.1016/S0262-8856(03)00070-2},
	abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90\%.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Image and Vision Computing},
	author = {Chen, Feng-Sheng and Fu, Chih-Ming and Huang, Chung-Lin},
	month = aug,
	year = {2003},
	note = {528 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm},
	pages = {745--758},
	file = {ScienceDirect Full Text PDF:/Users/brk/Zotero/storage/7FR9BDWF/Chen et al. - 2003 - Hand gesture recognition using a real-time trackin.pdf:application/pdf;ScienceDirect Snapshot:/Users/brk/Zotero/storage/9ZPXDS8C/S0262885603000702.html:text/html},
}

@article{baudel_charade_1993,
	title = {Charade: remote control of objects using free-hand gestures},
	volume = {36},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Charade},
	url = {https://dl.acm.org/doi/10.1145/159544.159562},
	doi = {10.1145/159544.159562},
	abstract = {This paper presents an application that uses hand gesture input to control a computer while giving a presentation. In order to develop a prototype of this application, we have defined an interaction model, a notation for gestures, and a set of guidelines to design gestural command sets. This works aims to define interaction styles that work in computerized reality environments. In our application, gestures are used for interacting with the computer as well as for communicating with other people or operating other devices.},
	language = {en},
	number = {7},
	urldate = {2023-06-23},
	journal = {Communications of the ACM},
	author = {Baudel, Thomas and Beaudouin-Lafon, Michel},
	month = jul,
	year = {1993},
	note = {582 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, tech:flex, hardware:charade, hardware:dataglove},
	pages = {28--35},
	annote = {[TLDR] An application that uses hand gesture input to control a computer while giving a presentation and an interaction model, a notation for gestures, and a set of guidelines to design gestural command sets are presented.},
	file = {Full Text:/Users/brk/Zotero/storage/WRSWXDSK/Baudel and Beaudouin-Lafon - 1993 - Charade remote control of objects using free-hand.pdf:application/pdf},
}

@article{sturman_survey_1994,
	title = {A survey of glove-based input},
	volume = {14},
	issn = {0272-1716, 1558-1756},
	url = {https://ieeexplore.ieee.org/document/250916/},
	doi = {10.1109/38.250916},
	abstract = {Clumsy intermediary devices constrain our interaction with computers and their applications. Glove-based input devices let us apply our manual dexterity to the task. We provide a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input. The bulk of development in glove-based input has taken place very recently, and not all of it is easily accessible in the literature. We present a cross-section of the field to date. Hand-tracking devices may use the following technologies: position tracking, optical tracking, marker systems, silhouette analysis, magnetic tracking or acoustic tracking. Actual glove technologies on the market include: Sayre glove, MIT LED glove, Digital Data Entry Glove, DataGlove, Dexterous HandMaster, Power Glove, CyberGlove and Space Glove. Various applications of glove technologies include projects into the pursuit of natural interfaces, systems for understanding signed languages, teleoperation and robotic control, computer-based puppetry, and musical performance.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	number = {1},
	urldate = {2023-06-23},
	journal = {IEEE Computer Graphics and Applications},
	author = {Sturman, D.J. and Zeltzer, D.},
	month = jan,
	year = {1994},
	note = {853 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {read-priority-1, type:seminal, type:survey},
	pages = {30--39},
	annote = {[TLDR] This work provides a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input, and presents a cross-section of the field to date.},
	annote = {[TLDR] This work provides a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input, and presents a cross-section of the field to date.},
	file = {Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf:/Users/brk/Zotero/storage/UDDK6KNT/Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf:application/pdf},
}

@article{rekimoto_gesturewrist_2001,
	title = {{GestureWrist} and {GesturePad}: unobtrusive wearable interaction devices},
	shorttitle = {{GestureWrist} and {GesturePad}},
	url = {http://ieeexplore.ieee.org/document/962092/},
	doi = {10.1109/ISWC.2001.962092},
	abstract = {In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.},
	urldate = {2023-06-23},
	journal = {Proceedings Fifth International Symposium on Wearable Computers},
	author = {Rekimoto, Jun},
	year = {2001},
	note = {450 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Fifth International Symposium on Wearable Computers
ISBN: 9780769513188
Place: Zurich, Switzerland
Publisher: IEEE Comput. Soc},
	keywords = {tech:accelerometer, tech:capacitive},
	pages = {21--27},
	annote = {@rekimoto2001 developed a watch-like device named GestureWrist which recognises human hand gestures by measuring changes in wrist shape via capacitive sensors, as well as measuring forearm movement based on acceleration sensors. It requires the start and end of each gesture to be indicated by the user, and gestures were made up of simple full arm transitions such as palm facing upwards to palm facing downwards.

},
	annote = {[TLDR] Two input devices for wearable computers, called GestureWrist and GesturePad, are introduced that allow users to interact with wearable or nearby computers by using gesture-based commands and are designed to be as unobtrusive as possible.},
	file = {Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf:/Users/brk/Zotero/storage/CAUJY4HG/Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf:application/pdf},
}

@article{pavlovic_visual_1997,
	title = {Visual interpretation of hand gestures for human-computer interaction: a review},
	volume = {19},
	issn = {01628828},
	shorttitle = {Visual interpretation of hand gestures for human-computer interaction},
	url = {http://ieeexplore.ieee.org/document/598226/},
	doi = {10.1109/34.598226},
	abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3Dmodel of the human hand or an image appearancemodel of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient “purposive” approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction},
	number = {7},
	urldate = {2023-06-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Pavlovic, Vladimir I. and Sharma, R. and Huang, T.S.},
	month = jul,
	year = {1997},
	note = {1979 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {677--695},
	annote = {[TLDR] A fraction of the recycle slurry is treated with sulphuric acid to convert at least some of the gypsum to calcium sulphate hemihydrate and the slurry comprising hemihYDrate is returned to contact the mixture of phosphate rock, phosphoric acid and recycle Gypsum slurry.},
	annote = {[TLDR] A fraction of the recycle slurry is treated with sulphuric acid to convert at least some of the gypsum to calcium sulphate hemihydrate and the slurry comprising hemihYDrate is returned to contact the mixture of phosphate rock, phosphoric acid and recycle Gypsum slurry.},
	file = {Submitted Version:/Users/brk/Zotero/storage/7SQ99MKT/Pavlovic et al. - 1997 - Visual interpretation of hand gestures for human-c.pdf:application/pdf},
}

@article{kela_accelerometer-based_2006,
	title = {Accelerometer-based gesture control for a design environment},
	volume = {10},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-005-0033-8},
	doi = {10.1007/s00779-005-0033-8},
	abstract = {Accelerometer-based gesture control is studied as a supplementary or an alternative interaction modality. Gesture commands freely trainable by the user can be used for controlling external devices with handheld wireless sensor unit. Two user studies are presented. The first study concerns finding gestures for controlling a design environment (Smart Design Studio), TV, VCR, and lighting. The results indicate that different people usually prefer different gestures for the same task, and hence it should be possible to personalise them. The second user study concerns evaluating the usefulness of the gesture modality compared to other interaction modalities for controlling a design environment. The other modalities were speech, RFID-based physical tangible objects, laser-tracked pen, and PDA stylus. The results suggest that gestures are a natural modality for certain tasks, and can augment other modalities. Gesture commands were found to be natural, especially for commands with spatial association in design environment control.},
	language = {en},
	number = {5},
	urldate = {2023-06-23},
	journal = {Personal and Ubiquitous Computing},
	author = {Kela, Juha and Korpipää, Panu and Mäntyjärvi, Jani and Kallio, Sanna and Savino, Giuseppe and Jozzo, Luca and Marca, Sergio Di},
	month = aug,
	year = {2006},
	note = {201 citations (Crossref) [2023-07-11]
333 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, from:cite.bib, read-priority-7},
	pages = {285--299},
	annote = {[TLDR] Gesture commands were found to be natural, especially for commands with spatial association in design environment control, and can augment other modalities.},
}

@article{erol_vision-based_2007,
	title = {Vision-based hand pose estimation: {A} review},
	volume = {108},
	issn = {10773142},
	shorttitle = {Vision-based hand pose estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206002281},
	doi = {10.1016/j.cviu.2006.10.012},
	abstract = {Semantic Scholar extracted view of "Vision-based hand pose estimation: A review" by A. Erol et al.},
	language = {en},
	number = {1-2},
	urldate = {2023-06-23},
	journal = {Computer Vision and Image Understanding},
	author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D. and Twombly, Xander},
	month = oct,
	year = {2007},
	note = {885 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {52--73},
	file = {Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:/Users/brk/Zotero/storage/33ZZZF27/Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:application/pdf},
}

@article{chen_survey_2020,
	title = {A {Survey} on {Hand} {Pose} {Estimation} with {Wearable} {Sensors} and {Computer}-{Vision}-{Based} {Methods}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/4/1074},
	doi = {10.3390/s20041074},
	abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
	language = {en},
	number = {4},
	urldate = {2023-06-23},
	journal = {Sensors},
	author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
	month = feb,
	year = {2020},
	note = {54 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {1074},
	annote = {

Discusses deep learning based method


Gesture recognition (classification) vs hand pose estimation (regression)


Many reviews of the literature have been done


“Especially the rapid development of convolutional neural networks”


Doesn’t actually cover gesture recognition, but only pose estimation (figuring out the exact location of all digits, instead of classifying them into discrete gestures)




“The latest CyberGlove III [26] has reached a resolution of less than 1 degree and a data rate of up to 120 records/second. The VPL glove (no longer available) and 5DT glove [27] are also classical data gloves that are based on optical flex sensors” (Chen et al., 2020, p. 5)

},
	annote = {[TLDR] An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods, including deep-learning-based methods, which are very promising in hand pose estimation.},
	annote = {[TLDR] An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods, including deep-learning-based methods, which are very promising in hand pose estimation.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/Y67RWII7/Chen et al. - 2020 - A Survey on Hand Pose Estimation with Wearable Sen.pdf:application/pdf},
}

@article{sharma_toward_1998,
	title = {Toward multimodal human-computer interface},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/664275/},
	doi = {10.1109/5.664275},
	abstract = {Recent advances in various signal processing technologies, coupled with an explosion in the available computing power, have given rise to a number of novel human-computer interaction (HCI) modalities: speech, vision-based gesture recognition, eye tracking, electroencephalograph, etc. Successful embodiment of these modalities into an interface has the potential of easing the HCI bottleneck that has become noticeable with the advances in computing and communication. It has also become increasingly evident that the difficulties encountered in the analysis and interpretation of individual sensing modalities may be overcome by integrating them into a multimodal human-computer interface. We examine several promising directions toward achieving multimodal HCI. We consider some of the emerging novel input modalities for HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level. We discuss the different computational approaches that may be applied at the different levels of modality integration. We also briefly review several demonstrated multimodal HCI systems and applications. Despite all the recent developments, it is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI. This research can benefit from many disparate fields of study that increase our understanding of the different human communication modalities and their potential role in HCI.},
	number = {5},
	urldate = {2023-06-23},
	journal = {Proceedings of the IEEE},
	author = {Sharma, R. and Pavlovic, V.I. and Huang, T.S.},
	month = may,
	year = {1998},
	note = {341 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {853--869},
	annote = {[TLDR] It is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level.},
	annote = {[TLDR] It is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level.},
}

@article{kessler_evaluation_1995,
	title = {Evaluation of the {CyberGlove} as a whole-hand input device},
	volume = {2},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/212430.212431},
	doi = {10.1145/212430.212431},
	abstract = {We present a careful evaluation of the sensory characteristics of the CyberGlove model CG1801 whole-hand input device. In particular, we conducted an experimental study that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements. Among our results, we show that hand size differences among the subjects of the study did not have a statistical effect on the accuracy of the device. We also analyzed the effect of different software calibration approaches on accuracy of the sensors.},
	language = {en},
	number = {4},
	urldate = {2023-06-23},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Kessler, G. Drew and Hodges, Larry F. and Walker, Neff},
	month = dec,
	year = {1995},
	note = {218 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {hardware:cyberglove},
	pages = {263--283},
	annote = {[TLDR] An experimental study is conducted that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements of the CyberGlove model CG1801 whole-hand input device.},
	annote = {[TLDR] An experimental study is conducted that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements of the CyberGlove model CG1801 whole-hand input device.},
}

@article{li_wifinger_2016,
	title = {{WiFinger}: talk to your smart devices with finger-grained gesture},
	shorttitle = {{WiFinger}},
	url = {https://dl.acm.org/doi/10.1145/2971648.2971738},
	doi = {10.1145/2971648.2971738},
	abstract = {In recent literatures, WiFi signals have been widely used to "sense" people's locations and activities. Researchers have exploited the characteristics of wireless signals to "hear" people's talk and "see" keystrokes by human users. Inspired by the excellent work of relevant scholars, we turn to explore the field of human-computer interaction using finger-grained gestures under WiFi environment. In this paper, we present Wi-Finger - the first solution using ubiquitous wireless signals to achieve number text input in WiFi devices. We implement a prototype of WiFinger on a commercial Wi-Fi infrastructure. Our scheme is based on the key intuition that while performing a certain gesture, the fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time series of Channel State Information (CSI) values. WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices. As the results show, WiFinger achieves up to 90.4\% average classification accuracy for recognizing 9 digits finger-grained gestures from American Sign Language (ASL), and its average accuracy for single individual number text input in desktop reaches 82.67\% within 90 digits.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
	author = {Li, Hong and Yang, Wei and Wang, Jianxin and Xu, Yang and Huang, Liusheng},
	month = sep,
	year = {2016},
	note = {230 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
ISBN: 9781450344616
Place: Heidelberg Germany
Publisher: ACM},
	keywords = {tech:wifi, app:sign-language, read-priority-3, app:american-sl},
	pages = {250--261},
	annote = {[TLDR] WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices, and achieves up to 90.4\% average classification accuracy for recognizing 9 digits finger- grained gestures from American Sign Language (ASL).},
	file = {Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf:/Users/brk/Zotero/storage/DT2Z2XQS/Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf:application/pdf},
}

@article{pu_whole-home_2013,
	title = {Whole-home gesture recognition using wireless signals},
	url = {http://dl.acm.org/citation.cfm?doid=2500423.2500436},
	doi = {10.1145/2500423.2500436},
	abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two- bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94\%.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 19th annual international conference on Mobile computing \& networking - MobiCom '13},
	author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
	year = {2013},
	note = {1010 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: the 19th annual international conference
ISBN: 9781450319997
Place: Miami, Florida, USA
Publisher: ACM Press},
	keywords = {tech:wifi, classes:{\textless}10},
	pages = {27},
	annote = {[TLDR] WiSee is presented, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures and achieves this goal without requiring instrumentation of the human body with sensing devices.},
	annote = {[TLDR] WiSee is presented, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures and achieves this goal without requiring instrumentation of the human body with sensing devices.},
	file = {Submitted Version:/Users/brk/Zotero/storage/DRDJ9SJ8/Pu et al. - 2013 - Whole-home gesture recognition using wireless sign.pdf:application/pdf},
}

@article{abdelnasser_wigest_2014,
	title = {Wigest: {A} {Ubiquitous} {Wifi}-based {Gesture} {Recognition} {System}},
	shorttitle = {Wigest},
	url = {https://www.qscience.com/content/papers/10.5339/qfarc.2014.ITPP1127},
	doi = {10.5339/qfarc.2014.ITPP1127},
	abstract = {We present WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device. Compared to related work, WiGest is unique in using standard WiFi equipment, with no modifications, and no training for gesture recognition. The system identifies different signal change primitives, from which we construct mutually independent gesture families. These families can be mapped to distinguishable application actions. We address various challenges including cleaning the noisy signals, gesture type and attributes detection, reducing false positives due to interfering humans, and adapting to changing signal polarity. We implement a proof-of-concept prototype using off-the-shelf laptops and extensively evaluate the system in both an office environment and a typical apartment with standard WiFi access points. Our results show that WiGest detects the basic primitives with an accuracy of 87.5\% using a single AP only, including through-the-wall non-line-of-sight scenarios. This accuracy increases to 96\% using three overheard APs. In addition, when evaluating the system using a multi-media player application, we achieve a classification accuracy of 96\%. This accuracy is robust to the presence of other interfering humans, highlighting WiGest's ability to enable future ubiquitous hands-free gesture-based interaction with mobile devices.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Qatar Foundation Annual Research Conference Proceedings Volume 2014 Issue 1},
	author = {Abdelnasser, Heba and Harras, Khaled and Youssef, Moustafa},
	year = {2014},
	note = {415 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Qatar Foundation Annual Research Conference Proceedings
Place: Qatar National Convention Center (QNCC), Doha, Qatar,
Publisher: Hamad bin Khalifa University Press (HBKU Press)},
	keywords = {tech:wifi},
	annote = {[TLDR] This work presents WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device, using standard WiFi equipment, with no modifications, and no training for gesture recognition.},
	file = {Submitted Version:/Users/brk/Zotero/storage/J79GHKIF/Abdelnasser et al. - 2014 - Wigest A Ubiquitous Wifi-based Gesture Recognitio.pdf:application/pdf},
}

@inproceedings{laviola_survey_1999,
	title = {A {Survey} of {Hand} {Posture} and {Gesture} {Recognition} {Techniques} and {Technology}},
	url = {https://www.semanticscholar.org/paper/A-Survey-of-Hand-Posture-and-Gesture-Recognition-LaViola/856d4bf0f1f5d4480ce3115d828f34d4b2782e1c},
	abstract = {This paper surveys the use of hand postures and gestures as a mechanism for interaction with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based interaction. First, the technological requirements and limitations for using hand postures and gestures are described by discussing both glove-based and vision-based recognition systems along with advantages and disadvantages of each. Second, the various types of techniques used in recognizing hand postures and gestures are compared and contrasted. Third, the applications that have used hand posture and gesture interfaces are examined. The survey concludes with a summary and a discussion of future research directions.},
	urldate = {2023-06-23},
	author = {LaViola, Jr Joseph J.},
	month = jun,
	year = {1999},
	note = {253 Citations},
	keywords = {read-priority-3, type:survey},
	annote = {[TLDR] This paper surveys the use of hand postures and gestures as a mechanism for interaction with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based interaction.},
	file = {LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf:/Users/brk/Zotero/storage/M8XQ3QQL/LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf:application/pdf},
}

@article{harrison_omnitouch_2011,
	title = {{OmniTouch}: wearable multitouch interaction everywhere},
	shorttitle = {{OmniTouch}},
	url = {https://dl.acm.org/doi/10.1145/2047196.2047255},
	doi = {10.1145/2047196.2047255},
	abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
	author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
	month = oct,
	year = {2011},
	note = {587 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: UIST '11: The 24th Annual ACM Symposium on User Interface Software and Technology
ISBN: 9781450307161
Place: Santa Barbara California USA
Publisher: ACM},
	keywords = {tech:rgb, hardware:unique},
	pages = {441--450},
	annote = {[TLDR] OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces and is conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
	annote = {[TLDR] OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces and is conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
}

@article{dipietro_survey_2008,
	title = {A {Survey} of {Glove}-{Based} {Systems} and {Their} {Applications}},
	volume = {38},
	issn = {1094-6977, 1558-2442},
	url = {http://ieeexplore.ieee.org/document/4539650/},
	doi = {10.1109/TSMCC.2008.923862},
	abstract = {Hand movement data acquisition is used in many engineering applications ranging from the analysis of gestures to the biomedical sciences. Glove-based systems represent one of the most important efforts aimed at acquiring hand movement data. While they have been around for over three decades, they keep attracting the interest of researchers from increasingly diverse fields. This paper surveys such glove systems and their applications. It also analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research. A foremost goal of this paper is to provide readers who are new to the area with a basis for understanding glove systems technology and how it can be applied, while offering specialists an updated picture of the breadth of applications in several engineering and biomedical sciences areas.},
	number = {4},
	urldate = {2023-06-23},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Dipietro, L. and Sabatini, A.M. and Dario, P.},
	month = jul,
	year = {2008},
	note = {652 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {461--482},
	annote = {Seven areas:


“1. design and manufacturing, 2. information visualization, 3. robotics, 4. art and entertainment, 5. sign language understanding, 6. medicine and health care, and 7. wearable and portable computers” (Dipietro et al., 2008, p. 1)


Previous review was Sturman and Zeltzer in 1994

zimmerman made the dataglove in 1982
Fingernail glove (Mascaro[12-14] at MIT) “exploited changes in coloration of the fingernail due to touching, bending/extension, and shear” (Dipietro et al., 2008, p. 3)
“Artificial neural networks (ANNs) have been used for both (static) postural classification [48], [69], [79], [80] and gesture classification [57], [81], [83], [84]” (Dipietro et al., 2008, p. 16)
“Metrics such as number of characters the user can input per minute, number of discrete keys, and time interval for key press are commonly accepted and usually reported by researchers and manufacturers; this is helpful when comparing new devices and traditional ones, e.g., QWERTY keyboards [131].” (Dipietro et al., 2008, p. 17)
},
	annote = {[TLDR] This paper surveys glove systems and their applications, analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research.},
	file = {Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf:/Users/brk/Zotero/storage/RWUPUG9F/Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf:application/pdf},
}

@inproceedings{madadi_occlusion_2017,
	title = {Occlusion {Aware} {Hand} {Pose} {Recovery} from {Sequences} of {Depth} {Images}},
	doi = {10.1109/FG.2017.37},
	abstract = {State-of-the-art approaches on hand pose estimation from depth images have reported promising results under quite controlled considerations. In this paper we propose a two-step pipeline for recovering the hand pose from a sequence of depth images. The pipeline has been designed to deal with images taken from any viewpoint and exhibiting a high degree of finger occlusion. In a first step we initialize the hand pose using a part-based model, fitting a set of hand components in the depth images. In a second step we consider temporal data and estimate the parameters of a trained bilinear model consisting of shape and trajectory bases. Results on a synthetic, highly-occluded dataset demonstrate that the proposed method outperforms most recent pose recovering approaches, including those based on CNNs.},
	booktitle = {2017 12th {IEEE} {International} {Conference} on {Automatic} {Face} \& {Gesture} {Recognition} ({FG} 2017)},
	author = {Madadi, Meysam and Escalera, Sergio and Carruesco, Alex and Andujar, Carlos and Baró, Xavier and Gonzàlez, Jordi},
	month = may,
	year = {2017},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, tech:rgbd},
	pages = {230--237},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/MKR4QCS5/7961746.html:text/html;Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf:/Users/brk/Zotero/storage/Y54S29Q8/Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf:application/pdf},
}

@article{guyon_chalearn_2014,
	title = {The {ChaLearn} gesture dataset ({CGD} 2011)},
	volume = {25},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-014-0596-3},
	doi = {10.1007/s00138-014-0596-3},
	abstract = {This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shot-learning. The data include 54,000 hand and arm gestures recorded with an RGB-D \$\${\textbackslash}hbox \{Kinect\}{\textasciicircum}{\textbackslash}mathrm\{TM\}\$\$camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8–12 gestures, recorded by the same user. Short continuous sequences of 1–5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user’s horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, datacollection software and the gesture vocabularies are downloadable from http://gesture.chalearn.org. We set up a forum for researchers working on these data http://groups.google.com/group/gesturechallenge.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Machine Vision and Applications},
	author = {Guyon, Isabelle and Athitsos, Vassilis and Jangyodsuk, Pat and Escalante, Hugo Jair},
	month = nov,
	year = {2014},
	note = {70 Citations},
	keywords = {type:dataset},
	pages = {1929--1951},
}

@inproceedings{chai_two_2016,
	title = {Two streams {Recurrent} {Neural} {Networks} for {Large}-{Scale} {Continuous} {Gesture} {Recognition}},
	doi = {10.1109/ICPR.2016.7899603},
	abstract = {In this paper, we tackle the continuous gesture recognition problem with a two streams Recurrent Neural Networks (2S-RNN) for the RGB-D data input. In our framework, the spotting-recognition strategy is used, that means the continuous gestures are first segmented into separated gestures, and then each isolated gesture is recognized by using the 2S-RNN. Concretely, the gesture segmentation is based on the accurate hand positions provided by the hand detector trained from Faster R-CNN. While in the recognition module, 2S-RNN is designed to efficiently fuse multi-modal features, i.e. the RGB and depth channels. The experimental results on both the validation and test sets of the Continuous Gesture Dataset (ConGD) have shown promising performance of the proposed framework. We ranked 1st in the ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge with the mean Jaccard Index of 0.286915.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Chai, Xiujuan and Liu, Zhipeng and Yin, Fang and Liu, Zhuang and Chen, Xilin},
	month = dec,
	year = {2016},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgbd, model:rnn, dataset:continuous-gesture-dataset, dataset:chalearn-lap},
	pages = {31--36},
}

@article{rautaray_vision_2015,
	title = {Vision based hand gesture recognition for human computer interaction: a survey},
	volume = {43},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Vision based hand gesture recognition for human computer interaction},
	url = {http://link.springer.com/10.1007/s10462-012-9356-9},
	doi = {10.1007/s10462-012-9356-9},
	abstract = {As computers become more pervasive in society, facilitating natural human–computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human–computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {Artificial Intelligence Review},
	author = {Rautaray, Siddharth S. and Agrawal, Anupam},
	month = jan,
	year = {2015},
	note = {1303 Citations},
	keywords = {have-read, type:survey},
	pages = {1--54},
	annote = {

This is a survey of existing surveys(!?)


Only covers vision based techniques


This seems to have lots of bogus claims(?!) like



“The main disadvantage of contact based devices is the health hazards (Schultz et al. 2003),” (Rautaray and Agrawal, 2015, p. 6)




“magnetic devices which raises risk of cancer etc (Nishikawa et al. 2003)” (Rautaray and Agrawal, 2015, p. 6)





},
	annote = {[TLDR] An analysis of comparative surveys done in the field of gesture based HCI and an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters are provided.},
	file = {Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf:/Users/brk/Zotero/storage/78UXQTNQ/Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf:application/pdf},
}

@article{al-shamayleh_systematic_2018,
	title = {A systematic literature review on vision based gesture recognition techniques},
	volume = {77},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-5971-z},
	doi = {10.1007/s11042-018-5971-z},
	abstract = {Human Computer Interaction (HCI) technologies are rapidly evolving the way we interact with computing devices and adapting to the constantly increasing demands of modern paradigms. One of the most useful tools in this regard is the integration of Human-to-Human Interaction gestures to facilitate communication and expressing ideas. Gesture recognition requires the integration of postures, gestures, face expressions and movements for communicating or conveying certain messages. The aim of this study is to aggregate and synthesize experiences and accumulated knowledge about Vision-Based Recognition (VBR) techniques. The major objective of conducting this Systematic Literature Review (SLR) is to highlight the state-of-the-art in the context of vision-based gesture recognition with specific focus on hand gesture recognition (HGR) techniques and enabling technologies. After a careful systematic selection process, 100 studies relevant to the four research questions were selected. This process was followed by data collection, a detailed analysis, and a synthesis of the selected studies. The results reveal that among the VBR techniques, HGR is a predominant and highly focused area of research. Research focus is also found to be converging towards sign language recognition. Potential applications of HGR techniques include desktop applications, smart environments, entertainment, sign language interpretation, virtual reality and gamification. Although various experimental research efforts have been devoted to gestures recognition, there are still numerous open issues and research challenges in this field. Lastly, considering the results from this SLR, potential future research directions are suggested, including a much needed focus on grammatical interpretation, hybrid approaches, smartphone devices, normalization, and real-life systems.},
	language = {en},
	number = {21},
	urldate = {2023-06-26},
	journal = {Multimedia Tools and Applications},
	author = {Al-Shamayleh, Ahmad Sami and Ahmad, Rodina and Abushariah, Mohammad A. M. and Alam, Khubaib Amjad and Jomhari, Nazean},
	month = nov,
	year = {2018},
	note = {71 Citations},
	keywords = {app:sign-language, read-priority-1, type:survey},
	pages = {28121--28184},
	annote = {[TLDR] The results reveal that among the VBR techniques, HGR is a predominant and highly focused area of research, and research focus is also found to be converging towards sign language recognition.},
	file = {Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf:/Users/brk/Zotero/storage/BL4SNP7J/Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf:application/pdf},
}

@article{vuletic_systematic_2019,
	title = {Systematic literature review of hand gestures used in human computer interaction interfaces},
	volume = {129},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581918305676},
	doi = {10.1016/j.ijhcs.2019.03.011},
	abstract = {Semantic Scholar extracted view of "Systematic literature review of hand gestures used in human computer interaction interfaces" by T. Vuletic et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {International Journal of Human-Computer Studies},
	author = {Vuletic, Tijana and Duffy, Alex and Hay, Laura and McTeague, Chris and Campbell, Gerard and Grealy, Madeleine},
	month = sep,
	year = {2019},
	note = {82 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {74--94},
	annote = {This survey more explores the movements done while performing gestures, rather than the tech used to capture those gestures.
This survey only considers the different types of gestures, and doesn’t look at all into the tech used to capture the gestures. Gesture types include:


Purpose (manipulation vs communication)


Static vs dynamic


Prescribed/instructed or free-form


Application: 3D modelling, assistance (elderly/disabled), data input, manipulation/navigation, touchless control, unspecified


The survey also looks at the tech used at a high level, with the following categories:


Video camera, infrared 3D camera, capture system, accelerometers, EMG, capacitive, and misc. other wearable items


There seems to be multiple obvious typos and a lack of domain knowledge:
- “Final State Machine (FSM)” (Vuletic et al., 2019, p. 17)
- “Hidden Markov Method” (Vuletic et al., 2019, p. 17)

There is generally a lack of progress in the field, likely because there is no standardised wearable sensor system which all researchers can build off of. Similarly, there’s no standardised wearable sensor system because there is no progress in the field. A nifty catch-22. However, I have a hunch that this type of catch 22 (where research needs standardised hardware and the hardware needs research to advance) must already have happened in another field, so how did they solve it?
“This paper reports findings from a systematic review aiming to answer the following research question: What are the patterns of touchless hand gesture use during gesture based interface design for interfaces which have reached the prototype stage?” (Vuletic et al., 2019, p. 2)
},
	file = {Accepted Version:/Users/brk/Zotero/storage/7VJ7GZXV/Vuletic et al. - 2019 - Systematic literature review of hand gestures used.pdf:application/pdf},
}

@article{cheok_review_2019,
	title = {A review of hand gesture and sign language recognition techniques},
	volume = {10},
	issn = {1868-8071, 1868-808X},
	url = {http://link.springer.com/10.1007/s13042-017-0705-5},
	doi = {10.1007/s13042-017-0705-5},
	abstract = {Hand gesture recognition serves as a key for overcoming many difficulties and providing convenience for human life. The ability of machines to understand human activities and their meaning can be utilized in a vast array of applications. One specific field of interest is sign language recognition. This paper provides a thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research. The techniques reviewed are suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification, where the various algorithms at each stage are elaborated and their merits compared. Further, we also discuss the challenges and limitations faced by gesture recognition research in general, as well as those exclusive to sign language recognition. Overall, it is hoped that the study may provide readers with a comprehensive introduction into the field of automated gesture and sign language recognition, and further facilitate future research efforts in this area.},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
	month = jan,
	year = {2019},
	note = {301 Citations},
	keywords = {type:survey},
	pages = {131--153},
	annote = {[TLDR] A thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research, suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification.},
	file = {Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf:/Users/brk/Zotero/storage/62H2TEM2/Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf:application/pdf},
}

@article{park_advanced_2019,
	title = {Advanced {Machine} {Learning} for {Gesture} {Learning} and {Recognition} {Based} on {Intelligent} {Big} {Data} of {Heterogeneous} {Sensors}},
	volume = {11},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/11/7/929},
	doi = {10.3390/sym11070929},
	abstract = {With intelligent big data, a variety of gesture-based recognition systems have been developed to enable intuitive interaction by utilizing machine learning algorithms. Realizing a high gesture recognition accuracy is crucial, and current systems learn extensive gestures in advance to augment their recognition accuracies. However, the process of accurately recognizing gestures relies on identifying and editing numerous gestures collected from the actual end users of the system. This final end-user learning component remains troublesome for most existing gesture recognition systems. This paper proposes a method that facilitates end-user gesture learning and recognition by improving the editing process applied on intelligent big data, which is collected through end-user gestures. The proposed method realizes the recognition of more complex and precise gestures by merging gestures collected from multiple sensors and processing them as a single gesture. To evaluate the proposed method, it was used in a shadow puppet performance that could interact with on-screen animations. An average gesture recognition rate of 90\% was achieved in the experimental evaluation, demonstrating the efficacy and intuitiveness of the proposed method for editing visualized learning gestures.},
	language = {en},
	number = {7},
	urldate = {2023-06-26},
	journal = {Symmetry},
	author = {Park, Jisun and Jin, Yong and Cho, Seoungjae and Sung, Yunsick and Cho, Kyungeun},
	month = jul,
	year = {2019},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {929},
	annote = {[TLDR] This paper proposes a method that facilitates end-user gesture learning and recognition by improving the editing process applied on intelligent big data, which is collected through end- user gestures.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ALFHQ8X4/Park et al. - 2019 - Advanced Machine Learning for Gesture Learning and.pdf:application/pdf},
}

@inproceedings{stephanidis_vision-based_2009,
	title = {Vision-{Based} {Hand} {Gesture} {Recognition} for {Human}-{Computer} {Interaction}},
	volume = {20091047},
	isbn = {978-0-8058-6280-5 978-1-4200-6499-5},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420064995-c34},
	doi = {10.1201/9781420064995-c34},
	abstract = {In recent years, research efforts seeking to provide more natural, human-centered means of interacting with computers have gained growing interest. A particularly important direction is that of perceptive user interfaces, where the computer is endowed with perceptive capabilities that allow it to acquire both implicit and explicit information about the user and the environment. Vision has the potential of carrying a wealth of information in a non-intrusive manner and at a low cost, therefore it constitutes a very attractive sensing modality for developing perceptive user interfaces. Proposed approaches for vision-driven interactive user interfaces resort to technologies such as head tracking, face and facial expression recognition, eye tracking and gesture recognition. In this paper, we focus our attention to vision-based recognition of hand gestures. The first part of the paper provides an overview of the current state of the art regarding the recognition of hand gestures as these are observed and recorded by typical video cameras. In order to make the review of the related literature tractable, this paper does not discuss:},
	language = {en},
	urldate = {2023-06-26},
	publisher = {CRC Press},
	author = {Zabulis, Xenophon and Baltzakis, Haris and Argyros, Antonis},
	editor = {Stephanidis, Constantine},
	month = jun,
	year = {2009},
	doi = {10.1201/9781420064995-c34},
	note = {171 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: The Universal Access Handbook
Series Title: Human Factors and Ergonomics},
	keywords = {type:survey},
	pages = {1--30},
	annote = {[TLDR] This paper provides an overview of the current state of the art regarding the recognition of hand gestures as these are observed and recorded by typical video cameras and discusses proposed approaches for vision-driven interactive user interfaces.},
	file = {Submitted Version:/Users/brk/Zotero/storage/24YMVRP3/Zabulis et al. - 2009 - Vision-Based Hand Gesture Recognition for Human-Co.pdf:application/pdf},
}

@article{supancic_depth-based_2015,
	title = {Depth-{Based} {Hand} {Pose} {Estimation}: {Data}, {Methods}, and {Challenges}},
	shorttitle = {Depth-{Based} {Hand} {Pose} {Estimation}},
	url = {http://ieeexplore.ieee.org/document/7410574/},
	doi = {10.1109/ICCV.2015.217},
	abstract = {Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.},
	urldate = {2023-06-26},
	journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
	author = {Supancic, James S. and Rogez, Gregory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
	month = dec,
	year = {2015},
	note = {155 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 IEEE International Conference on Computer Vision (ICCV)
ISBN: 9781467383912
Place: Santiago
Publisher: IEEE},
	keywords = {type:survey},
	pages = {1868--1876},
	annote = {[TLDR] An extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame, introduces a simple nearest-neighbor baseline that outperforms most existing systems and reinforces the under-appreciated point that training data is as important as the model itself.},
	file = {Submitted Version:/Users/brk/Zotero/storage/HAGU84CM/Supancic et al. - 2015 - Depth-Based Hand Pose Estimation Data, Methods, a.pdf:application/pdf},
}

@article{li_survey_2019,
	title = {A survey on {3D} hand pose estimation: {Cameras}, methods, and datasets},
	volume = {93},
	issn = {00313203},
	shorttitle = {A survey on {3D} hand pose estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320319301724},
	doi = {10.1016/j.patcog.2019.04.026},
	abstract = {Semantic Scholar extracted view of "A survey on 3D hand pose estimation: Cameras, methods, and datasets" by Rui Li et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {Pattern Recognition},
	author = {Li, Rui and Liu, Zhenyu and Tan, Jianrong},
	month = sep,
	year = {2019},
	note = {51 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {251--272},
}

@article{chatzis_comprehensive_2020,
	title = {A {Comprehensive} {Study} on {Deep} {Learning}-{Based} {3D} {Hand} {Pose} {Estimation} {Methods}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/19/6850},
	doi = {10.3390/app10196850},
	abstract = {The field of 3D hand pose estimation has been gaining a lot of attention recently, due to its significance in several applications that require human-computer interaction (HCI). The utilization of technological advances, such as cost-efficient depth cameras coupled with the explosive progress of Deep Neural Networks (DNNs), has led to a significant boost in the development of robust markerless 3D hand pose estimation methods. Nonetheless, finger occlusions and rapid motions still pose significant challenges to the accuracy of such methods. In this survey, we provide a comprehensive study of the most representative deep learning-based methods in literature and propose a new taxonomy heavily based on the input data modality, being RGB, depth, or multimodal information. Finally, we demonstrate results on the most popular RGB and depth-based datasets and discuss potential research directions in this rapidly growing field.},
	language = {en},
	number = {19},
	urldate = {2023-06-26},
	journal = {Applied Sciences},
	author = {Chatzis, Theocharis and Stergioulas, Andreas and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas and Daras, Petros},
	month = sep,
	year = {2020},
	note = {25 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgbd, model:deep-learning, read-priority-9, type:survey},
	pages = {6850},
	annote = {[TLDR] A comprehensive study of the most representative deep learning-based methods in literature is provided and a new taxonomy heavily based on the input data modality, being RGB, depth, or multimodal information is proposed.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5NQQ3Z7T/Chatzis et al. - 2020 - A Comprehensive Study on Deep Learning-Based 3D Ha.pdf:application/pdf},
}

@article{rashid_wearable_2019,
	title = {Wearable technologies for hand joints monitoring for rehabilitation: {A} survey},
	volume = {88},
	issn = {00262692},
	shorttitle = {Wearable technologies for hand joints monitoring for rehabilitation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0026269217305207},
	doi = {10.1016/j.mejo.2018.01.014},
	abstract = {Semantic Scholar extracted view of "Wearable technologies for hand joints monitoring for rehabilitation: A survey" by Adnan Rashid et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {Microelectronics Journal},
	author = {Rashid, Adnan and Hasan, Osman},
	month = jun,
	year = {2019},
	note = {58 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {173--183},
	annote = {there are typically four types
      {\textgreater} of sensors that can be used for hand-related tasks: bend sensors,
      {\textgreater} stretch sensors, inertial measurement units (IMUs), and magnetic
      {\textgreater} sensors.

},
}

@misc{sturman_whole-hand_1992,
	title = {Whole-hand {Input}},
	url = {https://scholar.googleusercontent.com/scholar?q=cache:Yk_AfzsiGZsJ:scholar.google.com/+sturman+whole+hand+input&hl=en&as_sdt=0,5},
	abstract = {This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc},
	urldate = {2023-06-28},
	author = {Sturman, David Joel},
	year = {1992},
	note = {@phdthesis\{\vphantom{\}}10.5555/143615,
author = \{Sturman, David Joel\},
title = \{Whole-Hand Input\},
year = \{1992\},
publisher = \{Massachusetts Institute of Technology\},
address = \{USA\},
abstract = \{This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc.\},
note = \{Not available from Univ. Microfilms Int.\}
\vphantom{\{}\}},
	keywords = {read-priority-5, type:seminal},
	file = {Whole-hand Input:/Users/brk/Zotero/storage/RG78DTFU/scholar.html:text/html;Whole-hand Input.pdf:/Users/brk/Zotero/storage/I4VQXME4/Whole-hand Input.pdf:application/pdf},
}

@inproceedings{kolsch_keyboards_2002,
	title = {Keyboards without {Keyboards}: {A} {Survey} of {Virtual} {Keyboards}},
	shorttitle = {Keyboards without {Keyboards}},
	url = {https://www.semanticscholar.org/paper/Keyboards-without-Keyboards%3A-A-Survey-of-Virtual-K%C3%B6lsch-Turk/44355bea0b025ba8719a946ba6c009e0eba133f3},
	abstract = {Input to small devices is becoming an increasingly crucial factor in development for the ever-more powerful embedded market. Speech input promises to become a feasible alternative to tiny keypads, yet its limited reliability, robustness, and flexibility render it unsuitable for certain tasks and/or environments. Various attempts have been made to provide the common keyboard metaphor without the physical keyboard, to build “virtual keyboards”. This promises to leverage our familiarity with the device without incurring the constraints of the bulky physics. This paper surveys technologies for alphanumeric input devices and methods with a strong focus on touch-typing. We analyze the characteristics of the keyboard modality and show how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
	urldate = {2023-06-29},
	author = {Kölsch, M. and Turk, M.},
	year = {2002},
	keywords = {read-priority-1, read-priority-3, type:survey},
	annote = {[TLDR] The characteristics of the keyboard modality are analyzed and it is shown how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
	annote = {[TLDR] The characteristics of the keyboard modality are analyzed and it is shown how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/MB8CKX7D/Kölsch and Turk - 2002 - Keyboards without Keyboards A Survey of Virtual K.pdf:application/pdf},
}

@article{xu_hand_2006,
	title = {Hand {Gesture} {Interaction} for {Virtual} {Training} of {SPG}},
	url = {https://ieeexplore.ieee.org/document/4089336/},
	doi = {10.1109/ICAT.2006.68},
	abstract = {We develop a virtual reality based driving training system of self-propelled gun (SPG). In order to make the interface of the system more powerful and natural, hand gesture interaction need to be incorporated into the system's interface. This paper discusses the use of hand gestures for interaction with the virtual training environment. We employ static hand gestures which coupled with hand translations and rotations as the method of interacting with the virtual training environment. An 18-sensor data glove is chosen for monitoring the movements of the fingers and the wrist. The feed-forward neural network is developed for recognizing gestures for use in virtual training application of artillery self-propelled gun (SPG). We present our approach for the algorithm design and implementation, and the use of the gestures in our application. The presented hand gesture interaction method can be effectively used in our virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way},
	urldate = {2023-07-02},
	journal = {16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)},
	author = {Xu, Deyou and Yao, Wuyun and Zhang, Yongliang},
	month = nov,
	year = {2006},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 16th International Conference on Artificial Reality and Telexistence-Workshops (ICAT'06)
ISBN: 9780769527543
Place: Hangzhou
Publisher: IEEE},
	keywords = {model:ffnn, movement:static},
	pages = {672--676},
	annote = {[TLDR] The presented hand gesture interaction method can be effectively used in the virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way.},
}

@article{damasio_animating_2002,
	title = {Animating virtual humans using hand postures},
	url = {http://ieeexplore.ieee.org/document/1167207/},
	doi = {10.1109/SIBGRA.2002.1167207},
	abstract = {Interaction between human and computer has been used in a large scale in computer graphics and virtual reality. This paper presents a system to provide interaction between user and virtual humans. The system uses a data glove and an artificial neural network system responsible for the recognition of hand postures.},
	urldate = {2023-07-02},
	journal = {Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing},
	author = {Damasio, F.W. and Musse, S.R.},
	year = {2002},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 15th Brazilian Symposium on Computer Graphics and Image Processing
ISBN: 9780769518466
Place: Fortaleza-CE, Brazil
Publisher: IEEE Comput. Soc},
	keywords = {model:ffnn, movement:static},
	pages = {437},
	annote = {[TLDR] A system to provide interaction between user and virtual humans using a data glove and an artificial neural network system responsible for the recognition of hand postures is presented.},
}

@article{murakami_gesture_1991,
	title = {Gesture recognition using recurrent neural networks},
	url = {http://portal.acm.org/citation.cfm?doid=108844.108900},
	doi = {10.1145/108844.108900},
	abstract = {A gesture recognition method for Japanese sign language is presented. We have developed a posture recognition system using neural networks which could recognize a finger alphabet of 42 symbols. We then developed a gesture recognition system where each gesture specifies a word. Gesture recognition is more difficult than posture recognition because it has to handle dynamic processes. To deal with dynamic processes we use a recurrent neural network. Here, we describe a gesture recognition method which can recognize continuous gesture. We then discuss the results of our research.},
	language = {en},
	urldate = {2023-07-02},
	journal = {Proceedings of the SIGCHI conference on Human factors in computing systems Reaching through technology - CHI '91},
	author = {Murakami, Kouichi and Taguchi, Hitomi},
	year = {1991},
	note = {405 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: the SIGCHI conference
ISBN: 9780897913836
Place: New Orleans, Louisiana, United States
Publisher: ACM Press},
	keywords = {movement:static, model:rnn},
	pages = {237--242},
	annote = {@murakami1991 trained an Elman recurrent neural network to recognise ten dynamic gestures from the Japanese Finger Alphabet. These gestures were all single hand motions which involved a single hand in motion while the fingers remained still. The input data came from a "Data Glove", although the make and model of the glove was not specified. The Data Glove provided ten values describing the bend of the fingers and six auxiliary values describing the position of the Data Glove.

},
	annote = {[TLDR] A gesture recognition method for Japanese sign language which can recognize continuous gesture is described and a recurrent neural network is used to deal with dynamic processes.},
}

@article{mehdi_sign_2002,
	title = {Sign language recognition using sensor gloves},
	url = {http://ieeexplore.ieee.org/document/1201884/},
	doi = {10.1109/ICONIP.2002.1201884},
	abstract = {This paper examines the possibility of recognizing sign language gestures using sensor gloves. Previously sensor gloves are used in games or in applications with custom gestures. This paper explores their use in Sign Language recognition. This is done by implementing a project called "Talking Hands", and studying the results. The project uses a sensor glove to capture the signs of American Sign Language performed by a user and translates them into sentences of English language. Artificial neural networks are used to recognize the sensor values coming from the sensor glove. These values are then categorized in 24 alphabets of English language and two punctuation symbols introduced by the author. So, mute people can write complete sentences using this application.},
	urldate = {2023-07-02},
	journal = {Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.},
	author = {Mehdi, S.A. and Khan, Y.N.},
	year = {2002},
	note = {151 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 9th International Conference on Neural Information Processing
ISBN: 9789810475246
Place: Singapore
Publisher: IEEE},
	keywords = {model:ffnn, movement:static},
	pages = {2204--2206 vol.5},
	annote = {[TLDR] M mute people can write complete sentences using this application, which uses a sensor glove to capture the signs of American Sign Language performed by a user and translates them into sentences of English language.},
}

@article{jong-sung_kim_dynamic_1996,
	title = {A dynamic gesture recognition system for the {Korean} sign language ({KSL})},
	volume = {26},
	issn = {10834419},
	url = {http://ieeexplore.ieee.org/document/485888/},
	doi = {10.1109/3477.485888},
	abstract = {The sign language is a method of communication for the deaf-mute. Articulated gestures and postures of hands and fingers are commonly used for the sign language. This paper presents a system which recognizes the Korean sign language (KSL) and translates into a normal Korean text. A pair of data-gloves are used as the sensing device for detecting motions of hands and fingers. For efficient recognition of gestures and postures, a technique of efficient classification of motions is proposed and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
	number = {2},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
	author = {{Jong-Sung Kim} and {Won Jang} and {Zeungnam Bien}},
	month = apr,
	year = {1996},
	note = {192 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, model:ffnn, movement:dynamic, app:korean-sl},
	pages = {354--359},
	annote = {[TLDR] A system which recognizes the Korean sign language (KSL) and translates into a normal Korean text is presented and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
	annote = {[TLDR] A system which recognizes the Korean sign language (KSL) and translates into a normal Korean text is presented and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
}

@article{kadous_grasp_1995,
	title = {{GRASP}: {Recognition} of {Australian} {Sign} {Language} {Using} {Instrumented} {Gloves}},
	shorttitle = {{GRASP}},
	abstract = {Instrumented gloves -- gloves equipped with sensors for detecting finger bend, hand position and orientation -- were conceived to allow a more natural interface to computers. However, the extension of their use for recognising sign language, and in this case Auslan (Australian Sign Language), is possible. Several researchers have already explored these possibilities and have successfully achieved finger-spelling recognition with high levels of accuracy, but progress in the recognition of sign language as a whole has been limited.},
	author = {Kadous, Mohammed},
	month = dec,
	year = {1995},
	keywords = {model:hmm, app:sign-language, app:australian-sl, model:nn, model:instance-based-learning, from:cite.bib},
	annote = {@kadous1995 used a one-hand Nintendo PowerGlove to distinguish between 95 different signs from Australian Sign Language. The start and end of each sign had to be manually indicated by pressing a button on the PowerGlove. These signs are often dynamic gestures in which both hands trace some path but keep the finger positions still. Only one PowerGlove was used, despite many of the signs being two-handed. The author discarded the idea of building a PowerGlove equivalent themselves, as that effort was considered to be a thesis in and of itself.

},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ZVYPR4WQ/Kadous - 1995 - GRASP Recognition of Australian Sign Language Usi.pdf:application/pdf},
}

@inproceedings{vamplew_recognition_2007,
	title = {Recognition of sign language gestures using neural networks},
	url = {http://www.ledonline.it/NeuropsychologicalTrends/},
	doi = {10.7358/neur-2007-001-vamp},
	abstract = {This paper describes the structure and performance of the SLARTI sign language recognition system developed at the University of Tasmania. SLARTI uses a modular architecture consisting of multiple feature-recognition neural networks and a nearest-neighbour classifier to recognise Australian sign language (Auslan) hand gestures.},
	urldate = {2023-07-02},
	booktitle = {Neuropsychological {Trends}},
	author = {Vamplew, Simon},
	month = apr,
	year = {2007},
	note = {93 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1970321X, 19703201
Issue: 1},
	keywords = {model:knn, app:sign-language, model:ffnn, movement:dynamic, app:australian-sl},
	pages = {4},
	annote = {[TLDR] The structure and performance of the SLARTI sign language recognition system, which uses a modular architecture consisting of multiple feature-recognition neural networks and a nearest-neighbour classifier to recognise Australian sign language (Auslan) hand gestures, are described.},
}

@inproceedings{naidoo_south_2010,
	title = {South {African} sign language recognition using feature vectors and {Hidden} {Markov} {Models}},
	url = {https://www.semanticscholar.org/paper/South-African-sign-language-recognition-using-and-Naidoo/be177e5e05b402da3bd53db0ba8445f87596f7d3},
	abstract = {This thesis presents a system for performing whole gesture recognition for South African Sign Language. The system uses feature vectors combined with Hidden Markov models. In order to constuct a feature vector, dynamic segmentation must occur to extract the signer’s hand movements. Techniques and methods for normalising variations that occur when recording a signer performing a gesture, are investigated. The system has a classification rate of 69\%.},
	urldate = {2023-07-02},
	author = {Naidoo, Nathan Lyle},
	year = {2010},
	keywords = {model:hmm},
	annote = {[TLDR] This thesis presents a system for performing whole gesture recognition for South African Sign Language that uses feature vectors combined with Hidden Markov models and has a classification rate of 69\%.},
	file = {Naidoo - 2010 - South African sign language recognition using feat.pdf:/Users/brk/Zotero/storage/JX268W5D/Naidoo - 2010 - South African sign language recognition using feat.pdf:application/pdf},
}

@inproceedings{frieslaar_robust_2014,
	title = {Robust {South} {African} sign language gesture recognition using hand motion and shape},
	url = {https://www.semanticscholar.org/paper/Robust-South-African-sign-language-gesture-using-Frieslaar/af10d1bb6ef67ab9c8b624b5fcafd9dd2e2daf69},
	abstract = {Research has shown that five fundamental parameters are required to recognize any sign language gesture: hand shape, hand motion, hand location, hand orientation and facial expressions. The South African Sign Language (SASL) research group at the University of the Western Cape (UWC) has created several systems to recognize sign language gestures using single parameters. These systems are, however, limited to a vocabulary size of 20 – 23 signs, beyond which the recognition accuracy is expected to decrease. The first aim of this research is to investigate the use of two parameters – hand motion and hand shape – to recognise a larger vocabulary of SASL gestures at a high accuracy. Also, the majority of related work in the field of sign language gesture recognition using these two parameters makes use of Hidden Markov Models (HMMs) to classify gestures. Hidden Markov Support Vector Machines (HM-SVMs) are a relatively new technique that make use of Support Vector Machines (SVMs) to simulate the functions of HMMs. Research indicates that HM-SVMs may perform better than HMMs in some applications. To our knowledge, they have not been applied to the field of sign language gesture recognition. This research compares the use of these two techniques in the context of SASL gesture recognition. The results indicate that, using two parameters results in a 15\% increase in accuracy over the use of a single parameter. Also, it is shown that HM-SVMs are a more accurate technique than HMMs, generally performing better or at least as good as HMMs.},
	urldate = {2023-07-02},
	author = {Frieslaar, I.},
	year = {2014},
	note = {6 Citations},
	keywords = {model:svm, model:hmm},
	annote = {[TLDR] This research investigates the use of two parameters – hand motion and hand shape – to recognise a larger vocabulary of SASL gestures at a high accuracy and shows that HM-SVMs are a more accurate technique than HMMs, generally performing better or at least as good as HMMs.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/U9U3CGTS/Frieslaar - 2014 - Robust South African sign language gesture recogni.pdf:application/pdf},
}

@article{gao_sign_2000,
	title = {{SIGN} {LANGUAGE} {RECOGNITION} {BASED} {ON} {HMM}/{ANN}/{DP}},
	volume = {14},
	issn = {0218-0014, 1793-6381},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001400000386},
	doi = {10.1142/S0218001400000386},
	abstract = {In this paper, a system designed for helping the deaf to communicate with others is presented. Some useful new ideas are proposed in design and implementation. An algorithm based on geometrical analysis for the purpose of extracting invariant feature to signer position is presented. An ANN–DP combined approach is employed for segmenting subwords automatically from the data stream of sign signals. To tackle the epenthesis movement problem, a DP-based method has been used to obtain the context-dependent models. Some techniques for system implementation are also given, including fast matching, frame prediction and search algorithms. The implemented system is able to recognize continuous large vocabulary Chinese Sign Language. Experiments show that proposed techniques in this paper are efficient on either recognition speed or recognition performance.},
	language = {en},
	number = {05},
	urldate = {2023-07-02},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Gao, Wen and Ma, Jiyong and Wu, Jiangqin and Wang, Chunli},
	month = aug,
	year = {2000},
	note = {99 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, model:ffnn},
	pages = {587--602},
	annote = {[TLDR] A system designed for helping the deaf to communicate with others is presented and an algorithm based on geometrical analysis for the purpose of extracting invariant feature to signer position is presented.},
}

@inproceedings{goos_real-time_2002,
	address = {Berlin, Heidelberg},
	title = {A {Real}-{Time} {Large} {Vocabulary} {Recognition} {System} for {Chinese} {Sign} {Language}},
	volume = {2298},
	isbn = {978-3-540-43678-2 978-3-540-47873-7},
	url = {http://link.springer.com/10.1007/3-540-47873-6_9},
	doi = {10.1007/3-540-47873-6_9},
	abstract = {The major challenge that faces Sign Language recognition now is to develop methods that will scale well with increasing vocabulary size. In this paper, a real-time system designed for recognizing Chinese Sign Language (CSL) signs with a 5100 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. An algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed. Then the worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including modifying the transferring probability, clustering the Gaussians and fast matching algorithm. Experiments show that techniques proposed in this paper are efficient on either recognition speed or recognition performance.},
	urldate = {2023-07-02},
	publisher = {Springer Berlin Heidelberg},
	author = {Chunli, Wang and Wen, Gao and Jiyong, Ma},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Sowa, Timo},
	year = {2002},
	doi = {10.1007/3-540-47873-6_9},
	note = {38 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Gesture and Sign Language in Human-Computer Interaction
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, app:sign-language},
	pages = {86--95},
	annote = {[TLDR] A real-time system designed for recognizing Chinese Sign Language signs with a 5100 sign vocabulary is presented and an algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed.},
}

@inproceedings{zhang_vision-based_2004,
	address = {State College PA USA},
	title = {A vision-based sign language recognition system using tied-mixture density {HMM}},
	isbn = {978-1-58113-995-2},
	url = {https://dl.acm.org/doi/10.1145/1027933.1027967},
	doi = {10.1145/1027933.1027967},
	abstract = {In this paper, a vision-based medium vocabulary Chinese sign language recognition (SLR) system is presented. The proposed recognition system consists of two modules. In the first module, techniques of robust hands detection, background subtraction and pupils detection are efficiently combined to precisely extract the feature information with the aid of simple colored gloves in the unconstrained environment. Meanwhile, an effective and efficient hierarchical feature description scheme with different scale features to characterize sign language is proposed, where principal component analysis (PCA) is employed to characterize the finger features more elaborately. In the second part, a Tied-Mixture Density Hidden Markov Models (TMDHMM) framework for SLR is proposed, which can speed up the recognition without the significant loss of recognition accuracy compared with the continuous hidden Markov models (CHMM). Experimental results based on 439 frequently used Chinese sign language (CSL) words show that the proposed methods can work well for the medium vocabulary SLR in the environment without special constraints and the recognition accuracy is up to 92.5\%.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the 6th international conference on {Multimodal} interfaces},
	publisher = {ACM},
	author = {Zhang, Liang-Guo and Chen, Yiqiang and Fang, Gaolin and Chen, Xilin and Gao, Wen},
	month = oct,
	year = {2004},
	note = {69 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language},
	pages = {198--204},
	annote = {[TLDR] Experimental results show that the proposed methods can work well for the medium vocabulary SLR in the environment without special constraints and the recognition accuracy is up to 92.5\%.},
}

@inproceedings{goos_real-time_2001,
	address = {Berlin, Heidelberg},
	title = {A {Real}-{Time} {Large} {Vocabulary} {Continuous} {Recognition} {System} for {Chinese} {Sign} {Language}},
	volume = {2195},
	isbn = {978-3-540-42680-6 978-3-540-45453-3},
	url = {http://link.springer.com/10.1007/3-540-45453-5_20},
	doi = {10.1007/3-540-45453-5_20},
	abstract = {In this paper, a real-time system designed for recognizing continuous Chinese Sign Language (CSL) sentences with a 4800 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. The worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including states tying, still frame detecting and fast search algorithm. Experiments were carried out, and for real-time continuous sign recognition, the correct rate is over 90\%.},
	urldate = {2023-07-02},
	publisher = {Springer Berlin Heidelberg},
	author = {Wang, Chunli and Gao, Wen and Xuan, Zhaoguo},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Shum, Heung-Yeung and Liao, Mark and Chang, Shih-Fu},
	year = {2001},
	doi = {10.1007/3-540-45453-5_20},
	note = {31 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Advances in Multimedia Information Processing — PCM 2001
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, app:sign-language, read-priority-1, classes:1k-5k, app:chinese-sl},
	pages = {150--157},
	annote = {Somehow they are using HMMs for 4k classes

},
	annote = {[TLDR] A real-time system designed for recognizing continuous Chinese Sign Language sentences with a 4800 sign vocabulary with over 90\% accuracy is presented and some useful new ideas are proposed in design and implementation.},
	annote = {[TLDR] A real-time system designed for recognizing continuous Chinese Sign Language sentences with a 4800 sign vocabulary with over 90\% accuracy is presented and some useful new ideas are proposed in design and implementation.},
	file = {Wang et al. - 2001 - A Real-Time Large Vocabulary Continuous Recognitio.pdf:/Users/brk/Zotero/storage/QZ3Y7L3J/Wang et al. - 2001 - A Real-Time Large Vocabulary Continuous Recognitio.pdf:application/pdf},
}

@article{gao_chinese_2004,
	title = {A {Chinese} sign language recognition system based on {SOFM}/{SRN}/{HMM}},
	volume = {37},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320304001657},
	doi = {10.1016/S0031-3203(04)00165-7},
	abstract = {Semantic Scholar extracted view of "A Chinese sign language recognition system based on SOFM/SRN/HMM" by Wen Gao et al.},
	language = {en},
	number = {12},
	urldate = {2023-07-02},
	journal = {Pattern Recognition},
	author = {Gao, W and Fang, G and Zhao, D and Chen, Y},
	month = dec,
	year = {2004},
	note = {128 citations},
	keywords = {model:hmm, app:sign-language, app:chinese-sl, model:som},
	pages = {2389--2402},
}

@article{fatmi_comparing_2019,
	title = {Comparing {ANN}, {SVM}, and {HMM} based {Machine} {Learning} {Methods} for {American} {Sign} {Language} {Recognition} using {Wearable} {Motion} {Sensors}},
	url = {https://ieeexplore.ieee.org/document/8666491/},
	doi = {10.1109/CCWC.2019.8666491},
	abstract = {Millions of people with speech and hearing impairments, worldwide, communicate through sign languages every day. In the same way that voice recognition provides a simple communication platform for most users, gesture recognition is a natural means of correspondence for the hearing-impaired. In this paper, we explore the problem of translating/converting sign language to speech, and propose an improved solution using different machine learning techniques. We seek to build a system that can be employed in the daily lives of people with hearing impairments, in order to enhance communication and collaboration between the hearing-impaired community and those untrained in American Sign Language (ASL). The system architecture is based on using wearable motion sensors and machine learning techniques. In this study, we propose a solution using Artificial Neural Networks (ANN) and Support Vector Machines (SVM), and compare their accuracy with the Hidden Markov Model (HMM) results from our previous work to recognize ASL words. Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.},
	urldate = {2023-07-02},
	journal = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
	author = {Fatmi, Rabeet and Rashad, Sherif and Integlia, Ryan},
	month = jan,
	year = {2019},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)
ISBN: 9781728105543
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {model:svm, model:hmm, app:sign-language, model:ffnn, app:american-sl},
	pages = {0290--0297},
	annote = {[TLDR] Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.},
}

@inproceedings{nel_integrated_2013,
	address = {East London South Africa},
	title = {An integrated sign language recognition system},
	isbn = {978-1-4503-2112-9},
	url = {https://dl.acm.org/doi/10.1145/2513456.2513491},
	doi = {10.1145/2513456.2513491},
	abstract = {The South African Sign Language research group at the University of the Western Cape has created several systems to recognize Sign Language gestures using single parameters. Research has shown that five parameters are required to recognize any sign language gesture: hand shape, location, orientation and motion, as well as facial expressions. Using a single parameter can cause conflicts in recognition of signs that are similarly signed. This paper pioneers research at the group towards combining multiple parameters to better distinguish between similar signs. This eventually aims to enable the recognition of a large SASL vocabulary. The proposed methodology combines hand location and hand shape recognition into one combined recognition system. The recognition approach is applied to 12 SASL signs that consist of six pairs of signs with the same hand shape performed at two different locations. It is shown that the approach is able to achieve a high average recognition accuracy of 79\% across all signs and distinguish between the signs effectively. It is also shown to be robust to variations in test subjects.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the {South} {African} {Institute} for {Computer} {Scientists} and {Information} {Technologists} {Conference}},
	publisher = {ACM},
	author = {Nel, Warren and Ghaziasgar, Mehrdad and Connan, James},
	month = oct,
	year = {2013},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:south-african-sl},
	pages = {179--185},
	annote = {[TLDR] This paper pioneers research at the group towards combining multiple parameters to better distinguish between similar signs, which eventually aims to enable the recognition of a large SASL vocabulary.},
	file = {Submitted Version:/Users/brk/Zotero/storage/R5QLW7FJ/Nel et al. - 2013 - An integrated sign language recognition system.pdf:application/pdf},
}

@article{mejia-perez_automatic_2022,
	title = {Automatic {Recognition} of {Mexican} {Sign} {Language} {Using} a {Depth} {Camera} and {Recurrent} {Neural} {Networks}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/11/5523},
	doi = {10.3390/app12115523},
	abstract = {Automatic sign language recognition is a challenging task in machine learning and computer vision. Most works have focused on recognizing sign language using hand gestures only. However, body motion and facial gestures play an essential role in sign language interaction. Taking this into account, we introduce an automatic sign language recognition system based on multiple gestures, including hands, body, and face. We used a depth camera (OAK-D) to obtain the 3D coordinates of the motions and recurrent neural networks for classification. We compare multiple model architectures based on recurrent networks such as Long Short-Term Memories (LSTM) and Gated Recurrent Units (GRU) and develop a noise-robust approach. For this work, we collected a dataset of 3000 samples from 30 different signs of the Mexican Sign Language (MSL) containing features coordinates from the face, body, and hands in 3D spatial coordinates. After extensive evaluation and ablation studies, our best model obtained an accuracy of 97\% on clean test data and 90\% on highly noisy data.},
	language = {en},
	number = {11},
	urldate = {2023-07-02},
	journal = {Applied Sciences},
	author = {Mejía-Peréz, Kenneth and Córdova-Esparza, Diana-Margarita and Terven, Juan and Herrera-Navarro, Ana-Marcela and García-Ramírez, Teresa and Ramírez-Pedraza, Alfonso},
	month = may,
	year = {2022},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:mexican-sl, classes:30-50, model:gru, model:lstm},
	pages = {5523},
	annote = {[TLDR] This work introduces an automatic sign language recognition system based on multiple gestures, including hands, body, and face, and compares multiple model architectures based on recurrent networks such as Long Short-Term Memories (LSTM) and Gated Recurrent Units (GRU) and develops a noise-robust approach.},
	file = {Full Text:/Users/brk/Zotero/storage/BVMX8PCK/Mejía-Peréz et al. - 2022 - Automatic Recognition of Mexican Sign Language Usi.pdf:application/pdf},
}

@article{ismail_dynamic_2022,
	title = {Dynamic hand gesture recognition of {Arabic} sign language by using deep convolutional neural networks},
	volume = {25},
	issn = {2502-4760, 2502-4752},
	url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/26823},
	doi = {10.11591/ijeecs.v25.i2.pp952-962},
	abstract = {{\textless}p{\textgreater}In computer vision, one of the most difficult problems is human gestures in videos recognition Because of certain irrelevant environmental variables. This issue has been solved by using single deep networks to learn spatiotemporal characteristics from video data, and this approach is still insufficient to handle both problems at the same time. As a result, the researchers fused various models to allow for the effective collection of important shape information as well as precise spatiotemporal variation of gestures. In this study, we collected the dynamic dataset for twenty meaningful words of Arabic sign language (ArSL) using a Microsoft Kinect v2 camera. The recorded data included 7350 red, green, and blue (RGB) videos and 7350 depth videos. We proposed four deep neural networks models using 2D and 3D convolutional neural network (CNN) to cover all feature extraction methods and then passing these features to the recurrent neural network (RNN) for sequence classification. Long short-term memory (LSTM) and gated recurrent unit (GRU) are two types of using RNN. Also, the research included evaluation fusion techniques for several types of multiple models. The experiment results show the best multi-model for the dynamic dataset of the ArSL recognition achieved 100\% accuracy.{\textless}/p{\textgreater}},
	number = {2},
	urldate = {2023-07-02},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	author = {Ismail, Mohammad H. and Dawwd, Shefa A. and Ali, Fakhradeen H.},
	month = feb,
	year = {2022},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, app:sign-language, model:rnn, app:arabic-sl, model:gru, model:lstm},
	pages = {952},
	annote = {[TLDR] Four deep neural networks models are proposed using 2D and 3D convolutional neural network (CNN) to cover all feature extraction methods and then passing these features to the recurrent neuralnetwork (RNN) for sequence classification.},
	file = {Full Text:/Users/brk/Zotero/storage/UKX6R46H/Ismail et al. - 2022 - Dynamic hand gesture recognition of Arabic sign la.pdf:application/pdf},
}

@article{avola_exploiting_2019,
	title = {Exploiting {Recurrent} {Neural} {Networks} and {Leap} {Motion} {Controller} for the {Recognition} of {Sign} {Language} and {Semaphoric} {Hand} {Gestures}},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	url = {https://ieeexplore.ieee.org/document/8410764/},
	doi = {10.1109/TMM.2018.2856094},
	abstract = {Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human–human communication and human–computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96\% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.},
	number = {1},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Multimedia},
	author = {Avola, Danilo and Bernardi, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano},
	month = jan,
	year = {2019},
	note = {108 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, model:rnn, app:american-sl},
	pages = {234--245},
	annote = {[TLDR] In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands, chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners.},
	file = {Submitted Version:/Users/brk/Zotero/storage/SQ8WF2YE/Avola et al. - 2019 - Exploiting Recurrent Neural Networks and Leap Moti.pdf:application/pdf},
}

@inproceedings{liang_sign_1996,
	address = {Hong Kong},
	title = {A sign language recognition system using hidden markov model and context sensitive search},
	isbn = {978-0-89791-825-1},
	url = {http://dl.acm.org/citation.cfm?doid=3304181.3304194},
	doi = {10.1145/3304181.3304194},
	abstract = {Hand gesture is one of the most natural and expressive ways for the hearing impaired. However, because of the complexity of dynamic gestures, most researches are focused either on static gestures, postures, or a small set of dynamic gestures. As real-time recognition of a large set of dynamic gestures is considered, some efficient algorithms and models are needed. To solve this problem in Taiwanese Sign Language, a statistics based context sensitive model is presented and both gestures and postures can be successfully recognized. A gesture is decomposed as a sequence of postures and the postures can be quickly recognized using hidden Markov model. With the probability resulted from hidden Markov model and the probability of each gesture in a lexicon, a gesture can be easily recognized in a linguistic way in real-time.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology} - {VRST} '96},
	publisher = {ACM Press},
	author = {Liang, Rung-Huei and Ouhyoung, Ming},
	year = {1996},
	note = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, app:taiwanese-sl},
	pages = {59--66},
	annote = {[TLDR] To solve the problem of real-time recognition of a large set of dynamic gestures in Taiwanese Sign Language, a statistics based context sensitive model is presented and both gestures and postures can be successfully recognized.},
}

@article{moni_hmm_2009,
	title = {{HMM} based hand gesture recognition: {A} review on techniques and approaches},
	shorttitle = {{HMM} based hand gesture recognition},
	url = {http://ieeexplore.ieee.org/document/5234536/},
	doi = {10.1109/ICCSIT.2009.5234536},
	abstract = {Gesture is one of the most natural and expressive ways of communications between human and computer in a virtual reality system. We naturally use various gestures to express our own intentions in everyday life. Hand gesture is one of the important methods of non-verbal communication for human beings for its freer in movements and much more expressive than any other body parts. Hand gesture recognition has a number of potential applications in human-computer interaction, machine vision, virtual reality, machine control in industry, and so on. As a gesture is a continuous motion on a sequential time series, the HMMs (Hidden Markov Models) must be a prominent recognition tool. The most important thing in hand gesture recognition is what the input features are that best represent the characteristics of the moving hand gesture.This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.},
	urldate = {2023-07-02},
	journal = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	author = {Moni, M. A. and Ali, A. B. M. Shawkat},
	year = {2009},
	note = {76 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2009 2nd IEEE International Conference on Computer Science and Information Technology
ISBN: 9781424445196
Place: Beijing, China
Publisher: IEEE},
	keywords = {model:hmm, read-priority-1, type:survey},
	pages = {433--437},
	annote = {[TLDR] This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.},
	file = {Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf:/Users/brk/Zotero/storage/8F5BHH48/Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf:application/pdf},
}

@article{rung-huei_liang_real-time_1998,
	title = {A real-time continuous gesture recognition system for sign language},
	url = {http://ieeexplore.ieee.org/document/671007/},
	doi = {10.1109/AFGR.1998.671007},
	abstract = {A large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a data glove. Sign language, which is usually known as a set of natural language with formal semantic definitions and syntactic rules, is a large set of hand gestures that are daily used to communicate with the hearing impaired. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to four parameters in a gesture: posture, position, orientation, and motion. The authors have implemented a prototype system with a lexicon of 250 vocabularies and collected 196 training sentences in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signer-dependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%,.},
	urldate = {2023-07-02},
	journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},
	author = {{Rung-Huei Liang} and {Ming Ouhyoung}},
	year = {1998},
	note = {498 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Third IEEE International Conference on Automatic Face and Gesture Recognition
ISBN: 9780818683442
Place: Nara, Japan
Publisher: IEEE Comput. Soc},
	keywords = {model:hmm, app:sign-language, app:taiwanese-sl, classes:50-100},
	pages = {558--567},
	annote = {[TLDR] A large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a data glove using hidden Markov models for 51 fundamental postures, 6 orientations, and 8 motion primitives.},
}

@article{ong_automatic_2005,
	title = {Automatic {Sign} {Language} {Analysis}: {A} {Survey} and the {Future} beyond {Lexical} {Meaning}},
	volume = {27},
	issn = {0162-8828},
	shorttitle = {Automatic {Sign} {Language} {Analysis}},
	url = {http://ieeexplore.ieee.org/document/1432718/},
	doi = {10.1109/TPAMI.2005.112},
	abstract = {Research in automatic analysis of sign language has largely focused on recognizing the lexical (or citation) form of sign gestures as they appear in continuous signing, and developing algorithms that scale well to large vocabularies. However, successful recognition of lexical signs is not sufficient for a full understanding of sign language communication. Nonmanual signals and grammatical processes which result in systematic variations in sign appearance are integral aspects of this communication but have received comparatively little attention in the literature. In this survey, we examine data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures. These are discussed with respect to issues such as modeling transitions between signs in continuous signing, modeling inflectional processes, signer independence, and adaptation. We further examine works that attempt to analyze nonmanual signals and discuss issues related to integrating these with (hand) sign gestures. We also discuss the overall progress toward a true test of sign recognition systems--dealing with natural signing by native signers. We suggest some future directions for this research and also point to contributions it can make to other fields of research. Web-based supplemental materials (appendicies) which contain several illustrative examples and videos of signing can be found at www.computer.org/publications/dlib.},
	language = {en},
	number = {6},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ong, S.C.W. and Ranganath, S.},
	month = jun,
	year = {2005},
	note = {595 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, type:survey},
	pages = {873--891},
	annote = {[TLDR] Data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures are examined and the overall progress toward a true test of sign recognition systems--dealing with natural signing by native signers is discussed.},
	file = {Submitted Version:/Users/brk/Zotero/storage/7IUZNIGX/Ong and Ranganath - 2005 - Automatic Sign Language Analysis A Survey and the.pdf:application/pdf},
}

@article{sagayam_hand_2017,
	title = {Hand posture and gesture recognition techniques for virtual reality applications: a survey},
	volume = {21},
	issn = {1359-4338, 1434-9957},
	shorttitle = {Hand posture and gesture recognition techniques for virtual reality applications},
	url = {http://link.springer.com/10.1007/s10055-016-0301-0},
	doi = {10.1007/s10055-016-0301-0},
	abstract = {Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.},
	language = {en},
	number = {2},
	urldate = {2023-07-02},
	journal = {Virtual Reality},
	author = {Sagayam, K. Martin and Hemanth, D. Jude},
	month = jun,
	year = {2017},
	note = {9 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:vr, type:survey},
	pages = {91--107},
	annote = {[TLDR] A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques, and difficulties and future investigation bearing are also examined.},
	annote = {[TLDR] A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques, and difficulties and future investigation bearing are also examined.},
}

@inproceedings{sharma_gesture_2013,
	title = {Gesture {Recognition} : {A} {Survey} of {Gesture} {Recognition} {Techniques} using {Neural} {Networks}},
	shorttitle = {Gesture {Recognition}},
	url = {https://www.semanticscholar.org/paper/Gesture-Recognition-%3A-A-Survey-of-Gesture-using-Sharma-Chawla/70c11356829e83ba2976af1e6a6e69785d0d56ca},
	abstract = {Understanding human motions can be posed as a pattern recognition problem. In order to convey visual messages to a receiver, a human expresses motion patterns. Loosely called gestures, these patterns are variable but distinct and have an associated meaning. The Pattern recognition by any computer or machine can be implemented via various methods such as Hidden Harkov Models, Linear Programming and Neural Networks. Each method has its own advantages and disadvantages, which will be studied separately later on. This paper reviews why using ANNs in particular is better suited for analyzing human},
	urldate = {2023-07-02},
	author = {Sharma, M. and Chawla, Er Rama},
	year = {2013},
	keywords = {model:nn, type:survey},
	annote = {[TLDR] This paper reviews why using ANNs in particular is better suited for analyzing human motion patterns and explains why this method has its own advantages and disadvantages.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2GC2XFLL/Sharma and Chawla - 2013 - Gesture Recognition  A Survey of Gesture Recognit.pdf:application/pdf},
}

@inproceedings{watson_survey_1993,
	title = {A {Survey} of {Gesture} {Recognition} {Techniques}},
	url = {https://www.semanticscholar.org/paper/A-Survey-of-Gesture-Recognition-Techniques-Watson/23ec699d9a8c19e3807c53f85c564b9cfa172fff},
	abstract = {Processing speeds have increased dramatically bitmapped displays allow graph ics to be rendered and updated at increasing rates and in general computers have advanced to the point where they can assist humans in complex tasks Yet input technologies seem to cause the major bottleneck in performing these tasks under utilising the available resources and restricting the expressiveness of application use We use our hands constantly to interact with things pick them up move them transform their shape or activate them in some way In the same uncon scious way we gesticulate in communicating fundamental ideas stop come closer over there no agreed and so on Gestures are thus a natural and intuitive form of both interaction and communication This report develops the motivations for gestural input and surveys current gesture recognition techniques A recognition technique under development at TCD as part of the GLAD IN ART EP project is also introduced},
	urldate = {2023-07-02},
	author = {Watson, R.},
	year = {1993},
	keywords = {type:survey},
	annote = {[TLDR] This report develops the motivations for gestural input and surveys current gesture recognition techniques.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/EHEKTWGH/Watson - 1993 - A Survey of Gesture Recognition Techniques.pdf:application/pdf},
}

@article{fels_glove-talkii-neural-network_1998,
	title = {Glove-{TalkII}-a neural-network interface which maps gestures to parallel formant speech synthesizer controls},
	volume = {9},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/655042/},
	doi = {10.1109/72.655042},
	abstract = {Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.},
	number = {1},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Neural Networks},
	author = {Fels, S.S. and Hinton, G.E.},
	month = jan,
	year = {1998},
	note = {73 citations (Crossref) [2023-07-02]},
	keywords = {tech:accelerometer, tech:flex, model:ffnn},
	pages = {205--212},
	annote = {[TLDR] One subject has trained to speak intelligibly with Glove-TalkII, a system which translates hand gestures to speech through an adaptive interface that uses several input devices, a parallel formant speech synthesizer, and three neural networks.},
}

@article{sturman_design_1993,
	title = {A design method for “whole-hand” human-computer interaction},
	volume = {11},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/159161.159159},
	doi = {10.1145/159161.159159},
	abstract = {A disciplined investigation of “whole-hand interfaces (often glove based, currently) and their appropriate use for the control of complex task domains is embodied by the design method for whole-hand input. This is a series of procedures—including a common basis for the description, design, and evaluation of whole-hand input, together with an accompanying taxonomy—that enumerates key issues and points for consideration in the development of whole-hand input. The method helps designers focus on task requirements, isolate problem areas, and choose appropriate whole-hand input strategies for their specified tasks. Several experiments were conducted to validate and demonstrate the use of the design method. The results of the experiments are summarized and discussed.},
	language = {en},
	number = {3},
	urldate = {2023-07-02},
	journal = {ACM Transactions on Information Systems},
	author = {Sturman, David J. and Zeltzer, David},
	month = jul,
	year = {1993},
	note = {30 citations (Crossref) [2023-07-02]},
	keywords = {evaluation-of-whole-hand-input, type:seminal},
	pages = {219--238},
	annote = {[TLDR] This is a series of procedures that enumerates key issues and points for consideration in the development of whole-hand input that helps designers focus on task requirements, isolate problem areas, and choose appropriate whole- hand input strategies for their specified tasks.},
	file = {Full Text:/Users/brk/Zotero/storage/65J52Q6P/Sturman and Zeltzer - 1993 - A design method for “whole-hand” human-computer in.pdf:application/pdf},
}

@inproceedings{hernandez-rebollar_acceleglove_2002,
	address = {San Antonio Texas},
	title = {The {AcceleGlove}: a whole-hand input device for virtual reality},
	isbn = {978-1-58113-525-1},
	shorttitle = {The {AcceleGlove}},
	url = {https://dl.acm.org/doi/10.1145/1242073.1242272},
	doi = {10.1145/1242073.1242272},
	abstract = {We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {{ACM} {SIGGRAPH} 2002 conference abstracts and applications},
	publisher = {ACM},
	author = {Hernandez-Rebollar, Jose L. and Kyriakopoulos, Nicholas and Lindeman, Robert W.},
	month = jul,
	year = {2002},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:american-sl, classes:10-29, hardware:accelerometers, hardware:fabricless},
	pages = {259--259},
	annote = {[TLDR] The AcceleGlove is presented, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the ASL alphabet.},
}

@article{zhang_egogesture_2018,
	title = {{EgoGesture}: {A} {New} {Dataset} and {Benchmark} for {Egocentric} {Hand} {Gesture} {Recognition}},
	volume = {20},
	issn = {1520-9210, 1941-0077},
	shorttitle = {{EgoGesture}},
	url = {https://ieeexplore.ieee.org/document/8299578/},
	doi = {10.1109/TMM.2018.2808769},
	abstract = {Gesture is a natural interface in human–computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24 000 gesture samples and 3 000 000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
	number = {5},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Yifan and Cao, Congqi and Cheng, Jian and Lu, Hanqing},
	month = may,
	year = {2018},
	note = {143 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:dataset, classes:50-99},
	pages = {1038--1050},
	annote = {[TLDR] A new benchmark dataset named EgoGesture is introduced with sufficient size, variation, and reality to be able to train deep neural networks and provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
}

@article{atzori_electromyography_2014,
	title = {Electromyography data for non-invasive naturally-controlled robotic hand prostheses},
	volume = {1},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201453},
	doi = {10.1038/sdata.2014.53},
	abstract = {Abstract
            Recent advances in rehabilitation robotics suggest that it may be possible for hand-amputated subjects to recover at least a significant part of the lost hand functionality. The control of robotic prosthetic hands using non-invasive techniques is still a challenge in real life: myoelectric prostheses give limited control capabilities, the control is often unnatural and must be learned through long training times. Meanwhile, scientific literature results are promising but they are still far from fulfilling real-life needs. This work aims to close this gap by allowing worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark scientific database. The database is targeted at studying the relationship between surface electromyography, hand kinematics and hand forces, with the final goal of developing non-invasive, naturally controlled, robotic hand prostheses. The validation section verifies that the data are similar to data acquired in real-life conditions, and that recognition of different hand tasks by applying state-of-the-art signal features and machine-learning algorithms is possible.},
	language = {en},
	number = {1},
	urldate = {2023-07-02},
	journal = {Scientific Data},
	author = {Atzori, Manfredo and Gijsberts, Arjan and Castellini, Claudio and Caputo, Barbara and Hager, Anne-Gabrielle Mittaz and Elsig, Simone and Giatsidis, Giorgio and Bassetto, Franco and Müller, Henning},
	month = dec,
	year = {2014},
	note = {548 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, read-priority-1, app:robot-control, type:dataset},
	pages = {140053},
	annote = {[TLDR] This work aims to close this gap by allowing worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark scientific database, with the final goal of developing non-invasive, naturally controlled, robotic hand prostheses.},
	file = {Full Text:/Users/brk/Zotero/storage/46J7VGGV/Atzori et al. - 2014 - Electromyography data for non-invasive naturally-c.pdf:application/pdf},
}

@article{atzori_ninapro_2015,
	title = {The {Ninapro} database: {A} resource for {sEMG} naturally controlled robotic hand prosthetics},
	shorttitle = {The {Ninapro} database},
	url = {http://ieeexplore.ieee.org/document/7320041/},
	doi = {10.1109/EMBC.2015.7320041},
	abstract = {The dexterous natural control of robotic prosthetic hands with non-invasive techniques is still a challenge: surface electromyography gives some control capabilities but these are limited, often not natural and require long training times; the application of pattern recognition techniques recently started to be applied in practice. While results in the scientific literature are promising they have to be improved to reach the real needs. The Ninapro database aims to improve the field of naturally controlled robotic hand prosthetics by permitting to worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark database. Currently, the Ninapro database includes data from 67 intact subjects and 11 amputated subject performing approximately 50 different movements. The data are aimed at permitting the study of the relationships between surface electromyography, kinematics and dynamics. The Ninapro acquisition protocol was created in order to be easy to be reproduced. Currently, the number of datasets included in the database is increasing thanks to the collaboration of several research groups.},
	urldate = {2023-07-02},
	journal = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	author = {Atzori, Manfredo and Muller, Henning},
	month = aug,
	year = {2015},
	note = {37 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
ISBN: 9781424492718
Place: Milan
Publisher: IEEE},
	keywords = {tech:emg, type:dataset},
	pages = {7151--7154},
	annote = {[TLDR] The Ninapro database aims to improve the field of naturally controlled robotic hand prosthetics by permitting to worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark database.},
}

@article{liu_uwave_2009,
	title = {{uWave}: {Accelerometer}-based personalized gesture recognition and its applications},
	volume = {5},
	issn = {15741192},
	shorttitle = {{uWave}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119209000674},
	doi = {10.1016/j.pmcj.2009.07.007},
	abstract = {Semantic Scholar extracted view of "uWave: Accelerometer-based personalized gesture recognition and its applications" by Jiayang Liu et al.},
	language = {en},
	number = {6},
	urldate = {2023-07-02},
	journal = {Pervasive and Mobile Computing},
	author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
	month = dec,
	year = {2009},
	note = {739 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:uwave, participants:8},
	pages = {657--675},
	annote = {@liu2009 presented an efficient gesture recognition method based on Dynamic Time Warping called uWave. uWave accepts sensor readings from a single 3-axis accelerometer as input, and notably requires a single training sample per unique gesture. They learnt eight full hand dynamic gesture patterns with data recorded from multiple users.

},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/H47GNEKC/4912759.html:text/html;Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:/Users/brk/Zotero/storage/UMP9M3LQ/Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:application/pdf},
}

@inproceedings{buchmann_fingartips_2004,
	address = {Singapore},
	title = {{FingARtips}: gesture based direct manipulation in {Augmented} {Reality}},
	isbn = {978-1-58113-883-2},
	shorttitle = {{FingARtips}},
	url = {https://dl.acm.org/doi/10.1145/988834.988871},
	doi = {10.1145/988834.988871},
	abstract = {This paper presents a technique for natural, fingertip-based interaction with virtual objects in Augmented Reality (AR) environments. We use image processing software and finger- and hand-based fiducial markers to track gestures from the user, stencil buffering to enable the user to see their fingers at all times, and fingertip-based haptic feedback devices to enable the user to feel virtual objects. Unlike previous AR interfaces, this approach allows users to interact with virtual content using natural hand gestures. The paper describes how these techniques were applied in an urban planning interface, and also presents preliminary informal usability results.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the 2nd international conference on {Computer} graphics and interactive techniques in {Australasia} and {South} {East} {Asia}},
	publisher = {ACM},
	author = {Buchmann, Volkert and Violich, Stephen and Billinghurst, Mark and Cockburn, Andy},
	month = jun,
	year = {2004},
	note = {290 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, app:augmented-reality, fiducials},
	pages = {212--221},
	annote = {[TLDR] This paper uses image processing software and finger- and hand-based fiducial markers to track gestures from the user, stencil buffering to enable the user to see their fingers at all times, and fingertip-based haptic feedback devices to enableThe user to feel virtual objects.},
	file = {Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf:/Users/brk/Zotero/storage/ZSNVZFEU/Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf:application/pdf},
}

@inproceedings{bolt_put-that-there_1980,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '80},
	title = {“{Put}-that-there”: {Voice} and gesture at the graphics interface},
	isbn = {978-0-89791-021-7},
	shorttitle = {“{Put}-that-there”},
	url = {https://dl.acm.org/doi/10.1145/800250.807503},
	doi = {10.1145/800250.807503},
	abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality. The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 7th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Bolt, Richard A.},
	month = jul,
	year = {1980},
	note = {1893 citations (Semantic Scholar/DOI) [2023-07-03]},
	keywords = {Gesture, Graphics, Graphics interface, Man-machine interfaces, Space sensing, Spatial data management, Speech input, Voice input},
	pages = {262--270},
	file = {Full Text PDF:/Users/brk/Zotero/storage/8U4B3N9X/Bolt - 1980 - “Put-that-there” Voice and gesture at the graphic.pdf:application/pdf},
}

@article{neto_highlevel_2010,
	title = {High‐level programming and control for industrial robotics: using a hand‐held accelerometer‐based input device for gesture and posture recognition},
	volume = {37},
	issn = {0143-991X},
	shorttitle = {High‐level programming and control for industrial robotics},
	url = {https://www.emerald.com/insight/content/doi/10.1108/01439911011018911/full/html},
	doi = {10.1108/01439911011018911},
	abstract = {Purpose
              Most industrial robots are still programmed using the typical teaching process, through the use of the robot teach pendant. This is a tedious and time‐consuming task that requires some technical expertise, and hence new approaches to robot programming are required. The purpose of this paper is to present a robotic system that allows users to instruct and program a robot with a high‐level of abstraction from the robot language.


              Design/methodology/approach
              The paper presents in detail a robotic system that allows users, especially non‐expert programmers, to instruct and program a robot just showing it what it should do, in an intuitive way. This is done using the two most natural human interfaces (gestures and speech), a force control system and several code generation techniques. Special attention will be given to the recognition of gestures, where the data extracted from a motion sensor (three‐axis accelerometer) embedded in the Wii remote controller was used to capture human hand behaviours. Gestures (dynamic hand positions) as well as manual postures (static hand positions) are recognized using a statistical approach and artificial neural networks.


              Findings
              It is shown that the robotic system presented is suitable to enable users without programming expertise to rapidly create robot programs. The experimental tests showed that the developed system can be customized for different users and robotic platforms.


              Research limitations/implications
              The proposed system is tested on two different robotic platforms. Since the options adopted are mainly based on standards, it can be implemented with other robot controllers without significant changes. Future work will focus on improving the recognition rate of gestures and continuous gesture recognition.


              Practical implications
              The key contribution of this paper is that it offers a practical method to program robots by means of gestures and speech, improving work efficiency and saving time.


              Originality/value
              This paper presents an alternative to the typical robot teaching process, extending the concept of human‐robot interaction and co‐worker scenario. Since most companies do not have engineering resources to make changes or add new functionalities to their robotic manufacturing systems, this system constitutes a major advantage for small‐ to medium‐sized enterprises.},
	language = {en},
	number = {2},
	urldate = {2023-07-03},
	journal = {Industrial Robot: An International Journal},
	author = {Neto, Pedro and Norberto Pires, J. and Paulo Moreira, A.},
	month = mar,
	year = {2010},
	note = {95 citations (Semantic Scholar/DOI) [2023-07-03]},
	keywords = {hardware:wii, movement:static, movement:dynamic, app:robotics},
	pages = {137--147},
	annote = {[TLDR] A robotic system that allows users to instruct and program a robot with a high‐level of abstraction from the robot language, done using the two most natural human interfaces, a force control system and several code generation techniques.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/GM28KZG6/Neto et al. - 2010 - High‐level programming and control for industrial .pdf:application/pdf},
}

@inproceedings{freeman_orientation_1995,
	title = {Orientation {Histograms} for {Hand} {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth/2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2},
	abstract = {We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or {\textbackslash}dynamic gestures", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,},
	urldate = {2023-07-03},
	author = {Freeman, W. and Roth, Michal},
	year = {1995},
	keywords = {classes:10, app:gaming, type:seminal, technique:histogram-oriented-gradients},
	annote = {[TLDR] A method to recognize hand gestures, based on a pattern recognition technique developed by McConnell employing histograms of local orientation, which is simple and fast to compute, and which can distinguish a small vocabulary of about 10 hand gestures.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/N6NR3A34/Freeman and Roth - 1995 - Orientation Histograms for Hand Gesture Recognitio.pdf:application/pdf},
}

@article{zaman_khan_hand_2012,
	title = {Hand {Gesture} {Recognition}: {A} {Literature} {Review}},
	volume = {3},
	issn = {09762191},
	shorttitle = {Hand {Gesture} {Recognition}},
	url = {http://www.airccse.org/journal/ijaia/papers/3412ijaia12.pdf},
	doi = {10.5121/ijaia.2012.3412},
	abstract = {Hand gesture recognition system received great attention in the recent few years because of its manifoldness applications and the ability to interact with machine efficiently through human computer interaction. In this paper a survey of recent hand gesture recognition systems is presented. Key issues of hand gesture recognition system are presented with challenges of gesture system. Review methods of recent postures and gestures recognition system presented as well. Summary of research results of hand gesture methods, databases, and comparison between main gesture recognition phases are also given. Advantages and drawbacks of the discussed systems are explained finally.},
	number = {4},
	urldate = {2023-07-03},
	journal = {International Journal of Artificial Intelligence \& Applications},
	author = {Zaman Khan, Rafiqul},
	month = jul,
	year = {2012},
	note = {165 citations (Semantic Scholar/DOI) [2023-07-03]},
	keywords = {model:hmm, tech:rgb, model:nn, type:survey, model:fuzzy},
	pages = {161--174},
	file = {Full Text:/Users/brk/Zotero/storage/LTRX4QL9/Zaman Khan - 2012 - Hand Gesture Recognition A Literature Review.pdf:application/pdf},
}

@article{viterbi_error_1967,
	title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
	volume = {13},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/1054010/},
	doi = {10.1109/TIT.1967.1054010},
	abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R\_\{0\} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R\_\{0\} and whose performance bears certain similarities to that of sequential decoding algorithms.},
	number = {2},
	urldate = {2023-07-11},
	journal = {IEEE Transactions on Information Theory},
	author = {Viterbi, A.},
	month = apr,
	year = {1967},
	note = {5898 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, type:seminal, viterbi},
	pages = {260--269},
	annote = {[TLDR] The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R\_\{0\} and whose performance bears certain similarities to that of sequential decoding algorithms.},
}

@article{yamato_recognizing_1992,
	title = {Recognizing human action in time-sequential images using hidden {Markov} model},
	url = {http://ieeexplore.ieee.org/document/223161/},
	doi = {10.1109/CVPR.1992.223161},
	abstract = {A human action recognition method based on a hidden Markov model (HMM) is proposed. It is a feature-based bottom-up approach that is characterized by its learning capability and time-scale invariability. To apply HMMs, one set of time-sequential images is transformed into an image feature vector sequence, and the sequence is converted into a symbol sequence by vector quantization. In learning human action categories, the parameters of the HMMs, one per category, are optimized so as to best describe the training sequences from the category. To recognize an observed sequence, the HMM which best matches the sequence is chosen. Experimental results for real time-sequential images of sports scenes show recognition rates higher than 90\%. The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	urldate = {2023-07-11},
	journal = {Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Yamato, J. and Ohya, J. and Ishii, K.},
	year = {1992},
	note = {610 citations (Crossref) [2023-07-11]
1566 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
ISBN: 9780818628559
Place: Champaign, IL, USA
Publisher: IEEE Comput. Soc. Press},
	keywords = {model:hmm, tech:rgb, type:seminal, from:cite.bib, model:vector-quantization},
	pages = {379--385},
	annote = {Probably the first paper to use HMMs for human activity recognition

},
	annote = {[TLDR] The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.},
	annote = {[TLDR] The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.},
	file = {Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf:/Users/brk/Zotero/storage/M2J27ASX/Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf:application/pdf},
}

@inproceedings{kanade_linguistic_2004,
	address = {Berlin, Heidelberg},
	title = {A {Linguistic} {Feature} {Vector} for the {Visual} {Interpretation} of {Sign} {Language}},
	volume = {3021},
	isbn = {978-3-540-21984-2 978-3-540-24670-1},
	url = {http://link.springer.com/10.1007/978-3-540-24670-1_30},
	doi = {10.1007/978-3-540-24670-1_30},
	abstract = {This paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data. Key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion. This high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans. Moreover, such a description broadly generalises temporal activities naturally overcoming variability of people and environments. A second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of Markov chains combined with Independent Component Analysis. We demonstrate classification rates as high as 97.67\% for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required.},
	language = {en},
	urldate = {2023-07-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Bowden, Richard and Windridge, David and Kadir, Timor and Zisserman, Andrew and Brady, Michael},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Pajdla, Tomás and Matas, Jiří},
	year = {2004},
	doi = {10.1007/978-3-540-24670-1_30},
	note = {182 citations (Semantic Scholar/DOI) [2023-07-11]
Book Title: Computer Vision - ECCV 2004
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, app:sign-language, classes:43},
	pages = {390--401},
	annote = {[TLDR] A novel approach to sign language recognition that provides extremely high classification rates on minimal training data using only single instance training outperforming previous approaches where thousands of training examples are required.},
	file = {Full Text:/Users/brk/Zotero/storage/RZMUR5ET/Bowden et al. - 2004 - A Linguistic Feature Vector for the Visual Interpr.pdf:application/pdf},
}

@inproceedings{bowden_vision_2003,
	title = {Vision based {Interpretation} of {Natural} {Sign} {Languages}},
	url = {https://www.semanticscholar.org/paper/Vision-based-Interpretation-of-Natural-Sign-Bowden-Zisserman/86e9d0c4d14456932043fa0fb22f39e62f2de688},
	abstract = {This manuscript outlines our current demonstration system for translating visual Sign to written text. The system is based around a broad description of scene activity that naturally generalizes, reducing training requirements and allowing the knowledge base to be explicitly stated. This allows the same system to be used for different sign languages requiring only a change of the knowledge base.},
	urldate = {2023-07-11},
	author = {Bowden, R. and Zisserman, Andrew and Kadir, T. and Brady, M.},
	month = apr,
	year = {2003},
	keywords = {tech:rgb, app:sign-language},
	annote = {[TLDR] This manuscript outlines the current demonstration system for translating visual Sign to written text based around a broad description of scene activity that naturally generalizes, reducing training requirements and allowing the knowledge base to be explicitly stated.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/BRKUR3W3/Bowden et al. - 2003 - Vision based Interpretation of Natural Sign Langua.pdf:application/pdf},
}

@inproceedings{kadir_minimal_2004,
	address = {Kingston},
	title = {Minimal {Training}, {Large} {Lexicon}, {Unconstrained} {Sign} {Language} {Recognition}},
	isbn = {978-1-901725-25-4},
	url = {http://www.bmva.org/bmvc/2004/papers/paper_265.html},
	doi = {10.5244/C.18.96},
	abstract = {This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92\% for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2004},
	publisher = {British Machine Vision Association},
	author = {Kadir, T. and Bowden, R. and Ong, E. J. and Zisserman, A.},
	year = {2004},
	note = {105 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, app:sign-language, classes:164},
	pages = {96.1--96.10},
	annote = {[TLDR] A flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign is presented.},
	file = {Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf:/Users/brk/Zotero/storage/MYCSBHHF/Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf:application/pdf},
}

@article{yoon_hand_2001,
	title = {Hand gesture recognition using combined features of location, angle and velocity},
	volume = {34},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320300000960},
	doi = {10.1016/S0031-3203(00)00096-0},
	abstract = {The use of hand gesture provides an attractive alternative to cumbersome interface devices for human–computer interaction (HCI). Many hand gesture recognition methods using visual analysis have been proposed: syntactical analysis, neural networks, the hidden Markov model (HMM). In our research, an HMM is proposed for various types of hand gesture recognition. In the preprocessing stage, this approach consists of three different procedures for hand localization, hand tracking and gesture spotting. The hand location procedure detects hand candidate regions on the basis of skin-color and motion. The hand tracking algorithm finds the centroids of the moving hand regions, connects them, and produces a hand trajectory. The gesture spotting algorithm divides the trajectory into real and meaningless segments. To construct a feature database, this approach uses a combined and weighted location, angle and velocity feature codes, and employs a k-means clustering algorithm for the HMM codebook. In our experiments, 2400 trained gestures and 2400 untrained gestures are used for training and testing, respectively. Those experimental results demonstrate that the proposed approach yields a satisfactory and higher recognition rate for user images of different hand size, shape and skew angle.},
	language = {en},
	number = {7},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Yoon, Ho-Sub and Soh, Jung and Bae, Younglae J. and Seung Yang, Hyun},
	month = jan,
	year = {2001},
	note = {272 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm},
	pages = {1491--1501},
}

@inproceedings{binh_real-time_2005,
	title = {Real-{Time} {Hand} {Tracking} and {Gesture} {Recognition} {System}},
	url = {https://www.semanticscholar.org/paper/Real-Time-Hand-Tracking-and-Gesture-Recognition-Binh-Shuichi/04cad56dd4fda01a3fa03a47dca2fa11d02695bf},
	abstract = {In this paper, we introduce a hand gesture recognition system to recognize real time gesture in unconstrained environments. The system consists of three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). We have used a Kalman filter and hand blobs analysis for hand tracking to obtain motion descriptors and hand region. It is fairy robust to background cluster and uses skin color for hand gesture tracking and recognition. Furthermore, there have been proposed to improve the overall performance of the approach: (1) Intelligent selection of training images and (2) Adaptive threshold gesture to remove non-gesture pattern that helps to qualify an input pattern as a gesture. A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed. In the experiments, we have tested our system to vocabulary of 36 gestures including the America sign language (ASL) letter spelling alphabet and digits, and results effectiveness of the approach.},
	urldate = {2023-07-11},
	author = {Binh, N. and Shuichi, Enokida and Ejima, T.},
	year = {2005},
	keywords = {model:hmm, tech:rgb, app:sign-language, app:american-sl, model:kalman-filter, classes:36},
	annote = {[TLDR] A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed and there have been proposed to improve the overall performance of the approach.},
}

@article{elmezain_real-time_2008,
	title = {Real-{Time} {Capable} {System} for {Hand} {Gesture} {Recognition} {Using} {Hidden} {Markov} {Models} in {Stereo} {Color} {Image} {Sequences}},
	url = {https://www.semanticscholar.org/paper/Real-Time-Capable-System-for-Hand-Gesture-Using-in-Elmezain-Al-Hamadi/aa17055855518ac47031bc509f5b7aa1b82fdcff},
	abstract = {This paper proposes a system to recognize the alphabets and numbers in real time from color image sequences by the motion trajectory of a single hand using Hidden Markov Models (HMM). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, YCbCr color space and depth information are used to detect hands and face in connection with morphological operation where Gaussian Mixture Model (GMM) is used for computing the skin probability. After the hand is detected and the centroid point of the hand region is determined, the tracking will take place in the further steps to determine the hand motion trajectory by using a search area around the hand region. In the feature extraction stage, the orientation is determined between two consecutive points from hand motion trajectory and then it is quantized to give a discrete vector that is used as input to HMM. The final stage so-called classification, Baum-Welch algorithm (BW) is used to do a full train for HMM parameters. The gesture of alphabets and numbers is recognized by using Left-Right Banded model (LRB) in conjunction with Forward algorithm. In our experiment, 720 trained gestures are used for training and also 360 tested gestures for testing. Our system recognizes the alphabets from A to Z and numbers from 0 to 9 and achieves an average recognition rate of 94.72\%.},
	urldate = {2023-07-11},
	journal = {J. WSCG},
	author = {Elmezain, M. and Al-Hamadi, A. and Michaelis, B.},
	year = {2008},
	keywords = {model:hmm, tech:rgb, model:gmm, classes:36},
	annote = {[TLDR] This paper proposes a system to recognize the alphabets and numbers in real time from color image sequences by the motion trajectory of a single hand using Hidden Markov Models (HMM).},
}

@article{elmezain_gesture_2007,
	title = {Gesture {Recognition} for {Alphabets} from {Hand} {Motion} {Trajectory} {Using} {Hidden} {Markov} {Models}},
	url = {http://ieeexplore.ieee.org/document/4458209/},
	doi = {10.1109/ISSPIT.2007.4458209},
	abstract = {This paper describes a method to recognize the alphabets from a single hand motion using Hidden Markov Models (HMM). In our method, gesture recognition for alphabets is based on three main stages; preprocessing, feature extraction and classification. In preprocessing stage, color and depth information are used to detect both hands and face in connection with morphological operation. After the detection of the hand, the tracking will take place in further step in order to determine the motion trajectory so-called gesture path. The second stage, feature extraction enhances the gesture path which gives us a pure path and also determines the orientation between the center of gravity and each point in a pure path. Thereby, the orientation is quantized to give a discrete vector that used as input to HMM. In the final stage, the gesture of alphabets is recognized by using Left-Right Banded model (LRB) in conjunction with Baum-Welch algorithm (BW) for training the parameters of HMM. Therefore, the best path is obtained by Viterbi algorithm using a gesture database. In our experiment, 520 trained gestures are used for training and also 260 tested gestures for testing. Our method recognizes the alphabets from A to Z and achieves an average recognition rate of 92.3\%.},
	urldate = {2023-07-11},
	journal = {2007 IEEE International Symposium on Signal Processing and Information Technology},
	author = {Elmezain, Mahmoud and Al-Hamadi, Ayoub and Krell, Gerald and El-Etriby, Sherif and Michaelis, Bernd},
	month = dec,
	year = {2007},
	note = {49 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2007 IEEE International Symposium on Signal Processing and Information Technology
ISBN: 9781424418343 9781424418350
Place: Giza, Egypt
Publisher: IEEE},
	keywords = {model:hmm, tech:rgbd, classes:26},
	pages = {1192--1197},
	annote = {[TLDR] This paper describes a method to recognize the alphabets from a single hand motion using Hidden Markov Models (HMM) and achieves an average recognition rate of 92.3\%.},
}

@article{elmezain_hand_2009,
	title = {Hand {Gesture} {Recognition} {Based} on {Combined} {Features} {Extraction}},
	url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Based-on-Combined-Features-Elmezain-Al-Hamadi/92a80f36e199e0ddf73ba06f81b31ef60efcf5f8},
	abstract = {Hand gesture is an active area of research in the vision community, mainly for the purpose of sign language recognition and Human Computer Interaction. In this paper, we propose a system to recognize alphabet characters (A-Z) and numbers (0-9) in real-time from stereo color image sequences using Hidden Markov Models (HMMs). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, color and 3D depth map are used to detect hands where the hand trajectory will take place in further step using Mean-shift algorithm and Kalman filter. In the feature extraction stage, 3D combined features of location, orientation and velocity with respected to Cartesian systems are used. And then, k-means clustering is employed for HMMs codeword. The final stage so-called classification, BaumWelch algorithm is used to do a full train for HMMs parameters. The gesture of alphabets and numbers is recognized using Left-Right Banded model in conjunction with Viterbi algorithm. Experimental results demonstrate that, our system can successfully recognize hand gestures with 98.33\% recognition rate. Keywords—Gesture Recognition, Computer Vision \& Image Processing, Pattern Recognition.},
	urldate = {2023-07-11},
	journal = {International Journal of Electrical and Computer Engineering},
	author = {Elmezain, M. and Al-Hamadi, A. and Michaelis, B.},
	month = dec,
	year = {2009},
	keywords = {model:hmm, tech:rgb, model:kalman-filter, model:kmeans},
	annote = {[TLDR] A system to recognize alphabet characters (A-Z) and numbers (0-9) in real-time from stereo color image sequences using Hidden Markov Models (HMMs) based on automatic segmentation and preprocessing of the hand regions, feature extraction and classification.},
	file = {Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf:/Users/brk/Zotero/storage/GP7SDB8D/Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf:application/pdf},
}

@article{yang_dynamic_2012,
	title = {Dynamic hand gesture recognition using hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/6295092/},
	doi = {10.1109/ICCSE.2012.6295092},
	abstract = {Hand gesture has become a powerful means for human-computer interaction. Traditional gesture recognition just consider hand trajectory. For some specific applications, such as virtual reality, more natural gestures are needed, which are complex and contain movement in 3-D space. In this paper, we introduce an HMM-based method to recognize complex single hand gestures. Gesture images are gained by a common web camera. Skin color is used to segment hand area from the image to form a hand image sequence. Then we put forward a state-based spotting algorithm to split continuous gestures. After that, feature extraction is executed on each gesture. Features used in the system contain hand position, velocity, size, and shape. We raise a data aligning algorithm to align feature vector sequences for training. Then an HMM is trained alone for each gesture. The recognition results demonstrate that our methods are effective and accurate.},
	urldate = {2023-07-11},
	journal = {2012 7th International Conference on Computer Science \& Education (ICCSE)},
	author = {Yang, Zhong and Li, Yi and Chen, Weidong and Zheng, Yang},
	month = jul,
	year = {2012},
	note = {68 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2012 7th International Conference on Computer Science \& Education (ICCSE 2012)
ISBN: 9781467302425 9781467302418 9781467302401
Place: Melbourne, Australia
Publisher: IEEE},
	keywords = {model:hmm, tech:rgb},
	pages = {360--365},
	annote = {[TLDR] This paper introduces an HMM-based method to recognize complex single hand gestures, which contains hand position, velocity, size, and shape and raises a data aligning algorithm to align feature vector sequences for training.},
}

@article{ramamoorthy_recognition_2003,
	title = {Recognition of dynamic hand gestures},
	volume = {36},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320303000426},
	doi = {10.1016/S0031-3203(03)00042-6},
	abstract = {This paper is concerned with the problem of recognition of dynamic hand gestures. We have considered gestures which are sequences of distinct hand poses. In these gestures hand poses can undergo motion and discrete changes. However, continuous deformations of the hand shapes are not permitted. We have developed a recognition engine which can reliably recognize these gestures despite individual variations. The engine also has the ability to detect start and end of gesture sequences in an automated fashion. The recognition strategy uses a combination of static shape recognition (performed using contour discriminant analysis), Kalman filter based hand tracking and a HMM based temporal characterization scheme. The system is fairly robust to background clutter and uses skin color for static shape recognition and tracking. A real time implementation on standard hardware is developed. Experimental results establish the effectiveness of the approach.},
	language = {en},
	number = {9},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Ramamoorthy, Aditya and Vaswani, Namrata and Chaudhury, Santanu and Banerjee, Subhashis},
	month = sep,
	year = {2003},
	note = {169 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, movement:dynamic, app:robotics},
	pages = {2069--2081},
}

@inproceedings{davis_visual_1994,
	title = {Visual gesture recognition},
	volume = {141},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941058},
	doi = {10.1049/ip-vis:19941058},
	abstract = {Presents a method for recognising human-hand gestures using a model based approach. A finite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of left, right, up, down, grab, rotate, and stop.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {{IEE} {Proceedings} - {Vision}, {Image}, and {Signal} {Processing}},
	author = {Davis, J.},
	year = {1994},
	note = {272 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 1350245X
Issue: 2
Journal Abbreviation: IEE Proc., Vis. Image Process.},
	keywords = {model:fsm, type:seminal},
	pages = {101},
	annote = {[TLDR] A finite state machine is used to model four qualitatively distinct phases of a generic gesture and results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware.},
}

@article{pengyu_hong_gesture_2000,
	title = {Gesture modeling and recognition using finite state machines},
	url = {http://ieeexplore.ieee.org/document/840667/},
	doi = {10.1109/AFGR.2000.840667},
	abstract = {We propose a state-based approach to gesture learning and recognition. Using spatial clustering and temporal alignment, each gesture is defined to be an ordered sequence of states in spatial-temporal space. The 2D image positions of the centers of the head and both hands of the user are used as features; these are located by a color-based tracking method. From training data of a given gesture, we first learn the spatial information and then group the data into segments that are automatically aligned temporally. The temporal information is further integrated to build a finite state machine (FSM) recognizer. Each gesture has a FSM corresponding to it. The computational efficiency of the FSM recognizers allows us to achieve real-time on-line performance. We apply this technique to build an experimental system that plays a game of "Simon Says" with the user.},
	urldate = {2023-07-11},
	journal = {Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
	author = {{Pengyu Hong} and Turk, M. and Huang, T.S.},
	year = {2000},
	note = {276 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Fourth International Conference on Automatic Face and Gesture Recognition
ISBN: 9780769505800
Place: Grenoble, France
Publisher: IEEE Comput. Soc},
	keywords = {model:fsm},
	pages = {410--415},
	annote = {[TLDR] A state-based approach to gesture learning and recognition is proposed, using spatial clustering and temporal alignment to build a finite state machine (FSM) recognizer.},
	file = {Submitted Version:/Users/brk/Zotero/storage/G3CD967K/Pengyu Hong et al. - 2000 - Gesture modeling and recognition using finite stat.pdf:application/pdf},
}

@article{yeasin_visual_2000,
	title = {Visual understanding of dynamic hand gestures},
	volume = {33},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320399001752},
	doi = {10.1016/S0031-3203(99)00175-2},
	abstract = {Semantic Scholar extracted view of "Visual understanding of dynamic hand gestures" by M. Yeasin et al.},
	language = {en},
	number = {11},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Yeasin, M. and Chaudhuri, S.},
	month = nov,
	year = {2000},
	note = {107 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:fsm, type:seminal},
	pages = {1805--1817},
}

@article{ming-hsuan_yang_recognizing_1999,
	title = {Recognizing hand gesture using motion trajectories},
	url = {http://ieeexplore.ieee.org/document/786979/},
	doi = {10.1109/CVPR.1999.786979},
	abstract = {We present an algorithm for extracting and classifying two-dimensional motion in an image sequence based on motion trajectories. First, a multiscale segmentation is performed to generate homogeneous regions in each frame. Regions between consecutive frames are then matched to obtain 2-view correspondences. Affine transformations are computed from each pair of corresponding regions to define pixel matches. Pixels matches over consecutive images pairs are concatenated to obtain pixel-level motion trajectories across the image sequence. Motion patterns are learned from the extracted trajectories using a time-delay neural network. We apply the proposed method to recognize 40 hand gestures of American Sign Language. Experimental results show that motion patterns in hand gestures can be extracted and recognized with high recognition rate using motion trajectories.},
	urldate = {2023-07-11},
	journal = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
	author = {{Ming-Hsuan Yang} and Ahuja, N.},
	year = {1999},
	note = {155 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
ISBN: 9780769501499
Place: Fort Collins, CO, USA
Publisher: IEEE Comput. Soc},
	keywords = {tech:rgb, app:sign-language, app:american-sl, model:nn, classes:40},
	pages = {466--472},
	annote = {[TLDR] Experimental results show that motion patterns in hand gestures can be extracted and recognized with high recognition rate using motion trajectories using a time-delay neural network.},
}

@article{yanmin_zhu_vision_2013,
	title = {Vision {Based} {Hand} {Gesture} {Recognition}},
	url = {http://ieeexplore.ieee.org/document/6519802/},
	doi = {10.1109/ICSS.2013.40},
	abstract = {With the development of ubiquitous computing, current user interaction approaches with keyboard, mouse and pen are not sufficient. Due to the limitation of these devices the useable command set is also limited. Direct use of hands as an input device is an attractive method for providing natural Human Computer Interaction which has evolved from text-based interfaces through 2D graphical-based interfaces, multimedia-supported interfaces, to fully fledged multi-participant Virtual Environment (VE) systems. Imagine the human-computer interaction of the future: A 3D- application where you can move and rotate objects simply by moving and rotating your hand - all without touching any input device. In this paper a review of vision based hand gesture recognition is presented. The existing approaches are categorized into 3D model based approaches and appearance based approaches, highlighting their advantages and shortcomings and identifying the open issues.},
	urldate = {2023-07-11},
	journal = {2013 International Conference on Service Sciences (ICSS)},
	author = {{Yanmin Zhu} and {Zhibo Yang} and {Bo Yuan}},
	month = apr,
	year = {2013},
	note = {28 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2013 International Conference on Service Sciences (ICSS 2013)
ISBN: 9781467362580 9780769549729
Place: Shenzhen
Publisher: IEEE},
	keywords = {tech:rgb, type:survey},
	pages = {260--265},
	annote = {[TLDR] In this paper a review of vision based hand gesture recognition is presented, the existing approaches are categorized into 3D model based approaches and appearance based approaches, highlighting their advantages and shortcomings and identifying the open issues.},
}

@inproceedings{mehra_survey_2013,
	title = {Survey on {Multiclass} {Classification} {Methods}},
	url = {https://www.semanticscholar.org/paper/Survey-on-Multiclass-Classification-Methods-Mehra-Gupta/f0fcf860031b356a3c68b330735634c00e5d7602},
	abstract = {Supervised learning is based on the target value or the desired outputs. Various successful techniques have been proposed to solve the problem in the binary classification case. The multiclass classification case is more delicate one. In this short survey we investigate the various techniques for solving the multiclass classification problem. Various authors and research modified the multiclass classification approach such as one against one, one against all and Directed Acyclic Graph (DAG) which creates many binary classifiers and combines their results to determine the class label of a test pixel. They also describe the various extensible methods that are extended from binary class to solve the multiclass problem and also explain the method in which the classes are arranged into a tree.},
	urldate = {2023-07-11},
	author = {Mehra, Neha and Gupta, Surendra},
	year = {2013},
	keywords = {classification, multiclass},
	annote = {[TLDR] In this short survey, the various techniques for solving the multiclass classification problem are investigated and the method in which the classes are arranged into a tree is explained.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/FPR3M2AA/Mehra and Gupta - 2013 - Survey on Multiclass Classification Methods.pdf:application/pdf},
}

@article{kopuklu_real-time_2019,
	title = {Real-time {Hand} {Gesture} {Detection} and {Classification} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8756576/},
	doi = {10.1109/FG.2019.8756576},
	abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04\% and 83.82\% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available1.},
	urldate = {2023-07-11},
	journal = {2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)},
	author = {Kopuklu, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
	month = may,
	year = {2019},
	note = {112 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)
ISBN: 9781728100890
Place: Lille, France
Publisher: IEEE},
	keywords = {model:cnn, tech:rgb, dataset:egogesture, dataset:nvidia-dynamic-hand-gesture},
	pages = {1--8},
	annote = {[TLDR] A hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach is proposed by proposing a lightweight CNN architecture to detect gestures and a classifier which is a deep CNN to classify the detected gestures.},
	file = {Submitted Version:/Users/brk/Zotero/storage/GE2CVBAG/Kopuklu et al. - 2019 - Real-time Hand Gesture Detection and Classificatio.pdf:application/pdf},
}

@article{lu_one-shot_2019,
	title = {One-shot learning hand gesture recognition based on modified 3d convolutional neural networks},
	volume = {30},
	issn = {0932-8092, 1432-1769},
	url = {http://link.springer.com/10.1007/s00138-019-01043-7},
	doi = {10.1007/s00138-019-01043-7},
	abstract = {Though deep neural networks have played a very important role in the field of vision-based hand gesture recognition, however, it is challenging to acquire large numbers of annotated samples to support its deep learning or training. Furthermore, in practical applications it often encounters some case with only one single sample for a new gesture class so that conventional recognition method cannot be qualified with a satisfactory classification performance. In this paper, the methodology of transfer learning is employed to build an effective network architecture of one-shot learning so as to deal with such intractable problem. Then some useful knowledge from deep training with big dataset of relative objects can be transferred and utilized to strengthen one-shot learning hand gesture recognition (OSLHGR) rather than to train a network from scratch. According to this idea a well-designed convolutional network architecture with deeper layers, C3D (Tran et al. in: ICCV, pp 4489–4497, 2015), is modified as an effective tool to extract spatiotemporal feature by deep learning. Then continuous fine-tune training is performed on a sample of new classes to complete one-shot learning. Moreover, the test of classification is carried out by Softmax classifier and geometrical classification based on Euclidean distance. Finally, a series of experiments and tests on two benchmark datasets, VIVA (Vision for Intelligent Vehicles and Applications) and SKIG (Sheffield Kinect Gesture) are conducted to demonstrate its state-of-the-art recognition accuracy of our proposed method. Meanwhile, a special dataset of gestures, BSG, is built using SoftKinetic DS325 for the test of OSLHGR, and a series of test results verify and validate its well classification performance and real-time response speed.},
	language = {en},
	number = {7-8},
	urldate = {2023-07-11},
	journal = {Machine Vision and Applications},
	author = {Lu, Zhi and Qin, Shiyin and Li, Xiaojie and Li, Lianwei and Zhang, Dinghao},
	month = oct,
	year = {2019},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, model:nn, dataset:viva, dataset:skig, technique:one-shot},
	pages = {1157--1180},
	annote = {[TLDR] The methodology of transfer learning is employed to build an effective network architecture of one-shot learning of hand gesture recognition (OSLHGR) rather than to train a network from scratch.},
}

@article{jie_huang_sign_2015,
	title = {Sign {Language} {Recognition} using {3D} convolutional neural networks},
	url = {https://ieeexplore.ieee.org/document/7177428},
	doi = {10.1109/ICME.2015.7177428},
	abstract = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
	urldate = {2023-07-11},
	journal = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
	author = {{Jie Huang} and {Wengang Zhou} and {Houqiang Li} and {Weiping Li}},
	month = jun,
	year = {2015},
	note = {199 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2015 IEEE International Conference on Multimedia and Expo (ICME)
ISBN: 9781479970827
Place: Turin, Italy
Publisher: IEEE},
	keywords = {hardware:kinect, model:cnn, tech:rgbd, app:sign-language, claims-to-be-first},
	pages = {1--6},
	annote = {[TLDR] A novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features is proposed.},
}

@article{zhang_gesture_2020,
	title = {Gesture recognition based on deep deformable {3D} convolutional neural networks},
	volume = {107},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320320302193},
	doi = {10.1016/j.patcog.2020.107416},
	abstract = {Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jester’s official leader-board until the submission time. The code and the trained models are released for better communication and future works.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Zhang, Yifan and Shi, Lei and Wu, Yi and Cheng, Ke and Cheng, Jian and Lu, Hanqing},
	month = nov,
	year = {2020},
	note = {23 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, movement:dynamic, dataset:egogesture, dataset:jester, dataset:chalearn-isogd},
	pages = {107416},
}

@article{baum_maximization_1970,
	title = {A {Maximization} {Technique} {Occurring} in the {Statistical} {Analysis} of {Probabilistic} {Functions} of {Markov} {Chains}},
	volume = {41},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177697196},
	doi = {10.1214/aoms/1177697196},
	abstract = {Semantic Scholar extracted view of "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains" by L. Baum et al.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {The Annals of Mathematical Statistics},
	author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
	month = feb,
	year = {1970},
	note = {4728 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, background, baum-welsch},
	pages = {164--171},
	file = {Full Text:/Users/brk/Zotero/storage/888D4KE6/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf:application/pdf},
}

@article{knuth_two_1992,
	title = {Two {Notes} on {Notation}},
	volume = {99},
	issn = {00029890},
	url = {https://www.jstor.org/stable/2325085?origin=crossref},
	doi = {10.2307/2325085},
	abstract = {Mathematical notation evolves like all languages do. As new experiments are made, we sometimes witness the survival of the fittest, sometimes the survival of the most familiar. A healthy conservatism keeps things from changing too rapidly; a healthy radicalism keeps things in tune with new theoretical emphases. Our mathematical language continues to improve, just as "the d-ism of Leibniz overtook the dotage of Newton" in past centuries [4, Chapter 4]. In 1970 I began teaching a class at Stanford University entitled Concrete Mathematics. The students and I studied how to manipulate formulas in continuous and discrete mathematics, and the problems we investigated were often inspired by new developments in computer science. As the years went by we began to see that a few changes in notational traditions would greatly facilitate our work. The notes from that class have recently been published in a book [15], and as I wrote the final drafts of that book I learned to my surprise that two of the notations we had been using were considerably more useful than I had previously realized. The ideas "clicked" so well, in fact, that I've decided to write this article, blatantly attempting to promote these notations among the mathematicians who have no use for [15]. I hope that within five years everybody will be able to use these notations in published papers without needing to explain what they mean. The notations I'm talking about are (1) Iverson's convention for characteristic functions; and (2) the "right" notation for Stirling numbers, at last.},
	number = {5},
	urldate = {2023-07-11},
	journal = {The American Mathematical Monthly},
	author = {Knuth, Donald E.},
	month = may,
	year = {1992},
	note = {458 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {background},
	pages = {403},
	annote = {[TLDR] Two notations are being promoted that everybody will be able to use in published papers without needing to explain what they mean: (1) Iverson's convention for characteristic functions; and the "right" notation for Stirling numbers, at last.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/QUCL5Y47/Knuth - 1992 - Two Notes on Notation.pdf:application/pdf},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2023-07-11},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	keywords = {background, batch-normalization},
	annote = {[TLDR] Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ST7LJZZJ/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/35016072},
	doi = {10.1038/35016072},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	language = {en},
	number = {6789},
	urldate = {2023-07-11},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	month = jun,
	year = {2000},
	note = {1114 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {background, relu},
	pages = {947--951},
	annote = {[TLDR] The model of cortical processing is presented as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
}

@inproceedings{hochreiter_gradient_2001,
	title = {Gradient {Flow} in {Recurrent} {Nets}: the {Difficulty} of {Learning} {Long}-{Term} {Dependencies}},
	shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
	url = {https://www.semanticscholar.org/paper/Gradient-Flow-in-Recurrent-Nets%3A-the-Difficulty-of-Hochreiter-Bengio/aed054834e2c696807cc8b227ac7a4197196e211},
	abstract = {D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6{\textbackslash}M[ X N@]\_{\textasciicircum}O{\textbackslash}`JaNcb V RcQ W d EGKeL({\textasciicircum}(QgfhKeLOE?i){\textasciicircum}(QSj ETNPfPQkRl[ V R)m"[ X {\textasciicircum}(KeLOEG{\textasciicircum} npo qarpo m"[ X {\textasciicircum}(KeLOEG{\textasciicircum}tsAu EGNPb V {\textasciicircum} v wyx zlwO\{({\textbar}(\vphantom{\{}\}{\textless}{\textasciitilde}OC\}((xp\{ay.{\textasciitilde}A\vphantom{\{}\}\_{\textasciitilde} Cl3\#{\textbar}{\textless}Azw\#{\textbar}l6 ({\textbar}  JpfhL XV EG{\textasciicircum}O QgJ  ETFOR] {\textasciicircum}O{\textbackslash}JNPb V RcQ X E)ETR 6EGKeLOETNcKMLOE F ETN V RcQgJp{\textasciicircum}({\textasciicircum}OE ZgZ E i {\textasciicircum}(Qkj EGNPfhQSRO E OE2m1Jp{\textasciicircum} RcNY E VZ sO! ¡ q.n sCD X KGKa8¢EG{\textasciicircum} RPNhE¤£ ¥¦Q ZgZ Es m§J{\textasciicircum} RPNO E VZ s( ̈ X  EG©\#EKas\# V {\textasciicircum} V  V s(H a «a¬3­ ®\#{\textbar}.Y ̄y\vphantom{\{}\} xa°OC\}l\{x  yxlY{\textasciitilde}3\{{\textbar}  ±2Pz   V J Z J U N V fhKTJp{\textasciicircum}(Q  ETFOR J{\textbackslash} D vYf3RPEGb ́f V {\textasciicircum}(§JpbF X RPETN@D KTQEG{\textasciicircum}(KTE i {\textasciicircum}(QSjpEGNPfhQSR4vμJ{\textbackslash} U¶Z JaNPEG{\textasciicircum}(K·E jYQ V (Q ̧D V {\textasciicircum} R V m V N3R V aOs\#1 o ¡Ga r U QNhE{\textasciicircum}OoTE1⁄4»,] R VZ vC1⁄2 3⁄4  x ± x  \#¿ \vphantom{\{}\}À 3t\vphantom{\{}\}lC\}2P\}{\textless}{\textasciitilde} ¬t[ X NPE{\textasciicircum}§D KeL(b ́Qg(L X ©yETN ]  DY]\_Á JNPfhJÃÂ Z j EToQ V a rpopo2Ä X  V {\textasciicircum}(J(sCD Å)QSRPoTEGN ZgV {\textasciicircum}( Æ \#{\textbar}\{3 ̄{\textbar}.(C\vphantom{\{}\}.C¿Y\}p Pzw},
	urldate = {2023-07-11},
	author = {Hochreiter, S. and Bengio, Yoshua},
	year = {2001},
	keywords = {vanishing-gradient},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
	language = {en},
	number = {2},
	urldate = {2023-07-11},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	note = {247 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {background, backprop},
	pages = {146--160},
}

@misc{geoffrey_hinton_coursera_2012,
	title = {Coursera: {Neural} {Networks} for {Machine} {Learning}},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	author = {{Geoffrey Hinton}},
	year = {2012},
	keywords = {background, technique:rmsprop},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Adaptive} {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan/413c1142de9d91804d6d11c67ff3fed59c9fc279},
	abstract = {This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight. We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	language = {en},
	urldate = {2023-07-11},
	keywords = {background, adagrad},
	file = {[PDF] Adaptive Subgradient Methods for Online Lear.pdf:/Users/brk/Zotero/storage/E6YNVHRW/[PDF] Adaptive Subgradient Methods for Online Lear.pdf:application/pdf;Snapshot:/Users/brk/Zotero/storage/GGADQGGW/413c1142de9d91804d6d11c67ff3fed59c9fc279.html:text/html},
}

@misc{adan_arriaga_charachorder_2022,
	title = {{CharaChorder} - {Type} at the speed of thought},
	url = {https://www.charachorder.com/},
	author = {{Adan Arriaga}},
	year = {2022},
	keywords = {type:product},
}

@inproceedings{wang_traffic_2008,
	title = {Traffic {Police} {Gesture} {Recognition} using {Accelerometers}},
	url = {https://www.semanticscholar.org/paper/Traffic-Police-Gesture-Recognition-using-Wang/59715d7a74376e7a21ada77f3a7af6b854a35545},
	abstract = {When an automatic traffic light system is not used due to too heavy traffic, the traffic would be controlled by traffic police gesture. This paper is about the design of a system so that the traffic lights can follow the traffic police gestures. To simplify the system, a unique mapping between the traffic police gestures and the orientation and movement of hands is defined. The hand motion characters are extracted by fixing a 3-axis accelerometer on the back of each hand. A 2level hierarchical classifier is used to recognize the gestures. First the gestures are categorized into three groups, according to the movement of each hand. Then a gesture is recognized by comparing it with the predefined templates. This real-time recognition algorithm is implemented by a micro-controller. It is envisaged that this will help drivers.},
	urldate = {2023-07-11},
	author = {Wang, Ben},
	year = {2008},
	keywords = {tech:accelerometer, app:traffic-police},
	annote = {[TLDR] A unique mapping between the traffic police gestures and the orientation and movement of hands is defined and a real-time recognition algorithm is implemented by a micro-controller that will help drivers.},
	annote = {[TLDR] A unique mapping between the traffic police gestures and the orientation and movement of hands is defined and a real-time recognition algorithm is implemented by a micro-controller that will help drivers.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/69LXNATR/Wang - 2008 - Traffic Police Gesture Recognition using Accelerom.pdf:application/pdf},
}

@article{zhang_stacked_2021,
	title = {Stacked {LSTM}-{Based} {Dynamic} {Hand} {Gesture} {Recognition} with {Six}-{Axis} {Motion} {Sensors}},
	url = {https://ieeexplore.ieee.org/document/9658659/},
	doi = {10.1109/SMC52423.2021.9658659},
	abstract = {Hand gesture recognition can be exploited to benefit ubiquitous applications using sensors. Currently, the inherent complexity of human physical activities makes it difficult to accurately recognize gestures with wearable sensors, especially in real time. To this end, a real-time hand gesture recognition system is presented in this paper. In particular, sliding window technology and y-axis threshold are used to detect intended gestures from a continuous data stream and then the segmented data are classified by applying a stacked Long Short-Term Memory (LSTM) model. After noise is removed, six-axis sensor data from wrist-worn devices are fed into the model without requiring feature engineering. We use twelve common hand gestures to evaluate the performance of our model. The experimental results demonstrate the feasibility of our proposed system with an accuracy of 99.8\% on average. Our approach allows for an accurate and nonindividual hand gesture recognition. It holds potential to be integrated into a smart watch or other wearable devices for intuitive human computer interaction.},
	urldate = {2023-07-11},
	journal = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Zhang, Yidi and Ran, Mengyuan and Liao, Jun and Su, Guoxin and Liu, Ming and Liu, Li},
	month = oct,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
ISBN: 9781665442077
Place: Melbourne, Australia
Publisher: IEEE},
	keywords = {tech:accelerometer, classes:12, model:lstm},
	pages = {2568--2575},
	annote = {[TLDR] A real-time hand gesture recognition system using sliding window technology and y-axis threshold are used to detect intended gestures from a continuous data stream and then the segmented data are classified by applying a stacked Long Short-Term Memory (LSTM) model.},
}

@article{yuan_hand_2020,
	title = {Hand {Gesture} {Recognition} using {Deep} {Feature} {Fusion} {Network} based on {Wearable} {Sensors}},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {https://ieeexplore.ieee.org/document/9158332/},
	doi = {10.1109/JSEN.2020.3014276},
	abstract = {Hand gesture recognition is an important way for human machine interaction, and it is widely used in many areas, such as health care, smart home, virtual reality as well as other areas. While many valuable efforts have been made, it still lacks efficient ways to capture fine grain hand gesture as well as track data of long distance dependency in complex gesture. In this paper, we firstly design a novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles. Secondly, an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures. In order to track detailed motion features, a convolutional neural network based feature fusion strategy is given to fuse data from multi-sensors by extracting both shallow and deep features. Moreover, a residual module is introduced to avoid over fitting and gradient vanishing during deepening the neural network. Thirdly, a long short term memory (LSTM) model with fused feature vector as input is introduced to classify complex hand motions into corresponding categories. Results of comprehensive experiments demonstrate that our work performs better than related algorithms, especially in America sign language (with the precise of 99.93\%) and Chinese sign language (with the precise of 96.1\%).},
	urldate = {2023-07-11},
	journal = {IEEE Sensors Journal},
	author = {Yuan, Guan and Liu, Xiao and Yan, Qiuyan and Qiao, Shaojie and Wang, Zhixiao and Yuan, Li},
	year = {2020},
	note = {32 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:flex, model:cnn, app:sign-language, app:chinese-sl, app:american-sl, model:lstm, model:nn},
	pages = {1--1},
	annote = {[TLDR] A novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles is designed and an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures.},
	annote = {[TLDR] A novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles is designed and an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures.},
}

@inproceedings{dept_of_biomedical_engineering_kyung_hee_university_republic_of_korea_recognition_2017,
	title = {Recognition of {Human} {Hand} {Activities} {Based} on a {Single} {Wrist} {IMU} {Using} {Recurrent} {Neural} {Networks}},
	volume = {6},
	url = {http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf},
	doi = {10.18178/ijpmbs.6.4.114-118},
	abstract = {Recognition of hand activities could provide new information towards daily human activity logging and gesture interface applications. However, there is a technical challenge due to delicate hand motions and complex movement contexts. In this work, we proposed hand activity recognition (HAR) based on a single inertial measurement unit (IMU) sensor at one wrist via deep learning recurrent neural network. The proposed HAR works directly with signals from a tri-axial accelerometer, gyroscope, and magnetometer sensors within one IMU. We evaluated the performance of our HAR with a public human hand activity database for six hand activities including Open Door, Close Door, Open Fridge, Close Fridge, Clean Table and Drink from Cup. Our results show an overall recognition accuracy of 80.09\% with discrete standard epochs and 74.92\% with noise-added epochs. With continuous time series epochs, the accuracy of 71.75\% was obtained. },
	urldate = {2023-07-11},
	booktitle = {International {Journal} of {Pharma} {Medicine} and {Biological} {Sciences}},
	author = {{Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea} and Rivera, Patricio and Valarezo, Edwin and Choi, Mun-Taek and Kim, Tae-Seong},
	year = {2017},
	note = {19 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 22785221
Issue: 4
Journal Abbreviation: IJPMBS},
	keywords = {classes:6, model:rnn, tech:imu},
	pages = {114--118},
	annote = {[TLDR] The proposed hand activity recognition (HAR) based on a single inertial measurement unit (IMU) sensor at one wrist via deep learning recurrent neural network works directly with signals from a tri-axial accelerometer, gyroscope, and magnetometer sensors within one IMU.},
	file = {Full Text:/Users/brk/Zotero/storage/38I9KL9J/Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea et al. - 2017 - Recognition of Human Hand Activities Based on a Si.pdf:application/pdf},
}

@article{amma_airwriting_2014,
	title = {Airwriting: a wearable handwriting recognition system},
	volume = {18},
	issn = {1617-4909, 1617-4917},
	shorttitle = {Airwriting},
	url = {http://link.springer.com/10.1007/s00779-013-0637-3},
	doi = {10.1007/s00779-013-0637-3},
	abstract = {We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {Personal and Ubiquitous Computing},
	author = {Amma, Christoph and Georgi, Marcus and Schultz, Tanja},
	month = jan,
	year = {2014},
	note = {0 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:accelerometer, model:hmm, tech:imu, app:writing, participants:9},
	pages = {191--203},
	annote = {[TLDR] It is shown that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocABularies for known systems.},
}

@article{patil_handwriting_2016,
	title = {Handwriting {Recognition} in {Free} {Space} {Using} {WIMU}-{Based} {Hand} {Motion} {Analysis}},
	volume = {2016},
	issn = {1687-725X, 1687-7268},
	url = {http://www.hindawi.com/journals/js/2016/3692876/},
	doi = {10.1155/2016/3692876},
	abstract = {We present a wireless-inertial-measurement-unit- (WIMU-) based hand motion analysis technique for handwriting recognition in three-dimensional (3D) space. The proposed handwriting recognition system is not bounded by any limitations or constraints; users have the freedom and flexibility to write characters in free space. It uses hand motion analysis to segment hand motion data from a WIMU device that incorporates magnetic, angular rate, and gravity sensors (MARG) and a sensor fusion algorithm to automatically distinguish segments that represent handwriting from nonhandwriting data in continuous hand motion data. Dynamic time warping (DTW) recognition algorithm is used to recognize handwriting in real-time. We demonstrate that a user can freely write in air using an intuitive WIMU as an input and hand motion analysis device to recognize the handwriting in 3D space. The experimental results for recognizing handwriting in free space show that the proposed method is effective and efficient for other natural interaction techniques, such as in computer games and real-time hand gesture recognition applications.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Journal of Sensors},
	author = {Patil, Shashidhar and Kim, Dubeom and Park, Seongsill and Chai, Youngho},
	year = {2016},
	note = {21 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:dtw, tech:imu, app:writing},
	pages = {1--10},
	annote = {[TLDR] It is demonstrated that a user can freely write in air using an intuitive WIMU as an input and hand motion analysis device to recognize the handwriting in 3D space.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/T68RSGXI/Patil et al. - 2016 - Handwriting Recognition in Free Space Using WIMU-B.pdf:application/pdf},
}

@article{sahoo_real-time_2022,
	title = {Real-{Time} {Hand} {Gesture} {Recognition} {Using} {Fine}-{Tuned} {Convolutional} {Neural} {Network}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/3/706},
	doi = {10.3390/s22030706},
	abstract = {Hand gesture recognition is one of the most effective modes of interaction between humans and computers due to being highly flexible and user-friendly. A real-time hand gesture recognition system should aim to develop a user-independent interface with high recognition performance. Nowadays, convolutional neural networks (CNNs) show high recognition rates in image classification problems. Due to the unavailability of large labeled image samples in static hand gesture images, it is a challenging task to train deep CNN networks such as AlexNet, VGG-16 and ResNet from scratch. Therefore, inspired by CNN performance, an end-to-end fine-tuning method of a pre-trained CNN model with score-level fusion technique is proposed here to recognize hand gestures in a dataset with a low number of gesture images. The effectiveness of the proposed technique is evaluated using leave-one-subject-out cross-validation (LOO CV) and regular CV tests on two benchmark datasets. A real-time American sign language (ASL) recognition system is developed and tested using the proposed technique.},
	language = {en},
	number = {3},
	urldate = {2023-07-11},
	journal = {Sensors},
	author = {Sahoo, Jaya Prakash and Prakash, Allam Jaya and Pławiak, Paweł and Samantray, Saunak},
	month = jan,
	year = {2022},
	note = {28 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, app:sign-language, app:american-sl},
	pages = {706},
	annote = {[TLDR] An end-to-end fine-tuning method of a pre-trained CNN model with score-level fusion technique is proposed here to recognize hand gestures in a dataset with a low number of gesture images.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/CUKP99QW/Sahoo et al. - 2022 - Real-Time Hand Gesture Recognition Using Fine-Tune.pdf:application/pdf},
}

@article{colli_alfaro_user-independent_2022,
	title = {User-{Independent} {Hand} {Gesture} {Recognition} {Classification} {Models} {Using} {Sensor} {Fusion}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/4/1321},
	doi = {10.3390/s22041321},
	abstract = {Recently, it has been proven that targeting motor impairments as early as possible while using wearable mechatronic devices for assisted therapy can improve rehabilitation outcomes. However, despite the advanced progress on control methods for wearable mechatronic devices, the need for a more natural interface that allows for better control remains. To address this issue, electromyography (EMG)-based gesture recognition systems have been studied as a potential solution for human–machine interface applications. Recent studies have focused on developing user-independent gesture recognition interfaces to reduce calibration times for new users. Unfortunately, given the stochastic nature of EMG signals, the performance of these interfaces is negatively impacted. To address this issue, this work presents a user-independent gesture classification method based on a sensor fusion technique that combines EMG data and inertial measurement unit (IMU) data. The Myo Armband was used to measure muscle activity and motion data from healthy subjects. Participants were asked to perform seven types of gestures in four different arm positions while using the Myo on their dominant limb. Data obtained from 22 participants were used to classify the gestures using three different classification methods. Overall, average classification accuracies in the range of 67.5–84.6\% were obtained, with the Adaptive Least-Squares Support Vector Machine model obtaining accuracies as high as 92.9\%. These results suggest that by using the proposed sensor fusion approach, it is possible to achieve a more natural interface that allows better control of wearable mechatronic devices during robot assisted therapies.},
	language = {en},
	number = {4},
	urldate = {2023-07-11},
	journal = {Sensors},
	author = {Colli Alfaro, Jose Guillermo and Trejos, Ana Luisa},
	month = feb,
	year = {2022},
	note = {13 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:svm, tech:emg, tech:imu, app:medical, participants:22},
	pages = {1321},
	annote = {[TLDR] A user-independent gesture classification method based on a sensor fusion technique that combines EMG data and inertial measurement unit (IMU) data is presented, suggesting that by using the proposed sensor fusion approach, it is possible to achieve a more natural interface that allows better control of wearable mechatronic devices during robot assisted therapies.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/BBADBX3U/Colli Alfaro and Trejos - 2022 - User-Independent Hand Gesture Recognition Classifi.pdf:application/pdf},
}

@article{xu_finger-writing_2015,
	title = {Finger-writing with {Smartwatch}: {A} {Case} for {Finger} and {Hand} {Gesture} {Recognition} using {Smartwatch}},
	shorttitle = {Finger-writing with {Smartwatch}},
	url = {https://dl.acm.org/doi/10.1145/2699343.2699350},
	doi = {10.1145/2699343.2699350},
	abstract = {Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the fitness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user's arm, hand and possibly finger movements using its accelerometer and gyroscope sensors. Although user's arm and hand gestures are likely to be identified with ease using the smartwatch sensors, it is not clear how much of user's finger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures. We identify essential features of accelerometer and gyroscope data that reflect the movements of tendons (passing through the wrist) when performing a finger or a hand gesture. With these features, we build a classifier that can uniquely identify 37 (13 finger, 14 hand and 10 arm) gestures with an accuracy of 98{\textbackslash}\%. We further extend our gesture recognition to identify the characters written by the user with her index finger on a surface, and show that such finger-writing can also be accurately recognized with nearly 95\% accuracy. Our presented results will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
	author = {Xu, Chao and Pathak, Parth H. and Mohapatra, Prasant},
	month = feb,
	year = {2015},
	note = {228 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: HotMobile '15: The 16th International Workshop on Mobile Computing Systems and Applications
ISBN: 9781450333917
Place: Santa Fe New Mexico USA
Publisher: ACM},
	keywords = {tech:imu, classes:37, hardware:smartwatch},
	pages = {9--14},
	annote = {[TLDR] It is shown that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures and will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
	annote = {[TLDR] It is shown that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures and will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
}

@inproceedings{wang_evaluation_2009,
	address = {London},
	title = {Evaluation of local spatio-temporal features for action recognition},
	isbn = {978-1-901725-39-1},
	url = {http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html},
	doi = {10.5244/C.23.124},
	abstract = {Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2009},
	publisher = {British Machine Vision Association},
	author = {Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
	year = {2009},
	note = {1494 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:svm, type:seminal, classes:25},
	pages = {124.1--124.11},
	annote = {[TLDR] It is demonstrated that regular sampling of space-time features consistently outperforms all testedspace-time interest point detectors for human actions in realistic settings and is a consistent ranking for the majority of methods over different datasets.},
	annote = {[TLDR] It is demonstrated that regular sampling of space-time features consistently outperforms all testedspace-time interest point detectors for human actions in realistic settings and is a consistent ranking for the majority of methods over different datasets.},
	file = {Submitted Version:/Users/brk/Zotero/storage/QFB8QYCI/Wang et al. - 2009 - Evaluation of local spatio-temporal features for a.pdf:application/pdf},
}

@article{pham_hand_2020,
	title = {Hand detection and segmentation using multimodal information from {Kinect}},
	url = {https://ieeexplore.ieee.org/document/9237785/},
	doi = {10.1109/MAPR49794.2020.9237785},
	abstract = {Nowadays, hand gestures are becoming one of the most natural and intuitive ways of communication between human and computer. To this end, a complex process including hand gesture acquisition, hand detection, gesture representation and recognition must be carried out. This paper presents a method that detects hand and segments hand regions from images captured by a Kinect sensor. As Kinect sensor provides not only RGB images as conventional camera, but also depth and skeleton, in our work, we incorporate multi-modal data from Kinect to deal with hand detection and segmentation. Specifically, we use skeleton to approximately determine hand palm. Then a skin based detector will be applied to discard non-skin pixels from the region of interest. Using depth data helps to limit the human body regions and remove false positive regions from the previous steps. Finally, morphological operations will be applied to fill holes in the hand region. The main advantage of this method is very easy to implement and it performs in real-time on an ordinary computer. We evaluate the proposed method on a dataset of hand gestures captured from different viewpoints. Experiment shows that it provides reasonable accuracy at very high frame rate. It also produces comparable performance in comparison with deep learning based methods.},
	urldate = {2023-07-11},
	journal = {2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
	author = {Pham, Van-Tien and Le, Thi-Lan and Tran, Thanh-Hai and Nguyen, Thanh Phuong},
	month = oct,
	year = {2020},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)
ISBN: 9781728165554
Place: Ha Noi, Vietnam
Publisher: IEEE},
	keywords = {hardware:kinect, tech:rgbd, read-priority-9},
	pages = {1--6},
	annote = {[TLDR] This paper presents a method that detects hand and segments hand regions from images captured by a Kinect sensor and incorporates multi-modal data from Kinect to deal with hand detection and segmentation.},
}

@article{ghotkar_dynamic_2016,
	title = {Dynamic {Hand} {Gesture} {Recognition} using {Hidden} {Markov} {Model} by {Microsoft} {Kinect} {Sensor}},
	volume = {150},
	issn = {09758887},
	url = {http://www.ijcaonline.org/archives/volume150/number5/ghotkar-2016-ijca-911498.pdf},
	doi = {10.5120/ijca2016911498},
	abstract = {Hand gesture recognition is one of the leading applications of human computer interaction. With diversity of applications of hand gesture recognition, sign language interpretation is the most demanding application. In this paper, dynamic hand gesture recognition for few subset of Indian sign language recognition was considered. The use of depth camera such as Kinect sensor gave skeleton information of signer body. After detailed study of dynamic ISL vocabulary with reference to skeleton joint information, angle has identified as a feature with reference to two moving hand. Here, real time video has been captured and gesture was recognized using Hidden Markov Model (HMM). Ten state HMM model was designed and normalized angle feature of dynamic sign was being observed. Maximum likelihood probability symbol was considered as a recognized gesture. Algorithm has been tested on ISL 20 dynamic signs of total 800 training set of four persons and achieved 89.25\% average accuracy. General Terms Human Computer Interaction, Pattern Recognition, Machine Learning},
	number = {5},
	urldate = {2023-07-11},
	journal = {International Journal of Computer Applications},
	author = {Ghotkar, Archana and Vidap, Pujashree and Deo, Kshitish},
	month = sep,
	year = {2016},
	note = {24 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, hardware:kinect, app:sign-language, app:indian-sl, classes:20, participants:4},
	pages = {5--9},
	annote = {[TLDR] Dynamic hand gesture recognition for few subset of Indian sign language recognition was considered and angle has identified as a feature with reference to two moving hand with respect to skeleton joint information.},
	file = {Full Text:/Users/brk/Zotero/storage/7RH4TGA8/Ghotkar et al. - 2016 - Dynamic Hand Gesture Recognition using Hidden Mark.pdf:application/pdf},
}

@article{moeslund_survey_2006,
	title = {A survey of advances in vision-based human motion capture and analysis},
	volume = {104},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206001263},
	doi = {10.1016/j.cviu.2006.08.002},
	abstract = {This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [247]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement.},
	language = {en},
	number = {2-3},
	urldate = {2023-07-11},
	journal = {Computer Vision and Image Understanding},
	author = {Moeslund, Thomas B. and Hilton, Adrian and Krüger, Volker},
	month = nov,
	year = {2006},
	note = {2830 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, type:survey},
	pages = {90--126},
	file = {Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf:/Users/brk/Zotero/storage/7NIXC2ZU/Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf:application/pdf},
}

@article{lepetit_monocular_2005,
	title = {Monocular {Model}-{Based} {3D} {Tracking} of {Rigid} {Objects}: {A} {Survey}},
	volume = {1},
	issn = {1572-2740, 1572-2759},
	shorttitle = {Monocular {Model}-{Based} {3D} {Tracking} of {Rigid} {Objects}},
	url = {http://www.nowpublishers.com/article/Details/CGV-001},
	doi = {10.1561/0600000001},
	abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Augmented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated interfaces can use. Computer Vision offers solutions that are cheap, practical and non-invasive.This survey reviews the different techniques and approaches that have been developed by industry and research. First, important mathematical tools are introduced: Camera representation, robust estimation and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery concludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking.Because it encompasses many computer vision techniques from low-level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Lepetit, Vincent and Fua, Pascal},
	year = {2005},
	note = {747 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, type:survey, app:robotics},
	pages = {1--89},
	annote = {[TLDR] This survey reviews the different techniques and approaches that have been developed by industry and research on 3D tracking and includes a comprehensive study of the massive literature on the subject.},
	file = {Submitted Version:/Users/brk/Zotero/storage/8KHLPAGM/Lepetit and Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects.pdf:application/pdf},
}

@inproceedings{oikonomidis_efficient_2011,
	address = {Dundee},
	title = {Efficient model-based {3D} tracking of hand articulations using {Kinect}},
	isbn = {978-1-901725-43-8},
	url = {http://www.bmva.org/bmvc/2011/proceedings/paper101/index.html},
	doi = {10.5244/C.25.101},
	abstract = {We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor. We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup. Being model based, it provides continuous solutions to the problem of tracking hand articulations. Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz).},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2011},
	publisher = {British Machine Vision Association},
	author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis},
	year = {2011},
	note = {1008 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {hardware:kinect, model:pso},
	pages = {101.1--101.11},
	annote = {[TLDR] A novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor is presented.},
	file = {Oikonomidis et al. - 2011 - Efficient model-based 3D tracking of hand articula.pdf:/Users/brk/Zotero/storage/2JTTNNXG/Oikonomidis et al. - 2011 - Efficient model-based 3D tracking of hand articula.pdf:application/pdf},
}

@inproceedings{rogowitz_saliency_2015,
	address = {San Francisco, California, United States},
	title = {Saliency detection for videos using {3D} {FFT} local spectra},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2077762},
	doi = {10.1117/12.2077762},
	abstract = {Bottom-up spatio-temporal saliency detection identifies perceptually important regions of interest in video sequences. The center-surround model proves to be useful for visual saliency detection. In this work, we explore using 3D FFT local spectra as features for saliency detection within the center-surround framework. We develop a spectral location based decomposition scheme to divide a 3D FFT cube into two components, one related to temporal changes and the other related to spatial changes. Temporal saliency and spatial saliency are detected separately using features derived from each spectral component through a simple center-surround comparison method. The two detection results are then combined to yield a saliency map. We apply the same detection algorithm to different color channels (YIQ) and incorporate the results into the final saliency determination. The proposed technique is tested with the public CRCNS database. Both visual and numerical evaluations verify the promising performance of our technique.},
	urldate = {2023-07-11},
	author = {Long, Zhiling and AlRegib, Ghassan},
	editor = {Rogowitz, Bernice E. and Pappas, Thrasyvoulos N. and De Ridder, Huib},
	month = mar,
	year = {2015},
	note = {10 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, read-priority-3, model:fft, dataset:crcns},
	pages = {93941G},
	annote = {[TLDR] A spectral location based decomposition scheme to divide a 3D FFT cube into two components, one related to temporal changes and the other related to spatial changes is developed and combined to yield a saliency map.},
}

@article{wachs_vision-based_2011,
	title = {Vision-based hand-gesture applications},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1897816.1897838},
	doi = {10.1145/1897816.1897838},
	abstract = {there is strong evidence that future humancomputer interfaces will enable more natural, intuitive communication between people and all kinds of sensor-based devices, thus more closely resembling human-human communication. Progress in the field of human-computer interaction has introduced innovative technologies that empower users to interact with computer systems in increasingly natural and intuitive ways; systems adopting them show increased efficiency, speed, power, and realism. However, users comfortable with traditional interaction methods like mice and keyboards are often unwilling to embrace new, alternative interfaces. Ideally, new interface technologies should be more accessible without requiring long periods of learning and adaptation. They should also provide more natural human-machine communication.},
	language = {en},
	number = {2},
	urldate = {2023-07-11},
	journal = {Communications of the ACM},
	author = {Wachs, Juan Pablo and Kölsch, Mathias and Stern, Helman and Edan, Yael},
	month = feb,
	year = {2011},
	note = {653 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, type:survey},
	pages = {60--71},
	annote = {[TLDR] Body posture and finger pointing are a natural modality for human-machine interaction, but first the system must know what it's seeing.},
	file = {Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf:/Users/brk/Zotero/storage/AGVRM526/Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	urldate = {2023-07-11},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	keywords = {background, type:seminal, model:random-forests},
	pages = {5--32},
	annote = {[TLDR] Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression.},
	file = {Full Text:/Users/brk/Zotero/storage/HSTITUV6/Breiman - 2001 - [No title found].pdf:application/pdf},
}

@inproceedings{michahial_hand_2015,
	title = {Hand gesture recognition using support vector machine},
	url = {https://www.semanticscholar.org/paper/Hand-gesture-recognition-using-support-vector-Michahial-Azeez/4deb239f4366c461ab092f17828aed9a03b52e8d},
	abstract = {The images contain measurement information of key interest for a variety of research and application areas. On the other hand, computers have become an inseparable part of our society, influencing many aspects of our daily lives in terms of communication and interaction. The main motive is to develop a system that can simplify the way humans interact with Computers. The system is designed using Canny’s edge detection for edge detection and Histogram of gradients for feature extraction and the Support Vector Machine (SVM) Classifier which is widely used for classification and regression testing. SVM training algorithm builds a model that predicts whether a new example falls into one category or other. And the classifier learns from the data points in examples when they are classified belonging to their respective categories.},
	urldate = {2023-07-11},
	author = {Michahial, S. and Azeez, Beebi Hajira and Rani, R.},
	year = {2015},
	keywords = {model:svm, tech:rgb, read-priority-9},
	annote = {[TLDR] A system that can simplify the way humans interact with Computers is developed using Canny’s edge detection for edge detection and Histogram of gradients for feature extraction and the Support Vector Machine (SVM) Classifier which is widely used for classification and regression testing.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/JLWRYAFF/Michahial et al. - 2015 - Hand gesture recognition using support vector mach.pdf:application/pdf},
}

@article{karantonis_implementation_2006,
	title = {Implementation of a {Real}-{Time} {Human} {Movement} {Classifier} {Using} a {Triaxial} {Accelerometer} for {Ambulatory} {Monitoring}},
	volume = {10},
	issn = {1089-7771},
	url = {http://ieeexplore.ieee.org/document/1573717/},
	doi = {10.1109/TITB.2005.856864},
	abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {IEEE Transactions on Information Technology in Biomedicine},
	author = {Karantonis, D.M. and Narayanan, M.R. and Mathie, M. and Lovell, N.H. and Celler, B.G.},
	month = jan,
	year = {2006},
	note = {1273 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:accelerometer, tech:imu, type:seminal},
	pages = {156--167},
	annote = {[TLDR] Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence.},
}

@inproceedings{mustafa_hand_2007,
	title = {Hand {Gesture} {Recognition} {Using} {Artificial} {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-Artificial-Neural-Mustafa/64cbb00f06981b59ebea1faf471695280d10a2bb},
	abstract = {Hand gesture has been part of human communication, where, young children usually communicate by using gesture before they can talk. Adults may have to also gesture if they need to or they are indeed mute or deaf. Thus the idea of teaching a machine to also learn gestures is very appealing due to its unique mode of communications. A reliable hand gesture recognition system will make the remote control become obsolete. However, many of the new techniques proposed are complicated to be implemented in real time, especially as a human machine interface.
This thesis focuses on recognizing hand gesture in static posture. Since static hand postures not only can express some concepts, but also can act as special transition states in temporal gestures recognition, thus estimating static hand postures is in fact a big topics in gesture recognition. A database consists of 200 gesture images have been built, where five volunteers had help in the making of the database. The images were captured in a controlled environment and the postures are free from occlusion where the background is uncluttered and the hand is assumed to have been localized.

A system was then built to recognize the hand gesture. The captured image will be first preprocessed in order to binarize the palm region, where Sobel edge detection technique has been employed, with later followed by morphological operation. A new feature extraction technique has been developed, based on horizontal and vertical states transition count, and the ratio of hand area with respect to the whole area of image. These set of features have been proven to have high intra class dissimilarity attributes.

In order to have a system that can be easily trained, artificial neural networks has been chosen in the classification stage. A multilayer perceptron with back-propagation algorithm has been developed, thus the system is actually in-built to be used as a human machine interface. The gesture recognition system has been built and tested in Matlab, where simulations have shown promising results. The performance of recognition rate in this research is 95\% which shows a major improvement in comparison to the available methods.},
	urldate = {2023-07-11},
	author = {Mustafa, M. A.},
	year = {2007},
	keywords = {movement:static, model:nn},
	annote = {[TLDR] This thesis focuses on recognizing hand gesture in static posture by building a system that can be easily trained and tested in Matlab, and shows a major improvement in comparison to the available methods.},
}

@inproceedings{hassanpour_visionbased_2008,
	title = {Vision­{Based} {Hand} {Gesture} {Recognition} for {Human} {Computer} {Interaction}: {A} {Review}},
	shorttitle = {Vision­{Based} {Hand} {Gesture} {Recognition} for {Human} {Computer} {Interaction}},
	url = {https://www.semanticscholar.org/paper/Vision%C2%ADBased-Hand-Gesture-Recognition-for-Human-A-Hassanpour-Wong/1285c27b3a3b2304274b88676fc20086bf5cc3dd},
	abstract = {Evolution of user interfaces shapes the change in the human­computer interaction. With the rapid emergence of three­dimensional (3­D) applications; the need for a new type of interaction device arises as traditional devices such as mouse, keyboard, and joystick become inefficient and cumbersome within these virtual environments. Intuitive and naturalness characteristics of “Ha nd Gestures” in human computer interaction have been the driving force and motivation to develop an interaction device which can replace current unwieldy tools. This study is a survey on the methods of analyzing, modeling and recognizing hand gestures in the context of human­ computer interaction. Taxonomy of the methods based on the applications that they have been developed for and the approaches that they have used to represent gestures is presented. Direction of future developments is also discussed.},
	urldate = {2023-07-11},
	author = {Hassanpour, R. and Wong, Stephan and Shahbahrami, A. and Wong, J.},
	year = {2008},
	keywords = {tech:rgb, type:survey},
	annote = {[TLDR] This study is a survey on the methods of analyzing, modeling and recognizing hand gestures in the context of human­ computer interaction and taxonomy of the methods based on the applications they have been developed for and the approaches that they have used to represent gestures is presented.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/79Q9UGWM/Hassanpour et al. - 2008 - Vision­Based Hand Gesture Recognition for Human Co.pdf:application/pdf},
}

@article{rigoll_new_1997,
	title = {New improved feature extraction methods for real-time high performance image sequence recognition},
	volume = {4},
	url = {http://ieeexplore.ieee.org/document/595396/},
	doi = {10.1109/ICASSP.1997.595396},
	abstract = {This paper describes new feature extraction methods which can be used very effectively in combination with statistical methods for image sequence recognition. Although these feature extraction methods can be used for a wide variety of image sequence processing applications, the target application presented in this paper is gesture recognition. The novel feature extraction methods have been integrated into an HMM-based gesture recognition system and led to substantial improvements for this system. It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence. This resulted in the fact that it was possible to use the novel features in combination with a new architecture for statistical image sequence recognition. The result of this investigation is a high performance gesture recognition system with significantly improved recognition rates and real-time capabilities.},
	urldate = {2023-07-11},
	journal = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
	author = {Rigoll, G. and Kosmala, A.},
	year = {1997},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing
ISBN: 9780818679193
Place: Munich, Germany
Publisher: IEEE Comput. Soc. Press},
	keywords = {model:hmm},
	pages = {2901--2904},
	annote = {[TLDR] It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence.},
	annote = {[TLDR] It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence.},
}

@article{eickeler_hidden_1998,
	title = {Hidden {Markov} model based continuous online gesture recognition},
	volume = {2},
	url = {http://ieeexplore.ieee.org/document/711914/},
	doi = {10.1109/ICPR.1998.711914},
	abstract = {Presents the extension of an existing vision-based gesture recognition system using hidden Markov models(HMMs). Several improvements have been carried out in order to increase the capabilities and the functionality of the system. These improvements include position independent recognition, rejection of unknown gestures, and continuous online recognition of spontaneous gestures. We show that especially the latter requirement is highly complicated and demanding, if we allow the user to move in front of the camera without any restrictions and to perform the gestures spontaneously at any arbitrary moment. We present solutions to this problem by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.},
	urldate = {2023-07-11},
	journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
	author = {Eickeler, S. and Kosmala, A. and Rigoll, G.},
	year = {1998},
	note = {39 citations (Crossref) [2023-07-11]
140 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Fourteenth International Conference on Pattern Recognition
ISBN: 9780818685125
Place: Brisbane, Qld., Australia
Publisher: IEEE Comput. Soc},
	keywords = {model:hmm, tech:rgb, from:cite.bib},
	pages = {1206--1208},
	annote = {@eickeler1998 extended an existing HMM-based image classification system to facilitate continuous online recognition of spontaneous gestures. The input data was the difference of adjacent frames of a video (to help emphasise the motion in the video). They are able to classify six gestures in real time.

},
	annote = {[TLDR] This work presents the extension of an existing vision-based gesture recognition system using hidden Markov models by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.},
}

@inproceedings{shyamala_survey_2014,
	title = {A {SURVEY} {OF} {VISION} {BASED} {HAND} {GESTURE} {RECOGNITION}},
	url = {https://www.semanticscholar.org/paper/A-SURVEY-OF-VISION-BASED-HAND-GESTURE-RECOGNITION-Shyamala/01fb6988163209f16021dec3320956de74a8f45d},
	abstract = {Gesture recognition is to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. Hand Gestures have greater importance in designing an intelligent and efficient human–computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition, gesture taxonomies, representations. Vision based Gesture recognition has the potential to be a natural and powerful tool supporting efficient and intuitive interaction between the human and the computer. Visual interpretation of hand gestures can help in achieving the ease and naturalness desired for Human Computer Interaction (HCI). It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition.},
	urldate = {2023-07-11},
	author = {Shyamala, M.},
	year = {2014},
	keywords = {tech:rgb, movement:static, movement:dynamic, type:survey},
	annote = {[TLDR] This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition, gesture taxonomies, representations and visual interpretation of hand gestures i.e. detection, tracking and recognition.},
}

@inproceedings{mahmoud_convolutional_2021,
	title = {Convolutional neural networks framework for human hand gesture recognition},
	volume = {10},
	url = {https://beei.org/index.php/EEI/article/view/2926},
	doi = {10.11591/eei.v10i4.2926},
	abstract = {Recently, the recognition of human hand gestures is becoming a valuable technology for various applications like sign language recognition, virtual games and robotics control, video surveillance, and home automation. Owing to the recent development of deep learning and its excellent performance, deep learning-based hand gesture recognition systems can provide promising results. However, accurate recognition of hand gestures remains a substantial challenge that faces most of the recently existing recognition systems. In this paper, convolutional neural networks (CNN) framework with multiple layers for accurate, effective, and less complex human hand gesture recognition has been proposed. Since the images of the infrared hand gestures can provide accurate gesture information through the low illumination environment, the proposed system is tested and evaluated on a database of hand-based near-infrared which including ten gesture poses. Extensive experiments prove that the proposed system provides excellent results of accuracy, precision, sensitivity (recall), and F1-score. Furthermore, a comparison with recently existing systems is reported.},
	urldate = {2023-07-11},
	booktitle = {Bulletin of {Electrical} {Engineering} and {Informatics}},
	author = {Mahmoud, Aseel Ghazi and Hasan, Ahmed Mudheher and Hassan, Nadia Moqbel},
	month = aug,
	year = {2021},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 2302-9285, 2089-3191
Issue: 4
Journal Abbreviation: Bulletin EEI},
	keywords = {model:cnn, classes:10, tech:rgb, tech:rgbd, tech:infrared},
	pages = {2223--2230},
	file = {Full Text PDF:/Users/brk/Zotero/storage/NDPUWIZE/Mahmoud et al. - 2021 - Convolutional neural networks framework for human .pdf:application/pdf},
}

@inproceedings{meghanathan_survey_2011,
	address = {Berlin, Heidelberg},
	title = {A {Survey} on {Hand} {Gesture} {Recognition} in {Context} of {Soft} {Computing}},
	volume = {133},
	isbn = {978-3-642-17880-1 978-3-642-17881-8},
	url = {http://link.springer.com/10.1007/978-3-642-17881-8_5},
	doi = {10.1007/978-3-642-17881-8_5},
	abstract = {Hand gestures recognition is the natural way of Human Machine interaction and today many researchers in the academia and industry are interested in this direction. It enables human being to interact with machine very easily and conveniently without wearing any extra device. It can be applied from sign language recognition to robot control and from virtual reality to intelligent home systems. In this paper we are discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc. We also described hand detection methods in the preprocessed image for detecting the hand image. Most researchers used fingertips for hand detection in appearance based modeling. Finally we are comparing results given by different researchers after their implementation.},
	urldate = {2023-07-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Chaudhary, Ankit and Raheja, J. L. and Das, Karen and Raheja, Sonia},
	editor = {Meghanathan, Natarajan and Kaushik, Brajesh Kumar and Nagamalai, Dhinaharan},
	year = {2011},
	doi = {10.1007/978-3-642-17881-8_5},
	note = {67 citations (Semantic Scholar/DOI) [2023-07-11]
Book Title: Advanced Computing
Series Title: Communications in Computer and Information Science},
	keywords = {type:survey, model:fuzzy},
	pages = {46--55},
	annote = {[TLDR] This paper is discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc and comparing results given by different researchers after their implementation.},
	annote = {[TLDR] This paper is discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc and comparing results given by different researchers after their implementation.},
}

@article{chen_survey_2013,
	title = {A {Survey} on {Hand} {Gesture} {Recognition}},
	url = {http://ieeexplore.ieee.org/document/6835606/},
	doi = {10.1109/CSA.2013.79},
	abstract = {Hand gesture recognition has become one of the key techniques of human-computer interaction (HCI). Many researchers are devoted in this field. In this paper, firstly the history of hand gesture recognition is discussed and the technical difficulties are also enumerated. Then, we analyze the definition of hand gesture and introduce the basic principle of it. The approaches for hand gesture recognition, such as vision-based, glove-based and depth-based, are contrasted briefly in this paper. But the former two methods are too simple and not natural enough. Currently, the new finger identification and hand gesture recognition technique with Kinect depth data is the most popular research direction. Finally, we discuss the application prospective of hand gesture recognition based on Kinect.},
	urldate = {2023-07-11},
	journal = {2013 International Conference on Computer Sciences and Applications},
	author = {Chen, Lingchen and Wang, Feng and Deng, Hui and Ji, Kaifan},
	month = dec,
	year = {2013},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2013 International Conference on Computer Sciences and Applications (CSA)
ISBN: 9780769551258
Place: Wuhan, China
Publisher: IEEE},
	keywords = {hardware:kinect, type:survey},
	pages = {313--316},
	annote = {[TLDR] The definition of hand gesture is analyzed and the basic principle of it is introduced, and the new finger identification and hand gesture recognition technique with Kinect depth data is the most popular research direction.},
}

@article{waskom_seaborn_2021,
	title = {seaborn: statistical data visualization},
	volume = {6},
	url = {https://doi.org/10.21105/joss.03021},
	doi = {10.21105/joss.03021},
	number = {60},
	journal = {Journal of Open Source Software},
	author = {Waskom, Michael L.},
	year = {2021},
	note = {1622 citations (Crossref) [2023-07-11]
1607 citations (Semantic Scholar/DOI) [2023-07-11]
Publisher: The Open Journal},
	keywords = {from:cite.bib, type:software-lib},
	pages = {3021},
}

@misc{the_pandas_development_team_pandas-devpandas_2020,
	title = {pandas-dev/pandas: {Pandas}},
	url = {https://doi.org/10.5281/zenodo.3509134},
	publisher = {Zenodo},
	author = {{The Pandas Development Team}},
	month = feb,
	year = {2020},
	doi = {10.5281/zenodo.3509134},
	keywords = {from:cite.bib, type:software-lib},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} graphics environment},
	volume = {9},
	doi = {10.1109/MCSE.2007.55},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	year = {2007},
	note = {17522 citations (Crossref) [2023-07-11]
Publisher: IEEE COMPUTER SOC},
	keywords = {from:cite.bib, type:software-lib},
	pages = {90--95},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	url = {https://doi.org/10.1038/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	number = {7825},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, Stéfan J. van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and Río, Jaime Fernández del and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {7070 citations (Crossref) [2023-07-11]
Publisher: Springer Science and Business Media LLC},
	keywords = {from:cite.bib, type:software-lib},
	pages = {357--362},
}

@misc{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	keywords = {from:cite.bib, type:software-lib},
	annote = {Software available from tensorflow.org},
}

@book{van_rossum_python_2009,
	address = {Scotts Valley, CA},
	title = {Python 3 {Reference} {Manual}},
	isbn = {1-4414-1269-7},
	publisher = {CreateSpace},
	author = {Van Rossum, Guido and Drake, Fred L.},
	year = {2009},
	keywords = {from:cite.bib, type:software-lib},
}

@inproceedings{matsakis_rust_2014,
	title = {The {Rust} language},
	volume = {34},
	booktitle = {{ACM} {SIGAda} {Ada} {Letters}},
	publisher = {ACM},
	author = {Matsakis, Nicholas D and Klock II, Felix S},
	year = {2014},
	note = {Issue: 3},
	keywords = {from:cite.bib, type:software-lib},
	pages = {103--104},
}

@misc{analogdevices_adxl335_2010,
	title = {{ADXL335} {Datasheet} and {Product} {Info} {\textbar} {Analog} {Devices}},
	url = {https://www.analog.com/en/products/adxl335.html#product-overview},
	publisher = {www.analog.com},
	author = {{AnalogDevices}},
	year = {2010},
	note = {Publication Title: ADXL335 Datasheet and Product Info {\textbar} Analog Devices},
	keywords = {from:cite.bib, type:datasheet},
}

@misc{arduino_arduino_2005,
	title = {Arduino - {Home}},
	url = {https://www.arduino.cc/},
	publisher = {www.arduino.cc},
	author = {{Arduino}},
	year = {2005},
	note = {Publication Title: Arduino - Home},
	keywords = {from:cite.bib, type:datasheet},
}

@misc{texas_instruments_cd74hc4067_2003,
	title = {{CD74HC4067} data sheet, product information and support {\textbar} {TI}.com},
	url = {https://www.ti.com/product/CD74HC4067},
	publisher = {www.ti.com},
	author = {{Texas Instruments}},
	month = jul,
	year = {2003},
	note = {Publication Title: CD74HC4067 data sheet, product information and support {\textbar} TI.com},
	keywords = {from:cite.bib, type:datasheet},
}

@misc{arduino_arduino_2016,
	title = {Arduino {Nano} 33 {BLE} — {Arduino} {Official} {Store}},
	url = {https://store.arduino.cc/products/arduino-nano-33-ble},
	publisher = {store.arduino.cc},
	author = {{Arduino}},
	year = {2016},
	note = {Publication Title: Arduino Official Store},
	keywords = {from:cite.bib, type:datasheet},
}

@article{roll_effectiveness_2016,
	title = {Effectiveness of {Occupational} {Therapy} {Interventions} for {Adults} {With} {Musculoskeletal} {Conditions} of the {Forearm}, {Wrist}, and {Hand}: {A} {Systematic} {Review}},
	volume = {71},
	issn = {0272-9490},
	url = {https://doi.org/10.5014/ajot.2017.023234},
	doi = {10.5014/ajot.2017.023234},
	number = {1},
	journal = {The American Journal of Occupational Therapy},
	author = {Roll, Shawn C. and Hardison, Mark E.},
	month = dec,
	year = {2016},
	note = {38 citations (Crossref) [2023-07-11]
45 citations (Semantic Scholar/DOI) [2023-07-11]
\_eprint: https://research.aota.org/ajot/article-pdf/71/1/7101180010p1/68296/7101180010p1.pdf},
	keywords = {background, from:cite.bib, medical},
	pages = {7101180010p1--7101180010p12},
}

@article{rempel_effect_2008,
	title = {Effect of wrist posture on carpal tunnel pressure while typing},
	volume = {26},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jor.20599},
	doi = {https://doi.org/10.1002/jor.20599},
	number = {9},
	journal = {Journal of Orthopaedic Research},
	author = {Rempel, David M. and Keir, Peter J. and Bach, Joel M.},
	year = {2008},
	note = {62 citations (Crossref) [2023-07-11]
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jor.20599},
	keywords = {carpal tunnel syndrome, keyboard, neuropathy, occupation, overuse, from:cite.bib},
	pages = {1269--1273},
}

@article{whitehead_abstract_2014,
	title = {Abstract {Hidden} {Markov} {Models} have been effectively used in time series based pattern recognition problems in the past. {This} work explores using {Hidden} {Markov} {Models} ({HMM}) to do {3D} gesture recognition from accelerometer data. {Our} work differs from much of the previous work in that we examine the use of discreet {HMMs} rather than continuous {HMMs}. {An} interesting side effect of this is that method is therefore theoretically transportable to other devices that have a {3D} sensor output system. {In} essence this brings us a mechanism to use the {HMM} model across a series of different sensor devices for gesture recognition. {We} achieve recognition results with accuracy rates approaching 90 percent for users who are not in the training samples. {The} speed of our system is also of interest as we are able to classify gestures at a rate of several hundred times per second. {As} long as the sen-sor system is capable of outputting information about the 3 axes of motion, and the outputs can be discretized to volumetrically equivalent cubic sub-spaces; that information can then be used in this generic model for accurate, high speed gesture recognition.},
	volume = {3},
	doi = {10.7603/s40601-013-0042-9},
	journal = {GSTF Journal on Computing (JoC)},
	author = {Whitehead, A.D.},
	month = apr,
	year = {2014},
	note = {0 citations (Crossref) [2023-07-11]
1 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, from:cite.bib},
	annote = {@whitehead2014 used discrete HMMs to recognise gestures from three-axis linear accelerometer data captured by a watch-like device strapped to the user's forearm. Seven gestures were learnt by the trained models and only single handed gestures were considered.

},
	annote = {[TLDR] This work explores using Hidden Markov Models (HMM) to do 3D gesture recognition from accelerometer data using discreet HMMs rather than continuous HMMs, and achieves recognition results with accuracy rates approaching 90 percent for users who are not in the training samples.},
}

@article{keskin_real_2003,
	title = {Real time hand tracking and {3D} gesture recognition for interactive interfaces using {HMM}},
	journal = {Proceedings of the joint international conference ICANN/ICONIP},
	author = {Keskin, Cem and Erkan, A.N. and Akarun, L},
	month = jan,
	year = {2003},
	keywords = {model:hmm, tech:rgb, classes:8, movement:dynamic, from:cite.bib},
	annote = {@keskin2003 developed a HMM-based system which classifies gestures based on video input of brightly coloured gloves (similar to @starner1995). The system is able to classify eight dynamic gestures consisting of single hand motions which trace out simple shapes in front of the camera.

},
}

@article{marcelo_gefighters_2006,
	title = {{GeFighters}: an {Experiment} for {Gesture}-based {Interaction} {Analysis} in a {Fighting} {Game}},
	author = {Marcelo, João and Farias, Thiago and Pessoa, Saulo and Moura, Guilherme and Teichrieb, Veronica},
	month = jan,
	year = {2006},
	keywords = {app:gaming, from:cite.bib},
	annote = {@marcelo2006 created GeFighters, a fighting game where the primary means of interacting with the game comes from gestures performed in front of a camera. This system requires large fiducial markers to be held in the user's hands. These black and white markers are recognised by the computer system and their relative positioning is used as input to the GeFighters game engine. A total of nine gestures were recognisable by the system.


},
}

@article{starner_visual_1995,
	title = {Visual recognition of american sign language using hidden markov models},
	abstract = {Abstract—We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American Sign Language (ASL) using a single camera to track the user’s unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
	author = {Starner, Thad and Pentland, Alex},
	month = may,
	year = {1995},
	keywords = {model:hmm, tech:rgb, app:sign-language, app:american-sl, classes:40, from:cite.bib},
	annote = {@starner1995 describe a Hidden Markov Model (HMM) capable of classifying 40 gestures from the American Sign Language. These gestures usually include motion of both hands. The input data are frames from a video, at a rate of five frames per second. These frames are of a seated user who wear coloured gloves to assist with segmentation of the user's hands from the background.

},
	annote = {[TLDR] Using hidden Markov models (HMM's), an unobstrusive single view camera system is developed that can recognize hand gestures, namely, a subset of American Sign Language (ASL), achieving high recognition rates for full sentence ASL using only visual cues.},
	file = {Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf:/Users/brk/Zotero/storage/KBBCZYDF/Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf:application/pdf},
}

@misc{frank_hofmann_sensorglove_1995,
	title = {{SensorGlove}: {Anthropomorphic} {Robot} {Hand}},
	url = {https://pdv.cs.tu-berlin.de/forschung/SensorGlove2_engl.html},
	author = {{Frank Hofmann} and {Jürgen Henz}},
	year = {1995},
	keywords = {from:cite.bib, type:product},
}

@misc{arregui_sensor_2017,
	title = {Sensor {Glove}},
	url = {https://sensorglove.wixsite.com/sensorglove/producto},
	author = {Arregui, Patxi Xabier Quintana},
	year = {2017},
	keywords = {from:cite.bib, type:product},
}

@misc{immersion_inc_immersion_2005,
	title = {Immersion {Inc} {CyberGlove} {II}},
	url = {https://www.immersion.fr/en/cyberglove-ii/},
	author = {{Immersion Inc}},
	year = {2005},
	keywords = {from:cite.bib, type:product},
}

@misc{fifth_dimension_technologies_dataglove5_2005,
	title = {{DataGlove5}},
	url = {https://5dt.com/5dt-data-glove-ultra/},
	author = {{Fifth Dimension Technologies}},
	year = {2005},
	keywords = {from:cite.bib, type:product},
}

@article{elman_distributed_2004,
	title = {Distributed representations, simple recurrent networks, and grammatical structure},
	volume = {7},
	journal = {Machine Learning},
	author = {Elman, Jeffrey L.},
	year = {2004},
	keywords = {type:seminal, from:cite.bib, model:elman-nn},
	pages = {195--225},
}

@misc{cyberglove_inc_wireless_nodate,
	title = {Wireless {CyberGlove} {II} {Motion} {Capture} {Data} {Glove}},
	url = {http://www.cyberglovesystems.com/cyberglove-ii/},
	author = {{CyberGlove Inc}},
	keywords = {from:cite.bib, type:product},
}

@article{myers_comparative_1981,
	title = {A comparative study of several dynamic time-warping algorithms for connected-word recognition},
	volume = {60},
	doi = {10.1002/j.1538-7305.1981.tb00272.x},
	number = {7},
	journal = {The Bell System Technical Journal},
	author = {Myers, C. S. and Rabiner, L. R.},
	year = {1981},
	note = {274 citations (Crossref) [2023-07-11]
481 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:dtw, from:cite.bib},
	pages = {1389--1409},
}

@misc{karpathy_recipe_2019,
	title = {A {Recipe} for {Training} {Neural} {Networks}},
	url = {https://karpathy.github.io/2019/04/25/recipe/},
	publisher = {https://karpathy.github.io},
	author = {Karpathy, Andrej},
	year = {2019},
	note = {Publication Title: Andrej Karpathy Blog},
	keywords = {model:nn, from:cite.bib, type:blog},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	journal = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	keywords = {background, from:cite.bib, dropout},
	pages = {1929--1958},
}

@article{agarap_deep_2018,
	title = {Deep {Learning} using {Rectified} {Linear} {Units} ({ReLU})},
	volume = {abs/1803.08375},
	journal = {ArXiv},
	author = {Agarap, Abien Fred},
	year = {2018},
	keywords = {background, from:cite.bib},
}

@inproceedings{quinlan_c45_1992,
	title = {C4.5: {Programs} for {Machine} {Learning}},
	author = {Quinlan, J. Ross},
	year = {1992},
	keywords = {background, from:cite.bib, model:c4.5},
}

@inproceedings{senin_dynamic_2008,
	title = {Dynamic {Time} {Warping} {Algorithm} {Review}},
	author = {Senin, Pavel},
	year = {2008},
	keywords = {model:dtw, type:survey, from:cite.bib},
}

@inproceedings{gillian_gesture_2014,
	title = {The gesture recognition toolkit},
	booktitle = {J. {Mach}. {Learn}. {Res}.},
	author = {Gillian, Nicholas Edward and Paradiso, Joseph A.},
	year = {2014},
	keywords = {from:cite.bib, type:software-lib},
}

@article{bergstra_random_2012,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	volume = {13},
	issn = {1532-4435},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Bergstra, James and Bengio, Yoshua},
	month = feb,
	year = {2012},
	note = {Publisher: JMLR.org},
	keywords = {deep learning, global optimization, model selection, neural networks, response surface modeling, from:cite.bib},
	pages = {281--305},
}

@misc{noauthor_sensor_nodate,
	title = {Sensor {Glove}},
	url = {https://github.com/SensorGlove/SensorGlove},
	keywords = {hardware:accelerometers},
	annote = {The [Sensor Glove](https://github.com/SensorGlove/SensorGlove) (only available
in Spanish, and unrelated to the SensorGlove developed by the Technical
University of Berlin [@hofmann1995]) was a project in 2017 by six alumni from
the University of Madrid. They built a textile glove with an accelerometer
which could be used to control a remote control car by holding one's hand in
the correct orientation. There is not enough detail available on the project's
website to determine the number of gestures which could be recognised by the
Sensor Glove.

},
}

@misc{noauthor_lucidvr_nodate,
	title = {{LucidVR}},
	url = {https://github.com/LucidVR/lucidgloves},
	keywords = {hardware:unique},
	annote = {[LucidVR](https://github.com/LucidVR/lucidgloves) is a project which aims to
provide affordable hand tracking for Virtual Reality (VR) games. LucidVR has an
emphasis on integrated force feedback, which allows the user to sense solid
objects in VR. User input is limited to measuring the amount by which each
finger is bent.

},
}

@misc{atltvhead_atltvhead_nodate,
	title = {Atltvhead {Gesture} {Recognition} {Bracer}},
	abstract = {Atltvhead Gesture Recognition Bracer - A TensorflowLite gesture detector for the atltvhead project and exploration into Data Science

This repository is my spin on Jennifer Wang's and Google Tensorflow's magic wand project, but for arm gestures},
	author = {{ATLTVHEAD}},
	keywords = {tech:accelerometer},
	annote = {The [gesture recognition bracer](https://github.com/ATLTVHEAD/Atltvhead-Gesture-Recognition-Bracer) created by live streamer [atltvhead](https://www.twitch.tv/atltvhead) is a bracer that straps to the user's forearm and allows them to control various live streaming settings by moving their forearm. The bracer contains a linear 6-axis accelerometer which provides linear and rotational acceleration for the two machine learning models which are trained: a CNN and a Long-Short Term Memory (LSTM) network.

},
}

@misc{nick_gillian_gesture_nodate,
	title = {Gesture {Recognition} {Toolkit} ({GRT})},
	url = {https://github.com/nickgillian/grt},
	abstract = {The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.},
	author = {{Nick Gillian}},
	keywords = {type:software-lib},
	annote = {The [Gesture Recognition Toolkit](https://github.com/nickgillian/grt) (GRT) contains numerous machine learning algorithms for creating gesture recognition applications from sensor data. It was initially developed by @gillian2014 and later expanded on by volunteer contributors. The GRT is unmaintained as of 2019.

},
}

@misc{mellis_gesture_nodate,
	title = {Gesture {Recognition} {Using} {Accelerometer} and {ESP}},
	url = {https://create.arduino.cc/projecthub/mellis/gesture-recognition-using-accelerometer-and-esp-71faa1},
	author = {{Mellis}},
	keywords = {tech:accelerometer, hardware:arduino, hardware:esp},
}

@incollection{noauthor_dynamic_2007,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Time} {Warping}},
	isbn = {978-3-540-74048-3},
	url = {https://doi.org/10.1007/978-3-540-74048-3_4},
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
	booktitle = {Information {Retrieval} for {Music} and {Motion}},
	publisher = {Springer Berlin Heidelberg},
	year = {2007},
	doi = {10.1007/978-3-540-74048-3_4},
	keywords = {model:dtw, background},
	pages = {69--84},
}

@inproceedings{goos_soapbox_2002,
	address = {Berlin, Heidelberg},
	title = {{SoapBox}: {A} {Platform} for {Ubiquitous} {Computing} {Research} and {Applications}},
	volume = {2414},
	isbn = {978-3-540-44060-4 978-3-540-45866-1},
	shorttitle = {{SoapBox}},
	url = {http://link.springer.com/10.1007/3-540-45866-2_11},
	doi = {10.1007/3-540-45866-2_11},
	abstract = {Designing, implementing and evaluating prototypes is a normal way of doing technical research. In recent years we have seen lots of research prototypes specifically designed for context awareness, future user interfaces and intelligent environment research. The problem with this type of specialised prototypes is that their lifetime is rather short and the valuable work done for them is not easily reusable. Our approach has been different as we have deliberately aimed towards a multipurpose platform that would be suitable for various ubiquitous computing related research themes. In this article we present the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box). Its main features are wired and wireless communications, in-built sensors, small size and low power consumption. We also introduce some results of research projects that have already used the platform successfully. Finally we conclude the paper with application scenarios for further work.},
	urldate = {2023-07-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Tuulari, Esa and Ylisaukko-oja, Arto},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Mattern, Friedemann and Naghshineh, Mahmoud},
	year = {2002},
	doi = {10.1007/3-540-45866-2_11},
	note = {74 citations (Semantic Scholar/DOI) [2023-07-11]
Book Title: Pervasive Computing
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, hardware:unique, from:cite.bib},
	pages = {125--138},
	annote = {They tried to do what the kinect did, (but failed)
},
	annote = {[TLDR] This article presents the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box), its main features are wired and wireless communications, in-built sensors, small size and low power consumption.},
	annote = {[TLDR] This article presents the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box), its main features are wired and wireless communications, in-built sensors, small size and low power consumption.},
}

@article{izotov_recognition_2021,
	title = {Recognition of handwritten {MNIST} digits on low-memory 2 {Kb} {RAM} {Arduino} board using {LogNNet} reservoir neural network},
	volume = {1155},
	issn = {1757-8981, 1757-899X},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/1155/1/012056},
	doi = {10.1088/1757-899X/1155/1/012056},
	abstract = {Abstract
            The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82\%. The algorithm was tested on a low-memory Arduino board with 2 Kb static RAM low-power microcontroller. The dependences of the accuracy and time of image recognition on the number of neurons in the reservoir have been investigated. The memory allocation demonstrates that the algorithm stores all the necessary information in RAM without using additional data storage, and operates with original images without preliminary processing. The simple structure of the algorithm, with appropriate training, can be adapted for wide practical application, for example, for creating mobile biosensors for early diagnosis of adverse events in medicine. The study results are important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing.},
	number = {1},
	urldate = {2023-07-11},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Izotov, Y A and Velichko, A A and Ivshin, A A and Novitskiy, R E},
	month = jun,
	year = {2021},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:nn, from:cite.bib, hardware:arduino, hardware:iot, app:resource-constrained, dataset:mnist},
	pages = {012056},
	annote = {[TLDR] The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82\% and is important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing.},
	file = {Full Text:/Users/brk/Zotero/storage/ZZ94MAW3/Izotov et al. - 2021 - Recognition of handwritten MNIST digits on low-mem.pdf:application/pdf},
}

@article{abeje_ethiopian_2022,
	title = {Ethiopian sign language recognition using deep convolutional neural network},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-022-12768-5},
	doi = {10.1007/s11042-022-12768-5},
	abstract = {In recent years, several technologies have been utilized to bridge the communication gap between persons who have hearing or speaking impairments and those who don't. This paper presents the development of a novel sign language recognition system which translates Ethiopian sign language (ETHSL) to Amharic alphabets using computer vision technology and Deep Convolutional Neural Network (CNN). The system accepts sign language images as input and gives Amharic text as the desired output. The proposed system comprises of three main stages which are: preprocessing, feature extraction, and recognition. The methodology employed involves data acquisition, preprocessing the acquired data, background normalization, image resizing, region of interest (ROI) identification, noise removal, brightness adjustment, and feature extraction, while Deep Convolutional Neural Network (CNN) was used for end-to-end classification. The data used in this study was acquired from students with hearing impairments at the Debre Markos Teaching College with an iPhone 6s phone which has a resolution of 3024 × 4020. The images are in JPEG file format and were collected in a controlled environment. The proposed system was implemented using Kera’s (Tensorflow2.3.0 as backend) in python and tested using the image dataset collected from Debre Markos Teaching College graduating students of 2012. The results show that the running time was minimized by adjusting the images to a suitable size and color. In addition, the results show an improved recognition accuracy compared to previous works. The proposed model achieves 98.5\% training, 95.59\% validation, and 98.3\% testing accuracy of recognition.},
	language = {en},
	number = {20},
	urldate = {2023-07-11},
	journal = {Multimedia Tools and Applications},
	author = {Abeje, Bekalu Tadele and Salau, Ayodeji Olalekan and Mengistu, Abreham Debasu and Tamiru, Nigus Kefyalew},
	month = aug,
	year = {2022},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, app:sign-language, app:ethiopian-sl},
	pages = {29027--29043},
	annote = {[TLDR] The development of a novel sign language recognition system which translates Ethiopian sign language (ETHSL) to Amharic alphabets using computer vision technology and Deep Convolutional Neural Network (CNN).},
}

@article{hurroo_sign_2020,
	title = {Sign {Language} {Recognition} {System} using {Convolutional} {Neural} {Network} and {Computer} {Vision}},
	url = {https://www.semanticscholar.org/paper/Sign-Language-Recognition-System-using-Neural-and-Hurroo-Elham/5ecc9d2073755a1364433d83df2a4007e42b16e1},
	abstract = {Conversing to a person with hearing disability is always a major challenge. Sign language has indelibly become the ultimate panacea and is a very powerful tool for individuals with hearing and speech disability to communicate their feelings and opinions to the world. It makes the integration process between them and others smooth and less complex. However, the invention of sign language alone, is not enough . There are many strings attached to this boon.The sign gestures often get mixed and confused for someone who has never learnt it or knows it in a different language. However, this communication gap which has existed for years can now be narrowed with the introduction of various techniques to automate the detection of sign gestures . In this paper, we introduce a Sign Language recognition using American Sign Language. In this study, the user must be able to capture images of the hand gesture using web camera and the system shall predict and display the name of the captured image. We use the HSV colour algorithm to detect the hand gesture and set the background to black. The images undergo a series of processing steps which include various Computer vision techniques such as the conversion to grayscale, dilation and mask operation. And the region of interest which, in our case is the hand gesture is segmented. The features extracted are the binary pixels of the images. We make use of Convolutional Neural Network(CNN) for training and to classify the images. We are able to recognise 10 American Sign gesture alphabets with high accuracy. Our model has achieved a remarkable accuracy of above 90\%.},
	urldate = {2023-07-11},
	journal = {International journal of engineering research and technology},
	author = {Hurroo, Mehreen and Elham, M.},
	month = dec,
	year = {2020},
	keywords = {model:cnn, classes:10, app:sign-language, app:american-sl, from:cite.bib, claims-to-be-first},
	annote = {@hurroo2020 trained a 2D Convolutional Neural Network (CNN) to recognise segmented images of ten letters signed in American Sign Language. This work does not attempt live recognition, but rather takes images of a hand on a plain background and attempts to recognise the sign being performed.

},
	annote = {[TLDR] A Sign Language recognition using American Sign Language which is able to recognise 10 American Sign gesture alphabets with high accuracy and has achieved a remarkable accuracy of above 90\%.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/IQGA9ZVK/Hurroo and Elham - 2020 - Sign Language Recognition System using Convolution.pdf:application/pdf},
}

@article{gupta_defocus_2019,
	title = {A {Defocus} {Based} {Novel} {Keyboard} {Design}},
	url = {https://ieeexplore.ieee.org/document/9066224/},
	doi = {10.1109/CICT48419.2019.9066224},
	abstract = {Depth map estimation from Defocus is a computer vision technique which has wide applications such as constructing the \$3D\$ setup from \$2D\$ image(s), image refocusing and reconstructing \$3D\$ scenes. In this paper, we propose an application of Depth from Defocus to a novel keyboard design for detecting keystrokes. The proposed keyboard can be integrated with devices such as mobile, PC and tablets and can be generated by either printing on plain paper or by projection on a flat surface. The proposed design utilizes measured defocus together with a precalibrated relation between the defocus amount and the keyboard pattern to infer the depth, which, along with the azimuth position of the stroke identifies the key. As the proposed design does not require any other hardware besides a monocular camera, this makes the proposed approach a cost effective and feasible solution for a portable keyboard.},
	urldate = {2023-07-11},
	journal = {2019 IEEE Conference on Information and Communication Technology},
	author = {Gupta, Priyanshu and Goswamy, Tushar and Kumar, Himanshu and Venkatesh, K.S.},
	month = dec,
	year = {2019},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2019 IEEE Conference on Information and Communication Technology (CICT)
ISBN: 9781728153988
Place: Allahabad, India
Publisher: IEEE},
	keywords = {tech:rgb, read-priority-3},
	pages = {1--6},
	annote = {[TLDR] The proposed design utilizes measured defocus together with a precalibrated relation between the defocus amount and the keyboard pattern to infer the depth, which, along with the azimuth position of the stroke identifies the key.},
}

@inproceedings{kratz_wiizards_2007,
	address = {Toronto, Canada},
	title = {Wiizards: {3D} gesture recognition for game play input},
	isbn = {978-1-59593-943-2},
	shorttitle = {Wiizards},
	url = {http://portal.acm.org/citation.cfm?doid=1328202.1328241},
	doi = {10.1145/1328202.1328241},
	abstract = {Gesture based input is an emerging technology gaining widespread popularity in interactive entertainment. The use of gestures provides intuitive and natural input mechanics for games, presenting an easy to learn yet richly immersive experience. In Wiizards, we explore the use of 3D accelerometer gestures in a multiplayer, zero sum game. Hidden Markov models are constructed for gesture recognition, providing increased flexibility and fluid tolerance. Users can strategically effect the outcome via combinations of gestures with limitless scalability.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Proceedings of the 2007 conference on {Future} {Play}  - {Future} {Play} '07},
	publisher = {ACM Press},
	author = {Kratz, Louis and Smith, Matthew and Lee, Frank J.},
	year = {2007},
	note = {41 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:accelerometer, model:hmm, app:gaming},
	pages = {209},
	annote = {[TLDR] In Wiizards, the use of 3D accelerometer gestures in a multiplayer, zero sum game is explored, and Hidden Markov models are constructed for gesture recognition, providing increased flexibility and fluid tolerance.},
	annote = {[TLDR] In Wiizards, the use of 3D accelerometer gestures in a multiplayer, zero sum game is explored, and Hidden Markov models are constructed for gesture recognition, providing increased flexibility and fluid tolerance.},
}

@article{heumer_grasp_2007,
	title = {Grasp {Recognition} with {Uncalibrated} {Data} {Gloves} - {A} {Comparison} of {Classification} {Methods}},
	url = {https://ieeexplore.ieee.org/document/4161001/},
	doi = {10.1109/VR.2007.352459},
	abstract = {This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out, where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was analyzed with 28 classifiers including different types of neural networks, decision trees, Bayes nets, and lazy learners. Each classifier was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.},
	urldate = {2023-07-11},
	journal = {2007 IEEE Virtual Reality Conference},
	author = {Heumer, Guido and Amor, Heni Ben and Weber, Matthias and Jung, Bernhard},
	month = mar,
	year = {2007},
	note = {64 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2007 IEEE Virtual Reality Conference
ISBN: 9781424409051
Place: Charlotte, NC, USA
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:flex, model:dt, model:nn, hardware:dataglove},
	pages = {19--26},
	annote = {[TLDR] It is shown that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves, and the best performing classification methods are identified.},
}

@article{alvi_pakistan_2007,
	title = {Pakistan {Sign} {Language} {Recognition} {Using} {Statistical} {Template} {Matching}},
	url = {https://www.semanticscholar.org/paper/Pakistan-Sign-Language-Recognition-Using-Template-Alvi-Azhar/2ca46ccbde2346b336adc12948b234c75ed37686},
	abstract = {Sign language recognition has been a topic of research since the first data glove was developed. Many researchers have attempted to recognize sign language through various techniques. However none of them have ventured into the area of Pakistan Sign Language (PSL). The Boltay Haath project aims at recognizing PSL gestures using Statistical Template Matching. The primary input device is the DataGlove5 developed by 5DT. Alternative approaches use camera-based recognition which, being sensitive to environmental changes are not always a good choice. This paper explains the use of Statistical Template Matching for gesture recognition in Boltay Haath. The system recognizes one handed alphabet signs from PSL. Keywords—Gesture Recognition, Pakistan Sign Language, Data Glove, Human Computer Interaction, Template Matching, Boltay Haath},
	urldate = {2023-07-12},
	journal = {World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering},
	author = {Alvi, A. and Azhar, M. and Usman, M. and Mumtaz, Suleman and Rafiq, Sameer and Rehman, RaziUr and Ahmed, Israr},
	month = mar,
	year = {2007},
	keywords = {tech:accelerometer, tech:flex, app:sign-language, hardware:dataglove, from:cite.bib, app:pakistan-sl, model:statistical-template-matching},
	annote = {@alvi2007 used a single-handed DataGlove5 (developed by @fifthdimensiontechnologies) to recognise 31 signs from Pakistan Sign Language (PSL) based on the standard deviations a given input gesture was from any of several reference gestures. Gestures using two hands was not attempted.

},
	annote = {[TLDR] The use of Statistical Template Matching for gesture recognition in Boltay Haath is explained and the system recognizes one handed alphabet signs from PSL.},
}

@article{moeslund_survey_2001,
	title = {A {Survey} of {Computer} {Vision}-{Based} {Human} {Motion} {Capture}},
	volume = {81},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S107731420090897X},
	doi = {10.1006/cviu.2000.0897},
	abstract = {A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition. Each process is discussed and divided into subprocesses and/or categories of methods to provide a reference to describe and compare the more than 130 publications covered by the survey. References are included throughout the paper to exemplify important issues and their relations to the various methods. A number of general assumptions used in this research field are identified and the character of these assumptions indicates that the research field is still in an early stage of development. To evaluate the state of the art, the major application areas are identified and performances are analyzed in light of the methods presented in the survey. Finally, suggestions for future research directions are offered.},
	language = {en},
	number = {3},
	urldate = {2023-07-12},
	journal = {Computer Vision and Image Understanding},
	author = {Moeslund, Thomas B. and Granum, Erik},
	month = mar,
	year = {2001},
	note = {2025 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:rgb, type:survey},
	pages = {231--268},
	annote = {[TLDR] A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented, with a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition.},
}

@inproceedings{moeslund_computer_1999,
	title = {Computer {Vision}-{Based} {Human} {Motion} {Capture} - {A} {Survey}},
	url = {https://www.semanticscholar.org/paper/Computer-Vision-Based-Human-Motion-Capture-A-Survey-Moeslund-Bajers/4a59c198bdb7ce6b081f6d9477bdf1e2c4324fba},
	abstract = {This technical report is the documentation of a survey within computer vision-based human motion capture. The idea with this report is to present previous work in a structural manner using taxonomies at di erent levels. The report is structured in the following way. The rst chapter gives an introduction to new types of interfaces where one class is known as the "Looking at People" domain. Within this class computer vision-based human motion capture is de ned. In chapter two motion capture is described in a larger context describing the di erent sensors used for motion capture. Chapter three presents previous taxonomies which are commented and a new taxonomy is suggested. Chapter four describes a number of assumptions which are used in computer vision-based human motion capture. The next four chapters each give a detailed description of the four classes present in the taxonomy suggested in chapter three. Finally a conclusion is given in the last chapter. Throughout the descriptions of the taxonomy and its four classes a number of examples are given from the literature. Preface This technical report is the documentation of a survey within computer vision-based human motion capture. The writing of the report was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.study titled: "Multiple Cues for Model-Based Human Motion Capture". A large number of papers have been read to write this report. Approximately 2/3 of them (107) are directly concerned with computer vision-based human motion capture. Detailed summaries of these papers can be found in another concurrently written technical report: Summaries of 107 Computer Vision-Based HumanMotion Capture Papers [108]. Whenever I use 'he' throughout the report it should be read as he/she. Finally I would like to thanks Moritz Stoerring for his help editing the report. Aalborg, Denmark, March 1999 Thomas B. Moeslund (tbm@vision.auc.dk)},
	urldate = {2023-07-12},
	author = {Moeslund, T. and Bajers, Fredrik},
	year = {1999},
	keywords = {tech:rgb, type:survey, survey-finsh:1998},
	annote = {[TLDR] This technical report is the documentation of a survey within computer vision-based human motion capture to present previous work in a structural manner using taxonomies at di erent levels.},
	annote = {[TLDR] This technical report is the documentation of a survey within computer vision-based human motion capture to present previous work in a structural manner using taxonomies at di erent levels.},
}

@inproceedings{moeslund_summaries_1999,
	title = {Summaries of 107 {Computer} {Vision}-{Based} {Human} {Motion} {Capture} {Papers}},
	url = {https://www.semanticscholar.org/paper/Summaries-of-107-Computer-Vision-Based-Human-Motion-Moeslund-Bajers/4f0e8c2b9fd0d6852b773b405e81aa768e449813},
	abstract = {This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. The survey gives a taxonomy for the papers described in this report. The summaries are presented in alphabetic order, with respect to the surname of the rst Author. I have tried to give objective summaries of the di erent papers and only give subjective critique in an item, comments, reserved for this purpose. I have also tried to use the same amount of energy and space on the di erent summaries. But large variations between the length of the di erent summaries can be observed. This is mainly due to the length of the papers, but also due to my interest in the individual papers. Preface This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. This report has been written as a separate report due to its size alone. The writing of the two reports was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.-study titled: "Multiple Cues for Model-Based Human Motion Capture". Whenever I use 'he' throughout the report is should be read as he/she. Aalborg, Denmark, March 1999. Thomas B. Moeslund (tbm@vision.auc.dk)},
	urldate = {2023-07-12},
	author = {Moeslund, T. and Bajers, Fredrik},
	year = {1999},
	keywords = {tech:rgb, type:survey},
	annote = {[TLDR] This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture and gives a taxonomy for the papers described in this report.},
}

@article{asadi-aghbolaghi_survey_2017,
	title = {A {Survey} on {Deep} {Learning} {Based} {Approaches} for {Action} and {Gesture} {Recognition} in {Image} {Sequences}},
	url = {http://ieeexplore.ieee.org/document/7961779/},
	doi = {10.1109/FG.2017.150},
	abstract = {The interest in action and gesture recognition has grown considerably in the last years. In this paper, we present a survey on current deep learning methodologies for action and gesture recognition in image sequences. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. We review the details of the proposed architectures, fusion strategies, main datasets, and competitions. We summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, discussing their main features and identify opportunities and challenges for future research.},
	urldate = {2023-07-12},
	journal = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
	author = {Asadi-Aghbolaghi, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo Jair and Ponce-Lopez, Victor and Baro, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
	month = may,
	year = {2017},
	note = {160 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: 2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)
ISBN: 9781509040230
Place: Washington, DC, DC, USA
Publisher: IEEE},
	keywords = {model:cnn, tech:rgb, model:lstm, type:survey},
	pages = {476--483},
	annote = {[TLDR] A taxonomy that summarizes important aspects of deep learning for approaching both action and gesture recognition in image sequences is introduced, and the main works proposed so far are summarized.},
	annote = {[TLDR] A taxonomy that summarizes important aspects of deep learning for approaching both action and gesture recognition in image sequences is introduced, and the main works proposed so far are summarized.},
	file = {Submitted Version:/Users/brk/Zotero/storage/Y9X75IZ5/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf:application/pdf;Submitted Version:/Users/brk/Zotero/storage/4BDF37WP/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf:application/pdf},
}

@article{raysarkar_hand_2013,
	title = {Hand {Gesture} {Recognition} {Systems}: {A} {Survey}},
	volume = {71},
	issn = {09758887},
	shorttitle = {Hand {Gesture} {Recognition} {Systems}},
	url = {http://research.ijcaonline.org/volume71/number15/pxc3889123.pdf},
	doi = {10.5120/12435-9123},
	abstract = {Gesture was the first mode of communication for the primitive cave men. Later on human civilization has developed the verbal communication very well. But still nonverbal communication has not lost its weightage. Such non – verbal communication are being used not only for the physically challenged people, but also for different applications in diversified areas, such as aviation, surveying, music direction etc. It is the best method to interact with the computer without using other peripheral devices, such as keyboard, mouse. Researchers around the world are actively engaged in development of robust and efficient gesture recognition system, more specially, hand gesture recognition system for various applications. The major steps associated with the hand gesture recognition system are; data acquisition, gesture modeling, feature extraction and hand gesture recognition. There are several sub-steps and methodologies associated with the above steps. Different researchers have followed different algorithm or sometimes have devised their own algorithm. The current research work reviews the work carried out in last twenty years and a brief comparison has been performed to analyze the difficulties encountered by these systems, as well as the limitation. Finally the desired characteristics of a robust and efficient hand gesture recognition system have been described. General Terms Hand gesture recognition, comparison},
	number = {15},
	urldate = {2023-07-12},
	journal = {International Journal of Computer Applications},
	author = {RaySarkar, Arpita and Sanyal, G. and Majumder, S.},
	month = jun,
	year = {2013},
	note = {69 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:accelerometer, tech:rgb, tech:rgbd, type:survey},
	pages = {25--37},
	annote = {[TLDR] The current research work reviews the work carried out in last twenty years and a brief comparison has been performed to analyze the difficulties encountered by these systems, as well as the limitation.},
	file = {Full Text:/Users/brk/Zotero/storage/T734BQCQ/RaySarkar et al. - 2013 - Hand Gesture Recognition Systems A Survey.pdf:application/pdf},
}

@article{thariq_ahmed_device_2020,
	title = {Device free human gesture recognition using {Wi}-{Fi} {CSI}: {A} survey},
	volume = {87},
	issn = {09521976},
	shorttitle = {Device free human gesture recognition using {Wi}-{Fi} {CSI}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197619302441},
	doi = {10.1016/j.engappai.2019.103281},
	abstract = {Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Thariq Ahmed, Hasmath Farhana and Ahmad, Hafisoh and C.V., Aravind},
	month = jan,
	year = {2020},
	note = {53 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, type:survey},
	pages = {103281},
}

@article{ahmed_df-wislr_2020,
	title = {{DF}-{WiSLR}: {Device}-{Free} {Wi}-{Fi}-based {Sign} {Language} {Recognition}},
	volume = {69},
	issn = {15741192},
	shorttitle = {{DF}-{WiSLR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119220301267},
	doi = {10.1016/j.pmcj.2020.101289},
	abstract = {Recent advancements in wireless technologies enable pervasive and device free gesture recognition that enable assisted living utilizing off the shelf commercial Wi-Fi devices. This paper proposes a Device-Free Wi-Fi-based Sign Language Recognition (DF-WiSLR) for recognizing 30 static and 19 dynamic sign gestures. The raw Channel State Information (CSI) acquired from the Wi-Fi device for 49 sign gestures, with a volunteer performing the sign gestures in home and office environments. The proposed system adopts machine learning classifiers such as SVM, KNN, RF, NB, and a deep learning classifier CNN, for measuring the gesture recognition accuracy. To address the practical limitation of building a voluminous dataset, DF-WiSLR augments the originally acquired CSI values with Additive White Gaussian Noise (AWGN). Higher-order cumulant features of orders 2, 3, and 4 are extracted from the original and augmented data, as the machine learning classifiers demand manual feature extraction. To reduce the computational complexity of machine learning classifiers, an informative and reduced optimal feature subset is selected using MIFS. Whilst the pre-processed original and augmented CSI values directly fed as input to an 8-layer deep CNN, it performs auto feature extraction and selection. DF-WiSLR reported better recognition accuracies with SVM for static and dynamic gestures in both home and office environments. SVM achieved 93.4\% 98.8\% and 98.9\% accuracies in home and office environments respectively, for static gestures. For dynamic gestures, 92.3\% recognition accuracy achieved in home environment. On augmented data, the corresponding gesture recognition accuracy values reported are 97.1\%, 99.9\%, 99.9\%, and 98.5\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Pervasive and Mobile Computing},
	author = {Ahmed, Hasmath Farhana Thariq and Ahmad, Hafisoh and Narasingamurthi, Kulasekharan and Harkat, Houda and Phang, Swee King},
	month = nov,
	year = {2020},
	note = {10 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {model:svm, tech:wifi, model:cnn, model:knn, model:nb, app:sign-language, movement:static, movement:dynamic, model:random-forests, classes:49},
	pages = {101289},
}

@article{dang_air_2020,
	title = {Air {Gesture} {Recognition} {Using} {WLAN} {Physical} {Layer} {Information}},
	volume = {2020},
	issn = {1530-8669, 1530-8677},
	url = {https://www.hindawi.com/journals/wcmc/2020/8546237/},
	doi = {10.1155/2020/8546237},
	abstract = {In recent years, the researchers have witnessed the important role of air gesture recognition in human-computer interactive (HCI), smart home, and virtual reality (VR). The traditional air gesture recognition method mainly depends on external equipment (such as special sensors and cameras) whose costs are high and also with a limited application scene. In this paper, we attempt to utilize channel state information (CSI) derived from a WLAN physical layer, a Wi-Fibased air gesture recognition system, namely, WiNum, which solves the problems of users’ privacy and energy consumption compared with the approaches using wearable sensors and depth cameras. In the process of recognizing the WiNum method, the collected raw data of CSI should be screened, among which can reflect the gesture motion. Meanwhile, the screened data should be preprocessed by noise reduction and linear transformation. After preprocessing, the joint of amplitude information and phase information is extracted, to match and recognize different air gestures by using the S-DTW algorithm which combines dynamic time warping algorithm (DTW) and support vector machine (SVM) properties. Comprehensive experiments demonstrate that under two different indoor scenes, WiNum can achieve higher recognition accuracy for air number gestures; the average recognition accuracy of each motion reached more than 93\%, in order to achieve effective recognition of air gestures.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Wireless Communications and Mobile Computing},
	author = {Dang, Xiaochao and Liu, Yang and Hao, Zhanjun and Tang, Xuhao and Shao, Chenguang},
	month = aug,
	year = {2020},
	note = {9 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {model:svm, tech:wifi, classes:10, model:dtw},
	pages = {1--14},
	annote = {[TLDR] This paper attempts to utilize channel state information (CSI) derived from a WLAN physical layer, a Wi-Fibased air gesture recognition system, namely, WiNum, which solves the problems of users’ privacy and energy consumption compared with the approaches using wearable sensors and depth cameras.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/9WJ72HZQ/Dang et al. - 2020 - Air Gesture Recognition Using WLAN Physical Layer .pdf:application/pdf},
}

@article{hao_wi-sl_2020,
	title = {Wi-{SL}: {Contactless} {Fine}-{Grained} {Gesture} {Recognition} {Uses} {Channel} {State} {Information}},
	volume = {20},
	issn = {1424-8220},
	shorttitle = {Wi-{SL}},
	url = {https://www.mdpi.com/1424-8220/20/14/4025},
	doi = {10.3390/s20144025},
	abstract = {In recent years, with the development of wireless sensing technology and the widespread popularity of WiFi devices, human perception based on WiFi has become possible, and gesture recognition has become an active topic in the field of human-computer interaction. As a kind of gesture, sign language is widely used in life. The establishment of an effective sign language recognition system can help people with aphasia and hearing impairment to better interact with the computer and facilitate their daily life. For this reason, this paper proposes a contactless fine-grained gesture recognition method using Channel State Information (CSI), namely Wi-SL. This method uses a commercial WiFi device to establish the correlation mapping between the amplitude and phase difference information of the subcarrier level in the wireless signal and the sign language action, without requiring the user to wear any device. We combine an efficient denoising method to filter environmental interference with an effective selection of optimal subcarriers to reduce the computational cost of the system. We also use K-means combined with a Bagging algorithm to optimize the Support Vector Machine (SVM) classification (KSB) model to enhance the classification of sign language action data. We implemented the algorithms and evaluated them for three different scenarios. The experimental results show that the average accuracy of Wi-SL gesture recognition can reach 95.8\%, which realizes device-free, non-invasive, high-precision sign language gesture recognition.},
	language = {en},
	number = {14},
	urldate = {2023-07-12},
	journal = {Sensors},
	author = {Hao, Zhanjun and Duan, Yu and Dang, Xiaochao and Liu, Yang and Zhang, Daiyang},
	month = jul,
	year = {2020},
	note = {21 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {model:svm, tech:wifi, app:sign-language, model:k-means},
	pages = {4025},
	annote = {[TLDR] This paper proposes a contactless fine-grained gesture recognition method using Channel State Information (CSI), namely Wi-SL, which realizes device-free, non-invasive, high-precision sign language gesture recognition.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/IM4P4L7G/Hao et al. - 2020 - Wi-SL Contactless Fine-Grained Gesture Recognitio.pdf:application/pdf},
}

@article{hussain_review_2020,
	title = {A review and categorization of techniques on device-free human activity recognition},
	volume = {167},
	issn = {10848045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804520302125},
	doi = {10.1016/j.jnca.2020.102738},
	abstract = {Human activity recognition has gained importance in recent years due to its applications in various fields such as health, security and surveillance, entertainment, and intelligent environments. A significant amount of work has been done on human activity recognition and researchers have leveraged different approaches, such as wearable, object-tagged, and devicefree, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the period 2010-2018 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything, instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the field of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interactionbased. We further divide these areas into ten different sub-topics and present the latest research work in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these sub-areas. Specifically, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to give the reader, a complete overview of the state-of-the-art techniques and trends in different sub-areas of human activity recognition. In the end, we discuss open research issues and provide future research directions in the field of human activity recognition.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Journal of Network and Computer Applications},
	author = {Hussain, Zawar and Sheng, Quan Z. and Zhang, Wei Emma},
	month = oct,
	year = {2020},
	note = {57 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, read-priority-1, type:survey, survey-start:2010, survey-finsh:2018},
	pages = {102738},
	file = {Submitted Version:/Users/brk/Zotero/storage/5M8W2RNX/Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf:application/pdf},
}

@article{wang_csi-based_2021,
	title = {{CSI}-based human sensing using model-based approaches: a survey},
	volume = {8},
	issn = {2288-5048},
	shorttitle = {{CSI}-based human sensing using model-based approaches},
	url = {https://academic.oup.com/jcde/article/8/2/510/6137731},
	doi = {10.1093/jcde/qwab003},
	abstract = {Abstract
            Currently, human sensing draws much attention in the field of ubiquitous computing, and human sensing based on WiFi CSI (channel state information) becomes a hot research topic due to the easy deployment and availability of WiFi devices. Although various human sensing applications based on the CSI signal model are emerging, the model-based approach has not been studied thoroughly. This paper provides a comprehensive survey of the latest model-based human sensing methods and their applications. First, the CSI signal and framework of model-based human sensing methods are introduced. Then, related models and fundamental signal preprocessing techniques are described. Next, typical human sensing applications are investigated, and the crucial characteristics are summarized. Finally, the advantages, limitations, and future research trends of model-based human sensing methods are concluded in this paper.},
	language = {en},
	number = {2},
	urldate = {2023-07-12},
	journal = {Journal of Computational Design and Engineering},
	author = {Wang, Zhengjie and Huang, Zehua and Zhang, Chengming and Dou, Wenwen and Guo, Yinjing and Chen, Da},
	month = apr,
	year = {2021},
	note = {11 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, type:survey},
	pages = {510--523},
	annote = {[TLDR] A comprehensive survey of the latest model-based human sensing methods and their applications based on the CSI signal model and related models and fundamental signal preprocessing techniques is provided.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/JW8EU7FV/Wang et al. - 2021 - CSI-based human sensing using model-based approach.pdf:application/pdf},
}

@article{wu_witraj_2023,
	title = {{WiTraj}: {Robust} {Indoor} {Motion} {Tracking} {With} {WiFi} {Signals}},
	volume = {22},
	issn = {1536-1233, 1558-0660, 2161-9875},
	shorttitle = {{WiTraj}},
	url = {https://ieeexplore.ieee.org/document/9645160/},
	doi = {10.1109/TMC.2021.3133114},
	abstract = {WiFi-based device-free motion tracking systems track persons without requiring them to carry any device. Existing work has explored signal parameters such as time-of-flight (ToF), angle-of-arrival (AoA), and Doppler-frequency-shift (DFS) extracted from WiFi channel state information (CSI) to locate and track people in a room. However, they are not robust due to unreliable estimation of signal parameters. ToF and AoA estimations are not accurate for current standards-compliant WiFi devices that typically have only two antennas and limited channel bandwidth. On the other hand, DFS can be extracted relatively easily on current devices but is susceptible to the high noise level and random phase offset in CSI measurement, which results in a speed-sign-ambiguity problem and renders ambiguous walking speeds. This paper proposes WiTraj, a device-free indoor motion tracking system using commodity WiFi devices. WiTraj improves tracking robustness from three aspects: 1) It significantly improves DFS estimation quality by using the ratio of the CSI from two antennas of each receiver, 2) To better track human walking, it leverages multiple receivers placed at different viewing angles to capture human walking and then intelligently combines the best views to achieve a robust trajectory reconstruction, and, 3) It differentiates walking from in-place activities, which are typically interleaved in daily life, so that non-walking activities do not cause tracking errors. Experiments show that WiTraj can significantly improve tracking accuracy in typical environments compared to existing DFS-based systems. Evaluations across 9 participants and 3 different environments show that the median tracking error {\textless}inline-formula{\textgreater}{\textless}tex-math notation="LaTeX"{\textgreater}\${\textless}2.5{\textbackslash}\%\${\textless}/tex-math{\textgreater}{\textless}alternatives{\textgreater}{\textless}mml:math{\textgreater}{\textless}mml:mrow{\textgreater}{\textless}mml:mo{\textgreater}{\textless}{\textless}/mml:mo{\textgreater}{\textless}mml:mn{\textgreater}2{\textless}/mml:mn{\textgreater}{\textless}mml:mo{\textgreater}.{\textless}/mml:mo{\textgreater}{\textless}mml:mn{\textgreater}5{\textless}/mml:mn{\textgreater}{\textless}mml:mo{\textgreater}\%{\textless}/mml:mo{\textgreater}{\textless}/mml:mrow{\textgreater}{\textless}/mml:math{\textgreater}{\textless}inline-graphic xlink:href="wu-ieq1-3133114.gif"/{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} for typical room-sized trajectories.},
	number = {5},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Wu, Dan and Zeng, Youwei and Gao, Ruiyang and Li, Shenjie and Li, Yang and Shah, Rahul C. and Lu, Hong and Zhang, Daqing},
	month = may,
	year = {2023},
	note = {13 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi},
	pages = {3062--3078},
	annote = {[TLDR] Experiments show that WiTraj can significantly improve tracking accuracy in typical environments compared to existing DFS-based systems, and differentiates walking from in-place activities, which are typically interleaved in daily life, so that non-walking activities do not cause tracking errors.},
	file = {Full Text:/Users/brk/Zotero/storage/CHVHRXH7/Wu et al. - 2023 - WiTraj Robust Indoor Motion Tracking With WiFi Si.pdf:application/pdf},
}

@article{alazrai_dataset_2020,
	title = {A dataset for {Wi}-{Fi}-based human-to-human interaction recognition},
	volume = {31},
	issn = {23523409},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S235234092030562X},
	doi = {10.1016/j.dib.2020.105668},
	abstract = {This paper presents a dataset for Wi-Fi-based human-tohuman interaction recognition that comprises twelve different interactions performed by 40 different pairs of subjects in an indoor environment. Each pair of subjects performed ten trials of each of the twelve interactions and the total number of trials recorded in our dataset for all the 40 pairs of subjects is 4800 trials (i.e., 40 pairs of subjects × 12 interactions × 10 trials). The publicly available CSI tool [1] is used to record the Wi-Fi signals transmitted from a commercial off-the-shelf access point, namely the Sagemcom 2704 access point, to a desktop computer that is equipped with an Intel 5300 network interface card. The recorded Wi-Fi signals consist of the Received Signal Strength Indicator (RSSI) values and the Channel State Information (CSI) values. Unlike the publicly available Wi-Fi-based human activity datasets, which mainly have focused on activities performed by a single human, our dataset provides a collection of Wi-Fi signals that are recorded for 40 different pairs of subjects while performing twelve two-person interactions. The presented dataset can be exploited to advance Wi-Fi-based human activity recognition in different aspects, such as the use of},
	language = {en},
	urldate = {2023-07-12},
	journal = {Data in Brief},
	author = {Alazrai, Rami and Awad, Ali and Alsaify, Baha’A. and Hababeh, Mohammad and Daoud, Mohammad I.},
	month = aug,
	year = {2020},
	note = {26 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {classes:12, tech:wifi, type:dataset, participants:40},
	pages = {105668},
	file = {Full Text:/Users/brk/Zotero/storage/TNBL8TTY/Alazrai et al. - 2020 - A dataset for Wi-Fi-based human-to-human interacti.pdf:application/pdf},
}

@article{chen_cross-domain_2023,
	title = {Cross-{Domain} {WiFi} {Sensing} with {Channel} {State} {Information}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Cross-{Domain} {WiFi} {Sensing} with {Channel} {State} {Information}},
	url = {https://dl.acm.org/doi/10.1145/3570325},
	doi = {10.1145/3570325},
	abstract = {The past years have witnessed the rapid conceptualization and development of wireless sensing based on
              Channel State Information (CSI)
              with commodity WiFi devices. Recent studies have demonstrated the vast potential of WiFi sensing in detection, recognition, and estimation applications. However, the widespread deployment of WiFi sensing systems still faces a significant challenge: how to ensure the sensing performance when exposing a pre-trained sensing system to new domains, such as new environments, different configurations, and unseen users, without data collection and system retraining. This survey provides a comprehensive review of recent research efforts on cross-domain WiFi Sensing. We first introduce the mathematical model of CSI and explore the impact of different domains on CSI. Then we present a general workflow of cross-domain WiFi sensing systems, which consists of signal processing and cross-domain sensing. Five cross-domain sensing algorithms, including domain-invariant feature extraction, virtual sample generation, transfer learning, few-shot learning and big data solution, are summarized to show how they achieve high sensing accuracy when encountering new domains. The advantages and limitations of each algorithm are also summarized and the performance comparison is made based on different applications. Finally, we discuss the remaining challenges to further promote the practical usability of cross-domain WiFi sensing systems.},
	language = {en},
	number = {11},
	urldate = {2023-07-12},
	journal = {ACM Computing Surveys},
	author = {Chen, Chen and Zhou, Gang and Lin, Youfang},
	month = nov,
	year = {2023},
	note = {115 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, type:survey},
	pages = {1--37},
	annote = {[TLDR] Five cross-domain sensing algorithms, including domain-invariant feature extraction, virtual sample generation, transfer learning, few-shot learning and big data solution, are summarized to show how they achieve high sensing accuracy when encountering new domains.},
}

@article{wang_placement_2022,
	title = {Placement {Matters}: {Understanding} the {Effects} of {Device} {Placement} for {WiFi} {Sensing}},
	volume = {6},
	issn = {2474-9567},
	shorttitle = {Placement {Matters}},
	url = {https://dl.acm.org/doi/10.1145/3517237},
	doi = {10.1145/3517237},
	abstract = {WiFi-based contactless sensing has found numerous applications in the fields of smart home and health care owning to its low-cost, non-intrusive and privacy-preserving characteristics. While promising in many aspects, the limited sensing range and interference issues still exist, hindering the adoption of WiFi sensing in real world. In this paper, inspired by the SNR (signal-to-noise ratio) metric in communication theory, we propose a new metric named SSNR (sensing-signal-to-noise-ratio) to quantify the sensing capability of WiFi systems. We theoretically model the effect of transmitter-receiver distance on sensing coverage. We show that in LoS scenario, the sensing coverage area increases first from a small oval to a maximal one and then decreases. When the transmitter-receiver distance further increases, the coverage area is separated into two ovals located around the two transceivers respectively. We demonstrate that, instead of applying complex signal processing scheme or advanced hardware, by just properly placing the transmitter and receiver, the two well-known issues in WiFi sensing (i.e., small range and severe interference) can be greatly mitigated. Specifically, by properly placing the transmitter and receiver, the coverage of human walking sensing can be expanded by around 200\%. By increasing the transmitter-receiver distance, a target's fine-grained respiration can still be accurately sensed with one interferer sitting just 0.5 m away.},
	language = {en},
	number = {1},
	urldate = {2023-07-12},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Wang, Xuanzhi and Niu, Kai and Xiong, Jie and Qian, Bochong and Yao, Zhiyun and Lou, Tairong and Zhang, Daqing},
	month = mar,
	year = {2022},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi},
	pages = {1--25},
	annote = {[TLDR] A new metric named SSNR (sensing-signal-to-noise-ratio) is proposed to quantify the sensing capability of WiFi systems and it is demonstrated that by properly placing the transmitter and receiver, the coverage of human walking sensing can be expanded by around 200\%.},
}

@article{alazrai_end--end_2020,
	title = {An {End}-to-{End} {Deep} {Learning} {Framework} for {Recognizing} {Human}-to-{Human} {Interactions} {Using} {Wi}-{Fi} {Signals}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9243938/},
	doi = {10.1109/ACCESS.2020.3034849},
	abstract = {Channel state information (CSI)-based human activity recognition plays an essential role in various application domains, such as security, healthcare, and Internet of Things. Most existing CSI-based activity recognition approaches rely on manually designed features that are classified using traditional classification methods. Furthermore, the use of deep learning methods for CSI-based activity recognition is still at its infancy with most of the existing approaches focus on recognizing single-human activities. The current study explores the feasibility of utilizing deep learning methods to recognize human-to-human interactions (HHIs) using CSI signals. Particularly, we introduce an end-to-end deep learning framework that comprises three phases, which are the input, feature extraction, and recognition phases. The input phase converts the raw CSI signals into CSI images that comprise time, frequency, and spatial information. In the feature extraction phase, a novel convolutional neural network (CNN) is designed to automatically extract deep features from the CSI images. Finally, the extracted features are fed to the recognition phase to identify the class of the HHI associated with each CSI image. The performance of our proposed framework is assessed using a publicly available CSI dataset that was acquired from 40 different pairs of subjects while performing 13 HHIs. Our proposed framework achieved an average recognition accuracy of 86.3\% across all HHIs. Moreover, the experiments indicate that our proposed framework enabled significant improvements over the results achieved using three state-of-the-art pre-trained CNNs as well as the results obtained using four different conventional classifiers that employs traditional handcrafted features.},
	urldate = {2023-07-12},
	journal = {IEEE Access},
	author = {Alazrai, Rami and Hababeh, Mohammad and Alsaify, Baha A. and Ali, Mostafa Z. and Daoud, Mohammad I.},
	year = {2020},
	note = {25 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {classes:13, model:cnn, participants:40, tech:wifi},
	pages = {197695--197710},
	annote = {[TLDR] This study introduces an end-to-end deep learning framework that comprises three phases, which are the input, feature extraction, and recognition phases and indicates that the proposed framework enabled significant improvements over the results achieved using three state-of-the-art pre-trained CNNs as well as the results obtained using four different conventional classifiers that employs traditional handcrafted features.},
	annote = {[TLDR] This study introduces an end-to-end deep learning framework that comprises three phases, which are the input, feature extraction, and recognition phases and indicates that the proposed framework enabled significant improvements over the results achieved using three state-of-the-art pre-trained CNNs as well as the results obtained using four different conventional classifiers that employs traditional handcrafted features.},
	file = {Full Text:/Users/brk/Zotero/storage/JAT5WT5H/Alazrai et al. - 2020 - An End-to-End Deep Learning Framework for Recogniz.pdf:application/pdf},
}

@article{tan_wifinger_2016,
	title = {{WiFinger}: leveraging commodity {WiFi} for fine-grained finger gesture recognition},
	shorttitle = {{WiFinger}},
	url = {https://dl.acm.org/doi/10.1145/2942358.2942393},
	doi = {10.1145/2942358.2942393},
	abstract = {Gesture recognition has become increasingly important in human-computer interaction (HCI) and can support a broad array of emerging applications, such as smart home, virtual reality, and mobile gaming. Traditional approaches usually rely on dedicated sensors that are worn by the user or cameras that require line of sight. In this paper, we present fine-grained finger gesture recognition by using a single commodity WiFi device without requiring user to wear any sensors. Our low-cost system, WiFinger, takes advantages of the fine-grained Channel State Information (CSI) available from commodity WiFi devices and the prevalence of WiFi network infrastructures. It senses and identifies subtle movements of finger gestures by examining the unique patterns exhibited in the detailed CSI. In WiFigner, we devise environmental noise removal mechanism to mitigate the effect of signal dynamic due to the environment changes. Moreover, we propose to capture the intrinsic gesture behavior to deal with individual diversity and gesture inconsistency. Our experimental evaluation in both home and office environments demonstrates that our system can achieve over 93\% recognition accuracy and is robust to both environment changes and individual diversity. Results also show that our system can work with WiFi beacon signals and provides accurate gesture recognition under NLOS scenarios.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
	author = {Tan, Sheng and Yang, Jie},
	month = jul,
	year = {2016},
	note = {234 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiHoc'16: The Seventeenth ACM International Symposium on Mobile Ad Hoc Networking and Computing
ISBN: 9781450341844
Place: Paderborn Germany
Publisher: ACM},
	keywords = {model:dtw, tech:wifi},
	pages = {201--210},
	annote = {[TLDR] This paper presents fine-grained finger gesture recognition by using a single commodity WiFi device without requiring user to wear any sensors and proposes to capture the intrinsic gesture behavior to deal with individual diversity and gesture inconsistency.},
	annote = {[TLDR] This paper presents fine-grained finger gesture recognition by using a single commodity WiFi device without requiring user to wear any sensors and proposes to capture the intrinsic gesture behavior to deal with individual diversity and gesture inconsistency.},
}

@article{sheng_deep_2020,
	title = {Deep {Spatial}–{Temporal} {Model} {Based} {Cross}-{Scene} {Action} {Recognition} {Using} {Commodity} {WiFi}},
	volume = {7},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/8993738/},
	doi = {10.1109/JIOT.2020.2973272},
	abstract = {With the popularization of Internet-of-Things (IoT) systems, passive action recognition on channel state information (CSI) has attracted much attention. Most conventional work under the machine-learning framework utilizes handcrafted features (e.g., statistic features) that are unable to sufficiently describe the sequence data and heavily rely on designers’ experiences. Therefore, how to automatically learn abundant spatial–temporal information from CSI data is a topic worthy of study. In this article, we propose a deep learning framework that integrates spatial features learned from the convolutional neural network (CNN) into the temporal model multilayer bidirectional long short-term memory (Bi-LSTM). Specifically, CSI streams are segmented into a series of patches, from which spatial features are extracted by our designed CNN structure. Considering long-term dependencies between adjacent sequences, the fully connected layer of CNN for each patch is taken as the Bi-LSTM sequential input to further capture temporal features. Our model is appealing in that it can simultaneously learn temporal dynamics and convolutional perceptual representations. To the best of our knowledge, this is the first work to explore deep spatial–temporal features for CSI-based action recognition. Furthermore, in order to solve the problem that the trained model fully fails with environmental changes, we use the off-the-shelf model as the pretrained model and fine-tune it in the new scenario. The transfer method is able to realize cross-scene action recognition with low computational consumption and satisfactory accuracy. We carry out experiments on indoor data and the experimental results validate the effectiveness of our algorithm.},
	number = {4},
	urldate = {2023-07-12},
	journal = {IEEE Internet of Things Journal},
	author = {Sheng, Biyun and Xiao, Fu and Sha, Letian and Sun, Lijuan},
	month = apr,
	year = {2020},
	note = {36 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, model:cnn, model:lstm},
	pages = {3592--3601},
	annote = {[TLDR] A deep learning framework that integrates spatial features learned from the convolutional neural network (CNN) into the temporal model multilayer bidirectional long short-term memory (Bi-LSTM) and is the first work to explore deep spatial–temporal features for CSI-based action recognition.},
}

@article{chen_wifi_2019,
	title = {{WiFi} {CSI} {Based} {Passive} {Human} {Activity} {Recognition} {Using} {Attention} {Based} {BLSTM}},
	volume = {18},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/8514811/},
	doi = {10.1109/TMC.2018.2878233},
	abstract = {Human activity recognition can benefit various applications including healthcare services and context awareness. Since human actions will influence WiFi signals, which can be captured by the channel state information (CSI) of WiFi, WiFi CSI based human activity recognition has gained more and more attention. Due to the complex relationship between human activities and WiFi CSI measurements, the accuracies of current recognition systems are far from satisfactory. In this paper, we propose a new deep learning based approach, i.e., attention based bi-directional long short-term memory (ABLSTM), for passive human activity recognition using WiFi CSI signals. The BLSTM is employed to learn representative features in two directions from raw sequential CSI measurements. Since the learned features may have different contributions for final activity recognition, we leverage on an attention mechanism to assign different weights for all the learned features. Real experiments have been carried out to evaluate the performance of the proposed ABLSTM for human activity recognition. The experimental results show that our proposed ABLSTM is able to achieve the best recognition performance for all activities when compared with some benchmark approaches.},
	number = {11},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Chen, Zhenghua and Zhang, Le and Jiang, Chaoyang and Cao, Zhiguang and Cui, Wei},
	month = nov,
	year = {2019},
	note = {192 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, model:lstm},
	pages = {2714--2724},
	annote = {[TLDR] This paper proposes a new deep learning based approach, i.e., attention based bi-directional long short-term memory (ABLSTM) for passive human activity recognition using WiFi CSI signals, employed to learn representative features in two directions from raw sequential CSI measurements.},
}

@article{feng_wi-multi_2019,
	title = {Wi-{Multi}: {A} {Three}-{Phase} {System} for {Multiple} {Human} {Activity} {Recognition} {With} {Commercial} {WiFi} {Devices}},
	volume = {6},
	issn = {2327-4662, 2372-2541},
	shorttitle = {Wi-{Multi}},
	url = {https://ieeexplore.ieee.org/document/8710328/},
	doi = {10.1109/JIOT.2019.2915989},
	abstract = {Channel state information-based activity recognition has gathered immense attention over recent years. Many existing works achieved desirable performance in various applications, including healthcare, security, and Internet of Things, with different machine learning algorithms. However, they usually fail to consider the availability of enough samples to be trained. Besides, many applications only focus on the scenario where only single subject presents. To address these challenges, in this paper, we propose a three-phase system Wi-multi that targets at recognizing multiple human activities in a wireless environment. Different system phases are applied according to the size of available collected samples. Specifically, distance-based classification using dynamic time warping is applied when there are few samples in the profile. Then, support vector machine is employed when representative features can be extracted from training samples. Lastly, recurrent neural networks is exploited when a large number of samples are available. Extensive experiments results show that Wi-multi achieves an accuracy of 96.1\% on average. It is also able to achieve a desirable tradeoff between accuracy and efficiency in different phases.},
	number = {4},
	urldate = {2023-07-12},
	journal = {IEEE Internet of Things Journal},
	author = {Feng, Chunhai and Arshad, Sheheryar and Zhou, Siwang and Cao, Dun and Liu, Yonghe},
	month = aug,
	year = {2019},
	note = {37 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {classes:5, tech:wifi, model:dtw, classes:4},
	pages = {7293--7304},
	annote = {[TLDR] A three-phase system Wi-multi that targets at recognizing multiple human activities in a wireless environment and is able to achieve a desirable tradeoff between accuracy and efficiency in different phases is proposed.},
	annote = {[TLDR] A three-phase system Wi-multi that targets at recognizing multiple human activities in a wireless environment and is able to achieve a desirable tradeoff between accuracy and efficiency in different phases is proposed.},
}

@article{yousefi_survey_2017,
	title = {A {Survey} on {Behavior} {Recognition} {Using} {WiFi} {Channel} {State} {Information}},
	volume = {55},
	issn = {0163-6804},
	url = {http://ieeexplore.ieee.org/document/8067693/},
	doi = {10.1109/MCOM.2017.1700082},
	abstract = {In this article, we present a survey of recent advances in passive human behavior recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems. The movement of the human body parts cause changes in the wireless signal reflections, which result in variations in the CSI. By analyzing the data streams of CSIs for different activities and comparing them against stored models, human behavior can be recognized. This is done by extracting features from CSI data streams and using machine learning techniques to build models and classifiers. The techniques from the literature that are presented herein have great performance; however, instead of the machine learning techniques employed in these works, we propose to use deep learning techniques such as long-short term memory (LSTM) recurrent neural networking (RNN) and show the improved performance. We also discuss different challenges such as environment change, frame rate selection, and the multi-user scenario; and finally suggest possible directions for future work.},
	number = {10},
	urldate = {2023-07-12},
	journal = {IEEE Communications Magazine},
	author = {Yousefi, Siamak and Narui, Hirokazu and Dayal, Sankalp and Ermon, Stefano and Valaee, Shahrokh},
	month = oct,
	year = {2017},
	note = {245 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, model:rnn, model:lstm, type:survey},
	pages = {98--104},
	annote = {[TLDR] A survey of recent advances in passive human behavior recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems is presented and deep learning techniques such as long-short term memory (LSTM) recurrent neural networking (RNN) are proposed and shown the improved performance.},
}

@article{zeng_farsense_2019,
	title = {{FarSense}: {Pushing} the {Range} {Limit} of {WiFi}-based {Respiration} {Sensing} with {CSI} {Ratio} of {Two} {Antennas}},
	volume = {3},
	issn = {2474-9567},
	shorttitle = {{FarSense}},
	url = {https://dl.acm.org/doi/10.1145/3351279},
	doi = {10.1145/3351279},
	abstract = {The past few years have witnessed the great potential of exploiting channel state information retrieved from commodity WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. On the other hand, most home environments only have one WiFi access point and it may not be located in the same room as the target. This sensing range constraint greatly limits the application of the proposed approaches in real life.
            This paper presents FarSense--the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair. FarSense works well even when one of the transceivers is located in another room, moving a big step towards real-life deployment. We propose two novel schemes to achieve this goal: (1) Instead of applying the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range; (2) The division operation further enables us to utilize the phase information which is not usable with one single antenna for sensing. The orthogonal amplitude and phase are elaborately combined to address the "blind spots" issue and further increase the sensing range. Extensive experiments show that FarSense is able to accurately monitor human respiration even when the target is 8 meters away from the transceiver pair, increasing the sensing range by more than 100\%.1 We believe this is the first system to enable through-wall respiration sensing with commodity WiFi devices and the proposed method could also benefit other sensing applications.},
	language = {en},
	number = {3},
	urldate = {2023-07-12},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Zeng, Youwei and Wu, Dan and Xiong, Jie and Yi, Enze and Gao, Ruiyang and Zhang, Daqing},
	month = sep,
	year = {2019},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi},
	pages = {1--26},
	annote = {[TLDR] FarSense is the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair and is believed to be the first system to enable through-wall respiration sensing with commodity WiFi devices.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/9DWYS5JZ/Zeng et al. - 2019 - FarSense Pushing the Range Limit of WiFi-based Re.pdf:application/pdf},
}

@article{halperin_tool_2011,
	title = {Tool release: gathering 802.11n traces with channel state information},
	volume = {41},
	issn = {0146-4833},
	shorttitle = {Tool release},
	url = {https://dl.acm.org/doi/10.1145/1925861.1925870},
	doi = {10.1145/1925861.1925870},
	abstract = {We are pleased to announce the release of a tool that records detailed measurements of the wireless channel along with received 802.11 packet traces. It runs on a commodity 802.11n NIC, and records Channel State Information (CSI) based on the 802.11 standard. Unlike Receive Signal Strength Indicator (RSSI) values, which merely capture the total power received at the listener, the CSI contains information about the channel between sender and receiver at the level of individual data subcarriers, for each pair of transmit and receive antennas.
            Our toolkit uses the Intel WiFi Link 5300 wireless NIC with 3 antennas. It works on up-to-date Linux operating systems: in our testbed we use Ubuntu 10.04 LTS with the 2.6.36 kernel. The measurement setup comprises our customized versions of Intel's close-source firmware and open-source iwlwifi wireless driver, userspace tools to enable these measurements, access point functionality for controlling both ends of the link, and Matlab (or Octave) scripts for data analysis. We are releasing the binary of the modified firmware, and the source code to all the other components.},
	language = {en},
	number = {1},
	urldate = {2023-07-12},
	journal = {ACM SIGCOMM Computer Communication Review},
	author = {Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David},
	month = jan,
	year = {2011},
	note = {1239 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, type:seminal},
	pages = {53--53},
	annote = {[TLDR] The measurement setup comprises the customized versions of Intel's close-source firmware and open-source iwlwifi wireless driver, userspace tools to enable these measurements, access point functionality for controlling both ends of the link, and Matlab scripts for data analysis.},
}

@article{adib_see_2013,
	title = {See through walls with {WiFi}!},
	url = {https://dl.acm.org/doi/10.1145/2486001.2486039},
	doi = {10.1145/2486001.2486039},
	abstract = {Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM},
	author = {Adib, Fadel and Katabi, Dina},
	month = aug,
	year = {2013},
	note = {681 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: SIGCOMM'13: ACM SIGCOMM 2013 Conference
ISBN: 9781450320566
Place: Hong Kong China
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference},
	pages = {75--86},
	annote = {[TLDR] This paper shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam, and shows how to use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target.},
	file = {Full Text:/Users/brk/Zotero/storage/QWSG2ZW8/Adib and Katabi - 2013 - See through walls with WiFi!.pdf:application/pdf},
}

@article{kotaru_spotfi_2015,
	title = {{SpotFi}: {Decimeter} {Level} {Localization} {Using} {WiFi}},
	shorttitle = {{SpotFi}},
	url = {https://dl.acm.org/doi/10.1145/2785956.2787487},
	doi = {10.1145/2785956.2787487},
	abstract = {This paper presents the design and implementation of SpotFi, an accurate indoor localization system that can be deployed on commodity WiFi infrastructure. SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems. SpotFi makes two key technical contributions. First, SpotFi incorporates super-resolution algorithms that can accurately compute the angle of arrival (AoA) of multipath components even when the access point (AP) has only three antennas. Second, it incorporates novel filtering and estimation techniques to identify AoA of direct path between the localization target and AP by assigning values for each path depending on how likely the particular path is the direct path. Our experiments in a multipath rich indoor environment show that SpotFi achieves a median accuracy of 40 cm and is robust to indoor hindrances such as obstacles and multipath.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
	author = {Kotaru, Manikanta and Joshi, Kiran and Bharadia, Dinesh and Katti, Sachin},
	month = aug,
	year = {2015},
	note = {1082 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: SIGCOMM '15: ACM SIGCOMM 2015 Conference
ISBN: 9781450335423
Place: London United Kingdom
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference},
	pages = {269--282},
	annote = {[TLDR] SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems.},
}

@article{wang_e-eyes_2014,
	title = {E-eyes: device-free location-oriented activity identification using fine-grained {WiFi} signatures},
	shorttitle = {E-eyes},
	url = {https://dl.acm.org/doi/10.1145/2639108.2639143},
	doi = {10.1145/2639108.2639143},
	abstract = {Activity monitoring in home environments has become increasingly important and has the potential to support a broad array of applications including elder care, well-being management, and latchkey child safety. Traditional approaches involve wearable sensors and specialized hardware installations. This paper presents device-free location-oriented activity identification at home through the use of existing WiFi access points and WiFi devices (e.g., desktops, thermostats, refrigerators, smartTVs, laptops). Our low-cost system takes advantage of the ever more complex web of WiFi links between such devices and the increasingly fine-grained channel state information that can be extracted from such links. It examines channel features and can uniquely identify both in-place activities and walking movements across a home by comparing them against signal profiles. Signal profiles construction can be semi-supervised and the profiles can be adaptively updated to accommodate the movement of the mobile devices and day-to-day signal calibration. Our experimental evaluation in two apartments of different size demonstrates that our approach can achieve over 96\% average true positive rate and less than 1\% average false positive rate to distinguish a set of in-place and walking activities with only a single WiFi access point. Our prototype also shows that our system can work with wider signal band (802.11ac) with even higher accuracy.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
	author = {Wang, Yan and Liu, Jian and Chen, Yingying and Gruteser, Marco and Yang, Jie and Liu, Hongbo},
	month = sep,
	year = {2014},
	note = {746 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiCom'14: The 20th Annual International Conference on Mobile Computing and Networking
ISBN: 9781450327831
Place: Maui Hawaii USA
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference},
	pages = {617--628},
	annote = {[TLDR] This paper presents device-free location-oriented activity identification at home through the use of existing WiFi access points and WiFi devices (e.g., desktops, thermostats, refrigerators, smartTVs, laptops) in a low-cost system that can uniquely identify both in-place activities and walking movements across a home by comparing them against signal profiles.},
}

@article{wang_wifall_2017,
	title = {{WiFall}: {Device}-{Free} {Fall} {Detection} by {Wireless} {Networks}},
	volume = {16},
	issn = {1536-1233},
	shorttitle = {{WiFall}},
	url = {http://ieeexplore.ieee.org/document/7458186/},
	doi = {10.1109/TMC.2016.2557792},
	abstract = {The world population is in the midst of a unique and irreversible process of aging. Fall, which is one of the major health threats and obstacles to independent living of elders, will aggravate the global pressure in elders' health care and injury rescue. Thus, automatic fall detection is highly in need. Current proposed fall detection systems either need hardware installation or disrupt people's daily life. These limitations make it hard to widely deploy fall detection systems in residential settings. In this work, we analyze the wireless signal propagation model considering human activities influence. We then propose a novel and truly unobtrusive detection method based on the advanced wireless technologies, which we call as WiFall. WiFall employs the time variability and special diversity of Channel State Information (CSI) as the indicator of human activities. As CSI is readily available in prevalent in-use wireless infrastructures, WiFall withdraws the need for hardware modification, environmental setup and worn or taken devices. We implement WiFall on laptops equipped with commercial 802.11n NICs. Two typical indoor scenarios and several layout schemes are examined. As demonstrated by the experimental results, WiFall yielded 87\% detection precision with false alarm rate of 18\% in average.},
	number = {2},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Wang, Yuxi and Wu, Kaishun and Ni, Lionel M.},
	month = feb,
	year = {2017},
	note = {721 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference, app:fall-detection},
	pages = {581--594},
	annote = {[TLDR] This work analyzes the wireless signal propagation model considering human activities influence and proposes a novel and truly unobtrusive detection method based on the advanced wireless technologies, which it is called as WiFall, which withdraws the need for hardware modification, environmental setup and worn or taken devices.},
}

@inproceedings{vasisht_decimeter-level_2016,
	title = {Decimeter-{Level} {Localization} with a {Single} {WiFi} {Access} {Point}},
	url = {https://www.semanticscholar.org/paper/Decimeter-Level-Localization-with-a-Single-WiFi-Vasisht-Kumar/3e204852e9315efe3df10b831246471cc52a8655},
	abstract = {We present Chronos, a system that enables a single WiFi access point to localize clients to within tens of centimeters. Such a system can bring indoor positioning to homes and small businesses which typically have a single access point.

The key enabler underlying Chronos is a novel algorithm that can compute sub-nanosecond time-of-flight using commodity WiFi cards. By multiplying the time-of-flight with the speed of light, a MIMO access point computes the distance between each of its antennas and the client, hence localizing it. Our implementation on commodity WiFi cards demonstrates that Chronos's accuracy is comparable to state-of-the-art localization systems, which use four or five access points.},
	urldate = {2023-07-12},
	author = {Vasisht, Deepak and Kumar, Swarun and Katabi, D.},
	month = mar,
	year = {2016},
	keywords = {tech:wifi, app:activity-inference},
	annote = {[TLDR] Chronos, a system that enables a single WiFi access point to localize clients to within tens of centimeters, demonstrates that Chronos's accuracy is comparable to state-of-the-art localization systems, which use four or five access points.},
}

@article{ali_keystroke_2015,
	title = {Keystroke {Recognition} {Using} {WiFi} {Signals}},
	url = {https://dl.acm.org/doi/10.1145/2789168.2790109},
	doi = {10.1145/2789168.2790109},
	abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of Channel State Information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal based keystroke recognition system called WiKey. WiKey consists of two Commercial Off-The-Shelf (COTS) WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves more than 97.5{\textbackslash}\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
	author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
	month = sep,
	year = {2015},
	note = {487 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking
ISBN: 9781450336192
Place: Paris France
Publisher: ACM},
	keywords = {app:activity-inference, classes:37, participants:10, read-priority-1, tech:wifi},
	pages = {90--102},
	annote = {[TLDR] It is shown for the first time that WiFi signals can also be exploited to recognize keystrokes, which is critical for ensuring the security of computer systems and the privacy of human users as what being typed could be passwords or privacy sensitive information.},
	annote = {[TLDR] It is shown for the first time that WiFi signals can also be exploited to recognize keystrokes, which is critical for ensuring the security of computer systems and the privacy of human users as what being typed could be passwords or privacy sensitive information.},
	file = {Submitted Version:/Users/brk/Zotero/storage/DGBM94AK/Ali et al. - 2015 - Keystroke Recognition Using WiFi Signals.pdf:application/pdf},
}

@article{shen_wipass_2020,
	title = {{WiPass}: {CSI}-based {Keystroke} {Recognition} for {Numerical} {Keypad} of {Smartphones}},
	shorttitle = {{WiPass}},
	url = {https://ieeexplore.ieee.org/document/9337673/},
	doi = {10.1109/YAC51587.2020.9337673},
	abstract = {Nowadays, smartphones are everywhere. They play an indispensable role in our lives and makes people convenient to communicate, pay, socialize, etc. However, they also bring a lot of security and privacy risks. Keystroke operations of numeric keypad are often required when users input password to perform mobile payment or input other privacy-sensitive information. Different keystrokes may cause different finger movements that will bring different interference to WiFi signal, which may be reflected by channel state information (CSI). In this paper, we propose WiPass, a password-keystroke recognition system for numerical keypad input on smartphones, which especially occurs frequently in mobile payment APPs. Based on only a public WiFi hotspot deployed in the victim payment scenario, WiPass would extracts and analyzes the CSI data generated by the password-keystroke operation of the smartphone user, and infers the user's payment password by comparing the CSI waveforms of different keystrokes. We implemented the WiPass system by using COTS WiFi AP devices and smartphones. The average keystroke segmentation accuracy was 80.45\%, and the average keystroke recognition accuracy was 74.24\%.},
	urldate = {2023-07-12},
	journal = {2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
	author = {Shen, Xingfa and Yan, Guo and Yang, Jian and Xu, Sheng},
	month = oct,
	year = {2020},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: 2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)
ISBN: 9781728176840
Place: Zhanjiang, China
Publisher: IEEE},
	keywords = {app:activity-inference, app:password-inference, tech:wifi},
	pages = {276--283},
	annote = {[TLDR] WiPass is proposed, a password-keystroke recognition system for numerical keypad input on smartphones, which especially occurs frequently in mobile payment APPs, based on only a public WiFi hotspot deployed in the victim payment scenario.},
}

@article{ali_recognizing_2017,
	title = {Recognizing {Keystrokes} {Using} {WiFi} {Devices}},
	volume = {35},
	issn = {0733-8716},
	url = {http://ieeexplore.ieee.org/document/7875144/},
	doi = {10.1109/JSAC.2017.2680998},
	abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what is being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of channel state information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal-based keystroke recognition system called WiKey. WiKey consists of two commercial off-the-shelf WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves over 97.5\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%. WiKey can also recognize complete words inside a sentence with over 85\% accuracy.},
	number = {5},
	urldate = {2023-07-12},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
	month = may,
	year = {2017},
	note = {86 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference, app:keystroke-inference},
	pages = {1175--1190},
	annote = {[TLDR] It is shown for the first time that WiFi signals can also be exploited to recognize keystrokes, which is critical for ensuring the security of computer systems and the privacy of human users as what is being typed could be passwords or privacy sensitive information.},
}

@article{wang_gait_2016,
	title = {Gait recognition using wifi signals},
	url = {https://dl.acm.org/doi/10.1145/2971648.2971670},
	doi = {10.1145/2971648.2971670},
	abstract = {In this paper, we propose WifiU, which uses commercial WiFi devices to capture fine-grained gait patterns to recognize humans. The intuition is that due to the differences in gaits of different people, the WiFi signal reflected by a walking human generates unique variations in the Channel State Information (CSI) on the WiFi receiver. To profile human movement using CSI, we use signal processing techniques to generate spectrograms from CSI measurements so that the resulting spectrograms are similar to those generated by specifically designed Doppler radars. To extract features from spectrograms that best characterize the walking pattern, we perform autocorrelation on the torso reflection to remove imperfection in spectrograms. We evaluated WifiU on a dataset with 2,800 gait instances collected from 50 human subjects walking in a room with an area of 50 square meters. Experimental results show that WifiU achieves top-1, top-2, and top-3 recognition accuracies of 79.28\%, 89.52\%, and 93.05\%, respectively.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
	author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad},
	month = sep,
	year = {2016},
	note = {420 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
ISBN: 9781450344616
Place: Heidelberg Germany
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference, app:gait-inference, app:human-identification, participants:50},
	pages = {363--373},
	annote = {[TLDR] The intuition is that due to the differences in gaits of different people, the WiFi signal reflected by a walking human generates unique variations in the Channel State Information on the WiFi receiver, so WifiU is proposed, which uses commercial WiFi devices to capture fine-grained gait patterns to recognize humans.},
}

@article{wang_rt-fall_2017,
	title = {{RT}-{Fall}: {A} {Real}-{Time} and {Contactless} {Fall} {Detection} {System} with {Commodity} {WiFi} {Devices}},
	volume = {16},
	issn = {1536-1233},
	shorttitle = {{RT}-{Fall}},
	url = {http://ieeexplore.ieee.org/document/7458198/},
	doi = {10.1109/TMC.2016.2557795},
	abstract = {This paper presents the design and implementation of RT-Fall, a real-time, contactless, low-cost yet accurate indoor fall detection system using the commodity WiFi devices. RT-Fall exploits the phase and amplitude of the fine-grained Channel State Information (CSI) accessible in commodity WiFi devices, and for the first time fulfills the goal of segmenting and detecting the falls automatically in real-time, which allows users to perform daily activities naturally and continuously without wearing any devices on the body. This work makes two key technical contributions. First, we find that the CSI phase difference over two antennas is a more sensitive base signal than amplitude for activity recognition, which can enable very reliable segmentation of fall and fall-like activities. Second, we discover the sharp power profile decline pattern of the fall in the time-frequency domain and further exploit the insight for new feature extraction and accurate fall segmentation/detection. Experimental results in four indoor scenarios demonstrate that RT-fall consistently outperforms the state-of-the-art approach WiFall with 14 percent higher sensitivity and 10 percent higher specificity on average.},
	number = {2},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Wang, Hao and Zhang, Daqing and Wang, Yasha and Ma, Junyi and Wang, Yuxiang and Li, Shengjie},
	month = feb,
	year = {2017},
	note = {390 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference, app:fall-detection},
	pages = {511--526},
	annote = {[TLDR] RT-Fall exploits the phase and amplitude of the fine-grained Channel State Information accessible in commodity WiFi devices, and for the first time fulfills the goal of segmenting and detecting the falls automatically in real-time, which allows users to perform daily activities naturally and continuously without wearing any devices on the body.},
}

@article{liu_tracking_2015,
	title = {Tracking {Vital} {Signs} {During} {Sleep} {Leveraging} {Off}-the-shelf {WiFi}},
	url = {https://dl.acm.org/doi/10.1145/2746285.2746303},
	doi = {10.1145/2746285.2746303},
	abstract = {Tracking human vital signs of breathing and heart rates during sleep is important as it can help to assess the general physical health of a person and provide useful clues for diagnosing possible diseases. Traditional approaches (e.g., Polysomnography (PSG)) are limited to clinic usage. Recent radio frequency (RF) based approaches require specialized devices or dedicated wireless sensors and are only able to track breathing rate. In this work, we propose to track the vital signs of both breathing rate and heart rate during sleep by using off-the-shelf WiFi without any wearable or dedicated devices. Our system re-uses existing WiFi network and exploits the fine-grained channel information to capture the minute movements caused by breathing and heart beats. Our system thus has the potential to be widely deployed and perform continuous long-term monitoring. The developed algorithm makes use of the channel information in both time and frequency domain to estimate breathing and heart rates, and it works well when either individual or two persons are in bed. Our extensive experiments demonstrate that our system can accurately capture vital signs during sleep under realistic settings, and achieve comparable or even better performance comparing to traditional and existing approaches, which is a strong indication of providing non-invasive, continuous fine-grained vital signs monitoring without any additional cost.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 16th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
	author = {Liu, Jian and Wang, Yan and Chen, Yingying and Yang, Jie and Chen, Xu and Cheng, Jerry},
	month = jun,
	year = {2015},
	note = {390 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiHoc'15: The Sixteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing
ISBN: 9781450334891
Place: Hangzhou China
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference, app:vital-signs},
	pages = {267--276},
	annote = {[TLDR] The extensive experiments demonstrate that the system can accurately capture vital signs during sleep under realistic settings, and achieve comparable or even better performance comparing to traditional and existing approaches, which is a strong indication of providing non-invasive, continuous fine-grained vital signs monitoring without any additional cost.},
}

@article{zeng_wiwho_2016,
	title = {{WiWho}: {WiFi}-{Based} {Person} {Identification} in {Smart} {Spaces}},
	shorttitle = {{WiWho}},
	url = {https://ieeexplore.ieee.org/document/7460727/},
	doi = {10.1109/IPSN.2016.7460727},
	abstract = {There has been a growing interest in equipping the objects and environment surrounding the user with sensing capabilities. Smart indoor spaces such as smart homes and offices can implement the sensing and processing functionality, relieving users from the need of wearing or carrying smart devices. Enabling such smart spaces requires device-free effortless sensing of user's identity and activities. Device-free sensing using WiFi has shown great potential in such scenarios, however, fundamental questions such as person identification have remained unsolved. In this paper, we present WiWho, a framework that can identify a person from a small group of people in a device-free manner using WiFi. We show that Channel State Information (CSI) used in recent WiFi can identify a person's steps and walking gait. The walking gait being distinguishing characteristics for different people, WiWho uses CSI-based gait for person identification. We demonstrate how step and walk analysis can be used to identify a person's walking gait from CSI, and how this information can be used to identify a person. WiWho does not require a person to carry any device and is effortless since it only requires the person to walk for a few steps (e.g. entering a home or an office). We evaluate WiWho using experiments at multiple locations with a total of 20 volunteers, and show that it can identify a person with average accuracy of 92\% to 80\% from a group of 2 to 6 people. We also show that in most cases walking as few as 2-3 meters is sufficient to recognize a person's gait and identify the person. We discuss the potential and challenges of WiFi- based person identification with respect to smart space applications.},
	urldate = {2023-07-12},
	journal = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
	author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
	month = apr,
	year = {2016},
	note = {303 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: 2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
ISBN: 9781509008025
Place: Vienna
Publisher: IEEE},
	keywords = {tech:wifi, app:activity-inference, app:gait-inference, app:human-identification, participants:20},
	pages = {1--12},
	annote = {[TLDR] WiWho is presented, a framework that can identify a person from a small group of people in a device-free manner using WiFi and it is shown that in most cases walking as few as 2-3 meters is sufficient to recognize a person's gait and identify the person.},
}

@article{wang_device-free_2017,
	title = {Device-{Free} {Human} {Activity} {Recognition} {Using} {Commercial} {WiFi} {Devices}},
	volume = {35},
	issn = {0733-8716},
	url = {http://ieeexplore.ieee.org/document/7875148/},
	doi = {10.1109/JSAC.2017.2679658},
	abstract = {Since human bodies are good reflectors of wireless signals, human activities can be recognized by monitoring changes in WiFi signals. However, existing WiFi-based human activity recognition systems do not build models that can quantify the correlation between WiFi signal dynamics and human activities. In this paper, we propose a Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM). CARM is based on two theoretical models. First, we propose a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds. Second, we propose a CSI-activity model that quantifies the relation between human movement speeds and human activities. Based on these two models, we implemented the CARM on commercial WiFi devices. Our experimental results show that the CARM achieves recognition accuracy of 96\% and is robust to environmental changes.},
	number = {5},
	urldate = {2023-07-12},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
	month = may,
	year = {2017},
	note = {293 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference},
	pages = {1118--1131},
	annote = {[TLDR] A Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM) based on a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds and human activities.},
}

@article{ma_wifi_2020,
	title = {{WiFi} {Sensing} with {Channel} {State} {Information}: {A} {Survey}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {{WiFi} {Sensing} with {Channel} {State} {Information}},
	url = {https://dl.acm.org/doi/10.1145/3310194},
	doi = {10.1145/3310194},
	abstract = {With the high demand for wireless data traffic, WiFi networks have experienced very rapid growth, because they provide high throughput and are easy to deploy. Recently, Channel State Information (CSI) measured by WiFi networks is widely used for different sensing purposes. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this survey gives a comprehensive review of the signal processing techniques, algorithms, applications, and performance results of WiFi sensing with CSI. Different WiFi sensing algorithms and signal processing techniques have their own advantages and limitations and are suitable for different WiFi sensing applications. The survey groups CSI-based WiFi sensing applications into three categories, detection, recognition, and estimation, depending on whether the outputs are binary/multi-class classifications or numerical values. With the development and deployment of new WiFi technologies, there will be more WiFi sensing opportunities wherein the targets may go beyond from humans to environments, animals, and objects. The survey highlights three challenges for WiFi sensing: robustness and generalization, privacy and security, and coexistence of WiFi sensing and networking. Finally, the survey presents three future WiFi sensing trends, i.e., integrating cross-layer network information, multi-device cooperation, and fusion of different sensors, for enhancing existing WiFi sensing capabilities and enabling new WiFi sensing opportunities.},
	language = {en},
	number = {3},
	urldate = {2023-07-12},
	journal = {ACM Computing Surveys},
	author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan},
	month = may,
	year = {2020},
	note = {277 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, type:survey, app:activity-inference},
	pages = {1--36},
	annote = {[TLDR] This survey gives a comprehensive review of the signal processing techniques, algorithms, applications, and performance results of WiFi sensing with CSI, and presents three future WiFi sensing trends, i.e., integrating cross-layer network information, multi-device cooperation, and fusion of different sensors for enhancing existing WiFi sensing capabilities and enabling new WiFi sensing opportunities.},
}

@article{ma_signfi_2018,
	title = {{SignFi}: {Sign} {Language} {Recognition} {Using} {WiFi}},
	volume = {2},
	issn = {2474-9567},
	shorttitle = {{SignFi}},
	url = {https://dl.acm.org/doi/10.1145/3191755},
	doi = {10.1145/3191755},
	abstract = {We propose SignFi to recognize sign language gestures using WiFi. SignFi uses Channel State Information (CSI) measured by WiFi packets as the input and a Convolutional Neural Network (CNN) as the classification algorithm. Existing WiFi-based sign gesture recognition technologies are tested on no more than 25 gestures that only involve hand and/or finger gestures. SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy. SignFi collects CSI measurements to capture wireless signal characteristics of sign gestures. Raw CSI measurements are pre-processed to remove noises and recover CSI changes over sub-carriers and sampling time. Pre-processed CSI measurements are fed to a 9-layer CNN for sign gesture classification. We collect CSI traces and evaluate SignFi in the lab and home environments. There are 8,280 gesture instances, 5,520 from the lab and 2,760 from the home, for 276 sign gestures in total. For 5-fold cross validation using CSI traces of one user, the average recognition accuracy of SignFi is 98.01\%, 98.91\%, and 94.81\% for the lab, home, and lab+home environment, respectively. We also run tests using CSI traces from 5 different users in the lab environment. The average recognition accuracy of SignFi is 86.66\% for 7,500 instances of 150 sign gestures performed by 5 different users.},
	language = {en},
	number = {1},
	urldate = {2023-07-12},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan and Zhao, Hongyang and Jung, Woosub},
	month = mar,
	year = {2018},
	note = {24 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, model:cnn, read-priority-1, app:activity-inference, classes:276, participants:5},
	pages = {1--21},
	annote = {[TLDR] SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy, using Channel State Information measured by WiFi packets as the input and a Convolutional Neural Network as the classification algorithm.},
}

@article{shang_robust_2017,
	title = {A {Robust} {Sign} {Language} {Recognition} {System} with {Multiple} {Wi}-{Fi} {Devices}},
	url = {https://dl.acm.org/doi/10.1145/3097620.3097624},
	doi = {10.1145/3097620.3097624},
	abstract = {Sign language is important since it provides a way for us to the deaf culture and more opportunities to communicate with those who are deaf or hard of hearing. Since sign language chiefly uses body languages to convey meaning, Human Activity Recognition (HAR) techniques can be used to recognize them for some sign language translation applications. In this paper, we show for the first time that Wi-Fi signals can be used to recognize sign language. The key intuition is that different hand and arm motions introduce different multi-path distortions in Wi-Fi signals and generate different unique patterns in the time-series of Channel State Information (CSI). More specifically, we propose a Wi-Fi signal-based sign language recognition system called WiSign. Different from existing Wi-Fi signal-based human activity recognition systems, WiSign uses 3 Wi-Fi devices to improve the recognition performance. We implemented the WiSign using a TP-Link TL-WR1043ND Wi-Fi router and two Lenovo X100e laptops. The evaluation results show that our system can achieve a mean prediction accuracy of 93.8\% and mean false positive of 1.55\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the Workshop on Mobility in the Evolving Internet Architecture},
	author = {Shang, Jiacheng and Wu, Jie},
	month = aug,
	year = {2017},
	note = {39 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: SIGCOMM '17: ACM SIGCOMM 2017 Conference
ISBN: 9781450350594
Place: Los Angeles CA USA
Publisher: ACM},
	keywords = {classes:5, tech:wifi, app:sign-language, claims-to-be-first},
	pages = {19--24},
	annote = {[TLDR] It is shown for the first time that Wi-Fi signals can be used to recognize sign language and the system can achieve a mean prediction accuracy of 93.8\% and mean false positive of 1.55\%.},
}

@article{virmani_position_2017,
	title = {Position and {Orientation} {Agnostic} {Gesture} {Recognition} {Using} {WiFi}},
	url = {https://dl.acm.org/doi/10.1145/3081333.3081340},
	doi = {10.1145/3081333.3081340},
	abstract = {WiFi based gesture recognition systems have recently proliferated due to the ubiquitous availability of WiFi in almost every modern building. The key limitation of existing WiFi based gesture recognition systems is that they require the user to be in the same configuration (i.e., at the same position and in same orientation) when performing gestures at runtime as when providing training samples, which significantly restricts their practical usability. In this paper, we propose a WiFi based gesture recognition system, namely WiAG, which recognizes the gestures of the user irrespective of his/her configuration. The key idea behind WiAG is that it first requests the user to provide training samples for all gestures in only one configuration and then automatically generates virtual samples for all gestures in all possible configurations by applying our novel translation function on the training samples. Next, for each configuration, it generates a classification model using virtual samples corresponding to that configuration. To recognize gestures of a user at runtime, as soon as the user performs a gesture, WiAG first automatically estimates the configuration of the user and then evaluates the gesture against the classification model corresponding to that estimated configuration. Our evaluation results show that when user's configuration is not the same at runtime as at the time of providing training samples, WiAG significantly improves the gesture recognition accuracy from just 51.4\% to 91.4\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
	author = {Virmani, Aditya and Shahzad, Muhammad},
	month = jun,
	year = {2017},
	note = {161 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiSys'17: The 15th Annual International Conference on Mobile Systems, Applications, and Services
ISBN: 9781450349284
Place: Niagara Falls New York USA
Publisher: ACM},
	keywords = {tech:wifi, classes:6, app:activity-inference},
	pages = {252--264},
	annote = {[TLDR] To recognize gestures of a user at runtime, as soon as the user performs a gesture, WiAG first automatically estimates the configuration of the user and then evaluates the gesture against the classification model corresponding to that estimated configuration.},
	file = {Full Text:/Users/brk/Zotero/storage/XLDUCJ4U/Virmani and Shahzad - 2017 - Position and Orientation Agnostic Gesture Recognit.pdf:application/pdf},
}

@article{zhang_mudra_2016,
	title = {Mudra: {User}-friendly {Fine}-grained {Gesture} {Recognition} using {WiFi} {Signals}},
	shorttitle = {Mudra},
	url = {https://dl.acm.org/doi/10.1145/2999572.2999582},
	doi = {10.1145/2999572.2999582},
	abstract = {There has been a great interest in recognizing gestures using wireless communication signals. We are motivated in detecting extremely fine, subtle finger gestures with WiFi signals. We envision this technology to find applications in finger-gesture control, disabled-friendly devices, physical therapy etc. The requirements of mm-level sensitivity and user-friendly feature using existing WiFi signals pose great challenges. Here, we present Mudra, a fine-grained finger gesture recognition system which leverages WiFi signals to enable a near-human-to-machine interaction with finger motion. Mudra uses a two-antenna receiver to detect and recognize finger gesture. It uses the signals received from one antenna to cancel the signal from the other. This "cancellation" is extremely sensitive to and enables us detect small variation in channel due to finger movements. Since Mudra decodes gestures with existing WiFi transmissions, Mudra enables gesture recognition without sacrificing WiFi transmission opportunities. Besides, Mudra is user-friendly with no need of user training. To demonstrate Mudra, we implement prototype on the NI-based SDR platform and use COTS WiFi adapter. We evaluate Mudra in a typical office environment. The results show that our system can achieve 96\% accuracy.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 12th International on Conference on emerging Networking EXperiments and Technologies},
	author = {Zhang, Ouyang and Srinivasan, Kannan},
	month = dec,
	year = {2016},
	note = {56 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: CoNEXT '16: The 12th International Conference on emerging Networking EXperiments and Technologies
ISBN: 9781450342926
Place: Irvine California USA
Publisher: ACM},
	keywords = {tech:wifi, classes:9, app:activity-inference},
	pages = {83--96},
	annote = {[TLDR] Mudra is a fine-grained finger gesture recognition system which leverages WiFi signals to enable a near-human-to-machine interaction with finger motion and is user-friendly with no need of user training.},
}

@inproceedings{kellogg_bringing_2014,
	title = {Bringing {Gesture} {Recognition} to {All} {Devices}},
	url = {https://www.semanticscholar.org/paper/Bringing-Gesture-Recognition-to-All-Devices-Kellogg-Talla/c17ac67f8c74c767073e5ef09516a70ede65adcf},
	abstract = {Existing gesture-recognition systems consume significant power and computational resources that limit how they may be used in low-end devices. We introduce AllSee, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries. AllSee consumes three to four orders of magnitude lower power than state-of-the-art systems and can enable always-on gesture recognition for smartphones and tablets. It extracts gesture information from existing wireless signals (e.g., TV transmissions), but does not incur the power and computational overheads of prior wireless approaches. We build AllSee prototypes that can recognize gestures on RFID tags and power-harvesting sensors. We also integrate our hardware with an off-the-shelf Nexus S phone and demonstrate gesture recognition in through-the-pocket scenarios. Our results show that AllSee achieves classification accuracies as high as 97\% over a set of eight gestures.},
	urldate = {2023-07-12},
	author = {Kellogg, Bryce and Talla, V. and Gollakota, Shyamnath},
	month = apr,
	year = {2014},
	keywords = {app:activity-inference, classes:8, tech:wifi},
	annote = {[TLDR] AllSee is introduced, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries and achieves classification accuracies as high as 97\% over a set of eight gestures.},
	annote = {[TLDR] AllSee is introduced, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries and achieves classification accuracies as high as 97\% over a set of eight gestures.},
}

@article{wang_we_2014,
	title = {We can hear you with {Wi}-{Fi}!},
	url = {https://dl.acm.org/doi/10.1145/2639108.2639112},
	doi = {10.1145/2639108.2639112},
	abstract = {Recent literature advances Wi-Fi signals to “see” people's motions and locations. This paper asks the following question: Can Wi-Fi “hear” our talks? We present WiHear, which enables Wi-Fi signals to “hear” our talks without deploying any devices. To achieve this, WiHear needs to detect and analyze fine-grained radio reflections from mouth movements. WiHear solves this micro-movement detection problem by introducing Mouth Motion Profile that leverages partial multipath effects and wavelet packet transformation. Since Wi-Fi signals do not require line-of-sight, WiHear can “hear” people talks within the radio range. Further, WiHear can simultaneously “hear” multiple people's talks leveraging MIMO technology. We implement WiHear on both USRP N210 platform and commercial Wi-Fi infrastructure. Results show that within our pre-defined vocabulary, WiHear can achieve detection accuracy of 91 percent on average for single individual speaking no more than six words and up to 74 percent for no more than three people talking simultaneously. Moreover, the detection accuracy can be further improved by deploying multiple receivers from different angles.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
	author = {Wang, Guanhua and Zou, Yongpan and Zhou, Zimu and Wu, Kaishun and Ni, Lionel M.},
	month = sep,
	year = {2014},
	note = {410 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiCom'14: The 20th Annual International Conference on Mobile Computing and Networking
ISBN: 9781450327831
Place: Maui Hawaii USA
Publisher: ACM},
	keywords = {tech:wifi, read-priority-1, app:activity-inference, app:speech-detection},
	pages = {593--604},
	annote = {[TLDR] WiHear is presented, which enables Wi-Fi signals to “hear” people talks within the radio range without deploying any devices and can simultaneously “ hear’ multiple people's talks leveraging MIMO technology.},
}

@article{sun_widraw_2015,
	title = {{WiDraw}: {Enabling} {Hands}-free {Drawing} in the {Air} on {Commodity} {WiFi} {Devices}},
	shorttitle = {{WiDraw}},
	url = {https://dl.acm.org/doi/10.1145/2789168.2790129},
	doi = {10.1145/2789168.2790129},
	abstract = {This paper demonstrates that it is possible to leverage WiFi signals from commodity mobile devices to enable hands-free drawing in the air. While prior solutions require the user to hold a wireless transmitter, or require custom wireless hardware, or can only determine a pre-defined set of hand gestures, this paper introduces WiDraw, the first hand motion tracking system using commodity WiFi cards, and without any user wearables. WiDraw harnesses the Angle-of-Arrival values of incoming wireless signals at the mobile device to track the user's hand trajectory. We utilize the intuition that whenever the user's hand occludes a signal coming from a certain direction, the signal strength of the angle representing the same direction will experience a drop. Our software prototype using commodity wireless cards can track the user's hand with a median error lower than 5 cm. We use WiDraw to implement an in-air handwriting application that allows the user to draw letters, words, and sentences, and achieves a mean word recognition accuracy of 91\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
	author = {Sun, Li and Sen, Souvik and Koutsonikolas, Dimitrios and Kim, Kyu-Han},
	month = sep,
	year = {2015},
	note = {261 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking
ISBN: 9781450336192
Place: Paris France
Publisher: ACM},
	keywords = {tech:wifi, app:writing, app:activity-inference},
	pages = {77--89},
	annote = {[TLDR] WiDraw is introduced, the first hand motion tracking system using commodity WiFi cards, and without any user wearables, that harnesses the Angle-of-Arrival values of incoming wireless signals at the mobile device to track the user's hand trajectory.},
}

@article{li_indotrack_2017,
	title = {{IndoTrack}: {Device}-{Free} {Indoor} {Human} {Tracking} with {Commodity} {Wi}-{Fi}},
	volume = {1},
	issn = {2474-9567},
	shorttitle = {{IndoTrack}},
	url = {https://dl.acm.org/doi/10.1145/3130940},
	doi = {10.1145/3130940},
	abstract = {Indoor human tracking is fundamental to many real-world applications such as security surveillance, behavioral analysis, and elderly care. Previous solutions usually require dedicated device being carried by the human target, which is inconvenient or even infeasible in scenarios such as elderly care and break-ins. However, compared with device-based tracking, device-free tracking is particularly challenging because the much weaker reflection signals are employed for tracking. The problem becomes even more difficult with commodity Wi-Fi devices, which have limited number of antennas, small bandwidth size, and severe hardware noise.
            In this work, we propose IndoTrack, a device-free indoor human tracking system that utilizes only commodity Wi-Fi devices. IndoTrack is composed of two innovative methods: (1) Doppler-MUSIC is able to extract accurate Doppler velocity information from noisy Wi-Fi Channel State Information (CSI) samples; and (2) Doppler-AoA is able to determine the absolute trajectory of the target by jointly estimating target velocity and location via probabilistic co-modeling of spatial-temporal Doppler and AoA information. Extensive experiments demonstrate that IndoTrack can achieve a 35cm median error in human trajectory estimation, outperforming the state-of-the-art systems and provide accurate location and velocity information for indoor human mobility and behavioral analysis.},
	language = {en},
	number = {3},
	urldate = {2023-07-12},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Li, Xiang and Zhang, Daqing and Lv, Qin and Xiong, Jie and Li, Shengjie and Zhang, Yue and Mei, Hong},
	month = sep,
	year = {2017},
	note = {41 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference},
	pages = {1--22},
	annote = {[TLDR] Extensive experiments demonstrate that IndoTrack can achieve a 35cm median error in human trajectory estimation, outperforming the state-of-the-art systems and provide accurate location and velocity information for indoor human mobility and behavioral analysis.},
}

@article{zhang_wifi-id_2016,
	title = {{WiFi}-{ID}: {Human} {Identification} {Using} {WiFi} {Signal}},
	shorttitle = {{WiFi}-{ID}},
	url = {http://ieeexplore.ieee.org/document/7536315/},
	doi = {10.1109/DCOSS.2016.30},
	abstract = {Prior research has shown the potential of device-free WiFi sensing for human activity recognition. In this paper, we show for the first time WiFi signals can also be used to uniquely identify people. There is strong evidence that suggests that all humans have a unique gait. An individual's gait will thus create unique perturbations in the WiFi spectrum. We propose a system called WiFi-ID that analyses the channel state information to extract unique features that are representative of the walking style of that individual and thus allow us to uniquely identify that person. We implement WiFi-ID on commercial off-the-shelf devices. We conduct extensive experiments to demonstrate that our system can uniquely identify people with average accuracy of 93\% to 77\% from a group of 2 to 6 people, respectively. We envisage that this technology can find many applications in small office or smart home settings.},
	urldate = {2023-07-12},
	journal = {2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)},
	author = {Zhang, Jin and Wei, Bo and Hu, Wen and Kanhere, Salil S.},
	month = may,
	year = {2016},
	note = {181 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: 2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)
ISBN: 9781509014606
Place: Washington, DC, USA
Publisher: IEEE},
	keywords = {tech:wifi, app:activity-inference, app:human-identification, participants:2, participants:6},
	pages = {75--82},
	annote = {[TLDR] For the first time WiFi signals can also be used to uniquely identify people and a system called WiFi-ID is proposed that analyses the channel state information to extract unique features that are representative of the walking style of that individual and thus allow for uniquely identify that person.},
	file = {Accepted Version:/Users/brk/Zotero/storage/PZ2KLPXF/Zhang et al. - 2016 - WiFi-ID Human Identification Using WiFi Signal.pdf:application/pdf},
}

@article{zeng_analyzing_2015,
	title = {Analyzing {Shopper}'s {Behavior} through {WiFi} {Signals}},
	url = {https://dl.acm.org/doi/10.1145/2753497.2753508},
	doi = {10.1145/2753497.2753508},
	abstract = {Substantial progress in WiFi-based indoor localization has proven that pervasiveness of WiFi can be exploited beyond its traditional use of internet access to enable a variety of sensing applications. Understanding shopper's behavior through physical analytics can provide crucial insights to the business owner in terms of effectiveness of promotions, arrangement of products and efficiency of services. However, analyzing shopper's behavior and browsing patterns is challenging. Since video surveillance can not used due to high cost and privacy concerns, it is necessary to design novel techniques that can provide accurate and efficient view of shopper's behavior. In this work, we propose WiFi-based sensing of shopper's behavior in a retail store. Specifically, we show that various states of a shopper such as standing near the entrance to view a promotion or walking quickly to proceed towards the intended item can be accurately classified by profiling Channel State Information (CSI) of WiFi. We recognize a few representative states of shopper's behavior at the entrance and inside the store, and show how CSI-based profile can be used to detect that a shopper is in one of the states with very high accuracy (≈ 90\%). We discuss the potential and limitations of CSI-based sensing of shopper's behavior and physical analytics in general.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 2nd workshop on Workshop on Physical Analytics},
	author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
	month = may,
	year = {2015},
	note = {104 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiSys'15: The 13th Annual International Conference on Mobile Systems, Applications, and Services
ISBN: 9781450334983
Place: Florence Italy
Publisher: ACM},
	keywords = {classes:5, tech:wifi, app:activity-inference},
	pages = {13--18},
	annote = {[TLDR] Wifi-based sensing of shopper's behavior in a retail store shows that various states of a shopper such as standing near the entrance to view a promotion or walking quickly to proceed towards the intended item can be accurately classified by profiling Channel State Information (CSI) of WiFi.},
}

@article{mantyjarvi_identifying_2005,
	title = {Identifying {Users} of {Portable} {Devices} from {Gait} {Pattern} with {Accelerometers}},
	volume = {2},
	url = {http://ieeexplore.ieee.org/document/1415569/},
	doi = {10.1109/ICASSP.2005.1415569},
	abstract = {Identifying users of portable devices from gait signals acquired with three-dimensional accelerometers was studied. Three approaches, correlation, frequency domain and data distribution statistics, were used. Test subjects (N=36) walked with fast, normal and slow walking speeds in enrolment and test sessions on separate days wearing the accelerometer device on their belt, at back. It was shown to be possible to identify users with this novel gait recognition method. Best equal error rate (EER=7\%) was achieved with the signal correlation method, while the frequency domain method and two variations of the data distribution statistics method produced EER of 10\%, 18\% and 19\%, respectively.},
	urldate = {2023-07-12},
	journal = {Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.},
	author = {Mantyjarvi, J. and Lindholm, M. and Vildjiounaite, E. and Makela, S. and Ailisto, H.},
	year = {2005},
	note = {434 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.
ISBN: 9780780388741
Place: Philadelphia, Pennsylvania, USA
Publisher: IEEE},
	keywords = {tech:accelerometer, app:activity-inference, app:gait-inference, app:human-identification, participants:36},
	pages = {973--976},
	annote = {[TLDR] It was shown to be possible to identify users of portable devices from gait signals acquired with three-dimensional accelerometers with this novel gait recognition method.},
}

@article{li_when_2016,
	title = {When {CSI} {Meets} {Public} {WiFi}: {Inferring} {Your} {Mobile} {Phone} {Password} via {WiFi} {Signals}},
	shorttitle = {When {CSI} {Meets} {Public} {WiFi}},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978397},
	doi = {10.1145/2976749.2978397},
	abstract = {In this study, we present WindTalker, a novel and practical keystroke inference framework that allows an attacker to infer the sensitive keystrokes on a mobile device through WiFi-based side-channel information. WindTalker is motivated from the observation that keystrokes on mobile devices will lead to different hand coverage and the finger motions, which will introduce a unique interference to the multi-path signals and can be reflected by the channel state information (CSI). The adversary can exploit the strong correlation between the CSI fluctuation and the keystrokes to infer the user's number input. WindTalker presents a novel approach to collect the target's CSI data by deploying a public WiFi hotspot. Compared with the previous keystroke inference approach, WindTalker neither deploys external devices close to the target device nor compromises the target device. Instead, it utilizes the public WiFi to collect user's CSI data, which is easy-to-deploy and difficult-to-detect. In addition, it jointly analyzes the traffic and the CSI to launch the keystroke inference only for the sensitive period where password entering occurs. WindTalker can be launched without the requirement of visually seeing the smart phone user's input process, backside motion, or installing any malware on the tablet. We implemented Windtalker on several mobile phones and performed a detailed case study to evaluate the practicality of the password inference towards Alipay, the largest mobile payment platform in the world. The evaluation results show that the attacker can recover the key with a high successful rate.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	author = {Li, Mengyuan and Meng, Yan and Liu, Junyi and Zhu, Haojin and Liang, Xiaohui and Liu, Yao and Ruan, Na},
	month = oct,
	year = {2016},
	note = {159 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: CCS'16: 2016 ACM SIGSAC Conference on Computer and Communications Security
ISBN: 9781450341394
Place: Vienna Austria
Publisher: ACM},
	keywords = {tech:wifi, classes:10, app:activity-inference, app:password-inference},
	pages = {1068--1079},
	annote = {[TLDR] A novel and practical keystroke inference framework that allows an attacker to infer the sensitive keystrokes on a mobile device through WiFi-based side-channel information and can recover the key with a high successful rate is presented.},
}

@article{wang_understanding_2015,
	title = {Understanding and {Modeling} of {WiFi} {Signal} {Based} {Human} {Activity} {Recognition}},
	url = {https://dl.acm.org/doi/10.1145/2789168.2790093},
	doi = {10.1145/2789168.2790093},
	abstract = {Some pioneer WiFi signal based human activity recognition systems have been proposed. Their key limitation lies in the lack of a model that can quantitatively correlate CSI dynamics and human activities. In this paper, we propose CARM, a CSI based human Activity Recognition and Monitoring system. CARM has two theoretical underpinnings: a CSI-speed model, which quantifies the correlation between CSI value dynamics and human movement speeds, and a CSI-activity model, which quantifies the correlation between the movement speeds of different human body parts and a specific human activity. By these two models, we quantitatively build the correlation between CSI value dynamics and a specific human activity. CARM uses this correlation as the profiling mechanism and recognizes a given activity by matching it to the best-fit profile. We implemented CARM using commercial WiFi devices and evaluated it in several different environments. Our results show that CARM achieves an average accuracy of greater than 96\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
	author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
	month = sep,
	year = {2015},
	note = {790 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking
ISBN: 9781450336192
Place: Paris France
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference},
	pages = {65--76},
	annote = {[TLDR] CARM is a CSI based human Activity Recognition and Monitoring system that quantitatively builds the correlation between CSI value dynamics and a specific human activity and recognizes a given activity by matching it to the best-fit profile.},
	file = {Submitted Version:/Users/brk/Zotero/storage/CCUH4UTT/Wang et al. - 2015 - Understanding and Modeling of WiFi Signal Based Hu.pdf:application/pdf},
}

@article{wang_human_2016,
	title = {Human respiration detection with commodity wifi devices: do user location and body orientation matter?},
	shorttitle = {Human respiration detection with commodity wifi devices},
	url = {https://dl.acm.org/doi/10.1145/2971648.2971744},
	doi = {10.1145/2971648.2971744},
	abstract = {Recent research has demonstrated the feasibility of detecting human respiration rate non-intrusively leveraging commodity WiFi devices. However, is it always possible to sense human respiration no matter where the subject stays and faces? What affects human respiration sensing and what's the theory behind? In this paper, we first introduce the Fresnel model in free space, then verify the Fresnel model for WiFi radio propagation in indoor environment. Leveraging the Fresnel model and WiFi radio propagation properties derived, we investigate the impact of human respiration on the receiving RF signals and develop the theory to relate one's breathing depth, location and orientation to the detectability of respiration. With the developed theory, not only when and why human respiration is detectable using WiFi devices become clear, it also sheds lights on understanding the physical limit and foundation of WiFi-based sensing systems. Intensive evaluations validate the developed theory and case studies demonstrate how to apply the theory to the respiration monitoring system design.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
	author = {Wang, Hao and Zhang, Daqing and Ma, Junyi and Wang, Yasha and Wang, Yuxiang and Wu, Dan and Gu, Tao and Xie, Bing},
	month = sep,
	year = {2016},
	note = {339 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
ISBN: 9781450344616
Place: Heidelberg Germany
Publisher: ACM},
	keywords = {app:activity-inference, app:breathing-detection, tech:wifi},
	pages = {25--36},
	annote = {[TLDR] Leveraging the Fresnel model and WiFi radio propagation properties derived, the impact of human respiration on the receiving RF signals is investigated and the theory to relate one's breathing depth, location and orientation to the detectability of respiration is developed.},
}

@article{wu_non-invasive_2015,
	title = {Non-{Invasive} {Detection} of {Moving} and {Stationary} {Human} {With} {WiFi}},
	volume = {33},
	issn = {0733-8716},
	url = {http://ieeexplore.ieee.org/document/7102722/},
	doi = {10.1109/JSAC.2015.2430294},
	abstract = {Non-invasive human sensing based on radio signals has attracted a great deal of research interest and fostered a broad range of innovative applications of localization, gesture recognition, smart health-care, etc., for which a primary primitive is to detect human presence. Previous works have studied the detection of moving humans via signal variations caused by human movements. For stationary people, however, existing approaches often employ a prerequisite scenario-tailored calibration of channel profile in human-free environments. Based on in-depth understanding of human motion induced signal attenuation reflected by PHY layer channel state information (CSI), we propose DeMan, a unified scheme for non-invasive detection of moving and stationary human on commodity WiFi devices. DeMan takes advantage of both amplitude and phase information of CSI to detect moving targets. In addition, DeMan considers human breathing as an intrinsic indicator of stationary human presence and adopts sophisticated mechanisms to detect particular signal patterns caused by minute chest motions, which could be destroyed by significant whole-body motion or hidden by environmental noises. By doing this, DeMan is capable of simultaneously detecting moving and stationary people with only a small number of prior measurements for model parameter determination, yet without the cumbersome scenario-specific calibration. Extensive experimental evaluation in typical indoor environments validates the great performance of DeMan in various human poses and locations and diverse channel conditions. Particularly, DeMan provides a detection rate of around 95\% for both moving and stationary people, while identifies human-free scenarios by 96\%, all of which outperforms existing methods by about 30\%.},
	number = {11},
	urldate = {2023-07-12},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Wu, Chenshu and Yang, Zheng and Zhou, Zimu and Liu, Xuefeng and Liu, Yunhao and Cao, Jiannong},
	month = nov,
	year = {2015},
	note = {266 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:wifi, app:activity-inference, app:breathing-detection},
	pages = {2329--2342},
	annote = {[TLDR] DeMan is a unified scheme for non-invasive detection of moving and stationary human on commodity WiFi devices that takes advantage of both amplitude and phase information of CSI to detect moving targets and considers human breathing as an intrinsic indicator of stationary human presence.},
}

@article{qian_widar20_2018,
	title = {Widar2.0: {Passive} {Human} {Tracking} with a {Single} {Wi}-{Fi} {Link}},
	shorttitle = {Widar2.0},
	url = {https://dl.acm.org/doi/10.1145/3210240.3210314},
	doi = {10.1145/3210240.3210314},
	abstract = {This paper presents Widar2.0, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices. Previous works based on either specialized or commercial hardware all require multiple links, preventing their wide adoption in scenarios like homes where typically only one single AP is installed. The key insight underlying Widar2.0 to circumvent the use of multiple links is to leverage multi-dimensional signal parameters from one single link. To this end, we build a unified model accounting for Angle-of-Arrival, Time-of-Flight, and Doppler shifts together and devise an efficient algorithm for their joint estimation. We then design a pipeline to translate the erroneous raw parameters into precise locations, which first finds parameters corresponding to the reflections of interests, then refines range estimates, and ultimately outputs target locations. Our implementation and evaluation on commodity WiFi devices demonstrate that Widar2.0 achieves better or comparable performance to state-of-the-art localization systems, which either use specialized hardwares or require 2 to 40 Wi-Fi links.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
	author = {Qian, Kun and Wu, Chenshu and Zhang, Yi and Zhang, Guidong and Yang, Zheng and Liu, Yunhao},
	month = jun,
	year = {2018},
	note = {246 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: MobiSys '18: The 16th Annual International Conference on Mobile Systems, Applications, and Services
ISBN: 9781450357203
Place: Munich Germany
Publisher: ACM},
	keywords = {tech:wifi, claims-to-be-first, app:activity-inference},
	pages = {350--361},
	annote = {[TLDR] Widar2.0 is presented, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices and achieves better or comparable performance to state-of- the-art localization systems.},
}

@article{liu_wi-sleep_2014,
	title = {Wi-{Sleep}: {Contactless} {Sleep} {Monitoring} via {WiFi} {Signals}},
	shorttitle = {Wi-{Sleep}},
	url = {https://ieeexplore.ieee.org/document/7010501/},
	doi = {10.1109/RTSS.2014.30},
	abstract = {Is it possible to leverage WiFi signals collected in bedrooms to monitor a person's sleep? In this paper, we show that with off-the-shelf WiFi devices, fine-grained sleep information like a person's respiration, sleeping postures and rollovers can be successfully extracted. We do this by introducing Wi-Sleep, the first sleep monitoring system based on WiFi signals. Wi-Sleep adopts off-the-shelf WiFi devices to continuously collect the fine-grained wireless channel state information (CSI) around a person. From the CSI, Wi-Sleep extracts rhythmic patterns associated with respiration and abrupt changes due to the body movement. Compared to existing sleep monitoring systems that usually require special devices attached to human body (i.e. Probes, head belt, and wrist band), Wi-Sleep is completely contact less. In addition, different from many vision-based sleep monitoring systems, Wi-Sleep is robust to low-light environments and does not raise privacy concerns. Preliminary testing results show that the Wi-Sleep can reliably track a person's respiration and sleeping postures in different conditions.},
	urldate = {2023-07-12},
	journal = {2014 IEEE Real-Time Systems Symposium},
	author = {Liu, Xuefeng and Cao, Jiannong and Tang, Shaojie and Wen, Jiaqi},
	month = dec,
	year = {2014},
	note = {243 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: 2014 IEEE Real-Time Systems Symposium (RTSS)
ISBN: 9781479972883
Place: Rome
Publisher: IEEE},
	keywords = {tech:wifi, app:activity-inference, app:breathing-detection, app:sleep-detection},
	pages = {346--355},
	annote = {[TLDR] It is shown that with off-the-shelf WiFi devices, fine-grained sleep information like a person's respiration, sleeping postures and rollovers can be successfully extracted.},
}

@article{qian_widar_2017,
	title = {Widar: {Decimeter}-{Level} {Passive} {Tracking} via {Velocity} {Monitoring} with {Commodity} {Wi}-{Fi}},
	shorttitle = {Widar},
	url = {https://dl.acm.org/doi/10.1145/3084041.3084067},
	doi = {10.1145/3084041.3084067},
	abstract = {Various pioneering approaches have been proposed for Wi-Fi-based sensing, which usually employ learning-based techniques to seek appropriate statistical features, yet do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we propose Widar, a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and the user's location and velocity. On this basis, we propose novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity. We implement Widar on commercial Wi-Fi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 25 cm given initial positions and 38 cm without them and a median relative velocity error of 13\%.},
	language = {en},
	urldate = {2023-07-12},
	journal = {Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
	author = {Qian, Kun and Wu, Chenshu and Yang, Zheng and Liu, Yunhao and Jamieson, Kyle},
	month = jul,
	year = {2017},
	note = {215 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: Mobihoc '17: The Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing
ISBN: 9781450349123
Place: Chennai India
Publisher: ACM},
	keywords = {tech:wifi, app:activity-inference},
	pages = {1--10},
	annote = {[TLDR] Widar is a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level and proposes novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity.},
}

@article{liming_chen_sensor-based_2012,
	title = {Sensor-{Based} {Activity} {Recognition}},
	volume = {42},
	issn = {1094-6977, 1558-2442},
	url = {http://ieeexplore.ieee.org/document/6208895/},
	doi = {10.1109/TSMCC.2012.2198883},
	abstract = {Research on sensor-based activity recognition has, recently, made significant progress and is attracting growing attention in a number of disciplines and application domains. However, there is a lack of high-level overview on this topic that can inform related communities of the research state of the art. In this paper, we present a comprehensive survey to examine the development and current status of various aspects of sensor-based activity recognition. We first discuss the general rationale and distinctions of vision-based and sensor-based activity recognition. Then, we review the major approaches and methods associated with sensor-based activity monitoring, modeling, and recognition from which strengths and weaknesses of those approaches are highlighted. We make a primary distinction in this paper between data-driven and knowledge-driven approaches, and use this distinction to structure our survey. We also discuss some promising directions for future research.},
	number = {6},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {{Liming Chen} and Hoey, J. and Nugent, C. D. and Cook, D. J. and {Zhiwen Yu}},
	month = nov,
	year = {2012},
	note = {910 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {type:survey},
	pages = {790--808},
	annote = {[TLDR] A comprehensive survey to examine the development and current status of various aspects of sensor-based activity recognition, making a primary distinction in this paper between data-driven and knowledge-driven approaches.},
}

@inproceedings{ahuja_hand_2015,
	title = {Hand {Gesture} {Recognition} {Using} {PCA}},
	url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-PCA-Ahuja-Singh/43b7e88adbcdfcf5b25d337821c94a0f0ee525b3},
	abstract = {Interacting with physical world using expressive body movements is much easier and effective than just speaking. Gesture recognition turns up to be important field in the recent years. Communication through gestures has been used since early ages not only by physically challenged persons but nowadays for many other applications. As most predominantly hand is use to perform gestures, Hand Gesture Recognition have been widely accepted for numerous applications such as human computer interactions, robotics, sign language recognition, etc. This paper focuses on bare hand gesture recognition system by proposing a scheme using a database-driven hand gesture recognition based upon skin color model approach and thresholding approach along with an effective template matching with can be effectively used for human robotics applications and similar other applications.. Initially, hand region is segmented by applying skin color model in YCbCr color space. In the next stage otsu thresholding is applied to separate foreground and background. Finally, template based matching technique is developed using Principal Component Analysis (PCA) for recognition. The system is tested with the controlled and uncontrolled database and shows 100\% accuracy with controlled database and 91.43\% with low brightness images.},
	urldate = {2023-07-12},
	author = {Ahuja, M. and Singh, Amardeep},
	year = {2015},
	keywords = {tech:rgb, app:hand-gesture-recognition, model:pca},
	annote = {[TLDR] A scheme using a database-driven hand gesture recognition based upon skin color model approach and thresholding approach along with an effective template matching with can be effectively used for human robotics applications and similar other applications.},
	file = {Full Text PDF:/Users/brk/Zotero/storage/6NS5M2E7/Ahuja and Singh - 2015 - Hand Gesture Recognition Using PCA.pdf:application/pdf},
}

@inproceedings{murase_gesture_2012,
	address = {Megève France},
	title = {Gesture keyboard with a machine learning requiring only one camera},
	isbn = {978-1-4503-1077-2},
	url = {https://dl.acm.org/doi/10.1145/2160125.2160154},
	doi = {10.1145/2160125.2160154},
	abstract = {In this paper, the authors propose a novel gesture-based virtual keyboard (Gesture Keyboard) that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique. Gesture Keyboard tracks the user's fingers and recognizes finger motions to judge keys input in the horizontal direction. Real-Adaboost (Adaptive Boosting), a machine learning technique, uses HOG (Histograms of Oriented Gradients) features in an image of the user's hands to estimate keys in the depth direction. Each virtual key follows a corresponding finger, so it is possible to input characters at the user's preferred hand position even if the user displaces his hands while inputting data. Additionally, because Gesture Keyboard requires only one camera, keyboard-less devices can implement this system easily. We show the effectiveness of utilizing a machine learning technique for estimating depth.},
	language = {en},
	urldate = {2023-07-12},
	booktitle = {Proceedings of the 3rd {Augmented} {Human} {International} {Conference}},
	publisher = {ACM},
	author = {Murase, Taichi and Moteki, Atsunori and Suzuki, Genta and Nakai, Takahiro and Hara, Nobuyuki and Matsuda, Takahiro},
	month = mar,
	year = {2012},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {tech:rgb, technique:histogram-oriented-gradients, model:adaboost},
	pages = {1--2},
	annote = {[TLDR] A novel gesture-based virtual keyboard that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique for estimating depth is proposed.},
	annote = {[TLDR] A novel gesture-based virtual keyboard that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique for estimating depth is proposed.},
	annote = {[TLDR] A novel gesture-based virtual keyboard that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique for estimating depth is proposed.},
	annote = {[TLDR] A novel gesture-based virtual keyboard that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique for estimating depth is proposed.},
}

@article{segen_human-computer_1998,
	title = {Human-computer interaction using gesture recognition and {3D} hand tracking},
	volume = {3},
	url = {http://ieeexplore.ieee.org/document/727164/},
	doi = {10.1109/ICIP.1998.727164},
	abstract = {This paper describes a real time system for human-computer interaction through gesture recognition and three dimensional hand-tracking. Using two cameras that are focused at the user's hand the system recognizes three gestures and tracks the hand in three dimensions. The system can simultaneously track two fingers (thumb and pointing finger) and output their poses. The pose for each finger consists of three positional coordinates and two angles (azimuth and elevation). By moving the thumb and the pointing finger in 3D, the user can control 10 degrees of freedom in a smooth and natural fashion. We have used this system as a multi-dimensional input-interface to computer games, terrain navigation software and graphical editors. In addition to providing 10 degrees of control, our system is much more natural and intuitive to use compared to traditional input devices. The system is user independent, and operates at the rate of 60 Hz.},
	urldate = {2023-07-12},
	journal = {Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)},
	author = {Segen, J. and Kumar, S.},
	year = {1998},
	note = {60 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: IPCIP'98 International Conference on Image Processing
ISBN: 9780818688218
Place: Chicago, IL, USA
Publisher: IEEE Comput. Soc},
	keywords = {tech:rgb},
	pages = {188--192},
	annote = {[TLDR] A real time system for human-computer interaction through gesture recognition and three dimensional hand-tracking using two cameras that are focused at the user's hand, which provides 10 degrees of freedom to the user.},
	annote = {[TLDR] A real time system for human-computer interaction through gesture recognition and three dimensional hand-tracking using two cameras that are focused at the user's hand, which provides 10 degrees of freedom to the user.},
}

@article{hyeon-kyu_lee_hmm-based_1999,
	title = {An {HMM}-based threshold model approach for gesture recognition},
	volume = {21},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/799904/},
	doi = {10.1109/34.799904},
	abstract = {A new method is developed using the hidden Markov model (HMM) based technique. To handle nongesture patterns, we introduce the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures in the sense that its likelihood is smaller than that of the dedicated gesture model for a given gesture. Consequently, the likelihood can be used as an adaptive threshold for selecting proper gesture model. It has, however, a large number of states and needs to be reduced because the threshold model is constructed by collecting the states of all gesture models in the system. To overcome this problem, the states with similar probability distributions are merged, utilizing the relative entropy measure. Experimental results show that the proposed method can successfully extract trained gestures from continuous hand motion with 93.14\% reliability.},
	number = {10},
	urldate = {2023-07-12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Hyeon-Kyu Lee} and Kim, J.H.},
	month = oct,
	year = {1999},
	note = {718 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {model:hmm, from:cite.bib},
	pages = {961--973},
	annote = {@lee1999 presented a HMM-based image classification system able to classify ten gestures, and is also able to discard unknown or unseen patterns. These gestures were single hand motions which traced arcs or patterns in front of the camera.

},
	annote = {[TLDR] A new method is developed using the hidden Markov model (HMM) based technique that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns.},
	annote = {[TLDR] A new method is developed using the hidden Markov model (HMM) based technique that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns.},
}

@article{segen_fast_1998,
	title = {Fast and accurate {3D} gesture recognition interface},
	volume = {1},
	url = {http://ieeexplore.ieee.org/document/711086/},
	doi = {10.1109/ICPR.1998.711086},
	abstract = {A video-based gesture recognition system can serve as a natural and accurate 3D user input device. We describe a two-camera system, that recognizes three gesture classes: two static and one dynamic. For one of these gestures (pointing), the system estimates five parameters of 3D pose: position and pointing direction. The recognition is robust, independent of the user and fast (60 Hz), and the estimated pose is very stable. We describe some of the interface applications that demonstrate the benefits of the system: control of a video game, piloting a virtual reality fly-through, and interaction with a 3D scene editor.},
	urldate = {2023-07-12},
	journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
	author = {Segen, J. and Kumar, S.},
	year = {1998},
	note = {41 citations (Semantic Scholar/DOI) [2023-07-12]
Conference Name: Fourteenth International Conference on Pattern Recognition
ISBN: 9780818685125
Place: Brisbane, Qld., Australia
Publisher: IEEE Comput. Soc},
	keywords = {tech:rgb, movement:static, movement:dynamic, classes:3},
	pages = {86--91},
	annote = {[TLDR] A video-based gesture recognition system that recognizes three gesture classes: two static and one dynamic, that is robust, independent of the user and fast (60 Hz), and the estimated pose is very stable.},
}

@inproceedings{carbonell_velocity_1998,
	address = {Berlin, Heidelberg},
	title = {Velocity profile based recognition of dynamic gestures with discrete {Hidden} {Markov} {Models}},
	volume = {1371},
	isbn = {978-3-540-64424-8 978-3-540-69782-4},
	url = {http://link.springer.com/10.1007/BFb0052991},
	doi = {10.1007/BFb0052991},
	abstract = {In this paper we present a method for the recognition of dynamic gestures with discrete Hidden Markov Models (HMMs) from a continuous stream of gesture input data. The segmentation problem is addressed by extracting two velocity profiles from the gesture data and using their extrema as segmentation cues. Gestures are captured with a TUB-SensorGlove. The paper focuses on the description of the gesture recognition method (including data preprocessing) and describes experiments for the evaluation of the performance of the recognition method. The paper combines and further develops ideas from some of our previous work.},
	urldate = {2023-07-12},
	publisher = {Springer Berlin Heidelberg},
	author = {Hofmann, Frank G. and Heyer, Peter and Hommel, Günter},
	editor = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Fröhlich, Martin},
	year = {1998},
	doi = {10.1007/BFb0052991},
	note = {108 citations (Semantic Scholar/DOI) [2023-07-12]
Book Title: Gesture and Sign Language in Human-Computer Interaction
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, from:cite.bib, hardware:tub-sensorglove},
	pages = {81--95},
	annote = {@hofmann1998 used the SensorGlove developed by the Technical University of Berlin [@hofmann1995] to segment and classify continuous input data into gesture predictions by using a HMM.

},
	annote = {[TLDR] This paper focuses on the description of the gesture recognition method (including data preprocessing) and describes experiments for the evaluation of the performance of the recognition method.},
}

@article{wheatland_state_2015,
	title = {State of the {Art} in {Hand} and {Finger} {Modeling} and {Animation}},
	volume = {34},
	issn = {01677055},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.12595},
	doi = {10.1111/cgf.12595},
	abstract = {The human hand is a complex biological system able to perform numerous tasks with impressive accuracy and dexterity. Gestures furthermore play an important role in our daily interactions, and humans are particularly skilled at perceiving and interpreting detailed signals in communications. Creating believable hand motions for virtual characters is an important and challenging task. Many new methods have been proposed in the Computer Graphics community within the last years, and significant progress has been made towards creating convincing, detailed hand and finger motions. This state of the art report presents a review of the research in the area of hand and finger modeling and animation. Starting with the biological structure of the hand and its implications for how the hand moves, we discuss current methods in motion capturing hands, data‐driven and physics‐based algorithms to synthesize their motions, and techniques to make the appearance of the hand model surface more realistic. We then focus on areas in which detailed hand motions are crucial such as manipulation and communication. Our report concludes by describing emerging trends and applications for virtual hand animation.},
	language = {en},
	number = {2},
	urldate = {2023-07-12},
	journal = {Computer Graphics Forum},
	author = {Wheatland, Nkenge and Wang, Yingying and Song, Huaguang and Neff, Michael and Zordan, Victor and Jörg, Sophie},
	month = may,
	year = {2015},
	note = {71 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {hand-modelling},
	pages = {735--760},
	annote = {[TLDR] This report discusses current methods in motion capturing hands, data‐driven and physics‐based algorithms to synthesize their motions, and techniques to make the appearance of the hand model surface more realistic, and describes emerging trends and applications for virtual hand animation.},
	file = {Wheatland et al. - 2015 - State of the Art in Hand and Finger Modeling and A.pdf:/Users/brk/Zotero/storage/B7NNE55B/Wheatland et al. - 2015 - State of the Art in Hand and Finger Modeling and A.pdf:application/pdf},
}

@article{cabibihan_suitability_2021,
	title = {Suitability of the {Openly} {Accessible} {3D} {Printed} {Prosthetic} {Hands} for {War}-{Wounded} {Children}},
	volume = {7},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2020.594196/full},
	doi = {10.3389/frobt.2020.594196},
	abstract = {The field of rehabilitation and assistive devices is being disrupted by innovations in desktop 3D printers and open-source designs. For upper limb prosthetics, those technologies have demonstrated a strong potential to aid those with missing hands. However, there are basic interfacing issues that need to be addressed for long term usage. The functionality, durability, and the price need to be considered especially for those in difficult living conditions. We evaluated the most popular designs of body-powered, 3D printed prosthetic hands. We selected a representative sample and evaluated its suitability for its grasping postures, durability, and cost. The prosthetic hand can perform three grasping postures out of the 33 grasps that a human hand can do. This corresponds to grasping objects similar to a coin, a golf ball, and a credit card. Results showed that the material used in the hand and the cables can withstand a 22 N normal grasping force, which is acceptable based on standards for accessibility design. The cost model showed that a 3D printed hand could be produced for as low as \$19. For the benefit of children with congenital missing limbs and for the war-wounded, the results can serve as a baseline study to advance the development of prosthetic hands that are functional yet low-cost.},
	urldate = {2023-07-12},
	journal = {Frontiers in Robotics and AI},
	author = {Cabibihan, John-John and Alkhatib, Farah and Mudassir, Mohammed and Lambert, Laurent A. and Al-Kwifi, Osama S. and Diab, Khaled and Mahdi, Elsadig},
	month = jan,
	year = {2021},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {medical},
	pages = {594196},
	annote = {[TLDR] Evaluated the most popular designs of body-powered, 3D printed prosthetic hands and showed that the material used in the hand and the cables can withstand a 22 N normal grasping force, which is acceptable based on standards for accessibility design.},
	file = {Full Text:/Users/brk/Zotero/storage/KXV4GLFK/Cabibihan et al. - 2021 - Suitability of the Openly Accessible 3D Printed Pr.pdf:application/pdf},
}

@article{noauthor_medical_2014,
	title = {Medical gallery of {Blausen} {Medical} 2014},
	volume = {1},
	issn = {20024436},
	url = {https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Blausen_Medical_2014},
	doi = {10.15347/wjm/2014.010},
	number = {2},
	urldate = {2023-07-12},
	journal = {WikiJournal of Medicine},
	year = {2014},
	file = {Full Text:/Users/brk/Zotero/storage/CI57MESF/2014 - Medical gallery of Blausen Medical 2014.pdf:application/pdf},
}


@inproceedings{fisher_telepresence_1987,
	address = {Cambridge, MA},
	title = {Telepresence {Master} {Glove} {Controller} {For} {Dexterous} {Robotic} {End}-{Effectors}},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.937753},
	doi = {10.1117/12.937753},
	abstract = {This paper describes recent research in the Aerospace Human Factors Research Division at NASA's Ames Research Center to develop a glove-like, control and data-recording device (DataGlove) that records and transmits to a host computerin real time, and at appropriate resolution, a numeric data-record of a user's hand/finger shape and dynamics. System configuration and performance specifications are detailed, and current research is discussed investigating its applications in operator control of dexterous robotic end-effectors and for use as a human factors research tool in evaluation of operator hand function requirements and performance in other specialized task environments.},
	urldate = {2023-07-12},
	author = {Fisher, Scott S.},
	editor = {Casasent, David P.},
	month = mar,
	year = {1987},
	note = {40 citations (Semantic Scholar/DOI) [2023-07-12]},
	keywords = {hardware:dataglove, tech:accelerometer, type:seminal},
	pages = {396},
	annote = {[TLDR] A glove-like, control and data-recording device (DataGlove) that records and transmits to a host computer in real time, and at appropriate resolution, a numeric data-record of a user's hand/finger shape and dynamics is described.},
}

@misc{experimental_television_center_computer_1969,
	title = {Computer {Image} {Corporation} {Archive}},
	url = {https://www.experimentaltvcenter.org/computer-image-corporation-archive/},
	abstract = {The Computer Image Corporation Archive contains an extensive film and video collection documenting work produced using machines developed by Computer Image Corporation of Denver, Colorado (1960-1985).},
	journal = {Computer Image Corporation Archive},
	author = {{\{Experimental Television Center\}}},
	year = {1969},
}

@misc{thomas_a_defanti_us_1977,
    title = {{US} {NEA} {R60}-34-163 {FINAL} {PROJECT} {REPORT}},
    publisher = {University of Illinois at Chicago Circle},
    author = {{Thomas A. DeFanti} and {Daniel J. Sandin}},
    year = {1977},
    file = {Thomas A. DeFanti and Daniel J. Sandin - 1977 - US NEA R60-34-163 FINAL PROJECT REPORT.pdf:/Users/brk/Zotero/storage/V95YJFCN/Thomas A. DeFanti and Daniel J. Sandin - 1977 - US NEA R60-34-163 FINAL PROJECT REPORT.pdf:application/pdf},
}

@article{jacobsen_utahmit_1984,
	title = {The {UTAH}/{M}.{I}.{T}. {Dextrous} {Hand}: {Work} in {Progress}},
	volume = {3},
	issn = {0278-3649, 1741-3176},
	shorttitle = {The {UTAH}/{M}.{I}.{T}. {Dextrous} {Hand}},
	url = {http://journals.sagepub.com/doi/10.1177/027836498400300402},
	doi = {10.1177/027836498400300402},
	abstract = {The Center for Biomedical Design at the University of Utah and the Artificial Intelligence Laboratory at the Massachu setts Institute of Technology are developing a tendon-oper ated multiple-degree-of-freedom dextrous hand (DH) with multichannel touch-sensing capability. Our goal is the design and fabrication of a high-performance yet well-behaved sys tem that is fast and stable and that includes considerable operational flexibility as a research tool. The paper reviews progress to date on project subtasks and discusses design issues important to hardware and control systems develop ment in terms of (1) structures that contain tendons, actua tors, joints, and sensors; (2) both pneumatic and electric tendon actuation systems; (3) optically based sensors that detect touch; (4) subcontrol systems that provide internal management of the DH; and (5) preliminary higher control systems that supervise general operation of the hand during execution of tasks and that provide integration of vision and tactile information.},
	language = {en},
	number = {4},
	urldate = {2023-07-13},
	journal = {The International Journal of Robotics Research},
	author = {Jacobsen, S.C. and Wood, J.E. and Knutti, D.F. and Biggers, K.B.},
	month = dec,
	year = {1984},
	note = {630 citations (Semantic Scholar/DOI) [2023-07-13]},
	pages = {21--50},
}

@article{jacobsen_design_1986,
	title = {Design of the {Utah}/{M}.{I}.{T}. {Dextrous} {Hand}},
	volume = {3},
	url = {http://ieeexplore.ieee.org/document/1087395/},
	doi = {10.1109/ROBOT.1986.1087395},
	abstract = {The Center for Engineering Design at the University of Utah, and the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology have developed a robotic end effector intended to function as a general purpose research tool for the study of machine dexterity. The high performance, multifingered hand will provide two important capabilities. First, it will permit the experimental investigation of basic concepts in manipulation theory, control system design and tactile sensing. Second, it will expand understanding required for the future design of physical machinery and will serve as a "test bed" for the development of tactile sensing systems. The paper includes: 1) a discussion of issues important to the development of manipulation machines; 2) general comments regarding design of the Utah/M.I.T. Dextrous Hand; and, 3) a detailed discussion of specific subsystems of the hand.},
	urldate = {2023-07-13},
	journal = {Proceedings. 1986 IEEE International Conference on Robotics and Automation},
	author = {Jacobsen, S. and Iversen, E. and Knutti, D. and Johnson, R. and Biggers, K.},
	year = {1986},
	note = {557 citations (Semantic Scholar/DOI) [2023-07-13]
Conference Name: 1986 IEEE International Conference on Robotics and Automation
Place: San Francisco, CA, USA
Publisher: Institute of Electrical and Electronics Engineers},
	keywords = {untagged},
	pages = {1520--1532},
	annote = {[TLDR] The Center for Engineering Design at the University of Utah, and the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology have developed a robotic end effector intended to function as a general purpose research tool for the study of machine dexterity.},
	file = {Jacobsen et al. - DESIGN OF THE UTAHh4.I.T. DEXTROUSHAND.pdf:/Users/brk/Zotero/storage/7U79YIGP/Jacobsen et al. - DESIGN OF THE UTAHh4.I.T. DEXTROUSHAND.pdf:application/pdf},
}

@misc{abrams_gentile_entertainment_powerglove_1989,
	title = {{PowerGlove}},
	url = {https://web.archive.org/web/20150525222703/http://www.ageinc.com/tech/index.html},
	author = {{Abrams Gentile Entertainment}},
	year = {1989},
}

@misc{immersion_corporation_cyber_2001,
	title = {Cyber {Glove}},
	url = {https://web.archive.org/web/20011005155130/https://www.immersion.com/products/3d/interaction/cyberglove.shtml},
	abstract = {Immersion 3D's award-winning instrumented glove
CyberGlove® The CyberGlove® is a fully instrumented glove that provides up to 22 high-accuracy joint-angle measurements. It uses proprietary resistive bend-sensing technology to accurately transform hand and finger motions into real-time digital joint-angle data. Our VirtualHand® Studio software converts the data into a graphical hand which mirrors the subtle movements of the physical hand. It is available in two models and for either hand.},
	author = {{Immersion Corporation}},
	year = {2001},
}

@misc{gary_j_grimes_us_1981,
	title = {{US} {Patent} for {Digital} data entry glove interface device {Patent} ({Patent} \# 4,414,537 issued {November} 8, 1983) - {Justia} {Patents} {Search}},
	url = {https://patents.justia.com/patent/4414537},
	abstract = {A man-machine interface is disclosed for translating discrete hand positions into electrical signals representing alpha-numeric characters. The interface comprises a glove having sensors positioned with respect to the hand for detecting the flex of finger joints and sensors for detecting the contact between various portions of the hand. Additional sensors detect the movement of the hand with respect to a gravitational vector and a horizontal plane of reference. Further additional sensors detect the twisting and flexing of the wrist. The additional sensors are associated with prescribed mode signals which determine whether subsequently formed or priorly formed character specifying hand positions are to be entered for transmission. The alpha-numeric characters associated with the formed character specifying hand positions are transmitted only when the appropriate mode signal results. The forming and moving of the hand actuates various combinations of sensors so that electrical signals representing the specified characters are generated and transmitted.},
	urldate = {2023-07-13},
	author = {{Gary J. Grimes}},
	year = {1981},
	keywords = {tech:flex, untagged, based-on-gloves},
	file = {US Patent for Digital data entry glove interface d.pdf:/Users/brk/Zotero/storage/J9WZWAX2/US Patent for Digital data entry glove interface d.pdf:application/pdf;US Patent for Digital data entry glove interface device Patent (Patent # 4,414,537 issued November 8, 1983) - Justia Patents Search:/Users/brk/Zotero/storage/FQNIK68R/4414537.html:text/html},
}

@inproceedings{marcus_sensing_1988,
	title = {Sensing human hand motions for controlling dexterous robots},
	url = {https://www.semanticscholar.org/paper/Sensing-human-hand-motions-for-controlling-robots-Marcus-Churchill/ca562cee0ce3781c0bc73e0392bc362e5a5f585c},
	abstract = {The Dexterous Hand Master (DHM) system is designed to control dexterous robot hands such as the UTAH/MIT and Stanford/JPL hands. It is the first commercially available device which makes it possible to accurately and confortably track the complex motion of the human finger joints. The DHM is adaptable to a wide variety of human hand sizes and shapes, throughout their full range of motion.},
	urldate = {2023-07-13},
	author = {Marcus, B. and Churchill, Philip J.},
	month = nov,
	year = {1988},
	keywords = {untagged},
	annote = {[TLDR] The Dexterous Hand Master system is designed to control dexterous robot hands such as the UTAH/MIT and Stanford/JPL hands and is the first commercially available device which makes it possible to accurately and confortably track the complex motion of the human finger joints.},
	file = {Marcus and Churchill - 1988 - Sensing human hand motions for controlling dextero.pdf:/Users/brk/Zotero/storage/TKRBDT3Q/Marcus and Churchill - 1988 - Sensing human hand motions for controlling dextero.pdf:application/pdf},
}

