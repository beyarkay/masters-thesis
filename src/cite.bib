
@inproceedings{rigoll_high_1998,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {High performance real-time gesture recognition using {Hidden} {Markov} {Models}},
	isbn = {978-3-540-69782-4},
	doi = {10.1007/BFb0052990},
	abstract = {An advanced real-time system for gesture recognition is presented, which is able to recognize complex dynamic gestures, such as ”hand waving”, ”spin”, ”pointing”, and ”head moving”. The recognition is based on global motion features, extracted from each difference image of the image sequence. The system uses Hidden Markov Models (HMMs) as statistical classifier. These HMMs are trained on a database of 24 isolated gestures, performed by 14 different people. With the use of global motion features, a recognition rate of 92.9\% is achieved for a person and background independent recognition.},
	language = {en},
	booktitle = {Gesture and {Sign} {Language} in {Human}-{Computer} {Interaction}},
	publisher = {Springer},
	author = {Rigoll, Gerhard and Kosmala, Andreas and Eickeler, Stefan},
	editor = {Wachsmuth, Ipke and Fröhlich, Martin},
	year = {1998},
	note = {27 citations (Crossref) [2023-07-11]
122 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {from:cite.bib, model:hmm, tech:rgb, untagged},
	pages = {69--80},
	file = {Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf:/Users/brk/Zotero/storage/BBCJSZZM/Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf:application/pdf},
}

@article{vasconez_hand_2022,
	title = {Hand {Gesture} {Recognition} {Using} {EMG}-{IMU} {Signals} and {Deep} {Q}-{Networks}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/24/9613},
	doi = {10.3390/s22249613},
	abstract = {Hand gesture recognition systems (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) have been studied for different applications in recent years. Most commonly, cutting-edge HGR methods are based on supervised machine learning methods. However, the potential benefits of reinforcement learning (RL) techniques have shown that these techniques could be a viable option for classifying EMGs. Methods based on RL have several advantages such as promising classification performance and online learning from experience. In this work, we developed an HGR system made up of the following stages: pre-processing, feature extraction, classification, and post-processing. For the classification stage, we built an RL-based agent capable of learning to classify and recognize eleven hand gestures—five static and six dynamic—using a deep Q-network (DQN) algorithm based on EMG and IMU information. The proposed system uses a feed-forward artificial neural network (ANN) for the representation of the agent policy. We carried out the same experiments with two different types of sensors to compare their performance, which are the Myo armband sensor and the G-force sensor. We performed experiments using training, validation, and test set distributions, and the results were evaluated for user-specific HGR models. The final accuracy results demonstrated that the best model was able to reach up to 97.50\%±1.13\% and 88.15\%±2.84\% for the classification and recognition, respectively, with regard to static gestures, and 98.95\%±0.62\% and 90.47\%±4.57\% for the classification and recognition, respectively, with regard to dynamic gestures with the Myo armband sensor. The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.},
	language = {en},
	number = {24},
	urldate = {2023-03-07},
	journal = {Sensors},
	author = {Vásconez, Juan Pablo and Barona López, Lorena Isabel and Valdivieso Caraguay, Ángel Leonardo and Benalcázar, Marco E.},
	month = dec,
	year = {2022},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg},
	pages = {9613},
	file = {Full Text PDF:/Users/brk/Zotero/storage/AJ2WMLPJ/Vásconez et al. - 2022 - Hand Gesture Recognition Using EMG-IMU Signals and.pdf:application/pdf},
}

@article{galka_inertial_2016,
	title = {Inertial {Motion} {Sensing} {Glove} for {Sign} {Language} {Gesture} {Acquisition} and {Recognition}},
	volume = {16},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {http://ieeexplore.ieee.org/document/7497574/},
	doi = {10.1109/JSEN.2016.2583542},
	abstract = {The most popular systems for automatic sign language recognition are based on vision. They are user-friendly, but very sensitive to changes in regard to recording conditions. This paper presents a description of the construction of a more robust system-an accelerometer glove-as well as its application in the recognition of sign language gestures. The basic data regarding inertial motion sensors and the design of the gesture acquisition system as well as project proposals are presented. The evaluation of the solution presents the results of the gesture recognition attempt by using a selected set of sign language gestures with a described method based on Hidden Markov Model (HMM) and parallel HMM approaches. The proposed usage of parallel HMM for sensor-fusion modeling reduced the equal error rate by more than 60\%, while preserving 99.75\% recognition accuracy.},
	number = {16},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Galka, Jakub and Masior, Mariusz and Zaborski, Mateusz and Barczewska, Katarzyna},
	month = aug,
	year = {2016},
	note = {81 citations (Crossref) [2023-07-11]
92 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, untagged, from:cite.bib},
	pages = {6310--6316},
	file = {Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf:/Users/brk/Zotero/storage/AI4LXML7/Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf:application/pdf},
}

@inproceedings{saliba_compact_2004,
	title = {A {Compact} {Glove} {Input} {Device} to {Measure} {Human} {Hand}, {Wrist} and {Forearm} {Joint} {Positions} for {Teleoperation} {Applications}},
	url = {https://www.semanticscholar.org/paper/A-Compact-Glove-Input-Device-to-Measure-Human-Hand%2C-Saliba-Farrugia/f99389f732525d8a5603da09b2d3de257b763e43},
	abstract = {In this work, we have developed a new glove input device that is able to measure the angular joint positions of two fingers and of the thumb on the human hand, as well as the pitch position of the wrist and the roll position of the radio-ulnar joint of the human forearm. The glove has various new features, including the measurement of forearm roll position, that are not found in other glove input devices described in the literature. The glove contains a number of flexible, plastic bands whose displacement, during joint rotation, is measured using linear potentiometers. The new glove is light, compact, easy to wear and use, robust, and inexpensive, and is intended for use in teleoperation applications in conjunction with a remotely located robot hand/wrist. Another property is that it can be easily adjusted to fit a wide range of human hand sizes. Preliminary testing of the glove has shown that it can achieve an accuracy in position measurement that compares well to that of a number of commercially-produced gloves that are presently in use. Keywords—glove input device; whole hand input device; teleoperation; human joint position sensing.},
	urldate = {2023-03-07},
	author = {Saliba, M. and Farrugia, F. and Giordmaina, A.},
	year = {2004},
	keywords = {model:none},
	file = {Full Text PDF:/Users/brk/Zotero/storage/446F9VJ8/Saliba et al. - 2004 - A Compact Glove Input Device to Measure Human Hand.pdf:application/pdf},
}

@article{alsaedi_efficient_2020,
	title = {An efficient hand gestures recognition system},
	volume = {745},
	issn = {1757-8981, 1757-899X},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/745/1/012045},
	doi = {10.1088/1757-899X/745/1/012045},
	abstract = {Abstract
            Talking about gestures make us return to the historical beginning of human communication because there is no language completely free of gestures. People cannot communicate without gestures. Any action or movement without gestures is free of real feelings and cannot express the thoughts. The purpose of any hand gesture recognition system is to recognize the hand gesture and used it to transfer a certain meaning or for computer control or/and a device. This paper introduced an efficient system to recognize hand gestures in real-time. Generally, the system is divided into five phases, first to image acquisition, second to pre-processing the image, third for detection and segmentation of the hand region, fourth to features extraction and fifth to count the numbers of fingers for gesture recognition. The system has been coded by Python language, PyAutoGUI library, OS Module of Python and the Open CV library.},
	number = {1},
	urldate = {2023-03-07},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {AlSaedi, Ahmed Kadem Hamed and AlAsadi, Abbas H. Hassin},
	month = feb,
	year = {2020},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, classes:10-29},
	pages = {012045},
	file = {Full Text:/Users/brk/Zotero/storage/2NTXV8NA/AlSaedi and AlAsadi - 2020 - An efficient hand gestures recognition system.pdf:application/pdf},
}

@inproceedings{zimmerman_hand_1987,
	address = {Toronto, Ontario, Canada},
	title = {A hand gesture interface device},
	isbn = {978-0-89791-213-6},
	url = {http://portal.acm.org/citation.cfm?doid=29933.275628},
	doi = {10.1145/29933.275628},
	abstract = {This paper reports on the development of a hand to machine interface device that provides real-time gesture, position and orientation information. The key element is a glove and the device as a whole incorporates a collection of technologies. Analog flex sensors on the glove measure finger bending. Hand position and orientation are measured either by ultrasonics, providing five degrees of freedom, or magnetic flux sensors, which provide six degrees of freedom. Piezoceramic benders provide the wearer of the glove with tactile feedback. These sensors are mounted on the light-weight glove and connected to the driving hardware via a small cable.
Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the {SIGCHI}/{GI} conference on {Human} factors in computing systems and graphics interface  - {CHI} '87},
	publisher = {ACM Press},
	author = {Zimmerman, Thomas G. and Lanier, Jaron and Blanchard, Chuck and Bryson, Steve and Harvill, Young},
	year = {1987},
	note = {501 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, model:none},
	pages = {189--192},
	file = {Full Text:/Users/brk/Zotero/storage/CTAKLA8C/Zimmerman et al. - 1987 - A hand gesture interface device.pdf:application/pdf},
}

@article{xu_zhang_framework_2011,
	title = {A {Framework} for {Hand} {Gesture} {Recognition} {Based} on {Accelerometer} and {EMG} {Sensors}},
	volume = {41},
	issn = {1083-4427, 1558-2426},
	url = {http://ieeexplore.ieee.org/document/5735233/},
	doi = {10.1109/TSMCA.2011.2116004},
	abstract = {This paper presents a framework for hand gesture recognition based on the information fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors. In our framework, the start and end points of meaningful gesture segments are detected automatically by the intensity of the EMG signals. A decision tree and multistream hidden Markov models are utilized as decision-level fusion to get the final results. For sign language recognition (SLR), experimental results on the classification of 72 Chinese Sign Language (CSL) words demonstrate the complementary functionality of the ACC and EMG sensors and the effectiveness of our framework. Additionally, the recognition of 40 CSL sentences is implemented to evaluate our framework for continuous SLR. For gesture-based control, a real-time interactive system is built as a virtual Rubik's cube game using 18 kinds of hand gestures as control commands. While ten subjects play the game, the performance is also examined in user-specific and user-independent classification. Our proposed framework facilitates intelligent and natural control in gesture-based interaction.},
	number = {6},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {{Xu Zhang} and {Xiang Chen} and {Yun Li} and Lantz, V. and {Kongqiao Wang} and {Jihai Yang}},
	month = nov,
	year = {2011},
	note = {518 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:hmm, tech:emg, model:decision-tree, classes:72, app:sign-language, classes:50-99},
	pages = {1064--1076},
}

@article{wu_wearable_2016,
	title = {A {Wearable} {System} for {Recognizing} {American} {Sign} {Language} in {Real}-{Time} {Using} {IMU} and {Surface} {EMG} {Sensors}},
	volume = {20},
	issn = {2168-2194, 2168-2208},
	url = {http://ieeexplore.ieee.org/document/7552525/},
	doi = {10.1109/JBHI.2016.2598302},
	abstract = {A sign language recognition system translates signs performed by deaf individuals into text/speech in real time. Inertial measurement unit and surface electromyography (sEMG) are both useful modalities to detect hand/arm gestures. They are able to capture signs and the fusion of these two complementary sensor modalities will enhance system performance. In this paper, a wearable system for recognizing American Sign Language (ASL) in real time is proposed, fusing information from an inertial sensor and sEMG sensors. An information gain-based feature selection scheme is used to select the best subset of features from a broad range of well-established features. Four popular classification algorithms are evaluated for 80 commonly used ASL signs on four subjects. The experimental results show 96.16\% and 85.24\% average accuracies for intra-subject and intra-subject cross session evaluation, respectively, with the selected feature subset and a support vector machine classifier. The significance of adding sEMG for ASL recognition is explored and the best channel of sEMG is highlighted.},
	number = {5},
	urldate = {2023-03-07},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Wu, Jian and Sun, Lu and Jafari, Roozbeh},
	month = sep,
	year = {2016},
	note = {166 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:svm, tech:emg, classes:80, model:dt, model:knn, model:nb, impressive, app:sign-language, classes:50-99},
	pages = {1281--1290},
	file = {Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf:/Users/brk/Zotero/storage/2J63995L/Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf:application/pdf},
}

@article{lee_smart_2018,
	title = {Smart {Wearable} {Hand} {Device} for {Sign} {Language} {Interpretation} {System} {With} {Sensors} {Fusion}},
	volume = {18},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {http://ieeexplore.ieee.org/document/8126796/},
	doi = {10.1109/JSEN.2017.2779466},
	abstract = {Gesturing is an instinctive way of communicating to present a specific meaning or intent. Therefore, research into sign language interpretation using gestures has been explored progressively during recent decades to serve as an auxiliary tool for deaf and mute people to blend into society without barriers. In this paper, a smart sign language interpretation system using a wearable hand device is proposed to meet this purpose. This wearable system utilizes five flex-sensors, two pressure sensors, and a three-axis inertial motion sensor to distinguish the characters in the American sign language alphabet. The entire system mainly consists of three modules: 1) a wearable device with a sensor module; 2) a processing module; and 3) a display unit mobile application module. Sensor data are collected and analyzed using a built-in embedded support vector machine classifier. Subsequently, the recognized alphabet is further transmitted to a mobile device through Bluetooth low energy wireless communication. An Android-based mobile application was developed with a text-to-speech function that converts the received textinto audible voice output. Experiment results indicate that a true sign language recognition accuracy rate of 65.7\% can be achieved on average in the first version without pressure sensors. A second version of the proposed wearable system with the fusion of pressure sensors on the middle finger increased the recognition accuracy rate dramatically to 98.2\%. The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Lee, Boon Giin and Lee, Su Min},
	month = feb,
	year = {2018},
	note = {102 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1224--1232},
	file = {Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf:/Users/brk/Zotero/storage/CQT634W2/Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf:application/pdf},
}

@article{kudrinko_wearable_2021,
	title = {Wearable {Sensor}-{Based} {Sign} {Language} {Recognition}: {A} {Comprehensive} {Review}},
	volume = {14},
	issn = {1937-3333, 1941-1189},
	shorttitle = {Wearable {Sensor}-{Based} {Sign} {Language} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/9178440/},
	doi = {10.1109/RBME.2020.3019769},
	abstract = {Sign language is used as a primary form of communication by many people who are Deaf, deafened, hard of hearing, and non-verbal. Communication barriers exist for members of these populations during daily interactions with those who are unable to understand or use sign language. Advancements in technology and machine learning techniques have led to the development of innovative approaches for gesture recognition. This literature review focuses on analyzing studies that use wearable sensor-based systems to classify sign language gestures. A review of 72 studies from 1991 to 2019 was performed to identify trends, best practices, and common challenges. Attributes including sign language variation, sensor configuration, classification method, study design, and performance metrics were analyzed and compared. Results from this literature review could aid in the development of user-centred and robust wearable sensor-based systems for sign language recognition.},
	urldate = {2023-03-07},
	journal = {IEEE Reviews in Biomedical Engineering},
	author = {Kudrinko, Karly and Flavin, Emile and Zhu, Xiaodan and Li, Qingguo},
	year = {2021},
	note = {37 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, type:survey},
	pages = {82--97},
}

@article{mardiyanto_development_2017,
	title = {Development of hand gesture recognition sensor based on accelerometer and gyroscope for controlling arm of underwater remotely operated robot},
	url = {http://ieeexplore.ieee.org/document/8124104/},
	doi = {10.1109/ISITIA.2017.8124104},
	abstract = {Hand Gesture Recognition sensor based on accelerometer and gyroscope is a sensor for capturing the positions of operator hand while controlling underwater remotely operated vehicle equipped with an arm. The proposed system has an advantage in its convenience by means of no training or exercise needed for operator before using it. The key issue here is how beginner operator could use easily the underwater remotely operated robot arm without any specific training. The conventional one uses a joystick for controlling the underwater system and it is inconvenience for beginner user as well as less precision. The proposed system consists of two main part: (1) ground station and (2) underwater remotely operated robot arm. This paper proposes the development of hand gesture recognition sensor used by operator at the ground station for controlling robot arm at the underwater robot. The proposed sensor uses accelerometers and gyroscopes installed in elbow, forearm, and wrist. These devices measure 3D position of each joints for constructing 3D position of hand. We design sensor's casing for its convenience of use by using CAD software. Each sensor is connected by Arduino Nano microcontroller having compact circuit and embedded it into sensor's casing. The sensors are connected to a microcontroller acting as master connected to microcontroller slave (sensor part). These sensors value are converted to 3D position by using forward kinematic. The forward kinematic values are sent to the underwater robot by using a wire utilizing Pulse Position Signal. Then, it converted again to servo's movement by using inverse kinematic. The result is operator able to control the underwater remotely robot arm by utilizing hand gesture directly. The last, operator could control the robot gripper based on flex sensor installed in operator's fingers. The accuracy of the sensor has been tested under laboratory condition, it has 98\% of accuracy.},
	urldate = {2023-03-07},
	journal = {2017 International Seminar on Intelligent Technology and Its Applications (ISITIA)},
	author = {Mardiyanto, Ronny and Utomo, Mochamad Fajar Rinaldi and Purwanto, Djoko and Suryoatmojo, Heri},
	month = aug,
	year = {2017},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 International Seminar on Intelligent Technology and its Applications (ISITIA)
ISBN: 9781538627082
Place: Surabaya
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:flex, app:robot-control, arduino-nano},
	pages = {329--333},
}

@article{chu_sensor-based_2021,
	title = {A {Sensor}-{Based} {Hand} {Gesture} {Recognition} {System} for {Japanese} {Sign} {Language}},
	url = {https://ieeexplore.ieee.org/document/9391981/},
	doi = {10.1109/LifeTech52111.2021.9391981},
	abstract = {In this paper, we propose a sensor-based data acquisition glove for Japanese Sign Language (JSL) hand gesture recognition. Five flex sensors, an Inertial Measurement Unit (IMU), and three Force Sensing Resistors (FSRs) are used to detect the bending degree of fingers and hand movement information. The detected data are transmitted to the computer by an Arduino Micro. The average accuracy of the hand gesture recognition for a single subject, using the Support Vector Machine (SVM) based and the Dynamic Time Wrapping (DTW) based algorithm are 96.9\% and 94.5\%, respectively. Our proposed system also achieves an average recognition accuracy of about 82.5\% for the cross-recognition among three subjects. The experimental results indicate that our proposed system has great potential for JSL hand gesture recognition.},
	urldate = {2023-03-07},
	journal = {2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)},
	author = {Chu, Xianzhi and Liu, Jiang and Shimamoto, Shigeru},
	month = mar,
	year = {2021},
	note = {8 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)
ISBN: 9781665418751
Place: Nara, Japan
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {311--312},
}

@inproceedings{xie_accelerometer_2014,
	title = {Accelerometer {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Gesture-Recognition-Xie/c8bb38e46b7d97cc1a4ccd4a9d6df9717c1c3ff7},
	abstract = {Our goal is to make gesture-based input for smartphones and smartwatches accurate and feasible to use. With a custom Android application to record accelerometer data for 5 gestures, we developed a highly accurate SVM classifier using only 1 training example per class. Our novel Dynamic-Threshold Truncation algorithm during preprocessing improved accuracy on 1 training example per class by 14\% and the addition of axis-wise Discrete Fourier Transform coefficient features improved accuracy on 1 training example per class by 5\%. With 5 gesture classes, 1 training example for each class, and 30 test examples for each class, our classifier achieves 96\% accuracy. With 5 training examples per class, the classifier achieves 98\% accuracy, which is greater than the 10-example accuracy of other efforts using HMM’s[1, 2]. This makes it feasible for a real-time implementation of accelerometer-based gesture recognition to identify user-defined gestures with high accuracy while requiring little training effort from the user.},
	urldate = {2023-03-07},
	author = {Xie, Michael},
	year = {2014},
	keywords = {tech:accelerometer, classes:5, model:svm, classes:{\textless}10},
	file = {Full Text PDF:/Users/brk/Zotero/storage/EC67HB44/Xie - 2014 - Accelerometer Gesture Recognition.pdf:application/pdf},
}

@article{patil_marathi_2022,
	title = {Marathi {Sign} {Language} {Hand} {Gesture} {Recognition} {Using} {Accelerometer} and {3D} {Printed} {Gloves}},
	url = {https://ieeexplore.ieee.org/document/10008290/},
	doi = {10.1109/CICN56167.2022.10008290},
	abstract = {Due to the communication abilities impairment, deaf \& dumb peoples are more or less isolated from mainstream societal activities. NGOs, social workers \& Governments in India are putting efforts with multiple initiatives to increase the involvement of these peoples in the mainstream activities happening in the rest of world. This paper proposes to establish an alternative method to streamline the communications among normal \& speech impaired persons by using smart hand glove system equipped with 3D accelerometer modules at the fingertips. The proposed system is delivering the measurement results in very proficient way. The experiments were carried out over 15 persons using the sign gestures, providing good co-relation between the Posed gestures versus system mapped gestures.},
	urldate = {2023-03-07},
	journal = {2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)},
	author = {Patil, Sachin and Joshi, Sarang and Kulkarni, Hrushikesh B. and Hagawane, Pradnesh and Shinde, Pradnya},
	month = dec,
	year = {2022},
	note = {0 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)
ISBN: 9781665487719
Place: Al-Khobar, Saudi Arabia
Publisher: IEEE},
	keywords = {tech:accelerometer, app:sign-language, app:marathi-sl},
	pages = {72--77},
}

@article{alzubaidi_novel_2023,
	title = {A {Novel} {Assistive} {Glove} to {Convert} {Arabic} {Sign} {Language} into {Speech}},
	volume = {22},
	issn = {2375-4699, 2375-4702},
	url = {https://dl.acm.org/doi/10.1145/3545113},
	doi = {10.1145/3545113},
	abstract = {People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98\%. A usability study was then conducted to determine the usefulness of the assistive device in real-time.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	author = {Alzubaidi, Mohammad A. and Otoom, Mwaffaq and Abu Rwaq, Areen M.},
	month = mar,
	year = {2023},
	note = {3 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1--16},
}

@article{ahmed_real-time_2021,
	title = {Real-time sign language framework based on wearable device: analysis of {MSL}, {DataGlove}, and gesture recognition},
	volume = {25},
	issn = {1432-7643, 1433-7479},
	shorttitle = {Real-time sign language framework based on wearable device},
	url = {https://link.springer.com/10.1007/s00500-021-05855-6},
	doi = {10.1007/s00500-021-05855-6},
	abstract = {Researchers have been inspired to use technology to enable people with hearing and speech impairment to communicate and engage with others around them. Sensory approach to recognition facilitates real-time and accurate recognition of signs. Thus, this study proposes a Malaysian Sign Language (MSL) recognition framework. The framework consists of three sub-modules for the recognition of static isolated signs based on data collected from a DataGlove. The first module focuses on the characteristics of signs, yielding sign recognition system requirements. The second module describes the different steps required to develop a wearable sign-capture device. The third module discusses the real-time SL recognition approach, which uses a template-matching algorithm to recognize acquired data. The final design of the DataGlove with 65 data channel fulfils the requirement identified from an analysis of MSL. The DataGlove is able to record data for all of the signs (both dynamic and static) of MSL due to the wide range of captured hand features. As a result, the recognition engine can accurately recognize complex signs.},
	language = {en},
	number = {16},
	urldate = {2023-03-07},
	journal = {Soft Computing},
	author = {Ahmed, M. A. and Zaidan, B. B. and Zaidan, A. A. and Alamoodi, A. H. and Albahri, O. S. and Al-Qaysi, Z. T. and Albahri, A. S. and Salih, Mahmood M.},
	month = aug,
	year = {2021},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {11101--11122},
}

@article{mummadi_real-time_2018,
	title = {Real-{Time} and {Embedded} {Detection} of {Hand} {Gestures} with an {IMU}-{Based} {Glove}},
	volume = {5},
	issn = {2227-9709},
	url = {http://www.mdpi.com/2227-9709/5/2/28},
	doi = {10.3390/informatics5020028},
	abstract = {This article focuses on the use of data gloves for human-computer interaction concepts, where external sensors cannot always fully observe the user’s hand. A good concept hereby allows to intuitively switch the interaction context on demand by using different hand gestures. The recognition of various, possibly complex hand gestures, however, introduces unintentional overhead to the system. Consequently, we present a data glove prototype comprising a glove-embedded gesture classifier utilizing data from Inertial Measurement Units (IMUs) in the fingertips. In an extensive set of experiments with 57 participants, our system was tested with 22 hand gestures, all taken from the French Sign Language (LSF) alphabet. Results show that our system is capable of detecting the LSF alphabet with a mean accuracy score of 92\% and an F1 score of 91\%, using complementary filter with a gyroscope-to-accelerometer ratio of 93\%. Our approach has also been compared to the local fusion algorithm on an IMU motion sensor, showing faster settling times and less delays after gesture changes. Real-time performance of the recognition is shown to occur within 63 milliseconds, allowing fluent use of the gestures via Bluetooth-connected systems.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {Informatics},
	author = {Mummadi, Chaithanya and Leo, Frederic and Verma, Keshav and Kasireddy, Shivaji and Scholl, Philipp and Kempfle, Jochen and Laerhoven, Kristof},
	month = jun,
	year = {2018},
	note = {46 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, app:sign-language, classes:10-29, app:french-sl},
	pages = {28},
	file = {Full Text PDF:/Users/brk/Zotero/storage/VFSAQLD7/Mummadi et al. - 2018 - Real-Time and Embedded Detection of Hand Gestures .pdf:application/pdf},
}

@article{lee_deep_2020,
	title = {Deep {Learning} {Based} {Real}-{Time} {Recognition} of {Dynamic} {Finger} {Gestures} {Using} a {Data} {Glove}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9264164/},
	doi = {10.1109/ACCESS.2020.3039401},
	abstract = {In this article, a real-time dynamic finger gesture recognition using a soft sensor embedded data glove is presented, which measures the metacarpophalangeal (MCP) and proximal interphalangeal (PIP) joint angles of five fingers. In the gesture recognition field, a challenging problem is that of separating meaningful dynamic gestures from a continuous data stream. Unconscious hand motions or sudden tremors, which can easily lead to segmentation ambiguity, makes this problem difficult. Furthermore, the hand shapes and speeds of users differ when performing the same dynamic gesture, and even those made by one user often vary. To solve the problem of separating meaningful dynamic gestures, we propose a deep learning-based gesture spotting algorithm that detects the start/end of a gesture sequence in a continuous data stream. The gesture spotting algorithm takes window data and estimates a scalar value named gesture progress sequence (GPS). GPS is a quantity that represents gesture progress. Moreover, to solve the gesture variation problem, we propose a sequence simplification algorithm and a deep learning-based gesture recognition algorithm. The proposed three algorithms (gesture spotting algorithm, sequence simplification algorithm, and gesture recognition algorithm) are unified into the real-time gesture recognition system and the system was tested with 11 dynamic finger gestures in real-time. The proposed system took only 6 ms to estimate a GPS and no more than 12 ms to recognize the completed gesture in real-time.},
	urldate = {2023-03-07},
	journal = {IEEE Access},
	author = {Lee, Minhyuk and Bae, Joonbum},
	year = {2020},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, movement:dynamic, classes:10-29},
	pages = {219923--219933},
	file = {Full Text:/Users/brk/Zotero/storage/NRM9LBZL/Lee and Bae - 2020 - Deep Learning Based Real-Time Recognition of Dynam.pdf:application/pdf},
}

@article{makaussov_low-cost_2020,
	title = {A {Low}-{Cost}, {IMU}-{Based} {Real}-{Time} {On} {Device} {Gesture} {Recognition} {Glove}},
	url = {https://ieeexplore.ieee.org/document/9283231/},
	doi = {10.1109/SMC42975.2020.9283231},
	abstract = {This paper evaluates the possibility of performing fine gesture recognition including finger movements on a low-tech device. In particular, we present a solution with a recognition model that is small enough to fit in the memory of a low- tech device and describe related difficulties associated with this approach. Several different Machine Learning techniques are employed and their individual advantages and drawbacks are explored for the task at hand. Our results indicate an average of 95\% accuracy during real-time testing for an eight class decoding task with a custom Recurrent Neural Network approach, that runs on the low-tech device, namely an Arduino Nano 33 BLE. The novelty and strength of this research lies in the fact that we are able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures. The recognition process is conducted on the low-tech device and as a result this solution has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.},
	urldate = {2023-03-07},
	journal = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Makaussov, Oleg and Krassavin, Mikhail and Zhabinets, Maxim and Fazli, Siamac},
	month = oct,
	year = {2020},
	note = {3 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
ISBN: 9781728185262
Place: Toronto, ON, Canada
Publisher: IEEE},
	keywords = {tech:accelerometer, read-priority-1, model:rnn, arduino-nano-ble, classes:{\textless}10},
	pages = {3346--3351},
}

@article{qaroush_smart_2021,
	title = {Smart, comfortable wearable system for recognizing {Arabic} {Sign} {Language} in real-time using {IMUs} and features-based fusion},
	volume = {184},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421008629},
	doi = {10.1016/j.eswa.2021.115448},
	abstract = {Semantic Scholar extracted view of "Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion" by Aziz Qaroush et al.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Expert Systems with Applications},
	author = {Qaroush, Aziz and Yassin, Sara and Al-Nubani, Ali and Alqam, Ameer},
	month = dec,
	year = {2021},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {115448},
}

@article{qi_multi-sensor_2021,
	title = {Multi-{Sensor} {Guided} {Hand} {Gesture} {Recognition} for a {Teleoperated} {Robot} {Using} a {Recurrent} {Neural} {Network}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9457168/},
	doi = {10.1109/LRA.2021.3089999},
	abstract = {Touch-free guided hand gesture recognition for human-robot interactions plays an increasingly significant role in teleoperated surgical robot systems. Indeed, despite depth cameras provide more practical information for recognition accuracy enhancement, the instability and computational burden of depth data represent a tricky problem. In this letter, we propose a novel multi-sensor guided hand gesture recognition system for surgical robot teleoperation. A multi-sensor data fusion model is designed for performing interference in the presence of occlusions. A multilayer Recurrent Neural Network (RNN) consisting of a Long Short-Term Memory (LSTM) module and a dropout layer (LSTM-RNN) is proposed for multiple hand gestures classification. Detected hand gestures are used to perform a set of human-robot collaboration tasks on a surgical robot platform. Classification performance and prediction time is compared among the LSTM-RNN model and several traditional Machine Learning (ML) algorithms, such as k-Nearest Neighbor (k-NN) and Support Vector Machines (SVM). Results show that the proposed LSTM-RNN classifier is able to achieve a higher recognition rate and faster inference speed. In addition, the present adaptive data fusion system shows a strong anti-interference capability for hand gesture recognition in real-time.},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Robotics and Automation Letters},
	author = {Qi, Wen and Ovur, Salih Ertug and Li, Zhijun and Marzullo, Aldo and Song, Rong},
	month = jul,
	year = {2021},
	note = {62 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:svm, model:knn, model:rnn, model:lstm, app:robot-control, app:surgery},
	pages = {6039--6045},
	file = {Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf:/Users/brk/Zotero/storage/8MHHF5ZG/Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf:application/pdf},
}

@article{liu_dynamic_2021,
	title = {Dynamic {Gesture} {Recognition} {Algorithm} {Based} on {3D} {Convolutional} {Neural} {Network}},
	volume = {2021},
	issn = {1687-5273, 1687-5265},
	url = {https://www.hindawi.com/journals/cin/2021/4828102/},
	doi = {10.1155/2021/4828102},
	abstract = {Gesture recognition is one of the important ways of human-computer interaction, which is mainly detected by visual technology. The temporal and spatial features are extracted by convolution of the video containing gesture. However, compared with the convolution calculation of a single image, multiframe image of dynamic gestures has more computation, more complex feature extraction, and more network parameters, which affects the recognition efficiency and real-time performance of the model. To solve above problems, a dynamic gesture recognition model based on CBAM-C3D is proposed. Key frame extraction technology, multimodal joint training, and network optimization with BN layer are used for making the network performance better. The experiments show that the recognition accuracy of the proposed 3D convolutional neural network combined with attention mechanism reaches 72.4\% on EgoGesture dataset, which is improved greatly compared with the current main dynamic gesture recognition methods, and the effectiveness of the proposed algorithm is verified.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Computational Intelligence and Neuroscience},
	author = {Liu, Yuting and Jiang, Du and Duan, Haojie and Sun, Ying and Li, Gongfa and Tao, Bo and Yun, Juntong and Liu, Ying and Chen, Baojia},
	editor = {Ahmed, Syed Hassan},
	month = aug,
	year = {2021},
	note = {40 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb},
	pages = {1--12},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ELPL8EWG/Liu et al. - 2021 - Dynamic Gesture Recognition Algorithm Based on 3D .pdf:application/pdf},
}

@article{mujahid_real-time_2021,
	title = {Real-{Time} {Hand} {Gesture} {Recognition} {Based} on {Deep} {Learning} {YOLOv3} {Model}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/9/4164},
	doi = {10.3390/app11094164},
	abstract = {Using gestures can help people with certain disabilities in communicating with other people. This paper proposes a lightweight model based on YOLO (You Only Look Once) v3 and DarkNet-53 convolutional neural networks for gesture recognition without additional preprocessing, image filtering, and enhancement of images. The proposed model achieved high accuracy even in a complex environment, and it successfully detected gestures even in low-resolution picture mode. The proposed model was evaluated on a labeled dataset of hand gestures in both Pascal VOC and YOLO format. We achieved better results by extracting features from the hand and recognized hand gestures of our proposed YOLOv3 based model with accuracy, precision, recall, and an F-1 score of 97.68, 94.88, 98.66, and 96.70\%, respectively. Further, we compared our model with Single Shot Detector (SSD) and Visual Geometry Group (VGG16), which achieved an accuracy between 82 and 85\%. The trained model can be used for real-time detection, both for static hand images and dynamic gestures recorded on a video.},
	language = {en},
	number = {9},
	urldate = {2023-03-07},
	journal = {Applied Sciences},
	author = {Mujahid, Abdullah and Awan, Mazhar Javed and Yasin, Awais and Mohammed, Mazin Abed and Damaševičius, Robertas and Maskeliūnas, Rytis and Abdulkareem, Karrar Hameed},
	month = may,
	year = {2021},
	note = {85 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb, movement:static, movement:dynamic},
	pages = {4164},
	file = {Full Text PDF:/Users/brk/Zotero/storage/VWFYIVN9/Mujahid et al. - 2021 - Real-Time Hand Gesture Recognition Based on Deep L.pdf:application/pdf},
}

@article{wen_machine_2020,
	title = {Machine {Learning} {Glove} {Using} {Self}‐{Powered} {Conductive} {Superhydrophobic} {Triboelectric} {Textile} for {Gesture} {Recognition} in {VR}/{AR} {Applications}},
	volume = {7},
	issn = {2198-3844, 2198-3844},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202000261},
	doi = {10.1002/advs.202000261},
	abstract = {The rapid progress of Internet of things (IoT) technology raises an imperative demand on human machine interfaces (HMIs) which provide a critical linkage between human and machines. Using a glove as an intuitive and low‐cost HMI can expediently track the motions of human fingers, resulting in a straightforward communication media of human–machine interactions. When combining several triboelectric textile sensors and proper machine learning technique, it has great potential to realize complex gesture recognition with the minimalist‐designed glove for the comprehensive control in both real and virtual space. However, humidity or sweat may negatively affect the triboelectric output as well as the textile itself. Hence, in this work, a facile carbon nanotubes/thermoplastic elastomer (CNTs/TPE) coating approach is investigated in detail to achieve superhydrophobicity of the triboelectric textile for performance improvement. With great energy harvesting and human motion sensing capabilities, the glove using the superhydrophobic textile realizes a low‐cost and self‐powered interface for gesture recognition. By leveraging machine learning technology, various gesture recognition tasks are done in real time by using gestures to achieve highly accurate virtual reality/augmented reality (VR/AR) controls including gun shooting, baseball pitching, and flower arrangement, with minimized effect from sweat during operation.},
	language = {en},
	number = {14},
	urldate = {2023-03-07},
	journal = {Advanced Science},
	author = {Wen, Feng and Sun, Zhongda and He, Tianyiyi and Shi, Qiongfeng and Zhu, Minglu and Zhang, Zixuan and Li, Lianhui and Zhang, Ting and Lee, Chengkuo},
	month = jul,
	year = {2020},
	note = {189 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:flex, model:cnn, classes:9, classes:{\textless}10},
	pages = {2000261},
	file = {Full Text:/Users/brk/Zotero/storage/ETVGGB62/Wen et al. - 2020 - Machine Learning Glove Using Self‐Powered Conducti.pdf:application/pdf},
}

@article{zhang_widar30_2021,
	title = {Widar3.0: {Zero}-{Effort} {Cross}-{Domain} {Gesture} {Recognition} with {Wi}-{Fi}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Widar3.0},
	url = {https://ieeexplore.ieee.org/document/9516988/},
	doi = {10.1109/TPAMI.2021.3105387},
	abstract = {With the development of signal processing technology, the ubiquitous Wi-Fi devices open an unprecedented opportunity to solve the challenging human gesture recognition problem by learning motion representations from wireless signals. Wi-Fi-based gesture recognition systems, although yield good performance on specific data domains, are still practically difficult to be used without explicit adaptation efforts to new domains. Various pioneering approaches have been proposed to resolve this contradiction but extra training efforts are still necessary for either data collection or model re-training when new data domains appear. To advance cross-domain recognition and achieve fully zero-effort recognition, we propose Widar3.0, a Wi-Fi-based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all general model that requires only one-time training but can adapt to different data domains. Experiments on various domain factors (i.e. environments, locations, and orientations of persons) demonstrate the accuracy of 92.7\% for in-domain recognition and 82.6\%-92.4\% for cross-domain recognition without model re-training, outperforming the state-of-the-art solutions.},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Yi and Zheng, Yue and Qian, Kun and Zhang, Guidong and Liu, Yunhao and Wu, Chenshu and Yang, Zheng},
	year = {2021},
	note = {256 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:wifi, model:cnn, model:rnn-gru, classes:6, classes:10, classes:10-29, classes:{\textless}10},
	pages = {1--1},
	file = {Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf:/Users/brk/Zotero/storage/9FJSTD3F/Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf:application/pdf},
}

@article{moin_wearable_2020,
	title = {A wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition},
	volume = {4},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-020-00510-8},
	doi = {10.1038/s41928-020-00510-8},
	abstract = {Wearable devices that monitor muscle activity based on surface electromyography could be of use in the development of hand gesture recognition applications. Such devices typically use machine-learning models, either locally or externally, for gesture classification. However, most devices with local processing cannot offer training and updating of the machine-learning model during use, resulting in suboptimal performance under practical conditions. Here we report a wearable surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities. Our system implements a neuro-inspired hyperdimensional computing algorithm locally for real-time gesture classification, as well as model training and updating under variable conditions such as different arm positions and sensor replacement. The system can classify 13 hand gestures with 97.12\% accuracy for two participants when training with a single trial per gesture. A high accuracy (92.87\%) is preserved on expanding to 21 gestures, and accuracy is recovered by 9.5\% by implementing model updates in response to varying conditions, without additional computation on an external device. A surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities can classify human gestures in real time and with high accuracy.},
	language = {en},
	number = {1},
	urldate = {2023-03-07},
	journal = {Nature Electronics},
	author = {Moin, Ali and Zhou, Andy and Rahimi, Abbas and Menon, Alisha and Benatti, Simone and Alexandrov, George and Tamakloe, Senam and Ting, Jonathan and Yamamoto, Natasha and Khan, Yasser and Burghardt, Fred and Benini, Luca and Arias, Ana C. and Rabaey, Jan M.},
	month = dec,
	year = {2020},
	note = {222 citations (Crossref) [2023-07-02]
0 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, classes:10-29},
	pages = {54--63},
	file = {Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf:/Users/brk/Zotero/storage/NMEFHCRT/Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf:application/pdf},
}

@article{wong_multi-features_2021,
	title = {Multi-{Features} {Capacitive} {Hand} {Gesture} {Recognition} {Sensor}: {A} {Machine} {Learning} {Approach}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	shorttitle = {Multi-{Features} {Capacitive} {Hand} {Gesture} {Recognition} {Sensor}},
	url = {https://ieeexplore.ieee.org/document/9314169/},
	doi = {10.1109/JSEN.2021.3049273},
	abstract = {Gesture recognition technology enables machines to understand human gestures. The technology is considered as a key enabler for gaming and virtual reality applications. In this paper, we propose an effective, low-cost capacitive sensor device to recognize hand gestures. In particular, we designed a prototype of a wearable capacitive sensor unit to capture the capacitance values from the electrodes placed on finger phalanges. The sensor captures finger capacitance values. Each gesture has specific finger capacitance values. We applied a running median filter to the output of the sensor and extracted 15 features for gesture classification training and testing tasks. Subsequently, various analyses were performed to provide more insights into the sensing data. We applied and compared two machine learning algorithms: Error Correction Output Code Support Vector Machines (ECOC-SVM) and \$\{K\}\$ -Nearest Neighbour (KNN) classifiers. The training and testing recognition rates were observed for both intra-participant and inter-participant data sets. Further, we introduced a feature compression approach derived from correlation analysis to reduce the complexity of the machine learning algorithms. Using cross validation, we achieved a classification rate of approximately 99\% for intra-participant data. We achieved a lower recognition rate of 97\% (average cross validation testing) for compressed feature data set using both machine learning approaches. For the inter-participant data, the recognition rate was 99\% (normalized feature data) using KNN and 97\% using ECOC-SVM. The research findings show that our recognition system is competitive and has an immense potential for further study.},
	number = {6},
	urldate = {2023-03-07},
	journal = {IEEE Sensors Journal},
	author = {Wong, W. K. and Juwono, Filbert H. and Khoo, Brendan Teng Thiam},
	month = mar,
	year = {2021},
	note = {35 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:svm, model:knn, tech:capacitive},
	pages = {8441--8450},
}

@inproceedings{klingmann_accelerometer-based_2009,
	title = {Accelerometer-{Based} {Gesture} {Recognition} with the {iPhone}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Gesture-Recognition-with-the-Klingmann/cbe0d68e8e9ec8d6b3d4129366214e98437c896b},
	abstract = {The growing number of small sensors built into consumer electronic devices, such as mobile phones, allow experiments with alternative interaction methods in favour of more physical, intuitive and pervasive human computer interaction. This paper examines hand gestures as an alternative or supplementary input modality for mobile devices. The iPhone is chosen as sensing and processing device. Based on its built-in accelerometer, hand movements are detected and classified into previously trained gestures. A software library for accelerometer-based gesture recognition and a demonstration iPhone application have been developed. The system allows the training and recognition of free-from hand gestures. Discrete hidden Markov models form the core part of the gesture recognition apparatus. Five test gestures have been defined and used to evaluate the performance of the application. The evaluation shows that with 10 training repetitions, an average recognition rate of over 90 percent can be achieved.},
	urldate = {2023-03-07},
	author = {Klingmann, Marco},
	year = {2009},
	note = {22 Citations},
	keywords = {tech:accelerometer, model:hmm, classes:{\textless}10},
}

@inproceedings{zhang_gesture_2009,
	address = {Berlin, Heidelberg},
	title = {Gesture {Recognition} with a 3-{D} {Accelerometer}},
	volume = {5585},
	isbn = {978-3-642-02829-8 978-3-642-02830-4},
	url = {http://link.springer.com/10.1007/978-3-642-02830-4_4},
	doi = {10.1007/978-3-642-02830-4_4},
	abstract = {Gesture-based interaction, as a natural way for human-computer interaction, has a wide range of applications in ubiquitous computing environment. This paper presents an acceleration-based gesture recognition approach, called FDSVM ( Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer. With FDSVM, firstly, the acceleration data of a gesture is collected and represented by a frame-based descriptor, to extract the discriminative information. Then a SVM-based multi-class gesture classifier is built for recognition in the nonlinear gesture feature space. Extensive experimental results on a data set with 3360 gesture samples of 12 gestures over weeks demonstrate that the proposed FDSVM approach significantly outperforms other four methods: DTW, Naive Bayes, C4.5 and HMM. In the user-dependent case, FDSVM achieves the recognition rate of 99.38\% for the 4 direction gestures and 95.21\% for all the 12 gestures. In the user-independent case, it obtains the recognition rate of 98.93\% for 4 gestures and 89.29\% for 12 gestures. Compared to other accelerometer-based gesture recognition approaches reported in literature FDSVM gives the best resulrs for both user-dependent and user-independent cases.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Wu, Jiahui and Pan, Gang and Zhang, Daqing and Qi, Guande and Li, Shijian},
	editor = {Zhang, Daqing and Portmann, Marius and Tan, Ah-Hwee and Indulska, Jadwiga},
	year = {2009},
	doi = {10.1007/978-3-642-02830-4_4},
	note = {133 citations (Crossref) [2023-07-11]
240 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Ubiquitous Intelligence and Computing
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:svm, classes:12, classes:10-29, untagged, from:cite.bib},
	pages = {25--38},
	file = {Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf:/Users/brk/Zotero/storage/UFY9XY4F/Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf:application/pdf},
}

@inproceedings{prekopcsak_accelerometer_2008,
	title = {Accelerometer {Based} {Real}-{Time} {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Real-Time-Gesture-Recognition-Prekopcs%C3%A1k/4e0b17d35696db6aa1cd0ea3565b47ff317e3ad3},
	abstract = {Gesture is a natural expression form for humans, but its recognition is a similarly hard problem as speech recognition. In this paper, I present a real-time hand gesture recognition system, which identifies relevant parts in the continous sensor data stream, and classifies them to the most probable gesture. Instead of the usual button-based segmentation, I have created an automatic segmentation method, which makes the interface more natural. The results showed that the two different classifiers reach 97.4\% and 96\% accuracy on a personalized gesture set, and these results can be improved for certain gesture sets with the combination of the two algorithms. Furthermore, the system has great performance and low response time, so the user experience is much better than with previous gesture recognizers.},
	urldate = {2023-03-07},
	author = {Prekopcsák, Zoltán},
	year = {2008},
	keywords = {model:svm, model:hmm, classes:10, classes:10-29},
	file = {Full Text PDF:/Users/brk/Zotero/storage/Q44WBJN9/Prekopcsák - 2008 - Accelerometer Based Real-Time Gesture Recognition.pdf:application/pdf},
}

@inproceedings{hutchison_accelerometer_2005,
	address = {Berlin, Heidelberg},
	title = {Accelerometer {Based} {Gesture} {Recognition} {Using} {Continuous} {HMMs}},
	volume = {3522},
	isbn = {978-3-540-26153-7 978-3-540-32237-5},
	url = {http://link.springer.com/10.1007/11492429_77},
	doi = {10.1007/11492429_77},
	abstract = {This paper presents a gesture recognition system based on continuous hidden Markov models. Gestures here are hand movements which are recorded by a 3D accelerometer embedded in a handheld device. In addition to standard hidden Markov model classifier, the recognition system has a preprocessing step which removes the effect of device orientation from the data. The performance of the recognizer is evaluated in both user dependent and user independent cases. The effects of sample resolution and sampling rate are studied in the user dependent case.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Pylvänäinen, Timo},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Marques, Jorge S. and Pérez de la Blanca, Nicolás and Pina, Pedro},
	year = {2005},
	doi = {10.1007/11492429_77},
	note = {150 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Pattern Recognition and Image Analysis
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:hmm},
	pages = {639--646},
}

@article{sethu_janaki_real_2013,
	title = {Real time recognition of {3D} gestures in mobile devices},
	url = {http://ieeexplore.ieee.org/document/6745463/},
	doi = {10.1109/RAICS.2013.6745463},
	abstract = {Gesture-based user interaction is increasingly relevant today as the use of personal computing devices becomes widespread. Smartphones have several inbuilt sensors like accelerometer, orientation sensor and gyroscope, which are able to provide data on motion of the device in 3D space. This paper proposes a mechanism for real-time recognition of 3D gestures using sensors in mobile devices. 3D gestures are space-drawn gestures computed from 3-axial accelerometer readings. The algorithms discussed in this paper include single value decomposition, dynamic time warping and Mahalanobis distance.},
	urldate = {2023-03-07},
	journal = {2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},
	author = {Sethu Janaki, V M and Babu, Satish and Sreekanth, S S},
	month = dec,
	year = {2013},
	note = {8 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)
ISBN: 9781479921782 9781479921775
Place: Trivandrum, India
Publisher: IEEE},
	keywords = {tech:accelerometer, model:knn, model:dtw, model:svd},
	pages = {149--152},
}

@article{kong_gesture_2009,
	title = {Gesture recognition model based on {3D} accelerations},
	url = {https://ieeexplore.ieee.org/document/5228524/},
	doi = {10.1109/ICCSE.2009.5228524},
	abstract = {This paper presents a recognition model on gesture interaction, gesture recognition based on accererations, which are collected by 3D accelerometer. Unlike most other vision-based gesture recognition, our method resorting to accelerations of the gesture in three directions. Gesture is divided into small units, and each unit is modeled by an HMM, the HMM classifies the gesture unit after being well trained, the gesture identified when all of the corresponding gesture units recognized successfully, then the instruction will be triggered, and the Human-Computer Interaction achieved. Through experiments, the method proved to be effective. This recognition model can be wildly used in the field of mobile computing and remote control, and people could use computer more friendly and naturally.},
	urldate = {2023-03-07},
	journal = {2009 4th International Conference on Computer Science \& Education},
	author = {Kong, Jun-qi and Wang, Hui and Zhang, Guang-quan},
	month = jul,
	year = {2009},
	note = {17 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2009 4th International Conference on Computer Science \&amp; Education (ICCSE 2009)
ISBN: 9781424435203
Place: Nanning
Publisher: IEEE},
	keywords = {tech:accelerometer, model:hmm},
	pages = {66--70},
}

@inproceedings{hutchison_comparative_2014,
	address = {Cham},
	title = {A {Comparative} {Study} of {User} {Dependent} and {Independent} {Accelerometer}-{Based} {Gesture} {Recognition} {Algorithms}},
	volume = {8530},
	isbn = {978-3-319-07787-1 978-3-319-07788-8},
	url = {http://link.springer.com/10.1007/978-3-319-07788-8_12},
	doi = {10.1007/978-3-319-07788-8_12},
	abstract = {In this paper, we introduce an evaluation of accelerometer-based gesture recognition algorithms in user dependent and independent cases. Gesture recognition has many algorithms and this evaluation includes Hidden Markov Models, Support Vector Machine, K-nearest neighbor, Artificial Neural Net-work and Dynamic Time Warping. Recognition results are based on acceleration data collected from 12 users. We evaluated the algorithms based on the recognition accuracy related to different number of gestures from two datasets. Evaluation results show that the best accuracy for 8 and 18 gestures is achieved with dynamic time warping and K-nearest neighbor algorithms.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer International Publishing},
	author = {Hamdy Ali, Aya and Atia, Ayman and Sami, Mostafa},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Streitz, Norbert and Markopoulos, Panos},
	year = {2014},
	doi = {10.1007/978-3-319-07788-8_12},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Distributed, Ambient, and Pervasive Interactions
Series Title: Lecture Notes in Computer Science},
	keywords = {tech:accelerometer, model:svm, model:hmm, model:knn, model:dtw, model:ffnn, classes:10-29, classes:{\textless}10},
	pages = {119--129},
	file = {Full Text:/Users/brk/Zotero/storage/CXIM8DZI/Hamdy Ali et al. - 2014 - A Comparative Study of User Dependent and Independ.pdf:application/pdf},
}

@inproceedings{tiwary_single_2009,
	address = {New Delhi},
	title = {A {Single} {Accelerometer} based {Wireless} {Embedded} {System} for {Predefined} {Dynamic} {Gesture} {Recognition}},
	isbn = {978-81-8489-404-2 978-81-8489-203-1},
	url = {http://link.springer.com/10.1007/978-81-8489-203-1_18},
	doi = {10.1007/978-81-8489-203-1_18},
	abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction. A complete embedded system which facilitates the data acquisition, analysis, recognition, and the transmission wirelessly, of human dynamic gestures to a computer, is described. An intuitive algorithm for processing the accelerometer data was implemented and tested. This method permits all the analysis to be done by the embedded system processor. The system is capable of recognizing gestures involving a combination of straight line motions in three dimensions. These gestures are then used to control a host computer which executes tasks based on the gesture received. A sample application showing how the gestures can be mapped to the English alphabet is also shown.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer India},
	author = {Parsani, Rahul and Singh, Karandeep},
	editor = {Tiwary, U. S. and Siddiqui, Tanveer J. and Radhakrishna, M. and Tiwari, M. D.},
	year = {2009},
	doi = {10.1007/978-81-8489-203-1_18},
	note = {4 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Proceedings of the First International Conference on Intelligent Human Computer Interaction},
	keywords = {tech:accelerometer},
	pages = {195--201},
}

@inproceedings{wang_user-independent_2013,
	title = {User-independent accelerometer-based gesture recognition for mobile devices},
	volume = {1},
	url = {https://revistas.usal.es/index.php/2255-2863/article/view/ADCAIJ20121311125},
	doi = {10.14201/ADCAIJ20121311125},
	abstract = {Many mobile devices embed nowadays inertial sensors. This enables new forms of human-computer interaction through the use of gestures (movements performed with the mobile device) as a way of communication. This paper presents an accelerometer-based gesture recognition system for mobile devices which is able to recognize a collection of 10 different hand gestures. The system was conceived to be light and to operate in a user-independent manner in real time. The recognition system was implemented in a smart phone and evaluated through a collection of user tests, which showed a recognition accuracy similar to other state-of-the art techniques and a lower computational complexity. The system was also used to build a human-robot interface that enables controlling a wheeled robot with the gestures made with the mobile phone},
	urldate = {2023-03-07},
	booktitle = {{ADCAIJ}: {Advances} in {Distributed} {Computing} and {Artificial} {Intelligence} {Journal}},
	author = {Wang, Xian and Tarrío, Paula and Bernardos, Ana María and Metola, Eduardo and Casar, José Ramón},
	month = jul,
	year = {2013},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 2255-2863
Issue: 3
Journal Abbreviation: ADCAIJ},
	keywords = {tech:accelerometer, classes:10-29, app:robot-control},
	pages = {11--25},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2YFHA534/Wang et al. - 2013 - User-independent accelerometer-based gesture recog.pdf:application/pdf},
}

@article{akl_accelerometer-based_2010,
	title = {Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, \&\#x00026; compressive sensing},
	url = {http://ieeexplore.ieee.org/document/5495895/},
	doi = {10.1109/ICASSP.2010.5495895},
	abstract = {We propose a gesture recognition system based primarily on a single 3-axis accelerometer. The system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition. A dictionary of 18 gestures is defined and a database of over 3,700 repetitions is created from 7 users. Our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition, to the best of our knowledge. The proposed system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
	urldate = {2023-03-07},
	journal = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
	author = {Akl, Ahmad and Valaee, Shahrokh},
	year = {2010},
	note = {142 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2010 IEEE International Conference on Acoustics, Speech and Signal Processing
ISBN: 9781424442959
Place: Dallas, TX, USA
Publisher: IEEE},
	keywords = {tech:accelerometer, classes:10-29, participants:7},
	pages = {2270--2273},
	file = {Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf:/Users/brk/Zotero/storage/85QXVKVJ/Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf:application/pdf},
}

@article{zinnen_new_2008,
	title = {A new approach to enable gesture recognition in continuous data streams},
	url = {http://ieeexplore.ieee.org/document/4911581/},
	doi = {10.1109/ISWC.2008.4911581},
	abstract = {Gesture recognition has great potential for mobile and wearable computing. Most papers in this area focus on classifying different gestures, but do not evaluate the distinctiveness of gestures in continuous recordings of gestures in daily life. This paper presents a new approach for the important and challenging problem of gesture recognition in continuous data streams. We use turning points of arm movements to identify segments of interest in the continuous data stream. The recognition algorithm considers both the direction of movements between turning points and the shape of the turning points for classification. Using the new method, seven gestures of different complexity are evaluated against a realistic background class of daily gestures in five different scenarios.},
	urldate = {2023-03-07},
	journal = {2008 12th IEEE International Symposium on Wearable Computers},
	author = {Zinnen, Andreas and Schiele, Bernt},
	year = {2008},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2008 12th IEEE International Symposium on Wearable Computers
ISBN: 9781424426379
Place: Pittaburgh, PA, USA
Publisher: IEEE},
	keywords = {classes:{\textless}10},
	pages = {33--40},
}

@inproceedings{hutchison_continuous_2010,
	address = {Berlin, Heidelberg},
	title = {Continuous {Realtime} {Gesture} {Following} and {Recognition}},
	volume = {5934},
	isbn = {978-3-642-12552-2 978-3-642-12553-9},
	url = {http://link.springer.com/10.1007/978-3-642-12553-9_7},
	doi = {10.1007/978-3-642-12553-9_7},
	abstract = {We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Bevilacqua, Frédéric and Zamborlin, Bruno and Sypniewski, Anthony and Schnell, Norbert and Guédy, Fabrice and Rasamimanana, Nicolas},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kopp, Stefan and Wachsmuth, Ipke},
	year = {2010},
	doi = {10.1007/978-3-642-12553-9_7},
	note = {207 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Gesture in Embodied Communication and Human-Computer Interaction
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm},
	pages = {73--84},
	file = {Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf:/Users/brk/Zotero/storage/9YT2MAGT/Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf:application/pdf},
}

@inproceedings{schlomer_gesture_2008,
	address = {Bonn Germany},
	title = {Gesture recognition with a {Wii} controller},
	isbn = {978-1-60558-004-3},
	url = {https://dl.acm.org/doi/10.1145/1347390.1347395},
	doi = {10.1145/1347390.1347395},
	abstract = {In many applications today user interaction is moving away from mouse and pens and is becoming pervasive and much more physical and tangible. New emerging interaction technologies allow developing and experimenting with new interaction methods on the long way to providing intuitive human computer interaction. In this paper, we aim at recognizing gestures to interact with an application and present the design and evaluation of our sensor-based gesture recognition. As input device we employ the Wii-controller (Wiimote) which recently gained much attention world wide. We use the Wiimote's acceleration sensor independent of the gaming console for gesture recognition. The system allows the training of arbitrary gestures by users which can then be recalled for interacting with systems like photo browsing on a home TV. The developed library exploits Wii-sensor data and employs a hidden Markov model for training and recognizing user-chosen gestures. Our evaluation shows that we can already recognize gestures with a small number of training samples. In addition to the gesture recognition we also present our experiences with the Wii-controller and the implementation of the gesture recognition. The system forms the basis for our ongoing work on multimodal intuitive media browsing and are available to other researchers in the field.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 2nd international conference on {Tangible} and embedded interaction},
	publisher = {ACM},
	author = {Schlömer, Thomas and Poppinga, Benjamin and Henze, Niels and Boll, Susanne},
	month = feb,
	year = {2008},
	note = {560 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, classes:5, model:hmm, hardware:wii, classes:{\textless}10},
	pages = {11--14},
	file = {Submitted Version:/Users/brk/Zotero/storage/QUVGMHED/Schlömer et al. - 2008 - Gesture recognition with a Wii controller.pdf:application/pdf},
}

@article{rajko_real-time_2007,
	title = {Real-time {Gesture} {Recognition} with {Minimal} {Training} {Requirements} and {On}-line {Learning}},
	url = {http://ieeexplore.ieee.org/document/4270328/},
	doi = {10.1109/CVPR.2007.383330},
	abstract = {In this paper, we introduce the semantic network model (SNM), a generalization of the hidden Markov model (HMM) that uses factorization of state transition probabilities to reduce training requirements, increase the efficiency of gesture recognition and on-line learning, and allow more precision in gesture modeling. We demonstrate the advantages both formally and experimentally, using examples such as full-body multimodal gesture recognition via optical motion capture and a pressure sensitive floor, as well as mouse/pen gesture recognition. Our results show that our algorithm performs much better than the traditional approach in situations where training samples are limited and/or the precision of the gesture model is high.},
	urldate = {2023-03-07},
	journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
	author = {Rajko, Stjepan and Qian, Gang and Ingalls, Todd and James, Jodi},
	month = jun,
	year = {2007},
	note = {69 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2007 IEEE Conference on Computer Vision and Pattern Recognition
ISBN: 9781424411795 9781424411801
Place: Minneapolis, MN, USA
Publisher: IEEE},
	keywords = {model:hmm},
	pages = {1--8},
	file = {Rajko et al. - 2007 - Real-time Gesture Recognition with Minimal Trainin.pdf:/Users/brk/Zotero/storage/7PDEU72Y/Rajko et al. - 2007 - Real-time Gesture Recognition with Minimal Trainin.pdf:application/pdf},
}

@article{ren_robust_2011,
	title = {Robust hand gesture recognition with kinect sensor},
	url = {https://dl.acm.org/doi/10.1145/2072298.2072443},
	doi = {10.1145/2072298.2072443},
	abstract = {Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Proceedings of the 19th ACM international conference on Multimedia},
	author = {Ren, Zhou and Meng, Jingjing and Yuan, Junsong and Zhang, Zhengyou},
	month = nov,
	year = {2011},
	note = {309 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: MM '11: ACM Multimedia Conference
ISBN: 9781450306164
Place: Scottsdale Arizona USA
Publisher: ACM},
	keywords = {hardware:kinect, classes:14, model:cnn, tech:rgbd, classes:10-29},
	pages = {759--760},
	file = {Ren et al. - 2011 - Robust hand gesture recognition with kinect sensor.pdf:/Users/brk/Zotero/storage/2NSG3HQQ/Ren et al. - 2011 - Robust hand gesture recognition with kinect sensor.pdf:application/pdf},
}

@inproceedings{mantyjarvi_enabling_2004,
	address = {College Park Maryland USA},
	title = {Enabling fast and effortless customisation in accelerometer based gesture interaction},
	isbn = {978-1-58113-981-5},
	url = {https://dl.acm.org/doi/10.1145/1052380.1052385},
	doi = {10.1145/1052380.1052385},
	abstract = {Accelerometer based gesture control is proposed as a complementary interaction modality for handheld devices. Predetermined gesture commands or freely trainable by the user can be used for controlling functions also in other devices. To support versatility of gesture commands in various types of personal device applications gestures should be customisable, easy and quick to train. In this paper we experiment with a procedure for training/recognizing customised accelerometer based gestures with minimum amount of user effort in training. Discrete Hidden Markov Models (HMM) are applied. Recognition results are presented for an external device, a DVD player gesture commands. A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training. For a set of eight gestures, each trained with two original gestures and with two Gaussian noise-distorted duplicates, the average recognition accuracy was 97\%, and with two original gestures and with four noise-distorted duplicates, the average recognition accuracy was 98\%, cross-validated from a total data set of 240 gestures. Use of procedure facilitates quick and effortless customisation in accelerometer based interaction.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 3rd international conference on {Mobile} and ubiquitous multimedia},
	publisher = {ACM},
	author = {Mäntyjärvi, Jani and Kela, Juha and Korpipää, Panu and Kallio, Sanna},
	month = oct,
	year = {2004},
	note = {78 citations (Crossref) [2023-07-11]
165 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:hmm, classes:8, untagged, classes:{\textless}10, from:cite.bib},
	pages = {25--31},
	file = {Mäntyjärvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf:/Users/brk/Zotero/storage/9GG99UUB/Mäntyjärvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf:application/pdf},
}

@article{mantyjarvi_gesture_2005,
	title = {Gesture {Interaction} for {Small} {Handheld} {Devices} to {Support} {Multimedia} {Applications}},
	url = {https://www.semanticscholar.org/paper/Gesture-Interaction-for-Small-Handheld-Devices-to-M%C3%A4ntyj%C3%A4rvi-Kallio/4945fca3ffa1fe292e0f61ffc4fd9cc51e9c86f8},
	abstract = {Accelerometer-based gesture control is proposed as a complementary interaction modality for small handheld devices to enable a variety of multimedia applications. The motivation for experimenting with gesture interaction is justified by the personal and public domain prototype applications developed. The challenges related to developing user-dependent and independent gesture control are presented. In this article, we experiment with methods for user-dependent gesture recognition with a low number of training repetitions, and for feasible user-independent gesture recognition from a moderately large set of gestures. The user-dependent gesture recognition performance of the continuous Hidden Markov Model (HMM) is better when compared to discrete HMM with three gesture repetitions in a training set. With continuous HMM, a recognition accuracy level of 95\% is obtained with or without tilt normalization, while for discrete HMM a best recognition accuracy of 90\% is obtained. The user-independent gesture recognition performance with continuous HMM of 89\% is considerably better compared to tests with discrete HMM, when both are obtained with cross-validation from 2,520 gestures. An important result is that the effect of using tilt normalization notably increases the user-independent gesture recognition performance by 10- 15\% depending on the method used. The chosen methods show great potential for gesture-based interaction in multimedia applications.},
	urldate = {2023-03-07},
	journal = {J. Mobile Multimedia},
	author = {Mäntyjärvi, Jani and Kallio, S. and Korpipää, Panu and Kela, J. and Plomp, J.},
	month = jun,
	year = {2005},
	note = {19 Citations},
	keywords = {tech:accelerometer, model:hmm},
}

@article{marasovic_motion-based_2015,
	title = {Motion-{Based} {Gesture} {Recognition} {Algorithms} for {Robot} {Manipulation}},
	volume = {12},
	issn = {1729-8814, 1729-8814},
	url = {http://journals.sagepub.com/doi/10.5772/60077},
	doi = {10.5772/60077},
	abstract = {The prevailing trend of integrating inertial sensors in consumer electronics devices has inspired research on new forms of human-computer interaction utilizing hand gestures, which may be set-up on mobile devices themselves. At present, motion gesture recognition is intensely studied, with various recognition techniques being employed and tested. This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings. The study takes two of the most popular and arguably the best recognition methods currently in use - dynamic time warping and hidden Markov model - and sets them against a relatively novel approach founded on distance metric learning. The three selected algorithms are evaluated in terms of their overall performance, accuracy, training time, execution time and storage efficacy. The optimal algorithm is further implemented in a prototype user application, aimed to serve as an interface for controlling the motion of a toy robot via gestures made with a smartphone.},
	language = {en},
	number = {5},
	urldate = {2023-03-07},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Marasović, Tea and Papić, Vladan and Marasović, Jadranka},
	month = may,
	year = {2015},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, model:dtw},
	pages = {51},
	file = {Full Text:/Users/brk/Zotero/storage/H6IC9DRX/Marasović et al. - 2015 - Motion-Based Gesture Recognition Algorithms for Ro.pdf:application/pdf},
}

@article{zhang_hand_2009,
	title = {Hand gesture recognition and virtual game control based on {3D} accelerometer and {EMG} sensors},
	url = {https://dl.acm.org/doi/10.1145/1502650.1502708},
	doi = {10.1145/1502650.1502708},
	abstract = {This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7\% in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.},
	language = {en},
	urldate = {2023-03-07},
	journal = {Proceedings of the 14th international conference on Intelligent user interfaces},
	author = {Zhang, Xu and Chen, Xiang and Wang, Wen-hui and Yang, Ji-hai and Lantz, Vuokko and Wang, Kong-qiao},
	month = feb,
	year = {2009},
	note = {229 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: IUI09: 14th International Conference on Intelligent User Interfaces
ISBN: 9781605581682
Place: Sanibel Island Florida USA
Publisher: ACM},
	keywords = {model:hmm, tech:rgb, classes:8, model:nearest-conflicting-neighbors, classes:{\textless}10},
	pages = {401--406},
	file = {Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf:/Users/brk/Zotero/storage/4QW723JU/Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf:application/pdf},
}

@article{jiang_development_2016,
	title = {Development of a real-time hand gesture recognition wristband based on {sEMG} and {IMU} sensing},
	url = {http://ieeexplore.ieee.org/document/7866498/},
	doi = {10.1109/ROBIO.2016.7866498},
	abstract = {Human computer interaction is becoming more integrated in daily life with the proliferation of mobile devices and virtual reality technology. Hand gesture recognition is a potentially promising mechanism to facilitate human computer interaction, however wrist-mounted surface electromyography (sEMG) hand gesture classification is particularly challenging given the relatively small sEMG signals as compared to traditional forearm-based sEMG sensing. This paper introduces the development of a wristband for detecting eight air gestures and four surface gestures at two different force levels through sEMG and inertial measurement unit (IMU) sensor fusion. To validate the wrist-worn device, ten healthy subjects performed hand gesture recognition experiments resulting in a total average recognition rate of 92.6\% for air gestures and 88.8\% for surface gestures. This paper demonstrates the potential of wrist-worn devices for accurate hand gesture recognition applications.},
	urldate = {2023-03-07},
	journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
	author = {Jiang, Shuo and Lv, Bo and Sheng, Xinjun and Zhang, Chao and Wang, Haitao and Shull, Peter B.},
	month = dec,
	year = {2016},
	note = {16 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)
ISBN: 9781509043644
Place: Qingdao, China
Publisher: IEEE},
	keywords = {tech:emg, classes:10-29},
	pages = {1256--1261},
}

@article{li_hand_2018,
	title = {Hand {Gesture} {Recognition} and {Real}-time {Game} {Control} {Based} on {A} {Wearable} {Band} with 6-axis {Sensors}},
	url = {https://ieeexplore.ieee.org/document/8489743/},
	doi = {10.1109/IJCNN.2018.8489743},
	abstract = {Human-computer interaction introduces critical open door with the proceeds with improvement of wearable gadgets. Gesture recognition through smart devices is becoming a popular research direction. This paper proposes a hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband. We utilize three-axis accelerator and gyroscope sensors embedded in smart band to collect hand motion information and use Kinect camera capture video information for manual segmentation during the training model phase. A continuous gesture segmentation algorithm based on sliding window and DTW algorithm is developed to detect meaningful gestures in the real-time game control stage. In addition, an android game named Fly Birds which is controlled by gesture recognition result is presented to simulate real-time human-computer interaction. Then, we classify the data in the window using common classifiers. Finally, our experimental results show that, we can accurately identify the designed gestures during the stage of static gesture recognition, and we also achieve a perfect interactive effect in the process of dynamic real-time game control. The experiment outcomes will advance the ascent of human-PC cooperation in view of hand gesture recognition and related applications will rise in vast numbers.},
	urldate = {2023-03-07},
	journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
	author = {Li, Yande and Wang, Taiqian and khan, Aamir and Li, Lian and Li, Caihong and Yang, Yi and Liu, Li},
	month = jul,
	year = {2018},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2018 International Joint Conference on Neural Networks (IJCNN)
ISBN: 9781509060146
Place: Rio de Janeiro
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:rgbd, model:dtw, app:gaming},
	pages = {1--6},
}

@article{akl_novel_2011,
	title = {A {Novel} {Accelerometer}-{Based} {Gesture} {Recognition} {System}},
	volume = {59},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5993550/},
	doi = {10.1109/TSP.2011.2165707},
	abstract = {In this paper, we address the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an ℓ1-minimization problem. The gesture recognition system operates primarily on data from a single 3-axis accelerometer and comprises two main stages: a training stage and a testing stage. For training, the system employs dynamic time warping as well as affinity propagation to create exemplars for each gesture while for testing, the system projects all candidate traces and also the unknown trace onto the same lower dimensional subspace for recognition. A dictionary of 18 gestures is defined and a database of over 3700 traces is created from seven subjects on which the system is tested and evaluated. To the best of our knowledge, our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition. The system achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
	number = {12},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Signal Processing},
	author = {Akl, Ahmad and Feng, Chen and Valaee, Shahrokh},
	month = dec,
	year = {2011},
	note = {100 citations (Crossref) [2023-07-11]
170 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, impressive, msc, model:dtw, classes:18, classes:10-29, untagged, from:cite.bib},
	pages = {6197--6205},
	file = {Full Text:/Users/brk/Zotero/storage/SAQUJ2PY/Akl et al. - 2011 - A Novel Accelerometer-Based Gesture Recognition Sy.pdf:application/pdf},
}

@article{zhao_real-time_2017,
	title = {Real-time head gesture recognition on head-mounted displays using cascaded hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/8122975/},
	doi = {10.1109/SMC.2017.8122975},
	abstract = {Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.},
	urldate = {2023-03-07},
	journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Zhao, Jingbo and Allison, Robert S.},
	month = oct,
	year = {2017},
	note = {11 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 IEEE International Conference on Systems, Man and Cybernetics (SMC)
ISBN: 9781538616451
Place: Banff, AB
Publisher: IEEE},
	keywords = {model:hmm, classes:9, tech:oculus-rift, classes:{\textless}10},
	pages = {2361--2366},
	file = {Submitted Version:/Users/brk/Zotero/storage/JUTLYHW4/Zhao and Allison - 2017 - Real-time head gesture recognition on head-mounted.pdf:application/pdf},
}

@inproceedings{hutchison_australian_2013,
	address = {Berlin, Heidelberg},
	title = {Australian {Sign} {Language} {Recognition} {Using} {Moment} {Invariants}},
	volume = {7996},
	isbn = {978-3-642-39481-2 978-3-642-39482-9},
	url = {http://link.springer.com/10.1007/978-3-642-39482-9_59},
	doi = {10.1007/978-3-642-39482-9_59},
	abstract = {Human Computer Interaction is geared towards seamless human machine integration without the need for LCDs, Keyboards or Gloves. Systems have already been developed to react to limited hand gestures especially in gaming and in consumer electronics control. Yet, it is a monumental task in bridging the well-developed sign languages in different parts of the world with a machine to interpret the meaning. One reason is the sheer extent of the vocabulary used in sign language and the sequence of gestures needed to communicate different words and phrases. Auslan the Australian Sign Language is comprised of numbers, finger spelling for words used in common practice and a medical dictionary. There are 7415 words listed in Auslan website. This research article tries to implement recognition of numerals using a computer using the static hand gesture recognition system developed for consumer electronics control at the University of Wollongong in Australia. The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures. The system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
	urldate = {2023-03-07},
	publisher = {Springer Berlin Heidelberg},
	author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Huang, De-Shuang and Jo, Kang-Hyun and Zhou, Yong-Quan and Han, Kyungsook},
	year = {2013},
	doi = {10.1007/978-3-642-39482-9_59},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Intelligent Computing Theories and Technology
Series Title: Lecture Notes in Computer Science},
	keywords = {app:sign-language},
	pages = {509--514},
}

@inproceedings{kadous_machine_1996,
	title = {Machine {Recognition} of {Auslan} {Signs} {Using} {PowerGloves}: {Towards} {Large}-{Lexicon} {Recognition} of {Sign} {Lan}},
	shorttitle = {Machine {Recognition} of {Auslan} {Signs} {Using} {PowerGloves}},
	url = {https://www.semanticscholar.org/paper/Machine-Recognition-of-Auslan-Signs-Using-Towards-Kadous/1ed3e20bbcf12c60e766189987074ac2935d7140},
	abstract = {Instrumented gloves use a variety of sensors to provide information about the user's hand. They can be used for recognition of gestures; especially well-deened gesture sets such as sign languages. However, recognising gestures is a diicult task, due to intrapersonal and inter-personal variations in performing them. One approach to solving this problem is to use machine learning. In this case, samples of 95 discrete Australian Sign Language (Auslan) signs were collected using a Power-Glove. Two machine learning techniques were applied \{\vphantom{\}} instance-based learning (IBL) and decision-tree learning \{\vphantom{\}} to the data after some simple features were extracted. Accuracy of approximately 80 per cent was achieved using IBL, despite the severe limitations of the glove.},
	urldate = {2023-03-07},
	author = {Kadous, M. W.},
	year = {1996},
	note = {115 Citations},
	keywords = {model:decision-tree, app:sign-language, app:australian-sl, classes:50-100, model:instance-based-learning},
}

@article{mohandes_automation_2004,
	title = {Automation of the arabic sign language recognition},
	url = {http://ieeexplore.ieee.org/document/1307840/},
	doi = {10.1109/ICTTA.2004.1307840},
	abstract = {This paper introduces a system to recognize the Arabic sign language using an instrumented glove and a machine learning method. Interfaces in sign language systems can be categorized as direct-device or vision-based. The direct-device approach uses measurement devices that are in direct contact with the hand such as instrumented gloves, flexion sensors, styli and position-tracking devices. On the other hand, the vision-based approach captures the movement of the singer's hand using a camera that is sometimes aided by making the signer wear a glove that has painted areas indicating the positions of the fingers or knuckles. The proposed system basically consists of a PowerGlove that is connected through the serial port to a workstation running the support vector machine algorithm. Obtained results are promising even though a simple and cheap glove with limited sensors was utilized.},
	urldate = {2023-03-07},
	journal = {Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.},
	author = {Mohandes, M. and A-Buraiky, S. and Halawani, T. and Al-Baiyat, S.},
	year = {2004},
	note = {41 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.
ISBN: 9780780384828
Place: Damascus, Syria
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {479--480},
}

@inproceedings{moeslund_sign_2011,
	address = {London},
	title = {Sign {Language} {Recognition}},
	isbn = {978-0-85729-996-3 978-0-85729-997-0},
	url = {https://link.springer.com/10.1007/978-0-85729-997-0_27},
	doi = {10.1007/978-0-85729-997-0_27},
	abstract = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a precis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {Springer London},
	author = {Cooper, Helen and Holt, Brian and Bowden, Richard},
	editor = {Moeslund, Thomas B. and Hilton, Adrian and Krüger, Volker and Sigal, Leonid},
	year = {2011},
	doi = {10.1007/978-0-85729-997-0_27},
	note = {293 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Visual Analysis of Humans},
	keywords = {app:sign-language},
	pages = {539--562},
}

@article{nath_hand_2019,
	title = {Hand {Gesture} {Recognition}: {A} {Survey}},
	volume = {511},
	shorttitle = {Hand {Gesture} {Recognition}},
	url = {http://link.springer.com/10.1007/978-981-13-0776-8_33},
	doi = {10.1007/978-981-13-0776-8_33},
	abstract = {A human–computer interaction is generally limited to taking input from the user using handheld devices like keyboard, mouse, or scanners. With the advancement in computers, the user interaction approaches have also advanced. Direct use of hands as an input device is an attractive method for providing natural Human–Computer Interaction. It is also helpful for people who use sign language. The chapter aims to study the existing methods for Hand Gesture Recognition and provide a comparative analysis of the same. The entire process of hand gesture recognition is divided into three phases: hand detection, hand tracking, and recognition. The chapter includes a review of the different methods used for the hand gesture recognition. The recognition phase is classified based on the way the input is received as glove based or vision based. For recognition, various methods like Feature extraction, Hidden Markov Model (HMM), Principal Component Analysis (PCA) are compared along with the reported accuracy.},
	language = {en},
	urldate = {2023-03-07},
	author = {Anwar, Shamama and Sinha, Subham Kumar and Vivek, Snehanshu and Ashank, Vishal},
	editor = {Nath, Vijay and Mandal, Jyotsna Kumar},
	year = {2019},
	doi = {10.1007/978-981-13-0776-8_33},
	note = {25 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Nanoelectronics, Circuits and Communication Systems
ISBN: 9789811307751 9789811307768
Place: Singapore
Publisher: Springer Singapore},
	keywords = {type:survey},
	pages = {365--371},
}

@article{bansal_gesture_2016,
	title = {Gesture {Recognition}: {A} {Survey}},
	volume = {139},
	issn = {09758887},
	shorttitle = {Gesture {Recognition}},
	url = {http://www.ijcaonline.org/research/volume139/number2/bansal-2016-ijca-909103.pdf},
	doi = {10.5120/ijca2016909103},
	abstract = {With increasing use of computers in our daily lives, lately there has been a rapid increase in the efforts to develop a better human computer interaction interface. The need of easy to use and advance types of human-computer interaction with natural interfaces is more than ever. In the present framework, the UI (User Interface) of a computer allows user to interact with electronic devices with graphical icons and visual indicators, which is still inconvenient and not suitable for working in virtual environments. An interface which allow user to communicate through gestures is the next step in the direction of advance human computer interface. In the present paper author explore different aspects of gesture recognition techniques.},
	number = {2},
	urldate = {2023-03-07},
	journal = {International Journal of Computer Applications},
	author = {Bansal, Bharti},
	month = apr,
	year = {2016},
	note = {650 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {8--10},
	file = {Full Text:/Users/brk/Zotero/storage/DXTI6K39/Bansal - 2016 - Gesture Recognition A Survey.pdf:application/pdf},
}

@article{mitra_gesture_2007,
	title = {Gesture {Recognition}: {A} {Survey}},
	volume = {37},
	issn = {1094-6977},
	shorttitle = {Gesture {Recognition}},
	url = {http://ieeexplore.ieee.org/document/4154947/},
	doi = {10.1109/TSMCC.2007.893280},
	abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
	number = {3},
	urldate = {2023-03-07},
	journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
	author = {Mitra, Sushmita and Acharya, Tinku},
	month = may,
	year = {2007},
	note = {1146 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, type:survey},
	pages = {311--324},
	file = {Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf:/Users/brk/Zotero/storage/32EDNKX8/Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf:application/pdf},
}

@article{khan_survey_2012,
	title = {Survey on {Gesture} {Recognition} for {Hand} {Image} {Postures}},
	volume = {5},
	issn = {1913-8997, 1913-8989},
	url = {http://www.ccsenet.org/journal/index.php/cis/article/view/16598},
	doi = {10.5539/cis.v5n3p110},
	abstract = {One of the attractive methods for providing natural human-computer interaction is the use of the hand as an input device rather than the cumbersome devices such as keyboards and mice, which need the user to be located in a specific location to use these devices. Since human hand is an articulated object, it is an open issue to discuss. The most important thing in hand gesture recognition system is the input features, and the selection of good features representation. This paper presents a review study on the hand postures and gesture recognition methods, which is considered to be a challenging problem in the human-computer interaction context and promising as well. Many applications and techniques were discussed here with the explanation of system recognition framework and its main phases.},
	number = {3},
	urldate = {2023-03-07},
	journal = {Computer and Information Science},
	author = {Khan, Rafiqul Zaman and Ibraheem, Noor Adnan},
	month = apr,
	year = {2012},
	note = {94 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {p110},
	file = {Full Text PDF:/Users/brk/Zotero/storage/J2T7FEZP/Khan and Ibraheem - 2012 - Survey on Gesture Recognition for Hand Image Postu.pdf:application/pdf},
}

@article{bloit_short-time_2008,
	title = {Short-time {Viterbi} for online {HMM} decoding: {Evaluation} on a real-time phone recognition task},
	issn = {1520-6149},
	shorttitle = {Short-time {Viterbi} for online {HMM} decoding},
	url = {http://ieeexplore.ieee.org/document/4518061/},
	doi = {10.1109/ICASSP.2008.4518061},
	abstract = {In this paper we present the implementation of an online HMM decoding process. The algorithm derives an online version of the Viterbi algorithm on successive variable length windows, iteratively storing portions of the optimal state path. We explicit the relation between the hidden layer's topology and the applicability and performance prediction of the algorithm. We evaluate the validity and performance of the algorithm on a phone recognition task on a database of continuous speech from a native French speaker. We specifically study the latency-accuracy performance of the algorithm.},
	urldate = {2023-03-07},
	journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
	author = {Bloit, Julien and Rodet, Xavier},
	month = mar,
	year = {2008},
	note = {52 citations (Semantic Scholar/DOI) [2023-03-07]
Conference Name: ICASSP 2008 - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
ISBN: 9781424414833 9781424414840
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {background},
	pages = {2121--2124},
	file = {Submitted Version:/Users/brk/Zotero/storage/BTJHRA6M/Bloit and Rodet - 2008 - Short-time Viterbi for online HMM decoding Evalua.pdf:application/pdf},
}

@article{wynants_three_2019,
	title = {Three myths about risk thresholds for prediction models},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1425-3},
	doi = {10.1186/s12916-019-1425-3},
	abstract = {Clinical prediction models are useful in estimating a patient’s risk of having a certain disease or experiencing an event in the future based on their current characteristics. Defining an appropriate risk threshold to recommend intervention is a key challenge in bringing a risk prediction model to clinical application; such risk thresholds are often defined in an ad hoc way. This is problematic because tacitly assumed costs of false positive and false negative classifications may not be clinically sensible. For example, when choosing the risk threshold that maximizes the proportion of patients correctly classified, false positives and false negatives are assumed equally costly. Furthermore, small to moderate sample sizes may lead to unstable optimal thresholds, which requires a particularly cautious interpretation of results.},
	number = {1},
	urldate = {2023-03-24},
	journal = {BMC Medicine},
	author = {Wynants, Laure and van Smeden, Maarten and McLernon, David J. and Timmerman, Dirk and Steyerberg, Ewout W. and Van Calster, Ben and {on behalf of the Topic Group ‘Evaluating diagnostic tests and prediction models’ of the STRATOS initiative}},
	month = oct,
	year = {2019},
	note = {68 citations (Semantic Scholar/DOI) [2023-03-24]},
	pages = {192},
	file = {Full Text PDF:/Users/brk/Zotero/storage/R6RTYDFE/Wynants et al. - 2019 - Three myths about risk thresholds for prediction m.pdf:application/pdf;Snapshot:/Users/brk/Zotero/storage/V9FAH7AE/s12916-019-1425-3.html:text/html},
}

@article{berrar_caveats_2012,
	title = {Caveats and pitfalls of {ROC} analysis in clinical microarray research (and how to avoid them)},
	volume = {13},
	issn = {1467-5463},
	url = {https://doi.org/10.1093/bib/bbr008},
	doi = {10.1093/bib/bbr008},
	abstract = {The receiver operating characteristic (ROC) has emerged as the gold standard for assessing and comparing the performance of classifiers in a wide range of disciplines including the life sciences. ROC curves are frequently summarized in a single scalar, the area under the curve (AUC). This article discusses the caveats and pitfalls of ROC analysis in clinical microarray research, particularly in relation to (i) the interpretation of AUC (especially a value close to 0.5); (ii) model comparisons based on AUC; (iii) the differences between ranking and classification; (iv) effects due to multiple hypotheses testing; (v) the importance of confidence intervals for AUC; and (vi) the choice of the appropriate performance metric. With a discussion of illustrative examples and concrete real-world studies, this article highlights critical misconceptions that can profoundly impact the conclusions about the observed performance.},
	number = {1},
	urldate = {2023-03-24},
	journal = {Briefings in Bioinformatics},
	author = {Berrar, Daniel and Flach, Peter},
	month = jan,
	year = {2012},
	note = {91 citations (Semantic Scholar/DOI) [2023-03-24]},
	keywords = {background},
	pages = {83--97},
	file = {Full Text PDF:/Users/brk/Zotero/storage/RLN68QM9/Berrar and Flach - 2012 - Caveats and pitfalls of ROC analysis in clinical m.pdf:application/pdf},
}

@inproceedings{flach_precision-recall-gain_2015,
	title = {Precision-{Recall}-{Gain} {Curves}: {PR} {Analysis} {Done} {Right}},
	volume = {28},
	shorttitle = {Precision-{Recall}-{Gain} {Curves}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
	urldate = {2023-03-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Flach, Peter and Kull, Meelis},
	year = {2015},
	keywords = {background},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5B3MNYUA/Flach and Kull - 2015 - Precision-Recall-Gain Curves PR Analysis Done Rig.pdf:application/pdf},
}

@article{page_cumulative_1961,
	title = {Cumulative {Sum} {Charts}},
	volume = {3},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1961.10489922},
	doi = {10.1080/00401706.1961.10489922},
	abstract = {This paper, presented orally to the Gordon Research Conference on Statistics in Chemistry in July 1960, traces the development of process inspection schemes from the original methods of Shewhart to the new charts using cumulative sums, and surveys the present practice in the operation of schemes based on cumulative sums. In spitc of the completely different appearance of the visual records kept for Shewhart and cumulative sum charts, a continuous sequence of development from the one type of scheme to the other can be established. The criteria by which a particular process inspection scheme is chosen are also developed and the practical choice of schemes is described.},
	number = {1},
	urldate = {2023-05-08},
	journal = {Technometrics},
	author = {Page, E. S.},
	month = feb,
	year = {1961},
	note = {250 citations (Semantic Scholar/DOI) [2023-05-08]
Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1961.10489922},
	keywords = {background, model:cusum},
	pages = {1--9},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to the {UCR} {Time} {Series} {Classification}/{Clustering} {Page}},
	url = {https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/},
	urldate = {2023-05-13},
	keywords = {type:dataset, dataset:ucr-time-series-classification},
	file = {Welcome to the UCR Time Series Classification/Clustering Page:/Users/brk/Zotero/storage/VML93JCD/time_series_data_2018.html:text/html},
}

@article{bagnall_great_2017,
	title = {The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
	volume = {31},
	issn = {1573-756X},
	shorttitle = {The great time series classification bake off},
	url = {https://doi.org/10.1007/s10618-016-0483-9},
	doi = {10.1007/s10618-016-0483-9},
	abstract = {In the last 5 years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.},
	language = {en},
	number = {3},
	urldate = {2023-05-13},
	journal = {Data Mining and Knowledge Discovery},
	author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
	month = may,
	year = {2017},
	note = {5 citations (Semantic Scholar/DOI) [2023-05-13]},
	keywords = {background, type:survey},
	pages = {606--660},
	file = {Full Text PDF:/Users/brk/Zotero/storage/SRFG34ZY/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf:application/pdf},
}

@misc{wang_time_2016,
	title = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}: {A} {Strong} {Baseline}},
	shorttitle = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.06455},
	doi = {10.48550/arXiv.1611.06455},
	abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
	urldate = {2023-05-13},
	publisher = {arXiv},
	author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
	month = dec,
	year = {2016},
	note = {1024 citations (Semantic Scholar/arXiv) [2023-05-13]
arXiv:1611.06455 [cs, stat]},
	file = {arXiv Fulltext PDF:/Users/brk/Zotero/storage/F8CWCQYK/Wang et al. - 2016 - Time Series Classification from Scratch with Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/brk/Zotero/storage/NUUI7HZK/1611.html:text/html},
}

@misc{norvig_how_2007,
	title = {How to {Write} a {Spelling} {Corrector}},
	url = {https://norvig.com/spell-correct.html},
	urldate = {2023-05-25},
	author = {Norvig, Peter},
	year = {2007},
	keywords = {background, autocorrect},
	file = {How to Write a Spelling Corrector:/Users/brk/Zotero/storage/ERETGY5C/spell-correct.html:text/html},
}

@article{page_continuous_1954,
	title = {{CONTINUOUS} {INSPECTION} {SCHEMES}},
	volume = {41},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/41.1-2.100},
	doi = {10.1093/biomet/41.1-2.100},
	number = {1-2},
	urldate = {2023-05-30},
	journal = {Biometrika},
	author = {PAGE, E. S.},
	month = jun,
	year = {1954},
	note = {4816 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background, untagged, model:cusum},
	pages = {100--115},
	file = {CONTINUOUS INSPECTION SCHEMES | Biometrika | Oxford Academic:/Users/brk/Zotero/storage/2CCAB6JV/456627.html:text/html;Snapshot:/Users/brk/Zotero/storage/886PZALL/456627.html:text/html},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
	language = {en},
	number = {5},
	urldate = {2023-05-30},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	note = {9990 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background, model:ffnn, universal-approximators},
	pages = {359--366},
}

@article{neal_pattern_2007,
	title = {Pattern {Recognition} and {Machine} {Learning}},
	volume = {49},
	issn = {0040-1706, 1537-2723},
	url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2007.s518},
	doi = {10.1198/tech.2007.s518},
	abstract = {the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.},
	language = {en},
	number = {3},
	urldate = {2023-05-30},
	journal = {Technometrics},
	author = {Neal, Radford M},
	month = aug,
	year = {2007},
	note = {9985 citations (Semantic Scholar/DOI) [2023-05-30]},
	keywords = {background},
	pages = {366--366},
}

@article{white_principles_1963,
	title = {Principles of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	volume = {76},
	issn = {00029556},
	shorttitle = {Principles of {Neurodynamics}},
	url = {https://www.jstor.org/stable/1419730?origin=crossref},
	doi = {10.2307/1419730},
	abstract = {Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.},
	number = {4},
	urldate = {2023-06-07},
	journal = {The American Journal of Psychology},
	author = {White, B. W. and Rosenblatt, Frank},
	month = dec,
	year = {1963},
	note = {2231 citations (Semantic Scholar/DOI) [2023-06-07]},
	keywords = {background, untagged, perceptrons},
	pages = {705},
}

@article{mcculloch_logical_1990,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {52},
	issn = {0092-8240, 1522-9602},
	url = {http://link.springer.com/10.1007/BF02459570},
	doi = {10.1007/BF02459570},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {1-2},
	urldate = {2023-06-07},
	journal = {Bulletin of Mathematical Biology},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = jan,
	year = {1990},
	note = {129 Citations},
	keywords = {background},
	pages = {99--115},
}

@article{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	language = {en},
	urldate = {2023-06-07},
	author = {Nielsen, Michael A.},
	year = {2015},
	note = {Publisher: Determination Press},
	keywords = {background},
}

@article{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-06-07},
	journal = {CoRR},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	keywords = {background},
	file = {Full Text PDF:/Users/brk/Zotero/storage/SB7A4J49/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2023-06-08},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {9997 citations (Semantic Scholar/DOI) [2023-06-08]},
	keywords = {background, backpropogation},
	pages = {533--536},
}

@article{baum_statistical_1966,
	title = {Statistical {Inference} for {Probabilistic} {Functions} of {Finite} {State} {Markov} {Chains}},
	volume = {37},
	url = {https://doi.org/10.1214/aoms/1177699147},
	doi = {10.1214/aoms/1177699147},
	number = {6},
	journal = {The Annals of Mathematical Statistics},
	author = {Baum, Leonard E. and Petrie, Ted},
	year = {1966},
	note = {2933 citations (Semantic Scholar/DOI) [2023-06-19]
Publisher: Institute of Mathematical Statistics},
	keywords = {model:hmm, background, untagged, model:finite-state-markov-chains},
	pages = {1554 -- 1563},
	file = {Full Text:/Users/brk/Zotero/storage/VMY4KUTL/Baum and Petrie - 1966 - Statistical Inference for Probabilistic Functions .pdf:application/pdf},
}

@article{wilson_parametric_1999,
	title = {Parametric hidden {Markov} models for gesture recognition},
	volume = {21},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/790429/},
	doi = {10.1109/34.790429},
	abstract = {A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction.},
	number = {9},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wilson, A.D. and Bobick, A.F.},
	month = sep,
	year = {1999},
	note = {675 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, untagged},
	pages = {884--900},
}

@article{starner_real-time_1995,
	title = {Real-time {American} {Sign} {Language} recognition from video using hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/477012/},
	doi = {10.1109/ISCV.1995.477012},
	abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
	urldate = {2023-06-22},
	journal = {Proceedings of International Symposium on Computer Vision - ISCV},
	author = {Starner, T. and Pentland, A.},
	year = {1995},
	note = {994 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: International Symposium on Computer Vision - ISCV
ISBN: 9780818671906
Place: Coral Gables, FL, USA
Publisher: IEEE Comput. Soc. Press},
	keywords = {app:american-sl, app:sign-language, author:starner, model:hmm, tech:rgb, type:seminal, untagged},
	pages = {265--270},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5LD7H7UI/Starner - 1995 - Visual Recognition of American Sign Language Using.pdf:application/pdf},
}

@article{starner_real-time_1998,
	title = {Real-time {American} sign language recognition using desk and wearable computer based video},
	volume = {20},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/735811/},
	doi = {10.1109/34.735811},
	abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
	number = {12},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Starner, T. and Weaver, J. and Pentland, A.},
	month = dec,
	year = {1998},
	note = {1394 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, type:seminal, author:starner},
	pages = {1371--1375},
	file = {Starner et al. - 1998 - Real-time American sign language recognition using.pdf:/Users/brk/Zotero/storage/JITL4ER8/Starner et al. - 1998 - Real-time American sign language recognition using.pdf:application/pdf},
}

@inproceedings{shen_using_2019,
	address = {Cham},
	title = {Using {3D} {Convolutional} {Neural} {Networks} to {Learn} {Spatiotemporal} {Features} for {Automatic} {Surgical} {Gesture} {Recognition} in {Video}},
	volume = {11768},
	isbn = {978-3-030-32253-3 978-3-030-32254-0},
	url = {http://link.springer.com/10.1007/978-3-030-32254-0_52},
	doi = {10.1007/978-3-030-32254-0_52},
	abstract = {Automatically recognizing surgical gestures is a crucial step towards a thorough understanding of surgical skill. Possible areas of application include automatic skill assessment, intra-operative monitoring of critical surgical steps, and semi-automation of surgical tasks. Solutions that rely only on the laparoscopic video and do not require additional sensor hardware are especially attractive as they can be implemented at low cost in many scenarios. However, surgical gesture recognition based only on video is a challenging problem that requires effective means to extract both visual and temporal information from the video. Previous approaches mainly rely on frame-wise feature extractors, either handcrafted or learned, which fail to capture the dynamics in surgical video. To address this issue, we propose to use a 3D Convolutional Neural Network (CNN) to learn spatiotemporal features from consecutive video frames. We evaluate our approach on recordings of robot-assisted suturing on a bench-top model, which are taken from the publicly available JIGSAWS dataset. Our approach achieves high frame-wise surgical gesture recognition accuracies of more than 84\%, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately. For the first time, these results demonstrate the benefit of spatiotemporal CNNs for video-based surgical gesture recognition.},
	language = {en},
	urldate = {2023-06-22},
	publisher = {Springer International Publishing},
	author = {Funke, Isabel and Bodenstedt, Sebastian and Oehme, Florian and Von Bechtolsheim, Felix and Weitz, Jürgen and Speidel, Stefanie},
	editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
	year = {2019},
	doi = {10.1007/978-3-030-32254-0_52},
	note = {61 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Medical Image Computing and Computer Assisted Intervention – MICCAI 2019
Series Title: Lecture Notes in Computer Science},
	keywords = {model:cnn, tech:rgb},
	pages = {467--475},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2P6JU4JY/Funke et al. - 2019 - Using 3D Convolutional Neural Networks to Learn Sp.pdf:application/pdf},
}

@article{ur_rehman_dynamic_2022,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Using} {3D}-{CNN} and {LSTM} {Networks}},
	volume = {70},
	issn = {1546-2226},
	url = {https://www.techscience.com/cmc/v70n3/44942},
	doi = {10.32604/cmc.2022.019586},
	abstract = {: Recognition of dynamic hand gestures in real-time is a difficult task because the system can never know when or from where the gesture starts and ends in a video stream. Many researchers have been working on vision-based gesture recognition due to its various applications. This paper proposes a deep learning architecture based on the combination of a 3D Convolutional Neural Network (3D-CNN) and a Long Short-Term Memory (LSTM) network. The proposed architecture extracts spatial-temporal information from video sequences input while avoiding extensive computation. The 3D-CNN is used for the extraction of spectral and spatial features which are then given to the LSTM network through which classification is carried out. The proposed model is a light-weight architecture with only 3.7 million training parameters. The model has been evaluated on 15 classes from the 20BN-jester dataset available publicly. The model was trained on 2000 video-clips per class which were separated into 80\% training and 20\% validation sets. An accuracy of 99\% and 97\% was achieved on training and testing data, respectively. We further show that the combination of 3D-CNN with LSTM gives superior results as compared to MobileNetv2 + LSTM.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Computers, Materials \& Continua},
	author = {Ur Rehman, Muneeb and Ahmed, Fawad and Attique Khan, Muhammad and Tariq, Usman and Abdulaziz Alfouzan, Faisal and M. Alzahrani, Nouf and Ahmad, Jawad},
	year = {2022},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb, movement:dynamic, model:lstm, dataset:20bn-jester},
	pages = {4675--4690},
	file = {Full Text PDF:/Users/brk/Zotero/storage/7NE5RC22/Ur Rehman et al. - 2022 - Dynamic Hand Gesture Recognition Using 3D-CNN and .pdf:application/pdf},
}

@article{wu_deep_2016,
	title = {Deep {Dynamic} {Neural} {Networks} for {Multimodal} {Gesture} {Segmentation} and {Recognition}},
	volume = {38},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/7423804/},
	doi = {10.1109/TPAMI.2016.2537340},
	abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
	number = {8},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wu, Di and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
	month = aug,
	year = {2016},
	note = {373 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, model:cnn, tech:rgbd, model:ffnn},
	pages = {1583--1597},
	file = {Accepted Version:/Users/brk/Zotero/storage/UNZIHXE3/Wu et al. - 2016 - Deep Dynamic Neural Networks for Multimodal Gestur.pdf:application/pdf},
}

@article{elbadawy_arabic_2017,
	title = {Arabic sign language recognition with {3D} convolutional neural networks},
	url = {http://ieeexplore.ieee.org/document/8260028/},
	doi = {10.1109/INTELCIS.2017.8260028},
	abstract = {Sign Language recognition is very important for communication purposes between Hearing Impaired (HI) people and hearing ones. Arabic Sign Language Recognition field became widespread because of its difficult nature and numerous details. Most researchers employed different input sensors, features extractors, and classifiers on static and dynamic data. These different ways were customized and employed in our previous work in the Arabic Sign Language Recognition field. In this paper, features extractor with deep behavior was used to deal with the minor details of Arabic Sign Language. 3D Convolutional Neural Network (CNN) was used to recognize 25 gestures from Arabic sign language dictionary. The recognition system was fed with data from depth maps. The system achieved 98\% accuracy for observed data and 85\% average accuracy for new data. The results could be improved as more data from more different signers are included.},
	urldate = {2023-06-22},
	journal = {2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)},
	author = {ElBadawy, Menna and Elons, A. S. and Shedeed, Howida A. and Tolba, M. F.},
	month = dec,
	year = {2017},
	note = {43 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)
ISBN: 9781538608210
Place: Cairo
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {66--71},
}

@article{liang_3d_2018,
	title = {{3D} {Convolutional} {Neural} {Networks} for {Dynamic} {Sign} {Language} {Recognition}},
	volume = {61},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article/61/11/1724/4995616},
	doi = {10.1093/comjnl/bxy049},
	abstract = {Automatic dynamic sign language recognition is even more challenging than gesture recognition due to the fact that the vocabularies are large and signs are context dependent. Previous works in this direction tend to build classifiers based on complex hand-crafted features computed from the raw inputs. As a type of deep learning model, convolutional neural networks (CNNs) have significantly advanced the accuracy of human gesture classification. However, such methods are currently used to treat video frames as 2D images and recognize gestures at the individual frame level. In this paper, we present a data driven system in which 3D-CNNs are applied to extract spatial and temporal features from video streams, and the motion information is captured by noting the variation in depth between each pair of consecutive frames. To further boost the performance, multi-modal of video streams, including infrared, contour and skeleton are used as input for the architecture and the prediction results estimated from the different sub-networks were fused together. In order to validate our method, we introduce a new challenging multi-modal dynamic sign language dataset captured with Kinect sensors. We evaluate the proposed approach on the collected dataset and achieve superior performance. Moreover, our method achieves a mean Jaccard Index score of 0.836 on the ChaLearn Looking at People Gesture datasets.},
	language = {en},
	number = {11},
	urldate = {2023-06-22},
	journal = {The Computer Journal},
	author = {Liang, Zhi-jie and Liao, Sheng-bin and Hu, Bing-zhang},
	editor = {Manolopoulos, Yannis},
	month = nov,
	year = {2018},
	note = {47 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {1724--1736},
}

@article{bhagat_indian_2019,
	title = {Indian {Sign} {Language} {Gesture} {Recognition} using {Image} {Processing} and {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8945850/},
	doi = {10.1109/DICTA47822.2019.8945850},
	abstract = {Speech impaired people use hand based gestures to communicate. Unfortunately, the vast majority of the people are not aware of the semantics of these gestures. In a attempt to bridge the same, we propose a real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera. Given that there is no one to one mapping between the pixels of the depth and the RGB camera, we used computer vision techniques like 3D contruction and affine transformation. After achieving one to one mapping, segmentation of the hand gestures was done from the background noise. Convolutional Neural Networks (CNNs) were utilised for training 36 static gestures relating to Indian Sign Language (ISL) alphabets and numbers. The model achieved an accuracy of 98.81\% on training using 45,000 RGB images and 45,000 depth images. Further Convolutional LSTMs were used for training 10 ISL dynamic word gestures and an accuracy of 99.08\% was obtained by training 1080 videos. The model showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures. The model also showed competitive adaptability to American Sign Language (ASL) gestures when the ISL models weights were transfer learned to ASL and it resulted in giving 97.71\% accuracy.},
	urldate = {2023-06-22},
	journal = {2019 Digital Image Computing: Techniques and Applications (DICTA)},
	author = {Bhagat, Neel Kamal and Vishnusai, Y. and Rathna, G. N.},
	month = dec,
	year = {2019},
	note = {26 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 Digital Image Computing: Techniques and Applications (DICTA)
ISBN: 9781728138572
Place: Perth, Australia
Publisher: IEEE},
	keywords = {app:sign-language},
	pages = {1--8},
}

@article{sharma_asl-3dcnn_2021,
	title = {{ASL}-{3DCNN}: {American} sign language recognition technique using 3-{D} convolutional neural networks},
	volume = {80},
	issn = {1380-7501, 1573-7721},
	shorttitle = {{ASL}-{3DCNN}},
	url = {https://link.springer.com/10.1007/s11042-021-10768-5},
	doi = {10.1007/s11042-021-10768-5},
	abstract = {The communication between a person from the impaired community with a person who does not understand sign language could be a tedious task. Sign language is the art of conveying messages using hand gestures. Recognition of dynamic hand gestures in American Sign Language (ASL) became a very important challenge that is still unresolved. In order to resolve the challenges of dynamic ASL recognition, a more advanced successor of the Convolutional Neural Networks (CNNs) called 3-D CNNs is employed, which can recognize the patterns in volumetric data like videos. The CNN is trained for classification of 100 words on Boston ASL (Lexicon Video Dataset) LVD dataset with more than 3300 English words signed by 6 different signers. 70\% of the dataset is used for Training while the remaining 30\% dataset is used for testing the model. The proposed work outperforms the existing state-of-art models in terms of precision (3.7\%), recall (4.3\%), and f-measure (3.9\%). The computing time (0.19 seconds per frame) of the proposed work shows that the proposal may be used in real-time applications.},
	language = {en},
	number = {17},
	urldate = {2023-06-22},
	journal = {Multimedia Tools and Applications},
	author = {Sharma, Shikhar and Kumar, Krishan},
	month = jul,
	year = {2021},
	note = {44 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language},
	pages = {26319--26331},
}

@article{cheng_survey_2016,
	title = {Survey on {3D} {Hand} {Gesture} {Recognition}},
	volume = {26},
	issn = {1051-8215, 1558-2205},
	url = {http://ieeexplore.ieee.org/document/7208833/},
	doi = {10.1109/TCSVT.2015.2469551},
	abstract = {Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners.},
	number = {9},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Cheng, Hong and Yang, Lu and Liu, Zicheng},
	month = sep,
	year = {2016},
	note = {265 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {1659--1673},
}

@article{harshith_survey_2010,
	title = {Survey on {Various} {Gesture} {Recognition} {Techniques} for {Interfacing} {Machines} {Based} on {Ambient} {Intelligence}},
	volume = {1},
	issn = {09763252},
	url = {http://www.airccse.org/journal/ijcses/papers/1110ijcses03.pdf},
	doi = {10.5121/ijcses.2010.1203},
	abstract = {Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions. Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods, appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.},
	number = {2},
	urldate = {2023-06-22},
	journal = {International Journal of Computer Science \& Engineering Survey},
	author = {Harshith, C and Shastry, Karthik.R. and Ravindran, Manoj and Srikanth, M.V.V.N.S and Lakshmikhanth, Naveen},
	month = nov,
	year = {2010},
	note = {53 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {31--42},
	file = {Full Text PDF:/Users/brk/Zotero/storage/UDQVJPM7/Harshith et al. - 2010 - Survey on Various Gesture Recognition Techniques f.pdf:application/pdf},
}

@article{cai_rgb-d_2017,
	title = {{RGB}-{D} datasets using microsoft kinect or similar sensors: a survey},
	volume = {76},
	issn = {1380-7501, 1573-7721},
	shorttitle = {{RGB}-{D} datasets using microsoft kinect or similar sensors},
	url = {http://link.springer.com/10.1007/s11042-016-3374-6},
	doi = {10.1007/s11042-016-3374-6},
	abstract = {RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems. It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color, illumination, rotation angle and scale. With the invention of the low-cost Microsoft Kinect sensor, which was initially used for gaming and later became a popular device for computer vision, high quality RGB-D data can be acquired easily. In recent years, more and more RGB-D image/video datasets dedicated to various applications have become available, which are of great importance to benchmark the state-of-the-art. In this paper, we systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation. We provide the insights into the characteristics of each important dataset, and compare the popularity and the difficulty of those datasets. Overall, the main goal of this survey is to give a comprehensive description about the available RGB-D datasets and thus to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
	language = {en},
	number = {3},
	urldate = {2023-06-22},
	journal = {Multimedia Tools and Applications},
	author = {Cai, Ziyun and Han, Jungong and Liu, Li and Shao, Ling},
	month = feb,
	year = {2017},
	note = {115 Citations},
	keywords = {hardware:kinect, tech:rgbd, read-priority-5, type:survey},
	pages = {4313--4355},
	file = {Full Text:/Users/brk/Zotero/storage/Q3HIZYJF/Cai et al. - 2017 - RGB-D datasets using microsoft kinect or similar s.pdf:application/pdf},
}

@article{al-qaness_wiger_2016,
	title = {{WiGeR}: {WiFi}-{Based} {Gesture} {Recognition} {System}},
	volume = {5},
	issn = {2220-9964},
	shorttitle = {{WiGeR}},
	url = {http://www.mdpi.com/2220-9964/5/6/92},
	doi = {10.3390/ijgi5060092},
	abstract = {Recently, researchers around the world have been striving to develop and modernize human–computer interaction systems by exploiting advances in modern communication systems. The priority in this field involves exploiting radio signals so human–computer interaction will require neither special devices nor vision-based technology. In this context, hand gesture recognition is one of the most important issues in human–computer interfaces. In this paper, we present a novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions. We extract CSI from any common WiFi router and then filter out the noise to obtain the CSI fluctuation trends generated by hand motions. We design a novel and agile segmentation and windowing algorithm based on wavelet analysis and short-time energy to reveal the specific pattern associated with each hand gesture and detect duration of the hand motion. Furthermore, we design a fast dynamic time warping algorithm to classify our system’s proposed hand gestures. We implement and test our system through experiments involving various scenarios. The results show that WiGeR can classify gestures with high accuracy, even in scenarios where the signal passes through multiple walls.},
	language = {en},
	number = {6},
	urldate = {2023-06-22},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Al-qaness, Mohammed and Li, Fangmin},
	month = jun,
	year = {2016},
	note = {84 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:wifi, model:dtw, movement:static, movement:dynamic, classes:10-29},
	pages = {92},
	file = {Full Text PDF:/Users/brk/Zotero/storage/DV7B98FA/Al-qaness and Li - 2016 - WiGeR WiFi-Based Gesture Recognition System.pdf:application/pdf},
}

@article{he_wig_2015,
	title = {{WiG}: {WiFi}-{Based} {Gesture} {Recognition} {System}},
	shorttitle = {{WiG}},
	url = {http://ieeexplore.ieee.org/document/7288485/},
	doi = {10.1109/ICCCN.2015.7288485},
	abstract = {Most recently, gesture recognition has increasingly attracted intense academic and industrial interest due to its various applications in daily life, such as home automation, mobile games. Present approaches for gesture recognition, mainly including vision-based, sensor-based and RF-based, all have certain limitations which hinder their practical use in some scenarios. For example, the vision-based approaches fail to work well in poor light conditions and the sensor-based ones require users to wear devices. To address these, we propose WiG in this paper, a device-free gesture recognition system based solely on Commercial Off-The-Shelf (COTS) WiFi infrastructures and devices. Compared with existing Radio Frequency (RF)-based systems, WiG stands out for its systematic simplicity, extremely low cost and high practicability. We implemented WiG in indoor environment and conducted experiments to evaluate its performance in two typical scenarios. The results demonstrate that WiG can achieve an average recognition accuracy of 92\% in line-of-sight scenario and average accuracy of 88\% in the none-line-of sight scenario.},
	urldate = {2023-06-22},
	journal = {2015 24th International Conference on Computer Communication and Networks (ICCCN)},
	author = {He, Wenfeng and Wu, Kaishun and Zou, Yongpan and Ming, Zhong},
	month = aug,
	year = {2015},
	note = {132 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 24th International Conference on Computer Communication and Networks (ICCCN)
ISBN: 9781479999644
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {tech:wifi},
	pages = {1--7},
}

@article{hakim_dynamic_2019,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Using} {3DCNN} and {LSTM} with {FSM} {Context}-{Aware} {Model}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/24/5429},
	doi = {10.3390/s19245429},
	abstract = {With the recent growth of Smart TV technology, the demand for unique and beneficial applications motivates the study of a unique gesture-based system for a smart TV-like environment. Combining movie recommendation, social media platform, call a friend application, weather updates, chatting app, and tourism platform into a single system regulated by natural-like gesture controller is proposed to allow the ease of use and natural interaction. Gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment. Dataset of a sequence of RGB and depth images were collected, preprocessed, and trained in the proposed deep learning architecture. Combination of three-dimensional Convolutional Neural Network (3DCNN) followed by Long Short-Term Memory (LSTM) model was used to extract the spatio-temporal features. At the end of the classification, Finite State Machine (FSM) communicates the model to control the class decision results based on application context. The result suggested the combination data of depth and RGB to hold 97.8\% of accuracy rate on eight selected gestures, while the FSM has improved the recognition rate from 89\% to 91\% in a real-time performance.},
	language = {en},
	number = {24},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {Hakim, Noorkholis Luthfil and Shih, Timothy K. and Kasthuri Arachchi, Sandeli Priyanwada and Aditya, Wisnu and Chen, Yi-Cheng and Lin, Chih-Yang},
	month = dec,
	year = {2019},
	note = {32 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgbd, movement:static, movement:dynamic, model:lstm, classes:10-29, model:fsm},
	pages = {5429},
	file = {Full Text PDF:/Users/brk/Zotero/storage/W6XRMHDD/Hakim et al. - 2019 - Dynamic Hand Gesture Recognition Using 3DCNN and L.pdf:application/pdf},
}

@article{zhao_multi-feature_2016,
	title = {Multi-feature gesture recognition based on {Kinect}},
	url = {http://ieeexplore.ieee.org/document/7574856/},
	doi = {10.1109/CYBER.2016.7574856},
	abstract = {Human Computer Interaction (HCI) has been a popular research area during the last few years. Compared with the tradition HCI methods such as using a keyboard or mouse, people prefer to have their tasks done in a more natural way. As an essential form of non-verbal communication in daily life, gesture is a good choice to turn the ideas into reality. Although various recognition methods are proposed to solve the problem, these methods are time-tensed, space-tensed or miscellaneous. This paper introduced a new method to recognize the hand gesture correctly and efficiently. The recognition is done through two phases: the skeleton phase concerning capturing and processing skeleton feature of the hand gesture, and the hand phase focusing on extracting hand contour feature of the hand gesture. Experimental results confirm an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
	urldate = {2023-06-22},
	journal = {2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
	author = {Zhao, Yue and Liu, Yunda and Dong, Min and Bi, Sheng},
	month = jun,
	year = {2016},
	note = {6 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)
ISBN: 9781509027330
Place: Chengdu, China
Publisher: IEEE},
	keywords = {hardware:kinect, tech:rgbd},
	pages = {392--396},
}

@article{koch_recurrent_2019,
	title = {A {Recurrent} {Neural} {Network} for {Hand} {Gesture} {Recognition} based on {Accelerometer} {Data}},
	url = {https://ieeexplore.ieee.org/document/8856844/},
	doi = {10.1109/EMBC.2019.8856844},
	abstract = {For many applications, hand gesture recognition systems that rely on biosignal data exclusively are mandatory. Usually, theses systems have to be affordable, reliable as well as mobile. The hand is moved due to muscle contractions that cause motions of the forearm skin. Theses motions can be captured with cheap and reliable accelerometers placed around the forearm. Since accelerometers can also be integrated into mobile systems easily, the possibility of a robust hand gesture recognition based on accelerometer signals is evaluated in this work. For this, a neural network architecture consisting of two different kinds of recurrent neural network (RNN) cells is proposed. Experiments on three databases reveal that this relatively small network outperforms by far state-of-the-art hand gesture recognition approaches that rely on multi-modal data. The combination of accelerometer data and an RNN forms a robust hand gesture classification system, i.e., the performance of the network does not vary a lot between subjects and it is outstanding for amputees. Furthermore, the proposed network uses only 5 ms short windows to classify the hand gestures. Consequently, this approach allows for a quick, and potentially delay-free hand gesture detection.},
	urldate = {2023-06-22},
	journal = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	author = {Koch, Philipp and Dreier, Mark and Maass, Marco and Bohme, Martina and Phan, Huy and Mertins, Alfred},
	month = jul,
	year = {2019},
	note = {16 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 41st Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC)
ISBN: 9781538613115
Place: Berlin, Germany
Publisher: IEEE},
	keywords = {tech:accelerometer, tech:emg, model:rnn},
	pages = {5088--5091},
	file = {Accepted Version:/Users/brk/Zotero/storage/ZF224WN7/Koch et al. - 2019 - A Recurrent Neural Network for Hand Gesture Recogn.pdf:application/pdf},
}

@article{zhang_real-time_2019,
	title = {Real-{Time} {Surface} {EMG} {Pattern} {Recognition} for {Hand} {Gestures} {Based} on an {Artificial} {Neural} {Network}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/14/3170},
	doi = {10.3390/s19143170},
	abstract = {In recent years, surface electromyography (sEMG) signals have been increasingly used in pattern recognition and rehabilitation. In this paper, a real-time hand gesture recognition model using sEMG is proposed. We use an armband to acquire sEMG signals and apply a sliding window approach to segment the data in extracting features. A feedforward artificial neural network (ANN) is founded and trained by the training dataset. A test method is used in which the gesture will be recognized when recognized label times reach the threshold of activation times by the ANN classifier. In the experiment, we collected real sEMG data from twelve subjects and used a set of five gestures from each subject to evaluate our model, with an average recognition rate of 98.7\% and an average response time of 227.76 ms, which is only one-third of the gesture time. Therefore, the pattern recognition system might be able to recognize a gesture before the gesture is completed.},
	language = {en},
	number = {14},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {{Zhang} and {Yang} and {Qian} and {Zhang}},
	month = jul,
	year = {2019},
	note = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, model:ffnn, classes:{\textless}10},
	pages = {3170},
	file = {Full Text PDF:/Users/brk/Zotero/storage/NV2YLHJB/Zhang et al. - 2019 - Real-Time Surface EMG Pattern Recognition for Hand.pdf:application/pdf},
}

@article{wan_explore_2016,
	title = {Explore {Efficient} {Local} {Features} from {RGB}-{D} {Data} for {One}-{Shot} {Learning} {Gesture} {Recognition}},
	volume = {38},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/7368923/},
	doi = {10.1109/TPAMI.2015.2513479},
	abstract = {Availability of handy RGB-D sensors has brought about a surge of gesture recognition research and applications. Among various approaches, one shot learning approach is advantageous because it requires minimum amount of data. Here, we provide a thorough review about one-shot learning gesture recognition from RGB-D data and propose a novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK). In the review, we analyze the challenges that we are facing, and point out some future research directions which may enlighten researchers in this field. The proposed MFSK feature is robust and invariant to scale, rotation and partial occlusions. To alleviate the insufficiency of one shot training samples, we augment the training samples by artificially synthesizing versions of various temporal scales, which is beneficial for coping with gestures performed at varying speed. We evaluate the proposed method on the Chalearn gesture dataset (CGD). The results show that our approach outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets. When applied to the RGB-D datasets that are not one-shot (e.g., the Cornell Activity Dataset-60 and MSR Daily Activity 3D dataset), the proposed feature also produces very promising results under leave-one-out cross validation or one-shot learning.},
	number = {8},
	urldate = {2023-06-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wan, Jun and Guo, Guodong and Li, Stan Z.},
	month = aug,
	year = {2016},
	note = {93 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgbd, app:robot-control, app:surgery, app:medical, dataset:chalearn-gesture, dataset:cornell-activity-60, dataset:msr-daily-activity-3d},
	pages = {1626--1639},
}

@article{mohammed_deep_2019,
	title = {A {Deep} {Learning}-{Based} {End}-to-{End} {Composite} {System} for {Hand} {Detection} and {Gesture} {Recognition}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/23/5282},
	doi = {10.3390/s19235282},
	abstract = {Recent research on hand detection and gesture recognition has attracted increasing interest due to its broad range of potential applications, such as human-computer interaction, sign language recognition, hand action analysis, driver hand behavior monitoring, and virtual reality. In recent years, several approaches have been proposed with the aim of developing a robust algorithm which functions in complex and cluttered environments. Although several researchers have addressed this challenging problem, a robust system is still elusive. Therefore, we propose a deep learning-based architecture to jointly detect and classify hand gestures. In the proposed architecture, the whole image is passed through a one-stage dense object detector to extract hand regions, which, in turn, pass through a lightweight convolutional neural network (CNN) for hand gesture recognition. To evaluate our approach, we conducted extensive experiments on four publicly available datasets for hand detection, including the Oxford, 5-signers, EgoHands, and Indian classical dance (ICD) datasets, along with two hand gesture datasets with different gesture vocabularies for hand gesture recognition, namely, the LaRED and TinyHands datasets. Here, experimental results demonstrate that the proposed architecture is efficient and robust. In addition, it outperforms other approaches in both the hand detection and gesture classification tasks.},
	language = {en},
	number = {23},
	urldate = {2023-06-22},
	journal = {Sensors},
	author = {Mohammed, Adam Ahmed Qaid and Lv, Jiancheng and Islam, Md. Sajjatul},
	month = nov,
	year = {2019},
	note = {34 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, tech:rgb},
	pages = {5282},
	file = {Full Text PDF:/Users/brk/Zotero/storage/V5WL63KJ/Mohammed et al. - 2019 - A Deep Learning-Based End-to-End Composite System .pdf:application/pdf},
}

@article{kundu_hand_2018,
	title = {Hand {Gesture} {Recognition} {Based} {Omnidirectional} {Wheelchair} {Control} {Using} {IMU} and {EMG} {Sensors}},
	volume = {91},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-017-0725-0},
	doi = {10.1007/s10846-017-0725-0},
	abstract = {This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors. Seven common gestures are recognized and classified using shape based feature extraction and Dendogram Support Vector Machine (DSVM) classifier. The dynamic gestures are mapped to the omnidirectional motion commands to navigate the wheelchair. A single IMU is used to measure the wrist tilt angle and acceleration in three axis. EMG signals are extracted from two forearm muscles namely Extensor Carpi Radialis and Flexor Carpi Radialis and processed to provide Root Mean Square (RMS) signal. Initiation and termination of dynamic activities are based on autonomous identification of static to dynamic or dynamic to static transition by setting static thresholds on processed IMU and myoelectric sensor data. Classification involves recognizing the activity pattern based on periodic shape of trajectories of the triaxial wrist tilt angle and EMG-RMS from the two selected muscles. Second order Polynomial coefficients extracted from the sensor trajectory templates during specific dynamic activity cycles are used as features to classify dynamic activities. Classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested by five healthy subjects. Classification accuracy of 94\% was achieved by DSVM classifier on ‘k’ fold cross validation data of 5 users. Classification accuracy while operating the wheelchair was 90.5\%.},
	language = {en},
	number = {3-4},
	urldate = {2023-06-22},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis},
	month = sep,
	year = {2018},
	note = {70 Citations},
	keywords = {tech:accelerometer, model:svm, tech:emg},
	pages = {529--541},
}

@article{ma_hand_2017,
	title = {Hand gesture recognition with convolutional neural networks for the multimodal {UAV} control},
	url = {http://ieeexplore.ieee.org/document/8101666/},
	doi = {10.1109/RED-UAS.2017.8101666},
	abstract = {We introduce a robust wearable sensor suite fusing arm motion and hand gesture recognition for operator control of UAVs. The sensor suite fuses mechanomyography (MMG) and an inertial measurement unit (IMU) to capture multi-modal (arm movement and hand gesture) command signals simultaneously. The IMU produces world referenced orientation and acceleration data while concomitant MMG tracks muscle activation through surface vibration. The use of surface muscle vibration for gesture recognition removes the need for electrical contact with the skin, which has impeded other forms of muscle measurement for gesture recognition in the field. This investigation presents hardware design, inertial recognition of arm movement, and the detailed structure of a convolutional neural network (CNN) system used for real-time hand gesture recognition based on MMG signals. The system achieved 94\% accuracy for five gestures with simple calibration for each user, thereby providing an intuitive gesture based UAV control system. To our knowledge this is the first wearable system enabling multimodal control of UAVs through intuitive gestures that does not require electrical skin contact. Future work involves testing the system with larger UAV swarms.},
	urldate = {2023-06-22},
	journal = {2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)},
	author = {Ma, Yuntao and Liu, Yuxuan and Jin, Ruiyang and Yuan, Xingyang and Sekha, Raza and Wilson, Samuel and Vaidyanathan, Ravi},
	month = oct,
	year = {2017},
	note = {35 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)
ISBN: 9781538609392
Place: Linkoping
Publisher: IEEE},
	keywords = {model:cnn, app:robot-control, classes:{\textless}10, tech:mechanomyography, tech:imu},
	pages = {198--203},
}

@article{aly_survey_2005,
	title = {Survey on multiclass classification methods},
	volume = {19},
	number = {1-9},
	journal = {Neural Netw},
	author = {Aly, Mohamed},
	year = {2005},
	note = {Publisher: Citeseer},
	keywords = {background},
	pages = {2},
}

@inproceedings{suarez_hand_2012,
	title = {Hand gesture recognition with depth images: {A} review},
	shorttitle = {Hand gesture recognition with depth images},
	doi = {10.1109/ROMAN.2012.6343787},
	abstract = {This paper presents a literature review on the use of depth for hand tracking and gesture recognition. The survey examines 37 papers describing depth-based gesture recognition systems in terms of (1) the hand localization and gesture classification methods developed and used, (2) the applications where gesture recognition has been tested, and (3) the effects of the low-cost Kinect and OpenNI software libraries on gesture recognition research. The survey is organized around a novel model of the hand gesture recognition process. In the reviewed literature, 13 methods were found for hand localization and 11 were found for gesture classification. 24 of the papers included real-world applications to test a gesture recognition system, but only 8 application categories were found (and three applications accounted for 18 of the papers). The papers that use the Kinect and the OpenNI libraries for hand tracking tend to focus more on applications than on localization and classification methods, and show that the OpenNI hand tracking method is good enough for the applications tested thus far. However, the limitations of the Kinect and other depth sensors for gesture recognition have yet to be tested in challenging applications and environments.},
	booktitle = {2012 {IEEE} {RO}-{MAN}: {The} 21st {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication}},
	author = {Suarez, Jesus and Murphy, Robin R.},
	month = sep,
	year = {2012},
	note = {364 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1944-9437},
	keywords = {tech:rgb, tech:rgbd, type:survey},
	pages = {411--417},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/EYLG8BUK/6343787.html:text/html},
}

@article{oudah_hand_2020,
	title = {Hand {Gesture} {Recognition} {Based} on {Computer} {Vision}: {A} {Review} of {Techniques}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-433X},
	shorttitle = {Hand {Gesture} {Recognition} {Based} on {Computer} {Vision}},
	url = {https://www.mdpi.com/2313-433X/6/8/73},
	doi = {10.3390/jimaging6080073},
	abstract = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human–computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Journal of Imaging},
	author = {Oudah, Munir and Al-Naji, Ali and Chahl, Javaan},
	month = aug,
	year = {2020},
	note = {178 citations (Semantic Scholar/DOI) [2023-07-02]
Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {tech:rgb, type:survey},
	pages = {73},
	file = {Full Text PDF:/Users/brk/Zotero/storage/WREJRICA/Oudah et al. - 2020 - Hand Gesture Recognition Based on Computer Vision.pdf:application/pdf},
}

@inproceedings{fang_real-time_2007,
	title = {A {Real}-{Time} {Hand} {Gesture} {Recognition} {Method}},
	doi = {10.1109/ICME.2007.4284820},
	abstract = {Compared with the traditional interaction approaches, such as keyboard, mouse, pen, etc, vision based hand interaction is more natural and efficient. In this paper, we proposed a robust real-time hand gesture recognition method. In our method, firstly, a specific gesture is required to trigger the hand detection followed by tracking; then hand is segmented using motion and color cues; finally, in order to break the limitation of aspect ratio encountered in most of learning based hand gesture methods, the scale-space feature detection is integrated into gesture recognition. Applying the proposed method to navigation of image browsing, experimental results show that our method achieves satisfactory performance.},
	booktitle = {2007 {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	author = {Fang, Yikai and Wang, Kongqiao and Cheng, Jian and Lu, Hanqing},
	month = jul,
	year = {2007},
	note = {227 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1945-788X},
	keywords = {tech:rgb, tech:rgbd},
	pages = {995--998},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/4T92MUB7/4284820.html:text/html},
}

@inproceedings{liu_hand_2004,
	title = {Hand gesture recognition using depth data},
	doi = {10.1109/AFGR.2004.1301587},
	abstract = {A method is presented for recognizing hand gestures by using a sequence of real-time depth image data acquired by an active sensing hardware. Hand posture and motion information extracted from a video is represented in a gesture space which consists of a number of aspects including hand shape, location and motion information. In this space, it is shown to be possible to recognize many types of gestures. Experimental results are shown to validate our approach and characteristics of our approach are discussed.},
	booktitle = {Sixth {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition}, 2004. {Proceedings}.},
	author = {Liu, Xia and Fujimura, K.},
	month = may,
	year = {2004},
	note = {243 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, tech:rgbd},
	pages = {529--534},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/R7WNTKII/1301587.html:text/html},
}

@article{chen_hand_2003,
	title = {Hand gesture recognition using a real-time tracking method and hidden {Markov} models},
	volume = {21},
	issn = {0262-8856},
	url = {https://www.sciencedirect.com/science/article/pii/S0262885603000702},
	doi = {10.1016/S0262-8856(03)00070-2},
	abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90\%.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Image and Vision Computing},
	author = {Chen, Feng-Sheng and Fu, Chih-Ming and Huang, Chung-Lin},
	month = aug,
	year = {2003},
	note = {528 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm},
	pages = {745--758},
	file = {ScienceDirect Full Text PDF:/Users/brk/Zotero/storage/7FR9BDWF/Chen et al. - 2003 - Hand gesture recognition using a real-time trackin.pdf:application/pdf;ScienceDirect Snapshot:/Users/brk/Zotero/storage/9ZPXDS8C/S0262885603000702.html:text/html},
}

@article{baudel_charade_1993,
	title = {Charade: remote control of objects using free-hand gestures},
	volume = {36},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Charade},
	url = {https://dl.acm.org/doi/10.1145/159544.159562},
	doi = {10.1145/159544.159562},
	abstract = {This paper presents an application that uses hand gesture input to control a computer while giving a presentation. In order to develop a prototype of this application, we have defined an interaction model, a notation for gestures, and a set of guidelines to design gestural command sets. This works aims to define interaction styles that work in computerized reality environments. In our application, gestures are used for interacting with the computer as well as for communicating with other people or operating other devices.},
	language = {en},
	number = {7},
	urldate = {2023-06-23},
	journal = {Communications of the ACM},
	author = {Baudel, Thomas and Beaudouin-Lafon, Michel},
	month = jul,
	year = {1993},
	note = {582 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, tech:flex, hardware:charade, hardware:dataglove},
	pages = {28--35},
	file = {Full Text:/Users/brk/Zotero/storage/WRSWXDSK/Baudel and Beaudouin-Lafon - 1993 - Charade remote control of objects using free-hand.pdf:application/pdf},
}

@article{sturman_survey_1994,
	title = {A survey of glove-based input},
	volume = {14},
	issn = {0272-1716, 1558-1756},
	url = {https://ieeexplore.ieee.org/document/250916/},
	doi = {10.1109/38.250916},
	abstract = {Clumsy intermediary devices constrain our interaction with computers and their applications. Glove-based input devices let us apply our manual dexterity to the task. We provide a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input. The bulk of development in glove-based input has taken place very recently, and not all of it is easily accessible in the literature. We present a cross-section of the field to date. Hand-tracking devices may use the following technologies: position tracking, optical tracking, marker systems, silhouette analysis, magnetic tracking or acoustic tracking. Actual glove technologies on the market include: Sayre glove, MIT LED glove, Digital Data Entry Glove, DataGlove, Dexterous HandMaster, Power Glove, CyberGlove and Space Glove. Various applications of glove technologies include projects into the pursuit of natural interfaces, systems for understanding signed languages, teleoperation and robotic control, computer-based puppetry, and musical performance.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	number = {1},
	urldate = {2023-06-23},
	journal = {IEEE Computer Graphics and Applications},
	author = {Sturman, D.J. and Zeltzer, D.},
	month = jan,
	year = {1994},
	note = {853 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {read-priority-1, type:seminal, type:survey},
	pages = {30--39},
	file = {Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf:/Users/brk/Zotero/storage/UDDK6KNT/Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf:application/pdf},
}

@article{rekimoto_gesturewrist_2001,
	title = {{GestureWrist} and {GesturePad}: unobtrusive wearable interaction devices},
	shorttitle = {{GestureWrist} and {GesturePad}},
	url = {http://ieeexplore.ieee.org/document/962092/},
	doi = {10.1109/ISWC.2001.962092},
	abstract = {In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.},
	urldate = {2023-06-23},
	journal = {Proceedings Fifth International Symposium on Wearable Computers},
	author = {Rekimoto, Jun},
	year = {2001},
	note = {450 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Fifth International Symposium on Wearable Computers
ISBN: 9780769513188
Place: Zurich, Switzerland
Publisher: IEEE Comput. Soc},
	keywords = {tech:accelerometer, tech:capacitive},
	pages = {21--27},
	file = {Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf:/Users/brk/Zotero/storage/CAUJY4HG/Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf:application/pdf},
}

@article{pavlovic_visual_1997,
	title = {Visual interpretation of hand gestures for human-computer interaction: a review},
	volume = {19},
	issn = {01628828},
	shorttitle = {Visual interpretation of hand gestures for human-computer interaction},
	url = {http://ieeexplore.ieee.org/document/598226/},
	doi = {10.1109/34.598226},
	abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3Dmodel of the human hand or an image appearancemodel of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient “purposive” approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction},
	number = {7},
	urldate = {2023-06-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Pavlovic, Vladimir I. and Sharma, R. and Huang, T.S.},
	month = jul,
	year = {1997},
	note = {1979 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {677--695},
	file = {Submitted Version:/Users/brk/Zotero/storage/7SQ99MKT/Pavlovic et al. - 1997 - Visual interpretation of hand gestures for human-c.pdf:application/pdf},
}

@article{kela_accelerometer-based_2006,
	title = {Accelerometer-based gesture control for a design environment},
	volume = {10},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-005-0033-8},
	doi = {10.1007/s00779-005-0033-8},
	abstract = {Accelerometer-based gesture control is studied as a supplementary or an alternative interaction modality. Gesture commands freely trainable by the user can be used for controlling external devices with handheld wireless sensor unit. Two user studies are presented. The first study concerns finding gestures for controlling a design environment (Smart Design Studio), TV, VCR, and lighting. The results indicate that different people usually prefer different gestures for the same task, and hence it should be possible to personalise them. The second user study concerns evaluating the usefulness of the gesture modality compared to other interaction modalities for controlling a design environment. The other modalities were speech, RFID-based physical tangible objects, laser-tracked pen, and PDA stylus. The results suggest that gestures are a natural modality for certain tasks, and can augment other modalities. Gesture commands were found to be natural, especially for commands with spatial association in design environment control.},
	language = {en},
	number = {5},
	urldate = {2023-06-23},
	journal = {Personal and Ubiquitous Computing},
	author = {Kela, Juha and Korpipää, Panu and Mäntyjärvi, Jani and Kallio, Sanna and Savino, Giuseppe and Jozzo, Luca and Marca, Sergio Di},
	month = aug,
	year = {2006},
	note = {201 citations (Crossref) [2023-07-11]
333 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, untagged, from:cite.bib},
	pages = {285--299},
}

@article{erol_vision-based_2007,
	title = {Vision-based hand pose estimation: {A} review},
	volume = {108},
	issn = {10773142},
	shorttitle = {Vision-based hand pose estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206002281},
	doi = {10.1016/j.cviu.2006.10.012},
	abstract = {Semantic Scholar extracted view of "Vision-based hand pose estimation: A review" by A. Erol et al.},
	language = {en},
	number = {1-2},
	urldate = {2023-06-23},
	journal = {Computer Vision and Image Understanding},
	author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D. and Twombly, Xander},
	month = oct,
	year = {2007},
	note = {885 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {52--73},
	file = {Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:/Users/brk/Zotero/storage/33ZZZF27/Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf:application/pdf},
}

@article{chen_survey_2020,
	title = {A {Survey} on {Hand} {Pose} {Estimation} with {Wearable} {Sensors} and {Computer}-{Vision}-{Based} {Methods}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/4/1074},
	doi = {10.3390/s20041074},
	abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
	language = {en},
	number = {4},
	urldate = {2023-06-23},
	journal = {Sensors},
	author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
	month = feb,
	year = {2020},
	note = {54 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {1074},
	file = {Full Text PDF:/Users/brk/Zotero/storage/Y67RWII7/Chen et al. - 2020 - A Survey on Hand Pose Estimation with Wearable Sen.pdf:application/pdf},
}

@article{sharma_toward_1998,
	title = {Toward multimodal human-computer interface},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/664275/},
	doi = {10.1109/5.664275},
	abstract = {Recent advances in various signal processing technologies, coupled with an explosion in the available computing power, have given rise to a number of novel human-computer interaction (HCI) modalities: speech, vision-based gesture recognition, eye tracking, electroencephalograph, etc. Successful embodiment of these modalities into an interface has the potential of easing the HCI bottleneck that has become noticeable with the advances in computing and communication. It has also become increasingly evident that the difficulties encountered in the analysis and interpretation of individual sensing modalities may be overcome by integrating them into a multimodal human-computer interface. We examine several promising directions toward achieving multimodal HCI. We consider some of the emerging novel input modalities for HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level. We discuss the different computational approaches that may be applied at the different levels of modality integration. We also briefly review several demonstrated multimodal HCI systems and applications. Despite all the recent developments, it is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI. This research can benefit from many disparate fields of study that increase our understanding of the different human communication modalities and their potential role in HCI.},
	number = {5},
	urldate = {2023-06-23},
	journal = {Proceedings of the IEEE},
	author = {Sharma, R. and Pavlovic, V.I. and Huang, T.S.},
	month = may,
	year = {1998},
	note = {341 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {853--869},
}

@article{kessler_evaluation_1995,
	title = {Evaluation of the {CyberGlove} as a whole-hand input device},
	volume = {2},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/212430.212431},
	doi = {10.1145/212430.212431},
	abstract = {We present a careful evaluation of the sensory characteristics of the CyberGlove model CG1801 whole-hand input device. In particular, we conducted an experimental study that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements. Among our results, we show that hand size differences among the subjects of the study did not have a statistical effect on the accuracy of the device. We also analyzed the effect of different software calibration approaches on accuracy of the sensors.},
	language = {en},
	number = {4},
	urldate = {2023-06-23},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Kessler, G. Drew and Hodges, Larry F. and Walker, Neff},
	month = dec,
	year = {1995},
	note = {218 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {hardware:cyberglove},
	pages = {263--283},
}

@article{li_wifinger_2016,
	title = {{WiFinger}: talk to your smart devices with finger-grained gesture},
	shorttitle = {{WiFinger}},
	url = {https://dl.acm.org/doi/10.1145/2971648.2971738},
	doi = {10.1145/2971648.2971738},
	abstract = {In recent literatures, WiFi signals have been widely used to "sense" people's locations and activities. Researchers have exploited the characteristics of wireless signals to "hear" people's talk and "see" keystrokes by human users. Inspired by the excellent work of relevant scholars, we turn to explore the field of human-computer interaction using finger-grained gestures under WiFi environment. In this paper, we present Wi-Finger - the first solution using ubiquitous wireless signals to achieve number text input in WiFi devices. We implement a prototype of WiFinger on a commercial Wi-Fi infrastructure. Our scheme is based on the key intuition that while performing a certain gesture, the fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time series of Channel State Information (CSI) values. WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices. As the results show, WiFinger achieves up to 90.4\% average classification accuracy for recognizing 9 digits finger-grained gestures from American Sign Language (ASL), and its average accuracy for single individual number text input in desktop reaches 82.67\% within 90 digits.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
	author = {Li, Hong and Yang, Wei and Wang, Jianxin and Xu, Yang and Huang, Liusheng},
	month = sep,
	year = {2016},
	note = {230 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
ISBN: 9781450344616
Place: Heidelberg Germany
Publisher: ACM},
	keywords = {tech:wifi, app:sign-language, read-priority-3, app:american-sl},
	pages = {250--261},
	file = {Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf:/Users/brk/Zotero/storage/DT2Z2XQS/Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf:application/pdf},
}

@article{pu_whole-home_2013,
	title = {Whole-home gesture recognition using wireless signals},
	url = {http://dl.acm.org/citation.cfm?doid=2500423.2500436},
	doi = {10.1145/2500423.2500436},
	abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two- bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94\%.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 19th annual international conference on Mobile computing \& networking - MobiCom '13},
	author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
	year = {2013},
	note = {1010 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: the 19th annual international conference
ISBN: 9781450319997
Place: Miami, Florida, USA
Publisher: ACM Press},
	keywords = {tech:wifi, classes:{\textless}10},
	pages = {27},
	file = {Submitted Version:/Users/brk/Zotero/storage/DRDJ9SJ8/Pu et al. - 2013 - Whole-home gesture recognition using wireless sign.pdf:application/pdf},
}

@article{abdelnasser_wigest_2014,
	title = {Wigest: {A} {Ubiquitous} {Wifi}-based {Gesture} {Recognition} {System}},
	shorttitle = {Wigest},
	url = {https://www.qscience.com/content/papers/10.5339/qfarc.2014.ITPP1127},
	doi = {10.5339/qfarc.2014.ITPP1127},
	abstract = {We present WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device. Compared to related work, WiGest is unique in using standard WiFi equipment, with no modifications, and no training for gesture recognition. The system identifies different signal change primitives, from which we construct mutually independent gesture families. These families can be mapped to distinguishable application actions. We address various challenges including cleaning the noisy signals, gesture type and attributes detection, reducing false positives due to interfering humans, and adapting to changing signal polarity. We implement a proof-of-concept prototype using off-the-shelf laptops and extensively evaluate the system in both an office environment and a typical apartment with standard WiFi access points. Our results show that WiGest detects the basic primitives with an accuracy of 87.5\% using a single AP only, including through-the-wall non-line-of-sight scenarios. This accuracy increases to 96\% using three overheard APs. In addition, when evaluating the system using a multi-media player application, we achieve a classification accuracy of 96\%. This accuracy is robust to the presence of other interfering humans, highlighting WiGest's ability to enable future ubiquitous hands-free gesture-based interaction with mobile devices.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Qatar Foundation Annual Research Conference Proceedings Volume 2014 Issue 1},
	author = {Abdelnasser, Heba and Harras, Khaled and Youssef, Moustafa},
	year = {2014},
	note = {415 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Qatar Foundation Annual Research Conference Proceedings
Place: Qatar National Convention Center (QNCC), Doha, Qatar,
Publisher: Hamad bin Khalifa University Press (HBKU Press)},
	keywords = {tech:wifi},
	file = {Submitted Version:/Users/brk/Zotero/storage/J79GHKIF/Abdelnasser et al. - 2014 - Wigest A Ubiquitous Wifi-based Gesture Recognitio.pdf:application/pdf},
}

@inproceedings{laviola_survey_1999,
	title = {A {Survey} of {Hand} {Posture} and {Gesture} {Recognition} {Techniques} and {Technology}},
	url = {https://www.semanticscholar.org/paper/A-Survey-of-Hand-Posture-and-Gesture-Recognition-LaViola/856d4bf0f1f5d4480ce3115d828f34d4b2782e1c},
	abstract = {This paper surveys the use of hand postures and gestures as a mechanism for interaction with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based interaction. First, the technological requirements and limitations for using hand postures and gestures are described by discussing both glove-based and vision-based recognition systems along with advantages and disadvantages of each. Second, the various types of techniques used in recognizing hand postures and gestures are compared and contrasted. Third, the applications that have used hand posture and gesture interfaces are examined. The survey concludes with a summary and a discussion of future research directions.},
	urldate = {2023-06-23},
	author = {LaViola, Jr Joseph J.},
	month = jun,
	year = {1999},
	note = {253 Citations},
	keywords = {read-priority-3, type:survey},
	file = {LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf:/Users/brk/Zotero/storage/M8XQ3QQL/LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf:application/pdf},
}

@article{harrison_omnitouch_2011,
	title = {{OmniTouch}: wearable multitouch interaction everywhere},
	shorttitle = {{OmniTouch}},
	url = {https://dl.acm.org/doi/10.1145/2047196.2047255},
	doi = {10.1145/2047196.2047255},
	abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
	language = {en},
	urldate = {2023-06-23},
	journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
	author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
	month = oct,
	year = {2011},
	note = {587 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: UIST '11: The 24th Annual ACM Symposium on User Interface Software and Technology
ISBN: 9781450307161
Place: Santa Barbara California USA
Publisher: ACM},
	keywords = {tech:rgb, hardware:unique},
	pages = {441--450},
}

@article{dipietro_survey_2008,
	title = {A {Survey} of {Glove}-{Based} {Systems} and {Their} {Applications}},
	volume = {38},
	issn = {1094-6977, 1558-2442},
	url = {http://ieeexplore.ieee.org/document/4539650/},
	doi = {10.1109/TSMCC.2008.923862},
	abstract = {Hand movement data acquisition is used in many engineering applications ranging from the analysis of gestures to the biomedical sciences. Glove-based systems represent one of the most important efforts aimed at acquiring hand movement data. While they have been around for over three decades, they keep attracting the interest of researchers from increasingly diverse fields. This paper surveys such glove systems and their applications. It also analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research. A foremost goal of this paper is to provide readers who are new to the area with a basis for understanding glove systems technology and how it can be applied, while offering specialists an updated picture of the breadth of applications in several engineering and biomedical sciences areas.},
	number = {4},
	urldate = {2023-06-23},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Dipietro, L. and Sabatini, A.M. and Dario, P.},
	month = jul,
	year = {2008},
	note = {652 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {461--482},
	file = {Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf:/Users/brk/Zotero/storage/RWUPUG9F/Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf:application/pdf},
}

@inproceedings{madadi_occlusion_2017,
	title = {Occlusion {Aware} {Hand} {Pose} {Recovery} from {Sequences} of {Depth} {Images}},
	doi = {10.1109/FG.2017.37},
	abstract = {State-of-the-art approaches on hand pose estimation from depth images have reported promising results under quite controlled considerations. In this paper we propose a two-step pipeline for recovering the hand pose from a sequence of depth images. The pipeline has been designed to deal with images taken from any viewpoint and exhibiting a high degree of finger occlusion. In a first step we initialize the hand pose using a part-based model, fitting a set of hand components in the depth images. In a second step we consider temporal data and estimate the parameters of a trained bilinear model consisting of shape and trajectory bases. Results on a synthetic, highly-occluded dataset demonstrate that the proposed method outperforms most recent pose recovering approaches, including those based on CNNs.},
	booktitle = {2017 12th {IEEE} {International} {Conference} on {Automatic} {Face} \& {Gesture} {Recognition} ({FG} 2017)},
	author = {Madadi, Meysam and Escalera, Sergio and Carruesco, Alex and Andujar, Carlos and Baró, Xavier and Gonzàlez, Jordi},
	month = may,
	year = {2017},
	note = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, tech:rgbd},
	pages = {230--237},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/MKR4QCS5/7961746.html:text/html;Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf:/Users/brk/Zotero/storage/Y54S29Q8/Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf:application/pdf},
}

@article{guyon_chalearn_2014,
	title = {The {ChaLearn} gesture dataset ({CGD} 2011)},
	volume = {25},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-014-0596-3},
	doi = {10.1007/s00138-014-0596-3},
	abstract = {This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shot-learning. The data include 54,000 hand and arm gestures recorded with an RGB-D \$\${\textbackslash}hbox \{Kinect\}{\textasciicircum}{\textbackslash}mathrm\{TM\}\$\$camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8–12 gestures, recorded by the same user. Short continuous sequences of 1–5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user’s horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, datacollection software and the gesture vocabularies are downloadable from http://gesture.chalearn.org. We set up a forum for researchers working on these data http://groups.google.com/group/gesturechallenge.},
	language = {en},
	number = {8},
	urldate = {2023-06-23},
	journal = {Machine Vision and Applications},
	author = {Guyon, Isabelle and Athitsos, Vassilis and Jangyodsuk, Pat and Escalante, Hugo Jair},
	month = nov,
	year = {2014},
	note = {70 Citations},
	pages = {1929--1951},
}

@inproceedings{chai_two_2016,
	title = {Two streams {Recurrent} {Neural} {Networks} for {Large}-{Scale} {Continuous} {Gesture} {Recognition}},
	doi = {10.1109/ICPR.2016.7899603},
	abstract = {In this paper, we tackle the continuous gesture recognition problem with a two streams Recurrent Neural Networks (2S-RNN) for the RGB-D data input. In our framework, the spotting-recognition strategy is used, that means the continuous gestures are first segmented into separated gestures, and then each isolated gesture is recognized by using the 2S-RNN. Concretely, the gesture segmentation is based on the accurate hand positions provided by the hand detector trained from Faster R-CNN. While in the recognition module, 2S-RNN is designed to efficiently fuse multi-modal features, i.e. the RGB and depth channels. The experimental results on both the validation and test sets of the Continuous Gesture Dataset (ConGD) have shown promising performance of the proposed framework. We ranked 1st in the ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge with the mean Jaccard Index of 0.286915.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Chai, Xiujuan and Liu, Zhipeng and Yin, Fang and Liu, Zhuang and Chen, Xilin},
	month = dec,
	year = {2016},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, untagged},
	pages = {31--36},
}

@article{rautaray_vision_2015,
	title = {Vision based hand gesture recognition for human computer interaction: a survey},
	volume = {43},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Vision based hand gesture recognition for human computer interaction},
	url = {http://link.springer.com/10.1007/s10462-012-9356-9},
	doi = {10.1007/s10462-012-9356-9},
	abstract = {As computers become more pervasive in society, facilitating natural human–computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human–computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {Artificial Intelligence Review},
	author = {Rautaray, Siddharth S. and Agrawal, Anupam},
	month = jan,
	year = {2015},
	note = {1303 Citations},
	keywords = {have-read, type:survey},
	pages = {1--54},
	file = {Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf:/Users/brk/Zotero/storage/78UXQTNQ/Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf:application/pdf},
}

@article{al-shamayleh_systematic_2018,
	title = {A systematic literature review on vision based gesture recognition techniques},
	volume = {77},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-5971-z},
	doi = {10.1007/s11042-018-5971-z},
	abstract = {Human Computer Interaction (HCI) technologies are rapidly evolving the way we interact with computing devices and adapting to the constantly increasing demands of modern paradigms. One of the most useful tools in this regard is the integration of Human-to-Human Interaction gestures to facilitate communication and expressing ideas. Gesture recognition requires the integration of postures, gestures, face expressions and movements for communicating or conveying certain messages. The aim of this study is to aggregate and synthesize experiences and accumulated knowledge about Vision-Based Recognition (VBR) techniques. The major objective of conducting this Systematic Literature Review (SLR) is to highlight the state-of-the-art in the context of vision-based gesture recognition with specific focus on hand gesture recognition (HGR) techniques and enabling technologies. After a careful systematic selection process, 100 studies relevant to the four research questions were selected. This process was followed by data collection, a detailed analysis, and a synthesis of the selected studies. The results reveal that among the VBR techniques, HGR is a predominant and highly focused area of research. Research focus is also found to be converging towards sign language recognition. Potential applications of HGR techniques include desktop applications, smart environments, entertainment, sign language interpretation, virtual reality and gamification. Although various experimental research efforts have been devoted to gestures recognition, there are still numerous open issues and research challenges in this field. Lastly, considering the results from this SLR, potential future research directions are suggested, including a much needed focus on grammatical interpretation, hybrid approaches, smartphone devices, normalization, and real-life systems.},
	language = {en},
	number = {21},
	urldate = {2023-06-26},
	journal = {Multimedia Tools and Applications},
	author = {Al-Shamayleh, Ahmad Sami and Ahmad, Rodina and Abushariah, Mohammad A. M. and Alam, Khubaib Amjad and Jomhari, Nazean},
	month = nov,
	year = {2018},
	note = {71 Citations},
	keywords = {untagged, type:survey},
	pages = {28121--28184},
	file = {Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf:/Users/brk/Zotero/storage/BL4SNP7J/Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf:application/pdf},
}

@article{vuletic_systematic_2019,
	title = {Systematic literature review of hand gestures used in human computer interaction interfaces},
	volume = {129},
	issn = {10715819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581918305676},
	doi = {10.1016/j.ijhcs.2019.03.011},
	abstract = {Semantic Scholar extracted view of "Systematic literature review of hand gestures used in human computer interaction interfaces" by T. Vuletic et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {International Journal of Human-Computer Studies},
	author = {Vuletic, Tijana and Duffy, Alex and Hay, Laura and McTeague, Chris and Campbell, Gerard and Grealy, Madeleine},
	month = sep,
	year = {2019},
	note = {82 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {have-read, type:survey},
	pages = {74--94},
	file = {Accepted Version:/Users/brk/Zotero/storage/7VJ7GZXV/Vuletic et al. - 2019 - Systematic literature review of hand gestures used.pdf:application/pdf},
}

@article{cheok_review_2019,
	title = {A review of hand gesture and sign language recognition techniques},
	volume = {10},
	issn = {1868-8071, 1868-808X},
	url = {http://link.springer.com/10.1007/s13042-017-0705-5},
	doi = {10.1007/s13042-017-0705-5},
	abstract = {Hand gesture recognition serves as a key for overcoming many difficulties and providing convenience for human life. The ability of machines to understand human activities and their meaning can be utilized in a vast array of applications. One specific field of interest is sign language recognition. This paper provides a thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research. The techniques reviewed are suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification, where the various algorithms at each stage are elaborated and their merits compared. Further, we also discuss the challenges and limitations faced by gesture recognition research in general, as well as those exclusive to sign language recognition. Overall, it is hoped that the study may provide readers with a comprehensive introduction into the field of automated gesture and sign language recognition, and further facilitate future research efforts in this area.},
	language = {en},
	number = {1},
	urldate = {2023-06-26},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
	month = jan,
	year = {2019},
	note = {301 Citations},
	keywords = {type:survey},
	pages = {131--153},
	file = {Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf:/Users/brk/Zotero/storage/62H2TEM2/Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf:application/pdf},
}

@article{park_advanced_2019,
	title = {Advanced {Machine} {Learning} for {Gesture} {Learning} and {Recognition} {Based} on {Intelligent} {Big} {Data} of {Heterogeneous} {Sensors}},
	volume = {11},
	issn = {2073-8994},
	url = {https://www.mdpi.com/2073-8994/11/7/929},
	doi = {10.3390/sym11070929},
	abstract = {With intelligent big data, a variety of gesture-based recognition systems have been developed to enable intuitive interaction by utilizing machine learning algorithms. Realizing a high gesture recognition accuracy is crucial, and current systems learn extensive gestures in advance to augment their recognition accuracies. However, the process of accurately recognizing gestures relies on identifying and editing numerous gestures collected from the actual end users of the system. This final end-user learning component remains troublesome for most existing gesture recognition systems. This paper proposes a method that facilitates end-user gesture learning and recognition by improving the editing process applied on intelligent big data, which is collected through end-user gestures. The proposed method realizes the recognition of more complex and precise gestures by merging gestures collected from multiple sensors and processing them as a single gesture. To evaluate the proposed method, it was used in a shadow puppet performance that could interact with on-screen animations. An average gesture recognition rate of 90\% was achieved in the experimental evaluation, demonstrating the efficacy and intuitiveness of the proposed method for editing visualized learning gestures.},
	language = {en},
	number = {7},
	urldate = {2023-06-26},
	journal = {Symmetry},
	author = {Park, Jisun and Jin, Yong and Cho, Seoungjae and Sung, Yunsick and Cho, Kyungeun},
	month = jul,
	year = {2019},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {929},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ALFHQ8X4/Park et al. - 2019 - Advanced Machine Learning for Gesture Learning and.pdf:application/pdf},
}

@inproceedings{stephanidis_vision-based_2009,
	title = {Vision-{Based} {Hand} {Gesture} {Recognition} for {Human}-{Computer} {Interaction}},
	volume = {20091047},
	isbn = {978-0-8058-6280-5 978-1-4200-6499-5},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420064995-c34},
	doi = {10.1201/9781420064995-c34},
	abstract = {In recent years, research efforts seeking to provide more natural, human-centered means of interacting with computers have gained growing interest. A particularly important direction is that of perceptive user interfaces, where the computer is endowed with perceptive capabilities that allow it to acquire both implicit and explicit information about the user and the environment. Vision has the potential of carrying a wealth of information in a non-intrusive manner and at a low cost, therefore it constitutes a very attractive sensing modality for developing perceptive user interfaces. Proposed approaches for vision-driven interactive user interfaces resort to technologies such as head tracking, face and facial expression recognition, eye tracking and gesture recognition. In this paper, we focus our attention to vision-based recognition of hand gestures. The first part of the paper provides an overview of the current state of the art regarding the recognition of hand gestures as these are observed and recorded by typical video cameras. In order to make the review of the related literature tractable, this paper does not discuss:},
	language = {en},
	urldate = {2023-06-26},
	publisher = {CRC Press},
	author = {Zabulis, Xenophon and Baltzakis, Haris and Argyros, Antonis},
	editor = {Stephanidis, Constantine},
	month = jun,
	year = {2009},
	doi = {10.1201/9781420064995-c34},
	note = {171 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: The Universal Access Handbook
Series Title: Human Factors and Ergonomics},
	keywords = {type:survey},
	pages = {1--30},
	file = {Submitted Version:/Users/brk/Zotero/storage/24YMVRP3/Zabulis et al. - 2009 - Vision-Based Hand Gesture Recognition for Human-Co.pdf:application/pdf},
}

@article{supancic_depth-based_2015,
	title = {Depth-{Based} {Hand} {Pose} {Estimation}: {Data}, {Methods}, and {Challenges}},
	shorttitle = {Depth-{Based} {Hand} {Pose} {Estimation}},
	url = {http://ieeexplore.ieee.org/document/7410574/},
	doi = {10.1109/ICCV.2015.217},
	abstract = {Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.},
	urldate = {2023-06-26},
	journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
	author = {Supancic, James S. and Rogez, Gregory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
	month = dec,
	year = {2015},
	note = {155 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 IEEE International Conference on Computer Vision (ICCV)
ISBN: 9781467383912
Place: Santiago
Publisher: IEEE},
	keywords = {type:survey},
	pages = {1868--1876},
	file = {Submitted Version:/Users/brk/Zotero/storage/HAGU84CM/Supancic et al. - 2015 - Depth-Based Hand Pose Estimation Data, Methods, a.pdf:application/pdf},
}

@article{li_survey_2019,
	title = {A survey on {3D} hand pose estimation: {Cameras}, methods, and datasets},
	volume = {93},
	issn = {00313203},
	shorttitle = {A survey on {3D} hand pose estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320319301724},
	doi = {10.1016/j.patcog.2019.04.026},
	abstract = {Semantic Scholar extracted view of "A survey on 3D hand pose estimation: Cameras, methods, and datasets" by Rui Li et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {Pattern Recognition},
	author = {Li, Rui and Liu, Zhenyu and Tan, Jianrong},
	month = sep,
	year = {2019},
	note = {51 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {251--272},
}

@article{chatzis_comprehensive_2020,
	title = {A {Comprehensive} {Study} on {Deep} {Learning}-{Based} {3D} {Hand} {Pose} {Estimation} {Methods}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/19/6850},
	doi = {10.3390/app10196850},
	abstract = {The field of 3D hand pose estimation has been gaining a lot of attention recently, due to its significance in several applications that require human-computer interaction (HCI). The utilization of technological advances, such as cost-efficient depth cameras coupled with the explosive progress of Deep Neural Networks (DNNs), has led to a significant boost in the development of robust markerless 3D hand pose estimation methods. Nonetheless, finger occlusions and rapid motions still pose significant challenges to the accuracy of such methods. In this survey, we provide a comprehensive study of the most representative deep learning-based methods in literature and propose a new taxonomy heavily based on the input data modality, being RGB, depth, or multimodal information. Finally, we demonstrate results on the most popular RGB and depth-based datasets and discuss potential research directions in this rapidly growing field.},
	language = {en},
	number = {19},
	urldate = {2023-06-26},
	journal = {Applied Sciences},
	author = {Chatzis, Theocharis and Stergioulas, Andreas and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas and Daras, Petros},
	month = sep,
	year = {2020},
	note = {25 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgbd, model:deep-learning, read-priority-9, type:survey},
	pages = {6850},
	file = {Full Text PDF:/Users/brk/Zotero/storage/5NQQ3Z7T/Chatzis et al. - 2020 - A Comprehensive Study on Deep Learning-Based 3D Ha.pdf:application/pdf},
}

@article{rashid_wearable_2019,
	title = {Wearable technologies for hand joints monitoring for rehabilitation: {A} survey},
	volume = {88},
	issn = {00262692},
	shorttitle = {Wearable technologies for hand joints monitoring for rehabilitation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0026269217305207},
	doi = {10.1016/j.mejo.2018.01.014},
	abstract = {Semantic Scholar extracted view of "Wearable technologies for hand joints monitoring for rehabilitation: A survey" by Adnan Rashid et al.},
	language = {en},
	urldate = {2023-06-26},
	journal = {Microelectronics Journal},
	author = {Rashid, Adnan and Hasan, Osman},
	month = jun,
	year = {2019},
	note = {58 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:survey},
	pages = {173--183},
}

@misc{sturman_whole-hand_1992,
	title = {Whole-hand {Input}},
	url = {https://scholar.googleusercontent.com/scholar?q=cache:Yk_AfzsiGZsJ:scholar.google.com/+sturman+whole+hand+input&hl=en&as_sdt=0,5},
	abstract = {This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc},
	urldate = {2023-06-28},
	author = {Sturman, David Joel},
	year = {1992},
	note = {@phdthesis\{\vphantom{\}}10.5555/143615,
author = \{Sturman, David Joel\},
title = \{Whole-Hand Input\},
year = \{1992\},
publisher = \{Massachusetts Institute of Technology\},
address = \{USA\},
abstract = \{This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc.\},
note = \{Not available from Univ. Microfilms Int.\}
\vphantom{\{}\}},
	keywords = {read-priority-5, type:seminal},
	file = {Whole-hand Input:/Users/brk/Zotero/storage/RG78DTFU/scholar.html:text/html;Whole-hand Input.pdf:/Users/brk/Zotero/storage/I4VQXME4/Whole-hand Input.pdf:application/pdf},
}

@inproceedings{kolsch_keyboards_2002,
	title = {Keyboards without {Keyboards}: {A} {Survey} of {Virtual} {Keyboards}},
	shorttitle = {Keyboards without {Keyboards}},
	url = {https://www.semanticscholar.org/paper/Keyboards-without-Keyboards%3A-A-Survey-of-Virtual-K%C3%B6lsch-Turk/44355bea0b025ba8719a946ba6c009e0eba133f3},
	abstract = {Input to small devices is becoming an increasingly crucial factor in development for the ever-more powerful embedded market. Speech input promises to become a feasible alternative to tiny keypads, yet its limited reliability, robustness, and flexibility render it unsuitable for certain tasks and/or environments. Various attempts have been made to provide the common keyboard metaphor without the physical keyboard, to build “virtual keyboards”. This promises to leverage our familiarity with the device without incurring the constraints of the bulky physics. This paper surveys technologies for alphanumeric input devices and methods with a strong focus on touch-typing. We analyze the characteristics of the keyboard modality and show how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
	urldate = {2023-06-29},
	author = {Kölsch, M. and Turk, M.},
	year = {2002},
	keywords = {read-priority-1, read-priority-3, type:survey},
	file = {Full Text PDF:/Users/brk/Zotero/storage/MB8CKX7D/Kölsch and Turk - 2002 - Keyboards without Keyboards A Survey of Virtual K.pdf:application/pdf},
}

@article{xu_hand_2006,
	title = {Hand {Gesture} {Interaction} for {Virtual} {Training} of {SPG}},
	url = {https://ieeexplore.ieee.org/document/4089336/},
	doi = {10.1109/ICAT.2006.68},
	abstract = {We develop a virtual reality based driving training system of self-propelled gun (SPG). In order to make the interface of the system more powerful and natural, hand gesture interaction need to be incorporated into the system's interface. This paper discusses the use of hand gestures for interaction with the virtual training environment. We employ static hand gestures which coupled with hand translations and rotations as the method of interacting with the virtual training environment. An 18-sensor data glove is chosen for monitoring the movements of the fingers and the wrist. The feed-forward neural network is developed for recognizing gestures for use in virtual training application of artillery self-propelled gun (SPG). We present our approach for the algorithm design and implementation, and the use of the gestures in our application. The presented hand gesture interaction method can be effectively used in our virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way},
	urldate = {2023-07-02},
	journal = {16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)},
	author = {Xu, Deyou and Yao, Wuyun and Zhang, Yongliang},
	month = nov,
	year = {2006},
	note = {14 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 16th International Conference on Artificial Reality and Telexistence-Workshops (ICAT'06)
ISBN: 9780769527543
Place: Hangzhou
Publisher: IEEE},
	keywords = {model:ffnn, movement:static},
	pages = {672--676},
}

@article{damasio_animating_2002,
	title = {Animating virtual humans using hand postures},
	url = {http://ieeexplore.ieee.org/document/1167207/},
	doi = {10.1109/SIBGRA.2002.1167207},
	abstract = {Interaction between human and computer has been used in a large scale in computer graphics and virtual reality. This paper presents a system to provide interaction between user and virtual humans. The system uses a data glove and an artificial neural network system responsible for the recognition of hand postures.},
	urldate = {2023-07-02},
	journal = {Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing},
	author = {Damasio, F.W. and Musse, S.R.},
	year = {2002},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 15th Brazilian Symposium on Computer Graphics and Image Processing
ISBN: 9780769518466
Place: Fortaleza-CE, Brazil
Publisher: IEEE Comput. Soc},
	keywords = {model:ffnn, movement:static},
	pages = {437},
}

@article{murakami_gesture_1991,
	title = {Gesture recognition using recurrent neural networks},
	url = {http://portal.acm.org/citation.cfm?doid=108844.108900},
	doi = {10.1145/108844.108900},
	abstract = {A gesture recognition method for Japanese sign language is presented. We have developed a posture recognition system using neural networks which could recognize a finger alphabet of 42 symbols. We then developed a gesture recognition system where each gesture specifies a word. Gesture recognition is more difficult than posture recognition because it has to handle dynamic processes. To deal with dynamic processes we use a recurrent neural network. Here, we describe a gesture recognition method which can recognize continuous gesture. We then discuss the results of our research.},
	language = {en},
	urldate = {2023-07-02},
	journal = {Proceedings of the SIGCHI conference on Human factors in computing systems Reaching through technology - CHI '91},
	author = {Murakami, Kouichi and Taguchi, Hitomi},
	year = {1991},
	note = {405 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: the SIGCHI conference
ISBN: 9780897913836
Place: New Orleans, Louisiana, United States
Publisher: ACM Press},
	keywords = {movement:static, model:rnn},
	pages = {237--242},
}

@article{mehdi_sign_2002,
	title = {Sign language recognition using sensor gloves},
	url = {http://ieeexplore.ieee.org/document/1201884/},
	doi = {10.1109/ICONIP.2002.1201884},
	abstract = {This paper examines the possibility of recognizing sign language gestures using sensor gloves. Previously sensor gloves are used in games or in applications with custom gestures. This paper explores their use in Sign Language recognition. This is done by implementing a project called "Talking Hands", and studying the results. The project uses a sensor glove to capture the signs of American Sign Language performed by a user and translates them into sentences of English language. Artificial neural networks are used to recognize the sensor values coming from the sensor glove. These values are then categorized in 24 alphabets of English language and two punctuation symbols introduced by the author. So, mute people can write complete sentences using this application.},
	urldate = {2023-07-02},
	journal = {Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.},
	author = {Mehdi, S.A. and Khan, Y.N.},
	year = {2002},
	note = {151 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 9th International Conference on Neural Information Processing
ISBN: 9789810475246
Place: Singapore
Publisher: IEEE},
	keywords = {model:ffnn, movement:static},
	pages = {2204--2206 vol.5},
}

@article{jong-sung_kim_dynamic_1996,
	title = {A dynamic gesture recognition system for the {Korean} sign language ({KSL})},
	volume = {26},
	issn = {10834419},
	url = {http://ieeexplore.ieee.org/document/485888/},
	doi = {10.1109/3477.485888},
	abstract = {The sign language is a method of communication for the deaf-mute. Articulated gestures and postures of hands and fingers are commonly used for the sign language. This paper presents a system which recognizes the Korean sign language (KSL) and translates into a normal Korean text. A pair of data-gloves are used as the sensing device for detecting motions of hands and fingers. For efficient recognition of gestures and postures, a technique of efficient classification of motions is proposed and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
	number = {2},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
	author = {{Jong-Sung Kim} and {Won Jang} and {Zeungnam Bien}},
	month = apr,
	year = {1996},
	note = {192 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, model:ffnn, movement:dynamic, app:korean-sl},
	pages = {354--359},
}

@article{kadous_grasp_1995,
	title = {{GRASP}: {Recognition} of {Australian} {Sign} {Language} {Using} {Instrumented} {Gloves}},
	shorttitle = {{GRASP}},
	abstract = {Instrumented gloves -- gloves equipped with sensors for detecting finger bend, hand position and orientation -- were conceived to allow a more natural interface to computers. However, the extension of their use for recognising sign language, and in this case Auslan (Australian Sign Language), is possible. Several researchers have already explored these possibilities and have successfully achieved finger-spelling recognition with high levels of accuracy, but progress in the recognition of sign language as a whole has been limited.},
	author = {Kadous, Mohammed},
	month = dec,
	year = {1995},
	keywords = {model:hmm, app:sign-language, app:australian-sl, model:nn, untagged, model:instance-based-learning, from:cite.bib},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ZVYPR4WQ/Kadous - 1995 - GRASP Recognition of Australian Sign Language Usi.pdf:application/pdf},
}

@inproceedings{vamplew_recognition_2007,
	title = {Recognition of sign language gestures using neural networks},
	url = {http://www.ledonline.it/NeuropsychologicalTrends/},
	doi = {10.7358/neur-2007-001-vamp},
	abstract = {This paper describes the structure and performance of the SLARTI sign language recognition system developed at the University of Tasmania. SLARTI uses a modular architecture consisting of multiple feature-recognition neural networks and a nearest-neighbour classifier to recognise Australian sign language (Auslan) hand gestures.},
	urldate = {2023-07-02},
	booktitle = {Neuropsychological {Trends}},
	author = {Vamplew, Simon},
	month = apr,
	year = {2007},
	note = {93 citations (Semantic Scholar/DOI) [2023-07-02]
ISSN: 1970321X, 19703201
Issue: 1},
	keywords = {model:knn, app:sign-language, model:ffnn, movement:dynamic, app:australian-sl},
	pages = {4},
}

@inproceedings{naidoo_south_2010,
	title = {South {African} sign language recognition using feature vectors and {Hidden} {Markov} {Models}},
	url = {https://www.semanticscholar.org/paper/South-African-sign-language-recognition-using-and-Naidoo/be177e5e05b402da3bd53db0ba8445f87596f7d3},
	abstract = {This thesis presents a system for performing whole gesture recognition for South African Sign Language. The system uses feature vectors combined with Hidden Markov models. In order to constuct a feature vector, dynamic segmentation must occur to extract the signer’s hand movements. Techniques and methods for normalising variations that occur when recording a signer performing a gesture, are investigated. The system has a classification rate of 69\%.},
	urldate = {2023-07-02},
	author = {Naidoo, Nathan Lyle},
	year = {2010},
	keywords = {model:hmm},
	file = {Naidoo - 2010 - South African sign language recognition using feat.pdf:/Users/brk/Zotero/storage/JX268W5D/Naidoo - 2010 - South African sign language recognition using feat.pdf:application/pdf},
}

@inproceedings{frieslaar_robust_2014,
	title = {Robust {South} {African} sign language gesture recognition using hand motion and shape},
	url = {https://www.semanticscholar.org/paper/Robust-South-African-sign-language-gesture-using-Frieslaar/af10d1bb6ef67ab9c8b624b5fcafd9dd2e2daf69},
	abstract = {Research has shown that five fundamental parameters are required to recognize any sign language gesture: hand shape, hand motion, hand location, hand orientation and facial expressions. The South African Sign Language (SASL) research group at the University of the Western Cape (UWC) has created several systems to recognize sign language gestures using single parameters. These systems are, however, limited to a vocabulary size of 20 – 23 signs, beyond which the recognition accuracy is expected to decrease. The first aim of this research is to investigate the use of two parameters – hand motion and hand shape – to recognise a larger vocabulary of SASL gestures at a high accuracy. Also, the majority of related work in the field of sign language gesture recognition using these two parameters makes use of Hidden Markov Models (HMMs) to classify gestures. Hidden Markov Support Vector Machines (HM-SVMs) are a relatively new technique that make use of Support Vector Machines (SVMs) to simulate the functions of HMMs. Research indicates that HM-SVMs may perform better than HMMs in some applications. To our knowledge, they have not been applied to the field of sign language gesture recognition. This research compares the use of these two techniques in the context of SASL gesture recognition. The results indicate that, using two parameters results in a 15\% increase in accuracy over the use of a single parameter. Also, it is shown that HM-SVMs are a more accurate technique than HMMs, generally performing better or at least as good as HMMs.},
	urldate = {2023-07-02},
	author = {Frieslaar, I.},
	year = {2014},
	note = {6 Citations},
	keywords = {model:svm, model:hmm},
	file = {Full Text PDF:/Users/brk/Zotero/storage/U9U3CGTS/Frieslaar - 2014 - Robust South African sign language gesture recogni.pdf:application/pdf},
}

@article{gao_sign_2000,
	title = {{SIGN} {LANGUAGE} {RECOGNITION} {BASED} {ON} {HMM}/{ANN}/{DP}},
	volume = {14},
	issn = {0218-0014, 1793-6381},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001400000386},
	doi = {10.1142/S0218001400000386},
	abstract = {In this paper, a system designed for helping the deaf to communicate with others is presented. Some useful new ideas are proposed in design and implementation. An algorithm based on geometrical analysis for the purpose of extracting invariant feature to signer position is presented. An ANN–DP combined approach is employed for segmenting subwords automatically from the data stream of sign signals. To tackle the epenthesis movement problem, a DP-based method has been used to obtain the context-dependent models. Some techniques for system implementation are also given, including fast matching, frame prediction and search algorithms. The implemented system is able to recognize continuous large vocabulary Chinese Sign Language. Experiments show that proposed techniques in this paper are efficient on either recognition speed or recognition performance.},
	language = {en},
	number = {05},
	urldate = {2023-07-02},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Gao, Wen and Ma, Jiyong and Wu, Jiangqin and Wang, Chunli},
	month = aug,
	year = {2000},
	note = {99 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, model:ffnn},
	pages = {587--602},
}

@inproceedings{goos_real-time_2002,
	address = {Berlin, Heidelberg},
	title = {A {Real}-{Time} {Large} {Vocabulary} {Recognition} {System} for {Chinese} {Sign} {Language}},
	volume = {2298},
	isbn = {978-3-540-43678-2 978-3-540-47873-7},
	url = {http://link.springer.com/10.1007/3-540-47873-6_9},
	doi = {10.1007/3-540-47873-6_9},
	abstract = {The major challenge that faces Sign Language recognition now is to develop methods that will scale well with increasing vocabulary size. In this paper, a real-time system designed for recognizing Chinese Sign Language (CSL) signs with a 5100 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. An algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed. Then the worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including modifying the transferring probability, clustering the Gaussians and fast matching algorithm. Experiments show that techniques proposed in this paper are efficient on either recognition speed or recognition performance.},
	urldate = {2023-07-02},
	publisher = {Springer Berlin Heidelberg},
	author = {Chunli, Wang and Wen, Gao and Jiyong, Ma},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Sowa, Timo},
	year = {2002},
	doi = {10.1007/3-540-47873-6_9},
	note = {38 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Gesture and Sign Language in Human-Computer Interaction
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, app:sign-language},
	pages = {86--95},
}

@inproceedings{zhang_vision-based_2004,
	address = {State College PA USA},
	title = {A vision-based sign language recognition system using tied-mixture density {HMM}},
	isbn = {978-1-58113-995-2},
	url = {https://dl.acm.org/doi/10.1145/1027933.1027967},
	doi = {10.1145/1027933.1027967},
	abstract = {In this paper, a vision-based medium vocabulary Chinese sign language recognition (SLR) system is presented. The proposed recognition system consists of two modules. In the first module, techniques of robust hands detection, background subtraction and pupils detection are efficiently combined to precisely extract the feature information with the aid of simple colored gloves in the unconstrained environment. Meanwhile, an effective and efficient hierarchical feature description scheme with different scale features to characterize sign language is proposed, where principal component analysis (PCA) is employed to characterize the finger features more elaborately. In the second part, a Tied-Mixture Density Hidden Markov Models (TMDHMM) framework for SLR is proposed, which can speed up the recognition without the significant loss of recognition accuracy compared with the continuous hidden Markov models (CHMM). Experimental results based on 439 frequently used Chinese sign language (CSL) words show that the proposed methods can work well for the medium vocabulary SLR in the environment without special constraints and the recognition accuracy is up to 92.5\%.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the 6th international conference on {Multimodal} interfaces},
	publisher = {ACM},
	author = {Zhang, Liang-Guo and Chen, Yiqiang and Fang, Gaolin and Chen, Xilin and Gao, Wen},
	month = oct,
	year = {2004},
	note = {69 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language},
	pages = {198--204},
}

@inproceedings{goos_real-time_2001,
	address = {Berlin, Heidelberg},
	title = {A {Real}-{Time} {Large} {Vocabulary} {Continuous} {Recognition} {System} for {Chinese} {Sign} {Language}},
	volume = {2195},
	isbn = {978-3-540-42680-6 978-3-540-45453-3},
	url = {http://link.springer.com/10.1007/3-540-45453-5_20},
	doi = {10.1007/3-540-45453-5_20},
	abstract = {In this paper, a real-time system designed for recognizing continuous Chinese Sign Language (CSL) sentences with a 4800 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. The worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including states tying, still frame detecting and fast search algorithm. Experiments were carried out, and for real-time continuous sign recognition, the correct rate is over 90\%.},
	urldate = {2023-07-02},
	publisher = {Springer Berlin Heidelberg},
	author = {Wang, Chunli and Gao, Wen and Xuan, Zhaoguo},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Shum, Heung-Yeung and Liao, Mark and Chang, Shih-Fu},
	year = {2001},
	doi = {10.1007/3-540-45453-5_20},
	note = {31 citations (Semantic Scholar/DOI) [2023-07-02]
Book Title: Advances in Multimedia Information Processing — PCM 2001
Series Title: Lecture Notes in Computer Science},
	keywords = {model:hmm, app:sign-language, read-priority-1, classes:1k-5k, app:chinese-sl},
	pages = {150--157},
	file = {Wang et al. - 2001 - A Real-Time Large Vocabulary Continuous Recognitio.pdf:/Users/brk/Zotero/storage/QZ3Y7L3J/Wang et al. - 2001 - A Real-Time Large Vocabulary Continuous Recognitio.pdf:application/pdf},
}

@article{gao_chinese_2004,
	title = {A {Chinese} sign language recognition system based on {SOFM}/{SRN}/{HMM}},
	volume = {37},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320304001657},
	doi = {10.1016/S0031-3203(04)00165-7},
	abstract = {Semantic Scholar extracted view of "A Chinese sign language recognition system based on SOFM/SRN/HMM" by Wen Gao et al.},
	language = {en},
	number = {12},
	urldate = {2023-07-02},
	journal = {Pattern Recognition},
	author = {Gao, W and Fang, G and Zhao, D and Chen, Y},
	month = dec,
	year = {2004},
	note = {128 citations},
	keywords = {model:hmm, app:sign-language, app:chinese-sl, model:som},
	pages = {2389--2402},
}

@article{fatmi_comparing_2019,
	title = {Comparing {ANN}, {SVM}, and {HMM} based {Machine} {Learning} {Methods} for {American} {Sign} {Language} {Recognition} using {Wearable} {Motion} {Sensors}},
	url = {https://ieeexplore.ieee.org/document/8666491/},
	doi = {10.1109/CCWC.2019.8666491},
	abstract = {Millions of people with speech and hearing impairments, worldwide, communicate through sign languages every day. In the same way that voice recognition provides a simple communication platform for most users, gesture recognition is a natural means of correspondence for the hearing-impaired. In this paper, we explore the problem of translating/converting sign language to speech, and propose an improved solution using different machine learning techniques. We seek to build a system that can be employed in the daily lives of people with hearing impairments, in order to enhance communication and collaboration between the hearing-impaired community and those untrained in American Sign Language (ASL). The system architecture is based on using wearable motion sensors and machine learning techniques. In this study, we propose a solution using Artificial Neural Networks (ANN) and Support Vector Machines (SVM), and compare their accuracy with the Hidden Markov Model (HMM) results from our previous work to recognize ASL words. Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.},
	urldate = {2023-07-02},
	journal = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
	author = {Fatmi, Rabeet and Rashad, Sherif and Integlia, Ryan},
	month = jan,
	year = {2019},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)
ISBN: 9781728105543
Place: Las Vegas, NV, USA
Publisher: IEEE},
	keywords = {model:svm, model:hmm, app:sign-language, model:ffnn, app:american-sl},
	pages = {0290--0297},
}

@inproceedings{nel_integrated_2013,
	address = {East London South Africa},
	title = {An integrated sign language recognition system},
	isbn = {978-1-4503-2112-9},
	url = {https://dl.acm.org/doi/10.1145/2513456.2513491},
	doi = {10.1145/2513456.2513491},
	abstract = {The South African Sign Language research group at the University of the Western Cape has created several systems to recognize Sign Language gestures using single parameters. Research has shown that five parameters are required to recognize any sign language gesture: hand shape, location, orientation and motion, as well as facial expressions. Using a single parameter can cause conflicts in recognition of signs that are similarly signed. This paper pioneers research at the group towards combining multiple parameters to better distinguish between similar signs. This eventually aims to enable the recognition of a large SASL vocabulary. The proposed methodology combines hand location and hand shape recognition into one combined recognition system. The recognition approach is applied to 12 SASL signs that consist of six pairs of signs with the same hand shape performed at two different locations. It is shown that the approach is able to achieve a high average recognition accuracy of 79\% across all signs and distinguish between the signs effectively. It is also shown to be robust to variations in test subjects.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the {South} {African} {Institute} for {Computer} {Scientists} and {Information} {Technologists} {Conference}},
	publisher = {ACM},
	author = {Nel, Warren and Ghaziasgar, Mehrdad and Connan, James},
	month = oct,
	year = {2013},
	note = {7 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:south-african-sl},
	pages = {179--185},
	file = {Submitted Version:/Users/brk/Zotero/storage/R5QLW7FJ/Nel et al. - 2013 - An integrated sign language recognition system.pdf:application/pdf},
}

@article{mejia-perez_automatic_2022,
	title = {Automatic {Recognition} of {Mexican} {Sign} {Language} {Using} a {Depth} {Camera} and {Recurrent} {Neural} {Networks}},
	volume = {12},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/11/5523},
	doi = {10.3390/app12115523},
	abstract = {Automatic sign language recognition is a challenging task in machine learning and computer vision. Most works have focused on recognizing sign language using hand gestures only. However, body motion and facial gestures play an essential role in sign language interaction. Taking this into account, we introduce an automatic sign language recognition system based on multiple gestures, including hands, body, and face. We used a depth camera (OAK-D) to obtain the 3D coordinates of the motions and recurrent neural networks for classification. We compare multiple model architectures based on recurrent networks such as Long Short-Term Memories (LSTM) and Gated Recurrent Units (GRU) and develop a noise-robust approach. For this work, we collected a dataset of 3000 samples from 30 different signs of the Mexican Sign Language (MSL) containing features coordinates from the face, body, and hands in 3D spatial coordinates. After extensive evaluation and ablation studies, our best model obtained an accuracy of 97\% on clean test data and 90\% on highly noisy data.},
	language = {en},
	number = {11},
	urldate = {2023-07-02},
	journal = {Applied Sciences},
	author = {Mejía-Peréz, Kenneth and Córdova-Esparza, Diana-Margarita and Terven, Juan and Herrera-Navarro, Ana-Marcela and García-Ramírez, Teresa and Ramírez-Pedraza, Alfonso},
	month = may,
	year = {2022},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:mexican-sl, classes:30-50, model:gru, model:lstm},
	pages = {5523},
	file = {Full Text:/Users/brk/Zotero/storage/BVMX8PCK/Mejía-Peréz et al. - 2022 - Automatic Recognition of Mexican Sign Language Usi.pdf:application/pdf},
}

@article{ismail_dynamic_2022,
	title = {Dynamic hand gesture recognition of {Arabic} sign language by using deep convolutional neural networks},
	volume = {25},
	issn = {2502-4760, 2502-4752},
	url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/26823},
	doi = {10.11591/ijeecs.v25.i2.pp952-962},
	abstract = {{\textless}p{\textgreater}In computer vision, one of the most difficult problems is human gestures in videos recognition Because of certain irrelevant environmental variables. This issue has been solved by using single deep networks to learn spatiotemporal characteristics from video data, and this approach is still insufficient to handle both problems at the same time. As a result, the researchers fused various models to allow for the effective collection of important shape information as well as precise spatiotemporal variation of gestures. In this study, we collected the dynamic dataset for twenty meaningful words of Arabic sign language (ArSL) using a Microsoft Kinect v2 camera. The recorded data included 7350 red, green, and blue (RGB) videos and 7350 depth videos. We proposed four deep neural networks models using 2D and 3D convolutional neural network (CNN) to cover all feature extraction methods and then passing these features to the recurrent neural network (RNN) for sequence classification. Long short-term memory (LSTM) and gated recurrent unit (GRU) are two types of using RNN. Also, the research included evaluation fusion techniques for several types of multiple models. The experiment results show the best multi-model for the dynamic dataset of the ArSL recognition achieved 100\% accuracy.{\textless}/p{\textgreater}},
	number = {2},
	urldate = {2023-07-02},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	author = {Ismail, Mohammad H. and Dawwd, Shefa A. and Ali, Fakhradeen H.},
	month = feb,
	year = {2022},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:cnn, app:sign-language, model:rnn, app:arabic-sl, model:gru, model:lstm},
	pages = {952},
	file = {Full Text:/Users/brk/Zotero/storage/UKX6R46H/Ismail et al. - 2022 - Dynamic hand gesture recognition of Arabic sign la.pdf:application/pdf},
}

@article{avola_exploiting_2019,
	title = {Exploiting {Recurrent} {Neural} {Networks} and {Leap} {Motion} {Controller} for the {Recognition} of {Sign} {Language} and {Semaphoric} {Hand} {Gestures}},
	volume = {21},
	issn = {1520-9210, 1941-0077},
	url = {https://ieeexplore.ieee.org/document/8410764/},
	doi = {10.1109/TMM.2018.2856094},
	abstract = {Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human–human communication and human–computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96\% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.},
	number = {1},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Multimedia},
	author = {Avola, Danilo and Bernardi, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano},
	month = jan,
	year = {2019},
	note = {108 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, model:rnn, app:american-sl},
	pages = {234--245},
	file = {Submitted Version:/Users/brk/Zotero/storage/SQ8WF2YE/Avola et al. - 2019 - Exploiting Recurrent Neural Networks and Leap Moti.pdf:application/pdf},
}

@inproceedings{liang_sign_1996,
	address = {Hong Kong},
	title = {A sign language recognition system using hidden markov model and context sensitive search},
	isbn = {978-0-89791-825-1},
	url = {http://dl.acm.org/citation.cfm?doid=3304181.3304194},
	doi = {10.1145/3304181.3304194},
	abstract = {Hand gesture is one of the most natural and expressive ways for the hearing impaired. However, because of the complexity of dynamic gestures, most researches are focused either on static gestures, postures, or a small set of dynamic gestures. As real-time recognition of a large set of dynamic gestures is considered, some efficient algorithms and models are needed. To solve this problem in Taiwanese Sign Language, a statistics based context sensitive model is presented and both gestures and postures can be successfully recognized. A gesture is decomposed as a sequence of postures and the postures can be quickly recognized using hidden Markov model. With the probability resulted from hidden Markov model and the probability of each gesture in a lexicon, a gesture can be easily recognized in a linguistic way in real-time.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology} - {VRST} '96},
	publisher = {ACM Press},
	author = {Liang, Rung-Huei and Ouhyoung, Ming},
	year = {1996},
	note = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:sign-language, app:taiwanese-sl},
	pages = {59--66},
}

@article{moni_hmm_2009,
	title = {{HMM} based hand gesture recognition: {A} review on techniques and approaches},
	shorttitle = {{HMM} based hand gesture recognition},
	url = {http://ieeexplore.ieee.org/document/5234536/},
	doi = {10.1109/ICCSIT.2009.5234536},
	abstract = {Gesture is one of the most natural and expressive ways of communications between human and computer in a virtual reality system. We naturally use various gestures to express our own intentions in everyday life. Hand gesture is one of the important methods of non-verbal communication for human beings for its freer in movements and much more expressive than any other body parts. Hand gesture recognition has a number of potential applications in human-computer interaction, machine vision, virtual reality, machine control in industry, and so on. As a gesture is a continuous motion on a sequential time series, the HMMs (Hidden Markov Models) must be a prominent recognition tool. The most important thing in hand gesture recognition is what the input features are that best represent the characteristics of the moving hand gesture.This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.},
	urldate = {2023-07-02},
	journal = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	author = {Moni, M. A. and Ali, A. B. M. Shawkat},
	year = {2009},
	note = {76 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2009 2nd IEEE International Conference on Computer Science and Information Technology
ISBN: 9781424445196
Place: Beijing, China
Publisher: IEEE},
	keywords = {model:hmm, read-priority-1, type:survey},
	pages = {433--437},
	file = {Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf:/Users/brk/Zotero/storage/8F5BHH48/Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf:application/pdf},
}

@article{rung-huei_liang_real-time_1998,
	title = {A real-time continuous gesture recognition system for sign language},
	url = {http://ieeexplore.ieee.org/document/671007/},
	doi = {10.1109/AFGR.1998.671007},
	abstract = {A large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a data glove. Sign language, which is usually known as a set of natural language with formal semantic definitions and syntactic rules, is a large set of hand gestures that are daily used to communicate with the hearing impaired. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to four parameters in a gesture: posture, position, orientation, and motion. The authors have implemented a prototype system with a lexicon of 250 vocabularies and collected 196 training sentences in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signer-dependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%,.},
	urldate = {2023-07-02},
	journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},
	author = {{Rung-Huei Liang} and {Ming Ouhyoung}},
	year = {1998},
	note = {498 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: Third IEEE International Conference on Automatic Face and Gesture Recognition
ISBN: 9780818683442
Place: Nara, Japan
Publisher: IEEE Comput. Soc},
	keywords = {model:hmm, app:sign-language, app:taiwanese-sl, classes:50-100},
	pages = {558--567},
}

@article{ong_automatic_2005,
	title = {Automatic {Sign} {Language} {Analysis}: {A} {Survey} and the {Future} beyond {Lexical} {Meaning}},
	volume = {27},
	issn = {0162-8828},
	shorttitle = {Automatic {Sign} {Language} {Analysis}},
	url = {http://ieeexplore.ieee.org/document/1432718/},
	doi = {10.1109/TPAMI.2005.112},
	abstract = {Research in automatic analysis of sign language has largely focused on recognizing the lexical (or citation) form of sign gestures as they appear in continuous signing, and developing algorithms that scale well to large vocabularies. However, successful recognition of lexical signs is not sufficient for a full understanding of sign language communication. Nonmanual signals and grammatical processes which result in systematic variations in sign appearance are integral aspects of this communication but have received comparatively little attention in the literature. In this survey, we examine data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures. These are discussed with respect to issues such as modeling transitions between signs in continuous signing, modeling inflectional processes, signer independence, and adaptation. We further examine works that attempt to analyze nonmanual signals and discuss issues related to integrating these with (hand) sign gestures. We also discuss the overall progress toward a true test of sign recognition systems--dealing with natural signing by native signers. We suggest some future directions for this research and also point to contributions it can make to other fields of research. Web-based supplemental materials (appendicies) which contain several illustrative examples and videos of signing can be found at www.computer.org/publications/dlib.},
	language = {en},
	number = {6},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ong, S.C.W. and Ranganath, S.},
	month = jun,
	year = {2005},
	note = {595 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, type:survey},
	pages = {873--891},
	file = {Submitted Version:/Users/brk/Zotero/storage/7IUZNIGX/Ong and Ranganath - 2005 - Automatic Sign Language Analysis A Survey and the.pdf:application/pdf},
}

@article{sagayam_hand_2017,
	title = {Hand posture and gesture recognition techniques for virtual reality applications: a survey},
	volume = {21},
	issn = {1359-4338, 1434-9957},
	shorttitle = {Hand posture and gesture recognition techniques for virtual reality applications},
	url = {http://link.springer.com/10.1007/s10055-016-0301-0},
	doi = {10.1007/s10055-016-0301-0},
	abstract = {Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.},
	language = {en},
	number = {2},
	urldate = {2023-07-02},
	journal = {Virtual Reality},
	author = {Sagayam, K. Martin and Hemanth, D. Jude},
	month = jun,
	year = {2017},
	note = {9 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {model:hmm, app:vr, untagged, type:survey},
	pages = {91--107},
}

@inproceedings{sharma_gesture_2013,
	title = {Gesture {Recognition} : {A} {Survey} of {Gesture} {Recognition} {Techniques} using {Neural} {Networks}},
	shorttitle = {Gesture {Recognition}},
	url = {https://www.semanticscholar.org/paper/Gesture-Recognition-%3A-A-Survey-of-Gesture-using-Sharma-Chawla/70c11356829e83ba2976af1e6a6e69785d0d56ca},
	abstract = {Understanding human motions can be posed as a pattern recognition problem. In order to convey visual messages to a receiver, a human expresses motion patterns. Loosely called gestures, these patterns are variable but distinct and have an associated meaning. The Pattern recognition by any computer or machine can be implemented via various methods such as Hidden Harkov Models, Linear Programming and Neural Networks. Each method has its own advantages and disadvantages, which will be studied separately later on. This paper reviews why using ANNs in particular is better suited for analyzing human},
	urldate = {2023-07-02},
	author = {Sharma, M. and Chawla, Er Rama},
	year = {2013},
	keywords = {model:nn, type:survey},
	file = {Full Text PDF:/Users/brk/Zotero/storage/2GC2XFLL/Sharma and Chawla - 2013 - Gesture Recognition  A Survey of Gesture Recognit.pdf:application/pdf},
}

@inproceedings{watson_survey_1993,
	title = {A {Survey} of {Gesture} {Recognition} {Techniques}},
	url = {https://www.semanticscholar.org/paper/A-Survey-of-Gesture-Recognition-Techniques-Watson/23ec699d9a8c19e3807c53f85c564b9cfa172fff},
	abstract = {Processing speeds have increased dramatically bitmapped displays allow graph ics to be rendered and updated at increasing rates and in general computers have advanced to the point where they can assist humans in complex tasks Yet input technologies seem to cause the major bottleneck in performing these tasks under utilising the available resources and restricting the expressiveness of application use We use our hands constantly to interact with things pick them up move them transform their shape or activate them in some way In the same uncon scious way we gesticulate in communicating fundamental ideas stop come closer over there no agreed and so on Gestures are thus a natural and intuitive form of both interaction and communication This report develops the motivations for gestural input and surveys current gesture recognition techniques A recognition technique under development at TCD as part of the GLAD IN ART EP project is also introduced},
	urldate = {2023-07-02},
	author = {Watson, R.},
	year = {1993},
	keywords = {type:survey},
	file = {Full Text PDF:/Users/brk/Zotero/storage/EHEKTWGH/Watson - 1993 - A Survey of Gesture Recognition Techniques.pdf:application/pdf},
}

@article{fels_glove-talkii-neural-network_1998,
	title = {Glove-{TalkII}-a neural-network interface which maps gestures to parallel formant speech synthesizer controls},
	volume = {9},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/655042/},
	doi = {10.1109/72.655042},
	abstract = {Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.},
	number = {1},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Neural Networks},
	author = {Fels, S.S. and Hinton, G.E.},
	month = jan,
	year = {1998},
	note = {73 citations (Crossref) [2023-07-02]},
	keywords = {tech:accelerometer, tech:flex, model:ffnn},
	pages = {205--212},
}

@article{sturman_design_1993,
	title = {A design method for “whole-hand” human-computer interaction},
	volume = {11},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/159161.159159},
	doi = {10.1145/159161.159159},
	abstract = {A disciplined investigation of “whole-hand interfaces (often glove based, currently) and their appropriate use for the control of complex task domains is embodied by the design method for whole-hand input. This is a series of procedures—including a common basis for the description, design, and evaluation of whole-hand input, together with an accompanying taxonomy—that enumerates key issues and points for consideration in the development of whole-hand input. The method helps designers focus on task requirements, isolate problem areas, and choose appropriate whole-hand input strategies for their specified tasks. Several experiments were conducted to validate and demonstrate the use of the design method. The results of the experiments are summarized and discussed.},
	language = {en},
	number = {3},
	urldate = {2023-07-02},
	journal = {ACM Transactions on Information Systems},
	author = {Sturman, David J. and Zeltzer, David},
	month = jul,
	year = {1993},
	note = {30 citations (Crossref) [2023-07-02]},
	keywords = {evaluation-of-whole-hand-input, type:seminal},
	pages = {219--238},
	file = {Full Text:/Users/brk/Zotero/storage/65J52Q6P/Sturman and Zeltzer - 1993 - A design method for “whole-hand” human-computer in.pdf:application/pdf},
}

@inproceedings{hernandez-rebollar_acceleglove_2002,
	address = {San Antonio Texas},
	title = {The {AcceleGlove}: a whole-hand input device for virtual reality},
	isbn = {978-1-58113-525-1},
	shorttitle = {The {AcceleGlove}},
	url = {https://dl.acm.org/doi/10.1145/1242073.1242272},
	doi = {10.1145/1242073.1242272},
	abstract = {We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {{ACM} {SIGGRAPH} 2002 conference abstracts and applications},
	publisher = {ACM},
	author = {Hernandez-Rebollar, Jose L. and Kyriakopoulos, Nicholas and Lindeman, Robert W.},
	month = jul,
	year = {2002},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {app:sign-language, app:american-sl, classes:10-29, hardware:accelerometers, hardware:fabricless},
	pages = {259--259},
}

@article{zhang_egogesture_2018,
	title = {{EgoGesture}: {A} {New} {Dataset} and {Benchmark} for {Egocentric} {Hand} {Gesture} {Recognition}},
	volume = {20},
	issn = {1520-9210, 1941-0077},
	shorttitle = {{EgoGesture}},
	url = {https://ieeexplore.ieee.org/document/8299578/},
	doi = {10.1109/TMM.2018.2808769},
	abstract = {Gesture is a natural interface in human–computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24 000 gesture samples and 3 000 000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
	number = {5},
	urldate = {2023-07-02},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Yifan and Cao, Congqi and Cheng, Jian and Lu, Hanqing},
	month = may,
	year = {2018},
	note = {143 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {type:dataset, classes:50-99},
	pages = {1038--1050},
}

@article{atzori_electromyography_2014,
	title = {Electromyography data for non-invasive naturally-controlled robotic hand prostheses},
	volume = {1},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201453},
	doi = {10.1038/sdata.2014.53},
	abstract = {Abstract
            Recent advances in rehabilitation robotics suggest that it may be possible for hand-amputated subjects to recover at least a significant part of the lost hand functionality. The control of robotic prosthetic hands using non-invasive techniques is still a challenge in real life: myoelectric prostheses give limited control capabilities, the control is often unnatural and must be learned through long training times. Meanwhile, scientific literature results are promising but they are still far from fulfilling real-life needs. This work aims to close this gap by allowing worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark scientific database. The database is targeted at studying the relationship between surface electromyography, hand kinematics and hand forces, with the final goal of developing non-invasive, naturally controlled, robotic hand prostheses. The validation section verifies that the data are similar to data acquired in real-life conditions, and that recognition of different hand tasks by applying state-of-the-art signal features and machine-learning algorithms is possible.},
	language = {en},
	number = {1},
	urldate = {2023-07-02},
	journal = {Scientific Data},
	author = {Atzori, Manfredo and Gijsberts, Arjan and Castellini, Claudio and Caputo, Barbara and Hager, Anne-Gabrielle Mittaz and Elsig, Simone and Giatsidis, Giorgio and Bassetto, Franco and Müller, Henning},
	month = dec,
	year = {2014},
	note = {548 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:emg, read-priority-1, app:robot-control, type:dataset},
	pages = {140053},
	file = {Full Text:/Users/brk/Zotero/storage/46J7VGGV/Atzori et al. - 2014 - Electromyography data for non-invasive naturally-c.pdf:application/pdf},
}

@article{atzori_ninapro_2015,
	title = {The {Ninapro} database: {A} resource for {sEMG} naturally controlled robotic hand prosthetics},
	shorttitle = {The {Ninapro} database},
	url = {http://ieeexplore.ieee.org/document/7320041/},
	doi = {10.1109/EMBC.2015.7320041},
	abstract = {The dexterous natural control of robotic prosthetic hands with non-invasive techniques is still a challenge: surface electromyography gives some control capabilities but these are limited, often not natural and require long training times; the application of pattern recognition techniques recently started to be applied in practice. While results in the scientific literature are promising they have to be improved to reach the real needs. The Ninapro database aims to improve the field of naturally controlled robotic hand prosthetics by permitting to worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark database. Currently, the Ninapro database includes data from 67 intact subjects and 11 amputated subject performing approximately 50 different movements. The data are aimed at permitting the study of the relationships between surface electromyography, kinematics and dynamics. The Ninapro acquisition protocol was created in order to be easy to be reproduced. Currently, the number of datasets included in the database is increasing thanks to the collaboration of several research groups.},
	urldate = {2023-07-02},
	journal = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
	author = {Atzori, Manfredo and Muller, Henning},
	month = aug,
	year = {2015},
	note = {37 citations (Semantic Scholar/DOI) [2023-07-02]
Conference Name: 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
ISBN: 9781424492718
Place: Milan
Publisher: IEEE},
	keywords = {tech:emg, type:dataset},
	pages = {7151--7154},
}

@article{liu_uwave_2009,
	title = {{uWave}: {Accelerometer}-based personalized gesture recognition and its applications},
	volume = {5},
	issn = {15741192},
	shorttitle = {{uWave}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119209000674},
	doi = {10.1016/j.pmcj.2009.07.007},
	abstract = {Semantic Scholar extracted view of "uWave: Accelerometer-based personalized gesture recognition and its applications" by Jiayang Liu et al.},
	language = {en},
	number = {6},
	urldate = {2023-07-02},
	journal = {Pervasive and Mobile Computing},
	author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
	month = dec,
	year = {2009},
	note = {739 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:accelerometer, model:uwave, participants:8},
	pages = {657--675},
	file = {IEEE Xplore Abstract Record:/Users/brk/Zotero/storage/H47GNEKC/4912759.html:text/html;Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:/Users/brk/Zotero/storage/UMP9M3LQ/Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf:application/pdf},
}

@inproceedings{buchmann_fingartips_2004,
	address = {Singapore},
	title = {{FingARtips}: gesture based direct manipulation in {Augmented} {Reality}},
	isbn = {978-1-58113-883-2},
	shorttitle = {{FingARtips}},
	url = {https://dl.acm.org/doi/10.1145/988834.988871},
	doi = {10.1145/988834.988871},
	abstract = {This paper presents a technique for natural, fingertip-based interaction with virtual objects in Augmented Reality (AR) environments. We use image processing software and finger- and hand-based fiducial markers to track gestures from the user, stencil buffering to enable the user to see their fingers at all times, and fingertip-based haptic feedback devices to enable the user to feel virtual objects. Unlike previous AR interfaces, this approach allows users to interact with virtual content using natural hand gestures. The paper describes how these techniques were applied in an urban planning interface, and also presents preliminary informal usability results.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the 2nd international conference on {Computer} graphics and interactive techniques in {Australasia} and {South} {East} {Asia}},
	publisher = {ACM},
	author = {Buchmann, Volkert and Violich, Stephen and Billinghurst, Mark and Cockburn, Andy},
	month = jun,
	year = {2004},
	note = {290 citations (Semantic Scholar/DOI) [2023-07-02]},
	keywords = {tech:rgb, app:augmented-reality, fiducials},
	pages = {212--221},
	file = {Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf:/Users/brk/Zotero/storage/ZSNVZFEU/Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf:application/pdf},
}

@inproceedings{bolt_put-that-there_1980,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '80},
	title = {“{Put}-that-there”: {Voice} and gesture at the graphics interface},
	isbn = {978-0-89791-021-7},
	shorttitle = {“{Put}-that-there”},
	url = {https://dl.acm.org/doi/10.1145/800250.807503},
	doi = {10.1145/800250.807503},
	abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality. The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 7th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Bolt, Richard A.},
	month = jul,
	year = {1980},
	note = {1893 citations (Semantic Scholar/DOI) [2023-07-03]},
	keywords = {Gesture, Graphics, Graphics interface, Man-machine interfaces, Space sensing, Spatial data management, Speech input, Voice input},
	pages = {262--270},
	file = {Full Text PDF:/Users/brk/Zotero/storage/8U4B3N9X/Bolt - 1980 - “Put-that-there” Voice and gesture at the graphic.pdf:application/pdf},
}

@article{neto_highlevel_2010,
	title = {High‐level programming and control for industrial robotics: using a hand‐held accelerometer‐based input device for gesture and posture recognition},
	volume = {37},
	issn = {0143-991X},
	shorttitle = {High‐level programming and control for industrial robotics},
	url = {https://www.emerald.com/insight/content/doi/10.1108/01439911011018911/full/html},
	doi = {10.1108/01439911011018911},
	abstract = {Purpose
              Most industrial robots are still programmed using the typical teaching process, through the use of the robot teach pendant. This is a tedious and time‐consuming task that requires some technical expertise, and hence new approaches to robot programming are required. The purpose of this paper is to present a robotic system that allows users to instruct and program a robot with a high‐level of abstraction from the robot language.
            
            
              Design/methodology/approach
              The paper presents in detail a robotic system that allows users, especially non‐expert programmers, to instruct and program a robot just showing it what it should do, in an intuitive way. This is done using the two most natural human interfaces (gestures and speech), a force control system and several code generation techniques. Special attention will be given to the recognition of gestures, where the data extracted from a motion sensor (three‐axis accelerometer) embedded in the Wii remote controller was used to capture human hand behaviours. Gestures (dynamic hand positions) as well as manual postures (static hand positions) are recognized using a statistical approach and artificial neural networks.
            
            
              Findings
              It is shown that the robotic system presented is suitable to enable users without programming expertise to rapidly create robot programs. The experimental tests showed that the developed system can be customized for different users and robotic platforms.
            
            
              Research limitations/implications
              The proposed system is tested on two different robotic platforms. Since the options adopted are mainly based on standards, it can be implemented with other robot controllers without significant changes. Future work will focus on improving the recognition rate of gestures and continuous gesture recognition.
            
            
              Practical implications
              The key contribution of this paper is that it offers a practical method to program robots by means of gestures and speech, improving work efficiency and saving time.
            
            
              Originality/value
              This paper presents an alternative to the typical robot teaching process, extending the concept of human‐robot interaction and co‐worker scenario. Since most companies do not have engineering resources to make changes or add new functionalities to their robotic manufacturing systems, this system constitutes a major advantage for small‐ to medium‐sized enterprises.},
	language = {en},
	number = {2},
	urldate = {2023-07-03},
	journal = {Industrial Robot: An International Journal},
	author = {Neto, Pedro and Norberto Pires, J. and Paulo Moreira, A.},
	month = mar,
	year = {2010},
	note = {95 citations (Semantic Scholar/DOI) [2023-07-03]},
	pages = {137--147},
	file = {Full Text PDF:/Users/brk/Zotero/storage/GM28KZG6/Neto et al. - 2010 - High‐level programming and control for industrial .pdf:application/pdf},
}

@inproceedings{freeman_orientation_1995,
	title = {Orientation {Histograms} for {Hand} {Gesture} {Recognition}},
	url = {https://www.semanticscholar.org/paper/Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth/2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2},
	abstract = {We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or {\textbackslash}dynamic gestures", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,},
	urldate = {2023-07-03},
	author = {Freeman, W. and Roth, Michal},
	year = {1995},
	keywords = {untagged, type:seminal},
	file = {Full Text PDF:/Users/brk/Zotero/storage/N6NR3A34/Freeman and Roth - 1995 - Orientation Histograms for Hand Gesture Recognitio.pdf:application/pdf},
}

@article{zaman_khan_hand_2012,
	title = {Hand {Gesture} {Recognition}: {A} {Literature} {Review}},
	volume = {3},
	issn = {09762191},
	shorttitle = {Hand {Gesture} {Recognition}},
	url = {http://www.airccse.org/journal/ijaia/papers/3412ijaia12.pdf},
	doi = {10.5121/ijaia.2012.3412},
	abstract = {Semantic Scholar extracted view of "Hand Gesture Recognition: A Literature Review" by R. Khan},
	number = {4},
	urldate = {2023-07-03},
	journal = {International Journal of Artificial Intelligence \& Applications},
	author = {Zaman Khan, Rafiqul},
	month = jul,
	year = {2012},
	note = {165 citations (Semantic Scholar/DOI) [2023-07-03]},
	keywords = {type:survey, untagged},
	pages = {161--174},
	file = {Full Text:/Users/brk/Zotero/storage/LTRX4QL9/Zaman Khan - 2012 - Hand Gesture Recognition A Literature Review.pdf:application/pdf},
}

@article{viterbi_error_1967,
	title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
	volume = {13},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/1054010/},
	doi = {10.1109/TIT.1967.1054010},
	abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R\_\{0\} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R\_\{0\} and whose performance bears certain similarities to that of sequential decoding algorithms.},
	number = {2},
	urldate = {2023-07-11},
	journal = {IEEE Transactions on Information Theory},
	author = {Viterbi, A.},
	month = apr,
	year = {1967},
	note = {5898 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, type:seminal, viterbi},
	pages = {260--269},
}

@article{yamato_recognizing_1992,
	title = {Recognizing human action in time-sequential images using hidden {Markov} model},
	url = {http://ieeexplore.ieee.org/document/223161/},
	doi = {10.1109/CVPR.1992.223161},
	abstract = {A human action recognition method based on a hidden Markov model (HMM) is proposed. It is a feature-based bottom-up approach that is characterized by its learning capability and time-scale invariability. To apply HMMs, one set of time-sequential images is transformed into an image feature vector sequence, and the sequence is converted into a symbol sequence by vector quantization. In learning human action categories, the parameters of the HMMs, one per category, are optimized so as to best describe the training sequences from the category. To recognize an observed sequence, the HMM which best matches the sequence is chosen. Experimental results for real time-sequential images of sports scenes show recognition rates higher than 90\%. The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	urldate = {2023-07-11},
	journal = {Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Yamato, J. and Ohya, J. and Ishii, K.},
	year = {1992},
	note = {610 citations (Crossref) [2023-07-11]
1566 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
ISBN: 9780818628559
Place: Champaign, IL, USA
Publisher: IEEE Comput. Soc. Press},
	keywords = {from:cite.bib, model:hmm, tech:rgb, type:seminal, untagged},
	pages = {379--385},
	file = {Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf:/Users/brk/Zotero/storage/M2J27ASX/Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf:application/pdf},
}

@inproceedings{kanade_linguistic_2004,
	address = {Berlin, Heidelberg},
	title = {A {Linguistic} {Feature} {Vector} for the {Visual} {Interpretation} of {Sign} {Language}},
	volume = {3021},
	isbn = {978-3-540-21984-2 978-3-540-24670-1},
	url = {http://link.springer.com/10.1007/978-3-540-24670-1_30},
	doi = {10.1007/978-3-540-24670-1_30},
	abstract = {This paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data. Key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion. This high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans. Moreover, such a description broadly generalises temporal activities naturally overcoming variability of people and environments. A second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of Markov chains combined with Independent Component Analysis. We demonstrate classification rates as high as 97.67\% for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required.},
	language = {en},
	urldate = {2023-07-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Bowden, Richard and Windridge, David and Kadir, Timor and Zisserman, Andrew and Brady, Michael},
	editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Pajdla, Tomás and Matas, Jiří},
	year = {2004},
	doi = {10.1007/978-3-540-24670-1_30},
	note = {182 citations (Semantic Scholar/DOI) [2023-07-11]
Book Title: Computer Vision - ECCV 2004
Series Title: Lecture Notes in Computer Science},
	keywords = {app:sign-language, untagged},
	pages = {390--401},
	file = {Full Text:/Users/brk/Zotero/storage/RZMUR5ET/Bowden et al. - 2004 - A Linguistic Feature Vector for the Visual Interpr.pdf:application/pdf},
}

@inproceedings{bowden_vision_2003,
	title = {Vision based {Interpretation} of {Natural} {Sign} {Languages}},
	url = {https://www.semanticscholar.org/paper/Vision-based-Interpretation-of-Natural-Sign-Bowden-Zisserman/86e9d0c4d14456932043fa0fb22f39e62f2de688},
	abstract = {This manuscript outlines our current demonstration system for translating visual Sign to written text. The system is based around a broad description of scene activity that naturally generalizes, reducing training requirements and allowing the knowledge base to be explicitly stated. This allows the same system to be used for different sign languages requiring only a change of the knowledge base.},
	urldate = {2023-07-11},
	author = {Bowden, R. and Zisserman, Andrew and Kadir, T. and Brady, M.},
	month = apr,
	year = {2003},
	keywords = {app:sign-language, untagged},
	file = {Full Text PDF:/Users/brk/Zotero/storage/BRKUR3W3/Bowden et al. - 2003 - Vision based Interpretation of Natural Sign Langua.pdf:application/pdf},
}

@inproceedings{kadir_minimal_2004,
	address = {Kingston},
	title = {Minimal {Training}, {Large} {Lexicon}, {Unconstrained} {Sign} {Language} {Recognition}},
	isbn = {978-1-901725-25-4},
	url = {http://www.bmva.org/bmvc/2004/papers/paper_265.html},
	doi = {10.5244/C.18.96},
	abstract = {This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92\% for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2004},
	publisher = {British Machine Vision Association},
	author = {Kadir, T. and Bowden, R. and Ong, E. J. and Zisserman, A.},
	year = {2004},
	note = {105 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {app:sign-language, untagged},
	pages = {96.1--96.10},
	file = {Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf:/Users/brk/Zotero/storage/MYCSBHHF/Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf:application/pdf},
}

@article{yoon_hand_2001,
	title = {Hand gesture recognition using combined features of location, angle and velocity},
	volume = {34},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320300000960},
	doi = {10.1016/S0031-3203(00)00096-0},
	abstract = {Semantic Scholar extracted view of "Hand gesture recognition using combined features of location, angle and velocity" by H. Yoon et al.},
	language = {en},
	number = {7},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Yoon, Ho-Sub and Soh, Jung and Bae, Younglae J. and Seung Yang, Hyun},
	month = jan,
	year = {2001},
	note = {272 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, untagged},
	pages = {1491--1501},
}

@inproceedings{binh_real-time_2005,
	title = {Real-{Time} {Hand} {Tracking} and {Gesture} {Recognition} {System}},
	url = {https://www.semanticscholar.org/paper/Real-Time-Hand-Tracking-and-Gesture-Recognition-Binh-Shuichi/04cad56dd4fda01a3fa03a47dca2fa11d02695bf},
	abstract = {In this paper, we introduce a hand gesture recognition system to recognize real time gesture in unconstrained environments. The system consists of three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). We have used a Kalman filter and hand blobs analysis for hand tracking to obtain motion descriptors and hand region. It is fairy robust to background cluster and uses skin color for hand gesture tracking and recognition. Furthermore, there have been proposed to improve the overall performance of the approach: (1) Intelligent selection of training images and (2) Adaptive threshold gesture to remove non-gesture pattern that helps to qualify an input pattern as a gesture. A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed. In the experiments, we have tested our system to vocabulary of 36 gestures including the America sign language (ASL) letter spelling alphabet and digits, and results effectiveness of the approach.},
	urldate = {2023-07-11},
	author = {Binh, N. and Shuichi, Enokida and Ejima, T.},
	year = {2005},
	keywords = {model:hmm, tech:rgb, app:sign-language, untagged, app:americal-sl, model:kalman-filter},
}

@article{elmezain_real-time_2008,
	title = {Real-{Time} {Capable} {System} for {Hand} {Gesture} {Recognition} {Using} {Hidden} {Markov} {Models} in {Stereo} {Color} {Image} {Sequences}},
	url = {https://www.semanticscholar.org/paper/Real-Time-Capable-System-for-Hand-Gesture-Using-in-Elmezain-Al-Hamadi/aa17055855518ac47031bc509f5b7aa1b82fdcff},
	abstract = {This paper proposes a system to recognize the alphabets and numbers in real time from color image sequences by the motion trajectory of a single hand using Hidden Markov Models (HMM). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, YCbCr color space and depth information are used to detect hands and face in connection with morphological operation where Gaussian Mixture Model (GMM) is used for computing the skin probability. After the hand is detected and the centroid point of the hand region is determined, the tracking will take place in the further steps to determine the hand motion trajectory by using a search area around the hand region. In the feature extraction stage, the orientation is determined between two consecutive points from hand motion trajectory and then it is quantized to give a discrete vector that is used as input to HMM. The final stage so-called classification, Baum-Welch algorithm (BW) is used to do a full train for HMM parameters. The gesture of alphabets and numbers is recognized by using Left-Right Banded model (LRB) in conjunction with Forward algorithm. In our experiment, 720 trained gestures are used for training and also 360 tested gestures for testing. Our system recognizes the alphabets from A to Z and numbers from 0 to 9 and achieves an average recognition rate of 94.72\%.},
	urldate = {2023-07-11},
	journal = {J. WSCG},
	author = {Elmezain, M. and Al-Hamadi, A. and Michaelis, B.},
	year = {2008},
	keywords = {model:hmm, tech:rgb, untagged},
}

@article{elmezain_gesture_2007,
	title = {Gesture {Recognition} for {Alphabets} from {Hand} {Motion} {Trajectory} {Using} {Hidden} {Markov} {Models}},
	url = {http://ieeexplore.ieee.org/document/4458209/},
	doi = {10.1109/ISSPIT.2007.4458209},
	abstract = {This paper describes a method to recognize the alphabets from a single hand motion using Hidden Markov Models (HMM). In our method, gesture recognition for alphabets is based on three main stages; preprocessing, feature extraction and classification. In preprocessing stage, color and depth information are used to detect both hands and face in connection with morphological operation. After the detection of the hand, the tracking will take place in further step in order to determine the motion trajectory so-called gesture path. The second stage, feature extraction enhances the gesture path which gives us a pure path and also determines the orientation between the center of gravity and each point in a pure path. Thereby, the orientation is quantized to give a discrete vector that used as input to HMM. In the final stage, the gesture of alphabets is recognized by using Left-Right Banded model (LRB) in conjunction with Baum-Welch algorithm (BW) for training the parameters of HMM. Therefore, the best path is obtained by Viterbi algorithm using a gesture database. In our experiment, 520 trained gestures are used for training and also 260 tested gestures for testing. Our method recognizes the alphabets from A to Z and achieves an average recognition rate of 92.3\%.},
	urldate = {2023-07-11},
	journal = {2007 IEEE International Symposium on Signal Processing and Information Technology},
	author = {Elmezain, Mahmoud and Al-Hamadi, Ayoub and Krell, Gerald and El-Etriby, Sherif and Michaelis, Bernd},
	month = dec,
	year = {2007},
	note = {49 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2007 IEEE International Symposium on Signal Processing and Information Technology
ISBN: 9781424418343 9781424418350
Place: Giza, Egypt
Publisher: IEEE},
	keywords = {model:hmm, tech:rgbd, untagged},
	pages = {1192--1197},
}

@article{elmezain_hand_2009,
	title = {Hand {Gesture} {Recognition} {Based} on {Combined} {Features} {Extraction}},
	url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Based-on-Combined-Features-Elmezain-Al-Hamadi/92a80f36e199e0ddf73ba06f81b31ef60efcf5f8},
	abstract = {Hand gesture is an active area of research in the vision community, mainly for the purpose of sign language recognition and Human Computer Interaction. In this paper, we propose a system to recognize alphabet characters (A-Z) and numbers (0-9) in real-time from stereo color image sequences using Hidden Markov Models (HMMs). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, color and 3D depth map are used to detect hands where the hand trajectory will take place in further step using Mean-shift algorithm and Kalman filter. In the feature extraction stage, 3D combined features of location, orientation and velocity with respected to Cartesian systems are used. And then, k-means clustering is employed for HMMs codeword. The final stage so-called classification, BaumWelch algorithm is used to do a full train for HMMs parameters. The gesture of alphabets and numbers is recognized using Left-Right Banded model in conjunction with Viterbi algorithm. Experimental results demonstrate that, our system can successfully recognize hand gestures with 98.33\% recognition rate. Keywords—Gesture Recognition, Computer Vision \& Image Processing, Pattern Recognition.},
	urldate = {2023-07-11},
	journal = {International Journal of Electrical and Computer Engineering},
	author = {Elmezain, M. and Al-Hamadi, A. and Michaelis, B.},
	month = dec,
	year = {2009},
	keywords = {model:hmm, tech:rgb, model:kalman-filter, model:kmeans},
	file = {Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf:/Users/brk/Zotero/storage/GP7SDB8D/Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf:application/pdf},
}

@article{yang_dynamic_2012,
	title = {Dynamic hand gesture recognition using hidden {Markov} models},
	url = {http://ieeexplore.ieee.org/document/6295092/},
	doi = {10.1109/ICCSE.2012.6295092},
	abstract = {Hand gesture has become a powerful means for human-computer interaction. Traditional gesture recognition just consider hand trajectory. For some specific applications, such as virtual reality, more natural gestures are needed, which are complex and contain movement in 3-D space. In this paper, we introduce an HMM-based method to recognize complex single hand gestures. Gesture images are gained by a common web camera. Skin color is used to segment hand area from the image to form a hand image sequence. Then we put forward a state-based spotting algorithm to split continuous gestures. After that, feature extraction is executed on each gesture. Features used in the system contain hand position, velocity, size, and shape. We raise a data aligning algorithm to align feature vector sequences for training. Then an HMM is trained alone for each gesture. The recognition results demonstrate that our methods are effective and accurate.},
	urldate = {2023-07-11},
	journal = {2012 7th International Conference on Computer Science \& Education (ICCSE)},
	author = {Yang, Zhong and Li, Yi and Chen, Weidong and Zheng, Yang},
	month = jul,
	year = {2012},
	note = {68 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2012 7th International Conference on Computer Science \& Education (ICCSE 2012)
ISBN: 9781467302425 9781467302418 9781467302401
Place: Melbourne, Australia
Publisher: IEEE},
	keywords = {model:hmm, untagged},
	pages = {360--365},
}

@article{ramamoorthy_recognition_2003,
	title = {Recognition of dynamic hand gestures},
	volume = {36},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320303000426},
	doi = {10.1016/S0031-3203(03)00042-6},
	abstract = {Semantic Scholar extracted view of "Recognition of dynamic hand gestures" by Aditya Ramamoorthy et al.},
	language = {en},
	number = {9},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Ramamoorthy, Aditya and Vaswani, Namrata and Chaudhury, Santanu and Banerjee, Subhashis},
	month = sep,
	year = {2003},
	note = {169 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {app:robotics, model:hmm, movement:dynamic, untagged},
	pages = {2069--2081},
}

@inproceedings{davis_visual_1994,
	title = {Visual gesture recognition},
	volume = {141},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941058},
	doi = {10.1049/ip-vis:19941058},
	abstract = {Presents a method for recognising human-hand gestures using a model based approach. A finite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of left, right, up, down, grab, rotate, and stop.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {{IEE} {Proceedings} - {Vision}, {Image}, and {Signal} {Processing}},
	author = {Davis, J.},
	year = {1994},
	note = {272 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 1350245X
Issue: 2
Journal Abbreviation: IEE Proc., Vis. Image Process.},
	keywords = {model:fsm, type:seminal},
	pages = {101},
}

@article{pengyu_hong_gesture_2000,
	title = {Gesture modeling and recognition using finite state machines},
	url = {http://ieeexplore.ieee.org/document/840667/},
	doi = {10.1109/AFGR.2000.840667},
	abstract = {We propose a state-based approach to gesture learning and recognition. Using spatial clustering and temporal alignment, each gesture is defined to be an ordered sequence of states in spatial-temporal space. The 2D image positions of the centers of the head and both hands of the user are used as features; these are located by a color-based tracking method. From training data of a given gesture, we first learn the spatial information and then group the data into segments that are automatically aligned temporally. The temporal information is further integrated to build a finite state machine (FSM) recognizer. Each gesture has a FSM corresponding to it. The computational efficiency of the FSM recognizers allows us to achieve real-time on-line performance. We apply this technique to build an experimental system that plays a game of "Simon Says" with the user.},
	urldate = {2023-07-11},
	journal = {Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
	author = {{Pengyu Hong} and Turk, M. and Huang, T.S.},
	year = {2000},
	note = {276 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Fourth International Conference on Automatic Face and Gesture Recognition
ISBN: 9780769505800
Place: Grenoble, France
Publisher: IEEE Comput. Soc},
	keywords = {model:fsm, untagged},
	pages = {410--415},
	file = {Submitted Version:/Users/brk/Zotero/storage/G3CD967K/Pengyu Hong et al. - 2000 - Gesture modeling and recognition using finite stat.pdf:application/pdf},
}

@article{yeasin_visual_2000,
	title = {Visual understanding of dynamic hand gestures},
	volume = {33},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320399001752},
	doi = {10.1016/S0031-3203(99)00175-2},
	abstract = {Semantic Scholar extracted view of "Visual understanding of dynamic hand gestures" by M. Yeasin et al.},
	language = {en},
	number = {11},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Yeasin, M. and Chaudhuri, S.},
	month = nov,
	year = {2000},
	note = {107 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:fsm, type:seminal},
	pages = {1805--1817},
}

@article{ming-hsuan_yang_recognizing_1999,
	title = {Recognizing hand gesture using motion trajectories},
	url = {http://ieeexplore.ieee.org/document/786979/},
	doi = {10.1109/CVPR.1999.786979},
	abstract = {We present an algorithm for extracting and classifying two-dimensional motion in an image sequence based on motion trajectories. First, a multiscale segmentation is performed to generate homogeneous regions in each frame. Regions between consecutive frames are then matched to obtain 2-view correspondences. Affine transformations are computed from each pair of corresponding regions to define pixel matches. Pixels matches over consecutive images pairs are concatenated to obtain pixel-level motion trajectories across the image sequence. Motion patterns are learned from the extracted trajectories using a time-delay neural network. We apply the proposed method to recognize 40 hand gestures of American Sign Language. Experimental results show that motion patterns in hand gestures can be extracted and recognized with high recognition rate using motion trajectories.},
	urldate = {2023-07-11},
	journal = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
	author = {{Ming-Hsuan Yang} and Ahuja, N.},
	year = {1999},
	note = {155 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
ISBN: 9780769501499
Place: Fort Collins, CO, USA
Publisher: IEEE Comput. Soc},
	keywords = {tech:rgb, app:sign-language, model:nn, untagged, app:americal-sl, classes:40},
	pages = {466--472},
}

@article{yanmin_zhu_vision_2013,
	title = {Vision {Based} {Hand} {Gesture} {Recognition}},
	url = {http://ieeexplore.ieee.org/document/6519802/},
	doi = {10.1109/ICSS.2013.40},
	abstract = {With the development of ubiquitous computing, current user interaction approaches with keyboard, mouse and pen are not sufficient. Due to the limitation of these devices the useable command set is also limited. Direct use of hands as an input device is an attractive method for providing natural Human Computer Interaction which has evolved from text-based interfaces through 2D graphical-based interfaces, multimedia-supported interfaces, to fully fledged multi-participant Virtual Environment (VE) systems. Imagine the human-computer interaction of the future: A 3D- application where you can move and rotate objects simply by moving and rotating your hand - all without touching any input device. In this paper a review of vision based hand gesture recognition is presented. The existing approaches are categorized into 3D model based approaches and appearance based approaches, highlighting their advantages and shortcomings and identifying the open issues.},
	urldate = {2023-07-11},
	journal = {2013 International Conference on Service Sciences (ICSS)},
	author = {{Yanmin Zhu} and {Zhibo Yang} and {Bo Yuan}},
	month = apr,
	year = {2013},
	note = {28 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2013 International Conference on Service Sciences (ICSS 2013)
ISBN: 9781467362580 9780769549729
Place: Shenzhen
Publisher: IEEE},
	pages = {260--265},
}

@inproceedings{mehra_survey_2013,
	title = {Survey on {Multiclass} {Classification} {Methods}},
	url = {https://www.semanticscholar.org/paper/Survey-on-Multiclass-Classification-Methods-Mehra-Gupta/f0fcf860031b356a3c68b330735634c00e5d7602},
	abstract = {Supervised learning is based on the target value or the desired outputs. Various successful techniques have been proposed to solve the problem in the binary classification case. The multiclass classification case is more delicate one. In this short survey we investigate the various techniques for solving the multiclass classification problem. Various authors and research modified the multiclass classification approach such as one against one, one against all and Directed Acyclic Graph (DAG) which creates many binary classifiers and combines their results to determine the class label of a test pixel. They also describe the various extensible methods that are extended from binary class to solve the multiclass problem and also explain the method in which the classes are arranged into a tree.},
	urldate = {2023-07-11},
	author = {Mehra, Neha and Gupta, Surendra},
	year = {2013},
	keywords = {classification, multiclass},
	file = {Full Text PDF:/Users/brk/Zotero/storage/FPR3M2AA/Mehra and Gupta - 2013 - Survey on Multiclass Classification Methods.pdf:application/pdf},
}

@article{kopuklu_real-time_2019,
	title = {Real-time {Hand} {Gesture} {Detection} and {Classification} {Using} {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8756576/},
	doi = {10.1109/FG.2019.8756576},
	abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04\% and 83.82\% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available1.},
	urldate = {2023-07-11},
	journal = {2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)},
	author = {Kopuklu, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
	month = may,
	year = {2019},
	note = {112 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)
ISBN: 9781728100890
Place: Lille, France
Publisher: IEEE},
	keywords = {dataset:egogesture, dataset:nvidia-dynamic-hand-gesture, model:cnn, tech:rgb, untagged},
	pages = {1--8},
	file = {Submitted Version:/Users/brk/Zotero/storage/GE2CVBAG/Kopuklu et al. - 2019 - Real-time Hand Gesture Detection and Classificatio.pdf:application/pdf},
}

@article{lu_one-shot_2019,
	title = {One-shot learning hand gesture recognition based on modified 3d convolutional neural networks},
	volume = {30},
	issn = {0932-8092, 1432-1769},
	url = {http://link.springer.com/10.1007/s00138-019-01043-7},
	doi = {10.1007/s00138-019-01043-7},
	abstract = {Though deep neural networks have played a very important role in the field of vision-based hand gesture recognition, however, it is challenging to acquire large numbers of annotated samples to support its deep learning or training. Furthermore, in practical applications it often encounters some case with only one single sample for a new gesture class so that conventional recognition method cannot be qualified with a satisfactory classification performance. In this paper, the methodology of transfer learning is employed to build an effective network architecture of one-shot learning so as to deal with such intractable problem. Then some useful knowledge from deep training with big dataset of relative objects can be transferred and utilized to strengthen one-shot learning hand gesture recognition (OSLHGR) rather than to train a network from scratch. According to this idea a well-designed convolutional network architecture with deeper layers, C3D (Tran et al. in: ICCV, pp 4489–4497, 2015), is modified as an effective tool to extract spatiotemporal feature by deep learning. Then continuous fine-tune training is performed on a sample of new classes to complete one-shot learning. Moreover, the test of classification is carried out by Softmax classifier and geometrical classification based on Euclidean distance. Finally, a series of experiments and tests on two benchmark datasets, VIVA (Vision for Intelligent Vehicles and Applications) and SKIG (Sheffield Kinect Gesture) are conducted to demonstrate its state-of-the-art recognition accuracy of our proposed method. Meanwhile, a special dataset of gestures, BSG, is built using SoftKinetic DS325 for the test of OSLHGR, and a series of test results verify and validate its well classification performance and real-time response speed.},
	language = {en},
	number = {7-8},
	urldate = {2023-07-11},
	journal = {Machine Vision and Applications},
	author = {Lu, Zhi and Qin, Shiyin and Li, Xiaojie and Li, Lianwei and Zhang, Dinghao},
	month = oct,
	year = {2019},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, model:nn, untagged, dataset:viva, datasetLskig},
	pages = {1157--1180},
}

@article{jie_huang_sign_2015,
	title = {Sign {Language} {Recognition} using {3D} convolutional neural networks},
	url = {https://ieeexplore.ieee.org/document/7177428},
	doi = {10.1109/ICME.2015.7177428},
	abstract = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
	urldate = {2023-07-11},
	journal = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
	author = {{Jie Huang} and {Wengang Zhou} and {Houqiang Li} and {Weiping Li}},
	month = jun,
	year = {2015},
	note = {199 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2015 IEEE International Conference on Multimedia and Expo (ICME)
ISBN: 9781479970827
Place: Turin, Italy
Publisher: IEEE},
	keywords = {app:sign-language, hardware:kinect, model:cnn, tech:rgbd, untagged},
	pages = {1--6},
}

@article{zhang_gesture_2020,
	title = {Gesture recognition based on deep deformable {3D} convolutional neural networks},
	volume = {107},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320320302193},
	doi = {10.1016/j.patcog.2020.107416},
	abstract = {Semantic Scholar extracted view of "Gesture recognition based on deep deformable 3D convolutional neural networks" by Yifan Zhang et al.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Pattern Recognition},
	author = {Zhang, Yifan and Shi, Lei and Wu, Yi and Cheng, Ke and Cheng, Jian and Lu, Hanqing},
	month = nov,
	year = {2020},
	note = {23 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, untagged},
	pages = {107416},
}

@article{baum_maximization_1970,
	title = {A {Maximization} {Technique} {Occurring} in the {Statistical} {Analysis} of {Probabilistic} {Functions} of {Markov} {Chains}},
	volume = {41},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177697196},
	doi = {10.1214/aoms/1177697196},
	abstract = {Semantic Scholar extracted view of "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains" by L. Baum et al.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {The Annals of Mathematical Statistics},
	author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
	month = feb,
	year = {1970},
	note = {4728 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:hmm, untagged, baum-welsch},
	pages = {164--171},
	file = {Full Text:/Users/brk/Zotero/storage/888D4KE6/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf:application/pdf},
}

@article{knuth_two_1992,
	title = {Two {Notes} on {Notation}},
	volume = {99},
	issn = {00029890},
	url = {https://www.jstor.org/stable/2325085?origin=crossref},
	doi = {10.2307/2325085},
	abstract = {Mathematical notation evolves like all languages do. As new experiments are made, we sometimes witness the survival of the fittest, sometimes the survival of the most familiar. A healthy conservatism keeps things from changing too rapidly; a healthy radicalism keeps things in tune with new theoretical emphases. Our mathematical language continues to improve, just as "the d-ism of Leibniz overtook the dotage of Newton" in past centuries [4, Chapter 4]. In 1970 I began teaching a class at Stanford University entitled Concrete Mathematics. The students and I studied how to manipulate formulas in continuous and discrete mathematics, and the problems we investigated were often inspired by new developments in computer science. As the years went by we began to see that a few changes in notational traditions would greatly facilitate our work. The notes from that class have recently been published in a book [15], and as I wrote the final drafts of that book I learned to my surprise that two of the notations we had been using were considerably more useful than I had previously realized. The ideas "clicked" so well, in fact, that I've decided to write this article, blatantly attempting to promote these notations among the mathematicians who have no use for [15]. I hope that within five years everybody will be able to use these notations in published papers without needing to explain what they mean. The notations I'm talking about are (1) Iverson's convention for characteristic functions; and (2) the "right" notation for Stirling numbers, at last.},
	number = {5},
	urldate = {2023-07-11},
	journal = {The American Mathematical Monthly},
	author = {Knuth, Donald E.},
	month = may,
	year = {1992},
	note = {458 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged},
	pages = {403},
	file = {Full Text PDF:/Users/brk/Zotero/storage/QUCL5Y47/Knuth - 1992 - Two Notes on Notation.pdf:application/pdf},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2023-07-11},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	keywords = {batch-normalization, untagged},
	file = {Full Text PDF:/Users/brk/Zotero/storage/ST7LJZZJ/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/35016072},
	doi = {10.1038/35016072},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	language = {en},
	number = {6789},
	urldate = {2023-07-11},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	month = jun,
	year = {2000},
	note = {1114 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {relu, untagged},
	pages = {947--951},
}

@inproceedings{hochreiter_gradient_2001,
	title = {Gradient {Flow} in {Recurrent} {Nets}: the {Difficulty} of {Learning} {Long}-{Term} {Dependencies}},
	shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
	url = {https://www.semanticscholar.org/paper/Gradient-Flow-in-Recurrent-Nets%3A-the-Difficulty-of-Hochreiter-Bengio/aed054834e2c696807cc8b227ac7a4197196e211},
	abstract = {D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6{\textbackslash}M[ X N@]\_{\textasciicircum}O{\textbackslash}`JaNcb V RcQ W d EGKeL({\textasciicircum}(QgfhKeLOE?i){\textasciicircum}(QSj ETNPfPQkRl[ V R)m"[ X {\textasciicircum}(KeLOEG{\textasciicircum} npo qarpo m"[ X {\textasciicircum}(KeLOEG{\textasciicircum}tsAu EGNPb V {\textasciicircum} v wyx zlwO\{({\textbar}(\vphantom{\{}\}{\textless}{\textasciitilde}OC\}((xp\{ay.{\textasciitilde}A\vphantom{\{}\}\_{\textasciitilde} Cl3\#{\textbar}{\textless}Azw\#{\textbar}l6 ({\textbar}  JpfhL XV EG{\textasciicircum}O QgJ  ETFOR] {\textasciicircum}O{\textbackslash}JNPb V RcQ X E)ETR 6EGKeLOETNcKMLOE F ETN V RcQgJp{\textasciicircum}({\textasciicircum}OE ZgZ E i {\textasciicircum}(Qkj EGNPfhQSRO E OE2m1Jp{\textasciicircum} RcNY E VZ sO! ¡ q.n sCD X KGKa8¢EG{\textasciicircum} RPNhE¤£ ¥¦Q ZgZ Es m§J{\textasciicircum} RPNO E VZ s( ̈ X  EG©\#EKas\# V {\textasciicircum} V  V s(H a «a¬3­ ®\#{\textbar}.Y ̄y\vphantom{\{}\} xa°OC\}l\{x  yxlY{\textasciitilde}3\{{\textbar}  ±2Pz   V J Z J U N V fhKTJp{\textasciicircum}(Q  ETFOR J{\textbackslash} D vYf3RPEGb ́f V {\textasciicircum}(§JpbF X RPETN@D KTQEG{\textasciicircum}(KTE i {\textasciicircum}(QSjpEGNPfhQSR4vμJ{\textbackslash} U¶Z JaNPEG{\textasciicircum}(K·E jYQ V (Q ̧D V {\textasciicircum} R V m V N3R V aOs\#1 o ¡Ga r U QNhE{\textasciicircum}OoTE1⁄4»,] R VZ vC1⁄2 3⁄4  x ± x  \#¿ \vphantom{\{}\}À 3t\vphantom{\{}\}lC\}2P\}{\textless}{\textasciitilde} ¬t[ X NPE{\textasciicircum}§D KeL(b ́Qg(L X ©yETN ]  DY]\_Á JNPfhJÃÂ Z j EToQ V a rpopo2Ä X  V {\textasciicircum}(J(sCD Å)QSRPoTEGN ZgV {\textasciicircum}( Æ \#{\textbar}\{3 ̄{\textbar}.(C\vphantom{\{}\}.C¿Y\}p Pzw},
	urldate = {2023-07-11},
	author = {Hochreiter, S. and Bengio, Yoshua},
	year = {2001},
	keywords = {vanishing-gradient},
}

@article{linnainmaa_taylor_1976,
	title = {Taylor expansion of the accumulated rounding error},
	volume = {16},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01931367},
	doi = {10.1007/BF01931367},
	abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
	language = {en},
	number = {2},
	urldate = {2023-07-11},
	journal = {BIT},
	author = {Linnainmaa, Seppo},
	month = jun,
	year = {1976},
	note = {247 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {backprop, untagged},
	pages = {146--160},
}

@misc{geoffrey_hinton_coursera_2012,
	title = {Coursera: {Neural} {Networks} for {Machine} {Learning}},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	author = {{Geoffrey Hinton}},
	year = {2012},
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Adaptive} {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan/413c1142de9d91804d6d11c67ff3fed59c9fc279},
	abstract = {This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight. We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	language = {en},
	urldate = {2023-07-11},
	keywords = {untagged, adagrad},
	file = {[PDF] Adaptive Subgradient Methods for Online Lear.pdf:/Users/brk/Zotero/storage/E6YNVHRW/[PDF] Adaptive Subgradient Methods for Online Lear.pdf:application/pdf;Snapshot:/Users/brk/Zotero/storage/GGADQGGW/413c1142de9d91804d6d11c67ff3fed59c9fc279.html:text/html},
}

@misc{adan_arriaga_charachorder_2022,
	title = {{CharaChorder} - {Type} at the speed of thought},
	url = {https://www.charachorder.com/},
	author = {{Adan Arriaga}},
	year = {2022},
}

@inproceedings{murase_gesture_2012,
	address = {Megève France},
	title = {Gesture keyboard with a machine learning requiring only one camera},
	isbn = {978-1-4503-1077-2},
	url = {https://dl.acm.org/doi/10.1145/2160125.2160154},
	doi = {10.1145/2160125.2160154},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Proceedings of the 3rd {Augmented} {Human} {International} {Conference}},
	publisher = {ACM},
	author = {Murase, Taichi and Moteki, Atsunori and Suzuki, Genta and Nakai, Takahiro and Hara, Nobuyuki and Matsuda, Takahiro},
	month = mar,
	year = {2012},
	note = {12 citations (Semantic Scholar/DOI) [2023-07-11]},
	pages = {1--2},
}

@inproceedings{wang_traffic_2008,
	title = {Traffic {Police} {Gesture} {Recognition} using {Accelerometers}},
	url = {https://www.semanticscholar.org/paper/Traffic-Police-Gesture-Recognition-using-Wang/59715d7a74376e7a21ada77f3a7af6b854a35545},
	abstract = {When an automatic traffic light system is not used due to too heavy traffic, the traffic would be controlled by traffic police gesture. This paper is about the design of a system so that the traffic lights can follow the traffic police gestures. To simplify the system, a unique mapping between the traffic police gestures and the orientation and movement of hands is defined. The hand motion characters are extracted by fixing a 3-axis accelerometer on the back of each hand. A 2level hierarchical classifier is used to recognize the gestures. First the gestures are categorized into three groups, according to the movement of each hand. Then a gesture is recognized by comparing it with the predefined templates. This real-time recognition algorithm is implemented by a micro-controller. It is envisaged that this will help drivers.},
	urldate = {2023-07-11},
	author = {Wang, Ben},
	year = {2008},
	keywords = {tech:accelerometer, app:traffic-police},
	file = {Full Text PDF:/Users/brk/Zotero/storage/69LXNATR/Wang - 2008 - Traffic Police Gesture Recognition using Accelerom.pdf:application/pdf},
}

@article{zhang_stacked_2021,
	title = {Stacked {LSTM}-{Based} {Dynamic} {Hand} {Gesture} {Recognition} with {Six}-{Axis} {Motion} {Sensors}},
	url = {https://ieeexplore.ieee.org/document/9658659/},
	doi = {10.1109/SMC52423.2021.9658659},
	abstract = {Hand gesture recognition can be exploited to benefit ubiquitous applications using sensors. Currently, the inherent complexity of human physical activities makes it difficult to accurately recognize gestures with wearable sensors, especially in real time. To this end, a real-time hand gesture recognition system is presented in this paper. In particular, sliding window technology and y-axis threshold are used to detect intended gestures from a continuous data stream and then the segmented data are classified by applying a stacked Long Short-Term Memory (LSTM) model. After noise is removed, six-axis sensor data from wrist-worn devices are fed into the model without requiring feature engineering. We use twelve common hand gestures to evaluate the performance of our model. The experimental results demonstrate the feasibility of our proposed system with an accuracy of 99.8\% on average. Our approach allows for an accurate and nonindividual hand gesture recognition. It holds potential to be integrated into a smart watch or other wearable devices for intuitive human computer interaction.},
	urldate = {2023-07-11},
	journal = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	author = {Zhang, Yidi and Ran, Mengyuan and Liao, Jun and Su, Guoxin and Liu, Ming and Liu, Li},
	month = oct,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
ISBN: 9781665442077
Place: Melbourne, Australia
Publisher: IEEE},
	keywords = {model:lstm, tech:accelerometers, untagged},
	pages = {2568--2575},
}

@article{yuan_hand_2020,
	title = {Hand {Gesture} {Recognition} using {Deep} {Feature} {Fusion} {Network} based on {Wearable} {Sensors}},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {https://ieeexplore.ieee.org/document/9158332/},
	doi = {10.1109/JSEN.2020.3014276},
	abstract = {Hand gesture recognition is an important way for human machine interaction, and it is widely used in many areas, such as health care, smart home, virtual reality as well as other areas. While many valuable efforts have been made, it still lacks efficient ways to capture fine grain hand gesture as well as track data of long distance dependency in complex gesture. In this paper, we firstly design a novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles. Secondly, an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures. In order to track detailed motion features, a convolutional neural network based feature fusion strategy is given to fuse data from multi-sensors by extracting both shallow and deep features. Moreover, a residual module is introduced to avoid over fitting and gradient vanishing during deepening the neural network. Thirdly, a long short term memory (LSTM) model with fused feature vector as input is introduced to classify complex hand motions into corresponding categories. Results of comprehensive experiments demonstrate that our work performs better than related algorithms, especially in America sign language (with the precise of 99.93\%) and Chinese sign language (with the precise of 96.1\%).},
	urldate = {2023-07-11},
	journal = {IEEE Sensors Journal},
	author = {Yuan, Guan and Liu, Xiao and Yan, Qiuyan and Qiao, Shaojie and Wang, Zhixiao and Yuan, Li},
	year = {2020},
	note = {32 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:flex, model:lstm, model:nn, untagged},
	pages = {1--1},
}

@inproceedings{dept_of_biomedical_engineering_kyung_hee_university_republic_of_korea_recognition_2017,
	title = {Recognition of {Human} {Hand} {Activities} {Based} on a {Single} {Wrist} {IMU} {Using} {Recurrent} {Neural} {Networks}},
	volume = {6},
	url = {http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf},
	doi = {10.18178/ijpmbs.6.4.114-118},
	abstract = {Recognition of hand activities could provide new information towards daily human activity logging and gesture interface applications. However, there is a technical challenge due to delicate hand motions and complex movement contexts. In this work, we proposed hand activity recognition (HAR) based on a single inertial measurement unit (IMU) sensor at one wrist via deep learning recurrent neural network. The proposed HAR works directly with signals from a tri-axial accelerometer, gyroscope, and magnetometer sensors within one IMU. We evaluated the performance of our HAR with a public human hand activity database for six hand activities including Open Door, Close Door, Open Fridge, Close Fridge, Clean Table and Drink from Cup. Our results show an overall recognition accuracy of 80.09\% with discrete standard epochs and 74.92\% with noise-added epochs. With continuous time series epochs, the accuracy of 71.75\% was obtained. },
	urldate = {2023-07-11},
	booktitle = {International {Journal} of {Pharma} {Medicine} and {Biological} {Sciences}},
	author = {{Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea} and Rivera, Patricio and Valarezo, Edwin and Choi, Mun-Taek and Kim, Tae-Seong},
	year = {2017},
	note = {19 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 22785221
Issue: 4
Journal Abbreviation: IJPMBS},
	keywords = {model:rnn, tech:imu, untagged},
	pages = {114--118},
	file = {Full Text:/Users/brk/Zotero/storage/38I9KL9J/Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea et al. - 2017 - Recognition of Human Hand Activities Based on a Si.pdf:application/pdf},
}

@article{amma_airwriting_2014,
	title = {Airwriting: a wearable handwriting recognition system},
	volume = {18},
	issn = {1617-4909, 1617-4917},
	shorttitle = {Airwriting},
	url = {http://link.springer.com/10.1007/s00779-013-0637-3},
	doi = {10.1007/s00779-013-0637-3},
	abstract = {We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {Personal and Ubiquitous Computing},
	author = {Amma, Christoph and Georgi, Marcus and Schultz, Tanja},
	month = jan,
	year = {2014},
	note = {0 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {app:writing, untagged},
	pages = {191--203},
}

@article{patil_handwriting_2016,
	title = {Handwriting {Recognition} in {Free} {Space} {Using} {WIMU}-{Based} {Hand} {Motion} {Analysis}},
	volume = {2016},
	issn = {1687-725X, 1687-7268},
	url = {http://www.hindawi.com/journals/js/2016/3692876/},
	doi = {10.1155/2016/3692876},
	abstract = {We present a wireless-inertial-measurement-unit- (WIMU-) based hand motion analysis technique for handwriting recognition in three-dimensional (3D) space. The proposed handwriting recognition system is not bounded by any limitations or constraints; users have the freedom and flexibility to write characters in free space. It uses hand motion analysis to segment hand motion data from a WIMU device that incorporates magnetic, angular rate, and gravity sensors (MARG) and a sensor fusion algorithm to automatically distinguish segments that represent handwriting from nonhandwriting data in continuous hand motion data. Dynamic time warping (DTW) recognition algorithm is used to recognize handwriting in real-time. We demonstrate that a user can freely write in air using an intuitive WIMU as an input and hand motion analysis device to recognize the handwriting in 3D space. The experimental results for recognizing handwriting in free space show that the proposed method is effective and efficient for other natural interaction techniques, such as in computer games and real-time hand gesture recognition applications.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Journal of Sensors},
	author = {Patil, Shashidhar and Kim, Dubeom and Park, Seongsill and Chai, Youngho},
	year = {2016},
	note = {21 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {app:writing, tech:imu, untagged},
	pages = {1--10},
	file = {Full Text PDF:/Users/brk/Zotero/storage/T68RSGXI/Patil et al. - 2016 - Handwriting Recognition in Free Space Using WIMU-B.pdf:application/pdf},
}

@article{sahoo_real-time_2022,
	title = {Real-{Time} {Hand} {Gesture} {Recognition} {Using} {Fine}-{Tuned} {Convolutional} {Neural} {Network}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/3/706},
	doi = {10.3390/s22030706},
	abstract = {Hand gesture recognition is one of the most effective modes of interaction between humans and computers due to being highly flexible and user-friendly. A real-time hand gesture recognition system should aim to develop a user-independent interface with high recognition performance. Nowadays, convolutional neural networks (CNNs) show high recognition rates in image classification problems. Due to the unavailability of large labeled image samples in static hand gesture images, it is a challenging task to train deep CNN networks such as AlexNet, VGG-16 and ResNet from scratch. Therefore, inspired by CNN performance, an end-to-end fine-tuning method of a pre-trained CNN model with score-level fusion technique is proposed here to recognize hand gestures in a dataset with a low number of gesture images. The effectiveness of the proposed technique is evaluated using leave-one-subject-out cross-validation (LOO CV) and regular CV tests on two benchmark datasets. A real-time American sign language (ASL) recognition system is developed and tested using the proposed technique.},
	language = {en},
	number = {3},
	urldate = {2023-07-11},
	journal = {Sensors},
	author = {Sahoo, Jaya Prakash and Prakash, Allam Jaya and Pławiak, Paweł and Samantray, Saunak},
	month = jan,
	year = {2022},
	note = {28 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:cnn, app:sign-language, app:american-sl, untagged},
	pages = {706},
	file = {Full Text PDF:/Users/brk/Zotero/storage/CUKP99QW/Sahoo et al. - 2022 - Real-Time Hand Gesture Recognition Using Fine-Tune.pdf:application/pdf},
}

@article{colli_alfaro_user-independent_2022,
	title = {User-{Independent} {Hand} {Gesture} {Recognition} {Classification} {Models} {Using} {Sensor} {Fusion}},
	volume = {22},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/4/1321},
	doi = {10.3390/s22041321},
	abstract = {Recently, it has been proven that targeting motor impairments as early as possible while using wearable mechatronic devices for assisted therapy can improve rehabilitation outcomes. However, despite the advanced progress on control methods for wearable mechatronic devices, the need for a more natural interface that allows for better control remains. To address this issue, electromyography (EMG)-based gesture recognition systems have been studied as a potential solution for human–machine interface applications. Recent studies have focused on developing user-independent gesture recognition interfaces to reduce calibration times for new users. Unfortunately, given the stochastic nature of EMG signals, the performance of these interfaces is negatively impacted. To address this issue, this work presents a user-independent gesture classification method based on a sensor fusion technique that combines EMG data and inertial measurement unit (IMU) data. The Myo Armband was used to measure muscle activity and motion data from healthy subjects. Participants were asked to perform seven types of gestures in four different arm positions while using the Myo on their dominant limb. Data obtained from 22 participants were used to classify the gestures using three different classification methods. Overall, average classification accuracies in the range of 67.5–84.6\% were obtained, with the Adaptive Least-Squares Support Vector Machine model obtaining accuracies as high as 92.9\%. These results suggest that by using the proposed sensor fusion approach, it is possible to achieve a more natural interface that allows better control of wearable mechatronic devices during robot assisted therapies.},
	language = {en},
	number = {4},
	urldate = {2023-07-11},
	journal = {Sensors},
	author = {Colli Alfaro, Jose Guillermo and Trejos, Ana Luisa},
	month = feb,
	year = {2022},
	note = {13 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:emg, tech:im, untagged},
	pages = {1321},
	file = {Full Text PDF:/Users/brk/Zotero/storage/BBADBX3U/Colli Alfaro and Trejos - 2022 - User-Independent Hand Gesture Recognition Classifi.pdf:application/pdf},
}

@article{xu_finger-writing_2015,
	title = {Finger-writing with {Smartwatch}: {A} {Case} for {Finger} and {Hand} {Gesture} {Recognition} using {Smartwatch}},
	shorttitle = {Finger-writing with {Smartwatch}},
	url = {https://dl.acm.org/doi/10.1145/2699343.2699350},
	doi = {10.1145/2699343.2699350},
	abstract = {Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the fitness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user's arm, hand and possibly finger movements using its accelerometer and gyroscope sensors. Although user's arm and hand gestures are likely to be identified with ease using the smartwatch sensors, it is not clear how much of user's finger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures. We identify essential features of accelerometer and gyroscope data that reflect the movements of tendons (passing through the wrist) when performing a finger or a hand gesture. With these features, we build a classifier that can uniquely identify 37 (13 finger, 14 hand and 10 arm) gestures with an accuracy of 98{\textbackslash}\%. We further extend our gesture recognition to identify the characters written by the user with her index finger on a surface, and show that such finger-writing can also be accurately recognized with nearly 95\% accuracy. Our presented results will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
	author = {Xu, Chao and Pathak, Parth H. and Mohapatra, Prasant},
	month = feb,
	year = {2015},
	note = {228 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: HotMobile '15: The 16th International Workshop on Mobile Computing Systems and Applications
ISBN: 9781450333917
Place: Santa Fe New Mexico USA
Publisher: ACM},
	keywords = {untagged, tech:imu, classes:37, hardware:smartwatch},
	pages = {9--14},
}

@inproceedings{wang_evaluation_2009,
	address = {London},
	title = {Evaluation of local spatio-temporal features for action recognition},
	isbn = {978-1-901725-39-1},
	url = {http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html},
	doi = {10.5244/C.23.124},
	abstract = {Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2009},
	publisher = {British Machine Vision Association},
	author = {Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
	year = {2009},
	note = {1494 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:svm, untagged, type:seminal, classes:25},
	pages = {124.1--124.11},
	file = {Submitted Version:/Users/brk/Zotero/storage/QFB8QYCI/Wang et al. - 2009 - Evaluation of local spatio-temporal features for a.pdf:application/pdf},
}

@article{pham_hand_2020,
	title = {Hand detection and segmentation using multimodal information from {Kinect}},
	url = {https://ieeexplore.ieee.org/document/9237785/},
	doi = {10.1109/MAPR49794.2020.9237785},
	abstract = {Nowadays, hand gestures are becoming one of the most natural and intuitive ways of communication between human and computer. To this end, a complex process including hand gesture acquisition, hand detection, gesture representation and recognition must be carried out. This paper presents a method that detects hand and segments hand regions from images captured by a Kinect sensor. As Kinect sensor provides not only RGB images as conventional camera, but also depth and skeleton, in our work, we incorporate multi-modal data from Kinect to deal with hand detection and segmentation. Specifically, we use skeleton to approximately determine hand palm. Then a skin based detector will be applied to discard non-skin pixels from the region of interest. Using depth data helps to limit the human body regions and remove false positive regions from the previous steps. Finally, morphological operations will be applied to fill holes in the hand region. The main advantage of this method is very easy to implement and it performs in real-time on an ordinary computer. We evaluate the proposed method on a dataset of hand gestures captured from different viewpoints. Experiment shows that it provides reasonable accuracy at very high frame rate. It also produces comparable performance in comparison with deep learning based methods.},
	urldate = {2023-07-11},
	journal = {2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
	author = {Pham, Van-Tien and Le, Thi-Lan and Tran, Thanh-Hai and Nguyen, Thanh Phuong},
	month = oct,
	year = {2020},
	note = {2 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)
ISBN: 9781728165554
Place: Ha Noi, Vietnam
Publisher: IEEE},
	keywords = {hardware:kinect, untagged},
	pages = {1--6},
}

@article{ghotkar_dynamic_2016,
	title = {Dynamic {Hand} {Gesture} {Recognition} using {Hidden} {Markov} {Model} by {Microsoft} {Kinect} {Sensor}},
	volume = {150},
	issn = {09758887},
	url = {http://www.ijcaonline.org/archives/volume150/number5/ghotkar-2016-ijca-911498.pdf},
	doi = {10.5120/ijca2016911498},
	abstract = {Hand gesture recognition is one of the leading applications of human computer interaction. With diversity of applications of hand gesture recognition, sign language interpretation is the most demanding application. In this paper, dynamic hand gesture recognition for few subset of Indian sign language recognition was considered. The use of depth camera such as Kinect sensor gave skeleton information of signer body. After detailed study of dynamic ISL vocabulary with reference to skeleton joint information, angle has identified as a feature with reference to two moving hand. Here, real time video has been captured and gesture was recognized using Hidden Markov Model (HMM). Ten state HMM model was designed and normalized angle feature of dynamic sign was being observed. Maximum likelihood probability symbol was considered as a recognized gesture. Algorithm has been tested on ISL 20 dynamic signs of total 800 training set of four persons and achieved 89.25\% average accuracy. General Terms Human Computer Interaction, Pattern Recognition, Machine Learning},
	number = {5},
	urldate = {2023-07-11},
	journal = {International Journal of Computer Applications},
	author = {Ghotkar, Archana and Vidap, Pujashree and Deo, Kshitish},
	month = sep,
	year = {2016},
	note = {24 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {app:indian-sl, app:sign-language, classes:20, hardware:kinect, model:hmm, untagged},
	pages = {5--9},
	file = {Full Text:/Users/brk/Zotero/storage/7RH4TGA8/Ghotkar et al. - 2016 - Dynamic Hand Gesture Recognition using Hidden Mark.pdf:application/pdf},
}

@article{moeslund_survey_2006,
	title = {A survey of advances in vision-based human motion capture and analysis},
	volume = {104},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206001263},
	doi = {10.1016/j.cviu.2006.08.002},
	abstract = {Semantic Scholar extracted view of "A survey of advances in vision-based human motion capture and analysis" by T. Moeslund et al.},
	language = {en},
	number = {2-3},
	urldate = {2023-07-11},
	journal = {Computer Vision and Image Understanding},
	author = {Moeslund, Thomas B. and Hilton, Adrian and Krüger, Volker},
	month = nov,
	year = {2006},
	note = {2830 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, type:survey, untagged},
	pages = {90--126},
	file = {Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf:/Users/brk/Zotero/storage/7NIXC2ZU/Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf:application/pdf},
}

@article{lepetit_monocular_2005,
	title = {Monocular {Model}-{Based} {3D} {Tracking} of {Rigid} {Objects}: {A} {Survey}},
	volume = {1},
	issn = {1572-2740, 1572-2759},
	shorttitle = {Monocular {Model}-{Based} {3D} {Tracking} of {Rigid} {Objects}},
	url = {http://www.nowpublishers.com/article/Details/CGV-001},
	doi = {10.1561/0600000001},
	abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Augmented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated interfaces can use. Computer Vision offers solutions that are cheap, practical and non-invasive.This survey reviews the different techniques and approaches that have been developed by industry and research. First, important mathematical tools are introduced: Camera representation, robust estimation and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery concludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking.Because it encompasses many computer vision techniques from low-level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Lepetit, Vincent and Fua, Pascal},
	year = {2005},
	note = {747 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, type:survey, untagged},
	pages = {1--89},
	file = {Submitted Version:/Users/brk/Zotero/storage/8KHLPAGM/Lepetit and Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects.pdf:application/pdf},
}

@inproceedings{oikonomidis_efficient_2011,
	address = {Dundee},
	title = {Efficient model-based {3D} tracking of hand articulations using {Kinect}},
	isbn = {978-1-901725-43-8},
	url = {http://www.bmva.org/bmvc/2011/proceedings/paper101/index.html},
	doi = {10.5244/C.25.101},
	abstract = {We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor. We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup. Being model based, it provides continuous solutions to the problem of tracking hand articulations. Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz).},
	language = {en},
	urldate = {2023-07-11},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2011},
	publisher = {British Machine Vision Association},
	author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis},
	year = {2011},
	note = {1008 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {hardware:kinect, untagged},
	pages = {101.1--101.11},
	file = {Oikonomidis et al. - 2011 - Efficient model-based 3D tracking of hand articula.pdf:/Users/brk/Zotero/storage/2JTTNNXG/Oikonomidis et al. - 2011 - Efficient model-based 3D tracking of hand articula.pdf:application/pdf},
}

@inproceedings{rogowitz_saliency_2015,
	address = {San Francisco, California, United States},
	title = {Saliency detection for videos using {3D} {FFT} local spectra},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2077762},
	doi = {10.1117/12.2077762},
	abstract = {Bottom-up spatio-temporal saliency detection identifies perceptually important regions of interest in video sequences. The center-surround model proves to be useful for visual saliency detection. In this work, we explore using 3D FFT local spectra as features for saliency detection within the center-surround framework. We develop a spectral location based decomposition scheme to divide a 3D FFT cube into two components, one related to temporal changes and the other related to spatial changes. Temporal saliency and spatial saliency are detected separately using features derived from each spectral component through a simple center-surround comparison method. The two detection results are then combined to yield a saliency map. We apply the same detection algorithm to different color channels (YIQ) and incorporate the results into the final saliency determination. The proposed technique is tested with the public CRCNS database. Both visual and numerical evaluations verify the promising performance of our technique.},
	urldate = {2023-07-11},
	author = {Long, Zhiling and AlRegib, Ghassan},
	editor = {Rogowitz, Bernice E. and Pappas, Thrasyvoulos N. and De Ridder, Huib},
	month = mar,
	year = {2015},
	note = {10 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {model:fft, tech:rgb, untagged},
	pages = {93941G},
}

@article{wachs_vision-based_2011,
	title = {Vision-based hand-gesture applications},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1897816.1897838},
	doi = {10.1145/1897816.1897838},
	abstract = {Body posture and finger pointing are a natural modality for human-machine interaction, but first the system must know what it's seeing.},
	language = {en},
	number = {2},
	urldate = {2023-07-11},
	journal = {Communications of the ACM},
	author = {Wachs, Juan Pablo and Kölsch, Mathias and Stern, Helman and Edan, Yael},
	month = feb,
	year = {2011},
	note = {653 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:rgb, untagged},
	pages = {60--71},
	file = {Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf:/Users/brk/Zotero/storage/AGVRM526/Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf:application/pdf},
}

@article{breiman_no_2001,
	title = {[{No} title found]},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	number = {1},
	urldate = {2023-07-11},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	keywords = {background, model:random-forests, type:seminal, untagged},
	pages = {5--32},
	file = {Full Text:/Users/brk/Zotero/storage/HSTITUV6/Breiman - 2001 - [No title found].pdf:application/pdf},
}

@inproceedings{michahial_hand_2015,
	title = {Hand gesture recognition using support vector machine},
	url = {https://www.semanticscholar.org/paper/Hand-gesture-recognition-using-support-vector-Michahial-Azeez/4deb239f4366c461ab092f17828aed9a03b52e8d},
	abstract = {----------------------------------------------------------ABSTRACT----------------------------------------------------------The images contain measurement information of key interest for a variety of research and application areas. On the other hand, computers have become an inseparable part of our society, influencing many aspects of our daily lives in terms of communication and interaction. The main motive is to develop a system that can simplify the way humans interact with Computers. The system is designed using Canny’s edge detection for edge detection and Histogram of gradients for feature extraction and the Support Vector Machine (SVM) Classifier which is widely used for classification and regression testing. SVM training algorithm builds a model that predicts whether a new example falls into one category or other. And the classifier learns from the data points in examples when they are classified belonging to their respective categories.},
	urldate = {2023-07-11},
	author = {Michahial, S. and Azeez, Beebi Hajira and Rani, R.},
	year = {2015},
	keywords = {model:svm, tech:rgb, untagged},
	file = {Full Text PDF:/Users/brk/Zotero/storage/JLWRYAFF/Michahial et al. - 2015 - Hand gesture recognition using support vector mach.pdf:application/pdf},
}

@article{harrison_tapsense_2011,
	title = {{TapSense}: enhancing finger interaction on touch surfaces},
	shorttitle = {{TapSense}},
	url = {https://dl.acm.org/doi/10.1145/2047196.2047279},
	doi = {10.1145/2047196.2047279},
	abstract = {We present TapSense, an enhancement to touch interaction that allows conventional surfaces to identify the type of object being used for input. This is achieved by segmenting and classifying sounds resulting from an object's impact. For example, the diverse anatomy of a human finger allows different parts to be recognized including the tip, pad, nail and knuckle - without having to instrument the user. This opens several new and powerful interaction opportunities for touch input, especially in mobile devices, where input is extremely constrained. Our system can also identify different sets of passive tools. We conclude with a comprehensive investigation of classification accuracy and training implications. Results show our proof-of-concept system can support sets with four input types at around 95\% accuracy. Small, but useful input sets of two (e.g., pen and finger discrimination) can operate in excess of 99\% accuracy.},
	language = {en},
	urldate = {2023-07-11},
	journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
	author = {Harrison, Chris and Schwarz, Julia and Hudson, Scott E.},
	month = oct,
	year = {2011},
	note = {232 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: UIST '11: The 24th Annual ACM Symposium on User Interface Software and Technology
ISBN: 9781450307161
Place: Santa Barbara California USA
Publisher: ACM},
	keywords = {untagged, tapsense},
	pages = {627--636},
}

@article{karantonis_implementation_2006,
	title = {Implementation of a {Real}-{Time} {Human} {Movement} {Classifier} {Using} a {Triaxial} {Accelerometer} for {Ambulatory} {Monitoring}},
	volume = {10},
	issn = {1089-7771},
	url = {http://ieeexplore.ieee.org/document/1573717/},
	doi = {10.1109/TITB.2005.856864},
	abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence},
	language = {en},
	number = {1},
	urldate = {2023-07-11},
	journal = {IEEE Transactions on Information Technology in Biomedicine},
	author = {Karantonis, D.M. and Narayanan, M.R. and Mathie, M. and Lovell, N.H. and Celler, B.G.},
	month = jan,
	year = {2006},
	note = {1273 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {tech:accelerometer, tech:imu, type:seminal},
	pages = {156--167},
}

@inproceedings{mustafa_hand_2007,
	title = {Hand {Gesture} {Recognition} {Using} {Artificial} {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-Artificial-Neural-Mustafa/64cbb00f06981b59ebea1faf471695280d10a2bb},
	abstract = {Hand gesture has been part of human communication, where, young children usually communicate by using gesture before they can talk. Adults may have to also gesture if they need to or they are indeed mute or deaf. Thus the idea of teaching a machine to also learn gestures is very appealing due to its unique mode of communications. A reliable hand gesture recognition system will make the remote control become obsolete. However, many of the new techniques proposed are complicated to be implemented in real time, especially as a human machine interface. 
This thesis focuses on recognizing hand gesture in static posture. Since static hand postures not only can express some concepts, but also can act as special transition states in temporal gestures recognition, thus estimating static hand postures is in fact a big topics in gesture recognition. A database consists of 200 gesture images have been built, where five volunteers had help in the making of the database. The images were captured in a controlled environment and the postures are free from occlusion where the background is uncluttered and the hand is assumed to have been localized. 
 
A system was then built to recognize the hand gesture. The captured image will be first preprocessed in order to binarize the palm region, where Sobel edge detection technique has been employed, with later followed by morphological operation. A new feature extraction technique has been developed, based on horizontal and vertical states transition count, and the ratio of hand area with respect to the whole area of image. These set of features have been proven to have high intra class dissimilarity attributes. 
 
In order to have a system that can be easily trained, artificial neural networks has been chosen in the classification stage. A multilayer perceptron with back-propagation algorithm has been developed, thus the system is actually in-built to be used as a human machine interface. The gesture recognition system has been built and tested in Matlab, where simulations have shown promising results. The performance of recognition rate in this research is 95\% which shows a major improvement in comparison to the available methods.},
	urldate = {2023-07-11},
	author = {Mustafa, M. A.},
	year = {2007},
	keywords = {movement:static, model:nn},
}

@inproceedings{hassanpour_visionbased_2008,
	title = {Vision­{Based} {Hand} {Gesture} {Recognition} for {Human} {Computer} {Interaction}: {A} {Review}},
	shorttitle = {Vision­{Based} {Hand} {Gesture} {Recognition} for {Human} {Computer} {Interaction}},
	url = {https://www.semanticscholar.org/paper/Vision%C2%ADBased-Hand-Gesture-Recognition-for-Human-A-Hassanpour-Wong/1285c27b3a3b2304274b88676fc20086bf5cc3dd},
	abstract = {Evolution of user interfaces shapes the change in the human­computer interaction. With the rapid emergence of three­dimensional (3­D) applications; the need for a new type of interaction device arises as traditional devices such as mouse, keyboard, and joystick become inefficient and cumbersome within these virtual environments. Intuitive and naturalness characteristics of “Ha nd Gestures” in human computer interaction have been the driving force and motivation to develop an interaction device which can replace current unwieldy tools. This study is a survey on the methods of analyzing, modeling and recognizing hand gestures in the context of human­ computer interaction. Taxonomy of the methods based on the applications that they have been developed for and the approaches that they have used to represent gestures is presented. Direction of future developments is also discussed.},
	urldate = {2023-07-11},
	author = {Hassanpour, R. and Wong, Stephan and Shahbahrami, A. and Wong, J.},
	year = {2008},
	keywords = {tech:rgb, type:survey, untagged},
	file = {Full Text PDF:/Users/brk/Zotero/storage/79Q9UGWM/Hassanpour et al. - 2008 - Vision­Based Hand Gesture Recognition for Human Co.pdf:application/pdf},
}

@article{rigoll_new_1997,
	title = {New improved feature extraction methods for real-time high performance image sequence recognition},
	volume = {4},
	url = {http://ieeexplore.ieee.org/document/595396/},
	doi = {10.1109/ICASSP.1997.595396},
	abstract = {This paper describes new feature extraction methods which can be used very effectively in combination with statistical methods for image sequence recognition. Although these feature extraction methods can be used for a wide variety of image sequence processing applications, the target application presented in this paper is gesture recognition. The novel feature extraction methods have been integrated into an HMM-based gesture recognition system and led to substantial improvements for this system. It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence. This resulted in the fact that it was possible to use the novel features in combination with a new architecture for statistical image sequence recognition. The result of this investigation is a high performance gesture recognition system with significantly improved recognition rates and real-time capabilities.},
	urldate = {2023-07-11},
	journal = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
	author = {Rigoll, G. and Kosmala, A.},
	year = {1997},
	note = {20 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing
ISBN: 9780818679193
Place: Munich, Germany
Publisher: IEEE Comput. Soc. Press},
	keywords = {model:hmm, untagged},
	pages = {2901--2904},
}

@article{eickeler_hidden_1998,
	title = {Hidden {Markov} model based continuous online gesture recognition},
	volume = {2},
	url = {http://ieeexplore.ieee.org/document/711914/},
	doi = {10.1109/ICPR.1998.711914},
	abstract = {Presents the extension of an existing vision-based gesture recognition system using hidden Markov models(HMMs). Several improvements have been carried out in order to increase the capabilities and the functionality of the system. These improvements include position independent recognition, rejection of unknown gestures, and continuous online recognition of spontaneous gestures. We show that especially the latter requirement is highly complicated and demanding, if we allow the user to move in front of the camera without any restrictions and to perform the gestures spontaneously at any arbitrary moment. We present solutions to this problem by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.},
	urldate = {2023-07-11},
	journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
	author = {Eickeler, S. and Kosmala, A. and Rigoll, G.},
	year = {1998},
	note = {39 citations (Crossref) [2023-07-11]
140 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: Fourteenth International Conference on Pattern Recognition
ISBN: 9780818685125
Place: Brisbane, Qld., Australia
Publisher: IEEE Comput. Soc},
	keywords = {from:cite.bib, model:hmm, tech:rgb, untagged},
	pages = {1206--1208},
}

@inproceedings{shyamala_survey_2014,
	title = {A {SURVEY} {OF} {VISION} {BASED} {HAND} {GESTURE} {RECOGNITION}},
	url = {https://www.semanticscholar.org/paper/A-SURVEY-OF-VISION-BASED-HAND-GESTURE-RECOGNITION-Shyamala/01fb6988163209f16021dec3320956de74a8f45d},
	abstract = {Gesture recognition is to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. Hand Gestures have greater importance in designing an intelligent and efficient human–computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition, gesture taxonomies, representations. Vision based Gesture recognition has the potential to be a natural and powerful tool supporting efficient and intuitive interaction between the human and the computer. Visual interpretation of hand gestures can help in achieving the ease and naturalness desired for Human Computer Interaction (HCI). It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition.},
	urldate = {2023-07-11},
	author = {Shyamala, M.},
	year = {2014},
	keywords = {movement:dynamic, movement:static, tech:rgb, type:survey, untagged},
}

@inproceedings{mahmoud_convolutional_2021,
	title = {Convolutional neural networks framework for human hand gesture recognition},
	volume = {10},
	url = {https://beei.org/index.php/EEI/article/view/2926},
	doi = {10.11591/eei.v10i4.2926},
	abstract = {Recently, the recognition of human hand gestures is becoming a valuable technology for various applications like sign language recognition, virtual games and robotics control, video surveillance, and home automation. Owing to the recent development of deep learning and its excellent performance, deep learning-based hand gesture recognition systems can provide promising results. However, accurate recognition of hand gestures remains a substantial challenge that faces most of the recently existing recognition systems. In this paper, convolutional neural networks (CNN) framework with multiple layers for accurate, effective, and less complex human hand gesture recognition has been proposed. Since the images of the infrared hand gestures can provide accurate gesture information through the low illumination environment, the proposed system is tested and evaluated on a database of hand-based near-infrared which including ten gesture poses. Extensive experiments prove that the proposed system provides excellent results of accuracy, precision, sensitivity (recall), and F1-score. Furthermore, a comparison with recently existing systems is reported.},
	urldate = {2023-07-11},
	booktitle = {Bulletin of {Electrical} {Engineering} and {Informatics}},
	author = {Mahmoud, Aseel Ghazi and Hasan, Ahmed Mudheher and Hassan, Nadia Moqbel},
	month = aug,
	year = {2021},
	note = {5 citations (Semantic Scholar/DOI) [2023-07-11]
ISSN: 2302-9285, 2089-3191
Issue: 4
Journal Abbreviation: Bulletin EEI},
	keywords = {model:cnn, tech:infrared, tech:rgb, tech:rgbd, untagged},
	pages = {2223--2230},
	file = {Full Text PDF:/Users/brk/Zotero/storage/NDPUWIZE/Mahmoud et al. - 2021 - Convolutional neural networks framework for human .pdf:application/pdf},
}

@inproceedings{meghanathan_survey_2011,
	address = {Berlin, Heidelberg},
	title = {A {Survey} on {Hand} {Gesture} {Recognition} in {Context} of {Soft} {Computing}},
	volume = {133},
	isbn = {978-3-642-17880-1 978-3-642-17881-8},
	url = {http://link.springer.com/10.1007/978-3-642-17881-8_5},
	doi = {10.1007/978-3-642-17881-8_5},
	abstract = {Hand gestures recognition is the natural way of Human Machine interaction and today many researchers in the academia and industry are interested in this direction. It enables human being to interact with machine very easily and conveniently without wearing any extra device. It can be applied from sign language recognition to robot control and from virtual reality to intelligent home systems. In this paper we are discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc. We also described hand detection methods in the preprocessed image for detecting the hand image. Most researchers used fingertips for hand detection in appearance based modeling. Finally we are comparing results given by different researchers after their implementation.},
	urldate = {2023-07-11},
	publisher = {Springer Berlin Heidelberg},
	author = {Chaudhary, Ankit and Raheja, J. L. and Das, Karen and Raheja, Sonia},
	editor = {Meghanathan, Natarajan and Kaushik, Brajesh Kumar and Nagamalai, Dhinaharan},
	year = {2011},
	doi = {10.1007/978-3-642-17881-8_5},
	note = {67 citations (Semantic Scholar/DOI) [2023-07-11]
Book Title: Advanced Computing
Series Title: Communications in Computer and Information Science},
	keywords = {untagged, type:survey, model:fuzzy},
	pages = {46--55},
}

@article{chen_survey_2013,
	title = {A {Survey} on {Hand} {Gesture} {Recognition}},
	url = {http://ieeexplore.ieee.org/document/6835606/},
	doi = {10.1109/CSA.2013.79},
	abstract = {Hand gesture recognition has become one of the key techniques of human-computer interaction (HCI). Many researchers are devoted in this field. In this paper, firstly the history of hand gesture recognition is discussed and the technical difficulties are also enumerated. Then, we analyze the definition of hand gesture and introduce the basic principle of it. The approaches for hand gesture recognition, such as vision-based, glove-based and depth-based, are contrasted briefly in this paper. But the former two methods are too simple and not natural enough. Currently, the new finger identification and hand gesture recognition technique with Kinect depth data is the most popular research direction. Finally, we discuss the application prospective of hand gesture recognition based on Kinect.},
	urldate = {2023-07-11},
	journal = {2013 International Conference on Computer Sciences and Applications},
	author = {Chen, Lingchen and Wang, Feng and Deng, Hui and Ji, Kaifan},
	month = dec,
	year = {2013},
	note = {75 citations (Semantic Scholar/DOI) [2023-07-11]
Conference Name: 2013 International Conference on Computer Sciences and Applications (CSA)
ISBN: 9780769551258
Place: Wuhan, China
Publisher: IEEE},
	keywords = {hardware:kinect, type:survey},
	pages = {313--316},
}

@article{alvi_pakistan_2007,
	title = {Pakistan {Sign} {Language} {Recognition} {Using} {Statistical} {Template} {Matching}},
	volume = {1},
	journal = {International Journal of Computer, Electrical, Automation, Control and Information Engineering, World Academy Of Science, Engineering and Technology},
	author = {Alvi, Aleem and Azhar, M.Y.B. and Usman, Mehmood and Mumtaz, Suleman and Rafiq, Sameer and Rehman, Razi and Ahmed, Israr},
	month = jan,
	year = {2007},
	keywords = {untagged, from:cite.bib},
}

@article{hurroo_sign_2020,
	title = {Sign {Language} {Recognition} {System} using {Convolutional} {Neural} {Network} and {Computer} {Vision}},
	volume = {9},
	journal = {International journal of engineering research and technology},
	author = {Hurroo, Mehreen and Elham, Mohammad Karim},
	year = {2020},
	keywords = {untagged, from:cite.bib},
}

@article{waskom_seaborn_2021,
	title = {seaborn: statistical data visualization},
	volume = {6},
	url = {https://doi.org/10.21105/joss.03021},
	doi = {10.21105/joss.03021},
	number = {60},
	journal = {Journal of Open Source Software},
	author = {Waskom, Michael L.},
	year = {2021},
	note = {1622 citations (Crossref) [2023-07-11]
1607 citations (Semantic Scholar/DOI) [2023-07-11]
Publisher: The Open Journal},
	keywords = {untagged, from:cite.bib, type:software-lib},
	pages = {3021},
}

@misc{the_pandas_development_team_pandas-devpandas_2020,
	title = {pandas-dev/pandas: {Pandas}},
	url = {https://doi.org/10.5281/zenodo.3509134},
	publisher = {Zenodo},
	author = {{The Pandas Development Team}},
	month = feb,
	year = {2020},
	doi = {10.5281/zenodo.3509134},
	keywords = {untagged, from:cite.bib, type:software-lib},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} graphics environment},
	volume = {9},
	doi = {10.1109/MCSE.2007.55},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	year = {2007},
	note = {17522 citations (Crossref) [2023-07-11]
Publisher: IEEE COMPUTER SOC},
	keywords = {from:cite.bib, type:software-lib},
	pages = {90--95},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	url = {https://doi.org/10.1038/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	number = {7825},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, Stéfan J. van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and Río, Jaime Fernández del and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {7070 citations (Crossref) [2023-07-11]
Publisher: Springer Science and Business Media LLC},
	keywords = {from:cite.bib, type:software-lib},
	pages = {357--362},
}

@misc{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	keywords = {untagged, from:cite.bib, type:software-lib},
}

@book{van_rossum_python_2009,
	address = {Scotts Valley, CA},
	title = {Python 3 {Reference} {Manual}},
	isbn = {1-4414-1269-7},
	publisher = {CreateSpace},
	author = {Van Rossum, Guido and Drake, Fred L.},
	year = {2009},
	keywords = {untagged, from:cite.bib, type:software-lib},
}

@inproceedings{matsakis_rust_2014,
	title = {The {Rust} language},
	volume = {34},
	booktitle = {{ACM} {SIGAda} {Ada} {Letters}},
	publisher = {ACM},
	author = {Matsakis, Nicholas D and Klock II, Felix S},
	year = {2014},
	note = {Issue: 3},
	keywords = {untagged, from:cite.bib, type:software-lib},
	pages = {103--104},
}

@misc{analogdevices_adxl335_2010,
	title = {{ADXL335} {Datasheet} and {Product} {Info} {\textbar} {Analog} {Devices}},
	url = {https://www.analog.com/en/products/adxl335.html#product-overview},
	publisher = {www.analog.com},
	author = {{AnalogDevices}},
	year = {2010},
	note = {Publication Title: ADXL335 Datasheet and Product Info {\textbar} Analog Devices},
	keywords = {untagged, from:cite.bib, type:datasheet},
}

@misc{arduino_arduino_2005,
	title = {Arduino - {Home}},
	url = {https://www.arduino.cc/},
	publisher = {www.arduino.cc},
	author = {{Arduino}},
	year = {2005},
	note = {Publication Title: Arduino - Home},
	keywords = {untagged, from:cite.bib, type:datasheet},
}

@misc{texas_instruments_cd74hc4067_2003,
	title = {{CD74HC4067} data sheet, product information and support {\textbar} {TI}.com},
	url = {https://www.ti.com/product/CD74HC4067},
	publisher = {www.ti.com},
	author = {{Texas Instruments}},
	month = jul,
	year = {2003},
	note = {Publication Title: CD74HC4067 data sheet, product information and support {\textbar} TI.com},
	keywords = {untagged, from:cite.bib},
}

@misc{arduino_arduino_2016,
	title = {Arduino {Nano} 33 {BLE} — {Arduino} {Official} {Store}},
	url = {https://store.arduino.cc/products/arduino-nano-33-ble},
	publisher = {store.arduino.cc},
	author = {{Arduino}},
	year = {2016},
	note = {Publication Title: Arduino Official Store},
	keywords = {untagged, from:cite.bib, type:datasheet},
}

@article{roll_effectiveness_2016,
	title = {Effectiveness of {Occupational} {Therapy} {Interventions} for {Adults} {With} {Musculoskeletal} {Conditions} of the {Forearm}, {Wrist}, and {Hand}: {A} {Systematic} {Review}},
	volume = {71},
	issn = {0272-9490},
	url = {https://doi.org/10.5014/ajot.2017.023234},
	doi = {10.5014/ajot.2017.023234},
	number = {1},
	journal = {The American Journal of Occupational Therapy},
	author = {Roll, Shawn C. and Hardison, Mark E.},
	month = dec,
	year = {2016},
	note = {38 citations (Crossref) [2023-07-11]
45 citations (Semantic Scholar/DOI) [2023-07-11]
\_eprint: https://research.aota.org/ajot/article-pdf/71/1/7101180010p1/68296/7101180010p1.pdf},
	keywords = {untagged, from:cite.bib},
	pages = {7101180010p1--7101180010p12},
}

@article{rempel_effect_2008,
	title = {Effect of wrist posture on carpal tunnel pressure while typing},
	volume = {26},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jor.20599},
	doi = {https://doi.org/10.1002/jor.20599},
	number = {9},
	journal = {Journal of Orthopaedic Research},
	author = {Rempel, David M. and Keir, Peter J. and Bach, Joel M.},
	year = {2008},
	note = {62 citations (Crossref) [2023-07-11]
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jor.20599},
	keywords = {untagged, carpal tunnel syndrome, keyboard, neuropathy, occupation, overuse, from:cite.bib},
	pages = {1269--1273},
}

@article{whitehead_gesture_2014,
	title = {Gesture {Recognition} with {Accelerometers} for {Game} {Controllers}, {Phones} and {Wearables}},
	volume = {3},
	doi = {10.7603/s40601-013-0042-9},
	journal = {GSTF Journal on Computing (JoC)},
	author = {Whitehead, A.D.},
	month = apr,
	year = {2014},
	note = {0 citations (Crossref) [2023-07-11]
1 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged, from:cite.bib},
}

@article{keskin_real_2003,
	title = {Real time hand tracking and {3D} gesture recognition for interactive interfaces using {HMM}},
	journal = {Proceedings of the joint international conference ICANN/ICONIP},
	author = {Keskin, Cem and Erkan, A.N. and Akarun, L},
	month = jan,
	year = {2003},
	keywords = {untagged, from:cite.bib},
}

@article{kratz_wiizards_2007,
	title = {Wiizards: {3D} gesture recognition for game play input},
	doi = {10.1145/1328202.1328241},
	journal = {Proceedings of the 2007 Conference on Future Play, Future Play '07},
	author = {Kratz, Louis and Smith, Matthew and Lee, Frank},
	month = jan,
	year = {2007},
	note = {19 citations (Crossref) [2023-07-11]
41 citations (Semantic Scholar/DOI) [2023-07-11]
ISBN: 978-1-59593-943-2},
	keywords = {untagged, from:cite.bib},
	pages = {209--212},
}

@inproceedings{segen_fast_1998,
	title = {Fast and accurate {3D} gesture recognition interface},
	volume = {1},
	isbn = {0-8186-8512-3},
	doi = {10.1109/ICPR.1998.711086},
	author = {Segen, Jakub and Kumar, S.},
	month = sep,
	year = {1998},
	note = {11 citations (Crossref) [2023-07-11]
41 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged, from:cite.bib},
	pages = {86 -- 91 vol.1},
}

@inproceedings{gupta_defocus_2019,
	title = {A {Defocus} {Based} {Novel} {Keyboard} {Design}},
	doi = {10.1109/CICT48419.2019.9066224},
	author = {Gupta, Priyanshu and Goswamy, Tushar and Kumar, Himanshu and Venkatesh, K.S.},
	month = dec,
	year = {2019},
	note = {1 citations (Crossref) [2023-07-11]
1 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged, from:cite.bib},
	pages = {1--6},
}

@article{izotov_recognition_2021,
	title = {Recognition of handwritten {MNIST} digits on low-memory 2 {Kb} {RAM} {Arduino} board using {LogNNet} reservoir neural network},
	author = {Izotov, Yuriy and Velichko, Andrei and Ivshin, Aleksandr and Novitskiy, Roman},
	month = apr,
	year = {2021},
	keywords = {untagged, from:cite.bib},
}

@inproceedings{cristina_manresa-yee_human-computer_2005,
	title = {Human-computer interaction using gesture recognition and {3D} hand tracking},
	author = {Cristina Manresa-Yee, Javier Varona, Ramon Mas and Perales, F. J.},
	year = {2005},
	keywords = {untagged, from:cite.bib},
}

@article{marcelo_gefighters_2006,
	title = {{GeFighters}: an {Experiment} for {Gesture}-based {Interaction} {Analysis} in a {Fighting} {Game}},
	author = {Marcelo, João and Farias, Thiago and Pessoa, Saulo and Moura, Guilherme and Teichrieb, Veronica},
	month = jan,
	year = {2006},
	keywords = {untagged, from:cite.bib},
}

@article{lee_hmm-based_1999,
	title = {An {HMM}-{Based} {Threshold} {Model} {Approach} for {Gesture} {Recognition}},
	volume = {21},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Lee, Hyeon-Kyu and Kim, Jin-Hyung},
	year = {1999},
	keywords = {untagged, from:cite.bib},
	pages = {961--973},
}

@article{starner_visual_1995,
	title = {Visual recognition of american sign language using hidden markov models},
	author = {Starner, Thad and Pentland, Alex},
	month = may,
	year = {1995},
	keywords = {untagged, from:cite.bib},
	file = {Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf:/Users/brk/Zotero/storage/KBBCZYDF/Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf:application/pdf},
}

@inproceedings{hofmann_velocity_1998,
	address = {Berlin, Heidelberg},
	title = {Velocity profile based recognition of dynamic gestures with discrete {Hidden} {Markov} {Models}},
	isbn = {978-3-540-69782-4},
	url = {https://link.springer.com/chapter/10.1007/BFb0052991},
	booktitle = {Gesture and {Sign} {Language} in {Human}-{Computer} {Interaction}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hofmann, Frank G. and Heyer, Peter and Hommel, Günter},
	editor = {Wachsmuth, Ipke and Fröhlich, Martin},
	year = {1998},
	keywords = {untagged, from:cite.bib},
	pages = {81--95},
}

@misc{frank_hofmann_sensorglove_1995,
	title = {{SensorGlove}: {Anthropomorphic} {Robot} {Hand}},
	url = {https://pdv.cs.tu-berlin.de/forschung/SensorGlove2_engl.html},
	author = {{Frank Hofmann} and {Jürgen Henz}},
	year = {1995},
	keywords = {untagged, from:cite.bib},
}

@misc{arregui_sensor_2017,
	title = {Sensor {Glove}},
	url = {https://sensorglove.wixsite.com/sensorglove/producto},
	author = {Arregui, Patxi Xabier Quintana},
	year = {2017},
	keywords = {untagged, from:cite.bib},
}

@inproceedings{tuulari_soapbox_2002,
	address = {Berlin, Heidelberg},
	title = {{SoapBox}: {A} {Platform} for {Ubiquitous} {Computing} {Research} and {Applications}},
	isbn = {978-3-540-45866-1},
	booktitle = {Pervasive {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tuulari, Esa and Ylisaukko-oja, Arto},
	editor = {Mattern, Friedemann and Naghshineh, Mahmoud},
	year = {2002},
	keywords = {untagged, from:cite.bib},
	pages = {125--138},
}

@misc{immersion_inc_immersion_2005,
	title = {Immersion {Inc} {CyberGlove} {II}},
	url = {https://www.immersion.fr/en/cyberglove-ii/},
	author = {{Immersion Inc}},
	year = {2005},
	keywords = {untagged, from:cite.bib},
}

@misc{fifth_dimension_technologies_dataglove5_2005,
	title = {{DataGlove5}},
	url = {https://5dt.com/5dt-data-glove-ultra/},
	author = {{Fifth Dimension Technologies}},
	year = {2005},
	keywords = {untagged, from:cite.bib},
}

@article{elman_distributed_2004,
	title = {Distributed representations, simple recurrent networks, and grammatical structure},
	volume = {7},
	journal = {Machine Learning},
	author = {Elman, Jeffrey L.},
	year = {2004},
	keywords = {untagged, from:cite.bib},
	pages = {195--225},
}

@inproceedings{heumer_grasp_2007,
	title = {Grasp {Recognition} with {Uncalibrated} {Data} {Gloves} - {A} {Comparison} of {Classification} {Methods}},
	doi = {10.1109/VR.2007.352459},
	booktitle = {Proceedings - {IEEE} {Virtual} {Reality}},
	author = {Heumer, Guido and Ben Amor, Heni and Weber, Matthias and Jung, Bernhard},
	month = mar,
	year = {2007},
	note = {33 citations (Crossref) [2023-07-11]
64 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged, from:cite.bib},
	pages = {19--26},
}

@misc{cyberglove_inc_wireless_nodate,
	title = {Wireless {CyberGlove} {II} {Motion} {Capture} {Data} {Glove}},
	url = {http://www.cyberglovesystems.com/cyberglove-ii/},
	author = {{CyberGlove Inc}},
	keywords = {untagged, from:cite.bib},
}

@article{myers_comparative_1981,
	title = {A comparative study of several dynamic time-warping algorithms for connected-word recognition},
	volume = {60},
	doi = {10.1002/j.1538-7305.1981.tb00272.x},
	number = {7},
	journal = {The Bell System Technical Journal},
	author = {Myers, C. S. and Rabiner, L. R.},
	year = {1981},
	note = {274 citations (Crossref) [2023-07-11]
481 citations (Semantic Scholar/DOI) [2023-07-11]},
	keywords = {untagged, from:cite.bib},
	pages = {1389--1409},
}

@misc{karpathy_recipe_2019,
	title = {A {Recipe} for {Training} {Neural} {Networks}},
	url = {https://karpathy.github.io/2019/04/25/recipe/},
	publisher = {https://karpathy.github.io},
	author = {Karpathy, Andrej},
	year = {2019},
	note = {Publication Title: Andrej Karpathy Blog},
	keywords = {untagged, from:cite.bib},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	journal = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	keywords = {untagged, from:cite.bib},
	pages = {1929--1958},
}

@article{agarap_deep_2018,
	title = {Deep {Learning} using {Rectified} {Linear} {Units} ({ReLU})},
	volume = {abs/1803.08375},
	journal = {ArXiv},
	author = {Agarap, Abien Fred},
	year = {2018},
	keywords = {untagged, from:cite.bib},
}

@inproceedings{quinlan_c45_1992,
	title = {C4.5: {Programs} for {Machine} {Learning}},
	author = {Quinlan, J. Ross},
	year = {1992},
	keywords = {untagged, from:cite.bib},
}

@inproceedings{senin_dynamic_2008,
	title = {Dynamic {Time} {Warping} {Algorithm} {Review}},
	author = {Senin, Pavel},
	year = {2008},
	keywords = {untagged, from:cite.bib},
}

@inproceedings{gillian_gesture_2014,
	title = {The gesture recognition toolkit},
	booktitle = {J. {Mach}. {Learn}. {Res}.},
	author = {Gillian, Nicholas Edward and Paradiso, Joseph A.},
	year = {2014},
	keywords = {untagged, from:cite.bib},
}

@article{bergstra_random_2012,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	volume = {13},
	issn = {1532-4435},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Bergstra, James and Bengio, Yoshua},
	month = feb,
	year = {2012},
	note = {Publisher: JMLR.org},
	keywords = {untagged, deep learning, global optimization, model selection, neural networks, response surface modeling, from:cite.bib},
	pages = {281--305},
}

@incollection{noauthor_dynamic_2007,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Time} {Warping}},
	isbn = {978-3-540-74048-3},
	url = {https://doi.org/10.1007/978-3-540-74048-3_4},
	booktitle = {Information {Retrieval} for {Music} and {Motion}},
	publisher = {Springer Berlin Heidelberg},
	year = {2007},
	doi = {10.1007/978-3-540-74048-3_4},
	keywords = {untagged, from:cite.bib},
	pages = {69--84},
}

@book{geron_hands-machine_2019,
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
	isbn = {978-1-4920-3261-8},
	url = {https://books.google.co.za/books?id=HHetDwAAQBAJ},
	publisher = {O'Reilly Media},
	author = {Géron, A.},
	year = {2019},
	keywords = {untagged, from:cite.bib},
}

@misc{noauthor_sensor_nodate,
	title = {Sensor {Glove}},
}

@misc{noauthor_lucidvr_nodate,
	title = {{LucidVR}},
	url = {https://github.com/LucidVR/lucidgloves},
}

@misc{atltvhead_atltvhead_nodate,
	title = {Atltvhead {Gesture} {Recognition} {Bracer}},
	abstract = {Atltvhead Gesture Recognition Bracer - A TensorflowLite gesture detector for the atltvhead project and exploration into Data Science

This repository is my spin on Jennifer Wang's and Google Tensorflow's magic wand project, but for arm gestures},
	author = {{ATLTVHEAD}},
}

@misc{nick_gillian_gesture_nodate,
	title = {Gesture {Recognition} {Toolkit} ({GRT})},
	url = {https://github.com/nickgillian/grt},
	abstract = {The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.},
	author = {{Nick Gillian}},
}

@misc{mellis_gesture_nodate,
	title = {Gesture {Recognition} {Using} {Accelerometer} and {ESP}},
	url = {https://create.arduino.cc/projecthub/mellis/gesture-recognition-using-accelerometer-and-esp-71faa1},
	author = {{Mellis}},
}
