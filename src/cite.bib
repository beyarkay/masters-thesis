@article{abdelnasserWigestUbiquitousWifibased2014,
  title = {Wigest: {{A Ubiquitous Wifi-based Gesture Recognition System}}},
  shorttitle = {Wigest},
  author = {Abdelnasser, Heba and Harras, Khaled and Youssef, Moustafa},
  year = {2014},
  journal = {Qatar Foundation Annual Research Conference Proceedings Volume 2014 Issue 1},
  publisher = {{Hamad bin Khalifa University Press (HBKU Press)}},
  address = {{Qatar National Convention Center (QNCC), Doha, Qatar,}},
  doi = {10.5339/qfarc.2014.ITPP1127},
  url = {https://www.qscience.com/content/papers/10.5339/qfarc.2014.ITPP1127},
  urldate = {2023-06-23},
  abstract = {We present WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device. Compared to related work, WiGest is unique in using standard WiFi equipment, with no modifications, and no training for gesture recognition. The system identifies different signal change primitives, from which we construct mutually independent gesture families. These families can be mapped to distinguishable application actions. We address various challenges including cleaning the noisy signals, gesture type and attributes detection, reducing false positives due to interfering humans, and adapting to changing signal polarity. We implement a proof-of-concept prototype using off-the-shelf laptops and extensively evaluate the system in both an office environment and a typical apartment with standard WiFi access points. Our results show that WiGest detects the basic primitives with an accuracy of 87.5\% using a single AP only, including through-the-wall non-line-of-sight scenarios. This accuracy increases to 96\% using three overheard APs. In addition, when evaluating the system using a multi-media player application, we achieve a classification accuracy of 96\%. This accuracy is robust to the presence of other interfering humans, highlighting WiGest's ability to enable future ubiquitous hands-free gesture-based interaction with mobile devices.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi},
  annotation = {415 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/J79GHKIF/Abdelnasser et al. - 2014 - Wigest A Ubiquitous Wifi-based Gesture Recognitio.pdf}
}

@article{abejeEthiopianSignLanguage2022,
  title = {Ethiopian Sign Language Recognition Using Deep Convolutional Neural Network},
  author = {Abeje, Bekalu Tadele and Salau, Ayodeji Olalekan and Mengistu, Abreham Debasu and Tamiru, Nigus Kefyalew},
  year = {2022},
  month = aug,
  journal = {Multimedia Tools and Applications},
  volume = {81},
  number = {20},
  pages = {29027--29043},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-022-12768-5},
  url = {https://link.springer.com/10.1007/s11042-022-12768-5},
  urldate = {2023-07-11},
  abstract = {In recent years, several technologies have been utilized to bridge the communication gap between persons who have hearing or speaking impairments and those who don't. This paper presents the development of a novel sign language recognition system which translates Ethiopian sign language (ETHSL) to Amharic alphabets using computer vision technology and Deep Convolutional Neural Network (CNN). The system accepts sign language images as input and gives Amharic text as the desired output. The proposed system comprises of three main stages which are: preprocessing, feature extraction, and recognition. The methodology employed involves data acquisition, preprocessing the acquired data, background normalization, image resizing, region of interest (ROI) identification, noise removal, brightness adjustment, and feature extraction, while Deep Convolutional Neural Network (CNN) was used for end-to-end classification. The data used in this study was acquired from students with hearing impairments at the Debre Markos Teaching College with an iPhone 6s phone which has a resolution of 3024\,\texttimes\,4020. The images are in JPEG file format and were collected in a controlled environment. The proposed system was implemented using Kera's (Tensorflow2.3.0 as backend) in python and tested using the image dataset collected from Debre Markos Teaching College graduating students of 2012. The results show that the running time was minimized by adjusting the images to a suitable size and color. In addition, the results show an improved recognition accuracy compared to previous works. The proposed model achieves 98.5\% training, 95.59\% validation, and 98.3\% testing accuracy of recognition.},
  langid = {english},
  keywords = {app:ethiopian-sl,app:sign-language,model:cnn},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@misc{abramsgentileentertainmentPowerGlove1989,
  title = {{{PowerGlove}}},
  author = {{Abrams Gentile Entertainment}},
  year = {1989},
  url = {https://web.archive.org/web/20150525222703/http://www.ageinc.com/tech/index.html},
  keywords = {hardware:powerglove,tech:flex}
}

@misc{adanarriagaCharaChorderTypeSpeed2022,
  title = {{{CharaChorder}} - {{Type}} at the Speed of Thought},
  author = {{Adan Arriaga}},
  year = {2022},
  url = {https://www.charachorder.com/},
  keywords = {type:product}
}

@article{adibSeeWallsWiFi2013,
  title = {See through Walls with {{WiFi}}!},
  author = {Adib, Fadel and Katabi, Dina},
  year = {2013},
  month = aug,
  journal = {Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM},
  pages = {75--86},
  publisher = {{ACM}},
  address = {{Hong Kong China}},
  doi = {10.1145/2486001.2486039},
  url = {https://dl.acm.org/doi/10.1145/2486001.2486039},
  urldate = {2023-07-12},
  abstract = {Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings.},
  isbn = {9781450320566},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {681 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/QWSG2ZW8/Adib and Katabi - 2013 - See through walls with WiFi!.pdf}
}

@article{agarapDeepLearningUsing2018,
  title = {Deep {{Learning}} Using {{Rectified Linear Units}} ({{ReLU}})},
  author = {Agarap, Abien Fred},
  year = {2018},
  journal = {ArXiv},
  volume = {abs/1803.08375},
  keywords = {background,from:cite.bib}
}

@article{ahaInstancebasedLearningAlgorithms1991,
  title = {Instance-Based Learning Algorithms},
  author = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
  year = {1991},
  month = jan,
  journal = {Machine Learning},
  volume = {6},
  number = {1},
  pages = {37--66},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00153759},
  url = {http://link.springer.com/10.1007/BF00153759},
  urldate = {2023-07-14},
  abstract = {Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-world databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.},
  langid = {english},
  keywords = {background,model:instance-based-learning},
  annotation = {2576 citations (Semantic Scholar/DOI) [2023-07-14]},
  file = {/Users/brk/Zotero/storage/QNMUDZH2/Aha et al. - 1991 - Instance-based learning algorithms.pdf}
}

@article{ahmedDFWiSLRDeviceFreeWiFibased2020,
  title = {{{DF-WiSLR}}: {{Device-Free Wi-Fi-based Sign Language Recognition}}},
  shorttitle = {{{DF-WiSLR}}},
  author = {Ahmed, Hasmath Farhana Thariq and Ahmad, Hafisoh and Narasingamurthi, Kulasekharan and Harkat, Houda and Phang, Swee King},
  year = {2020},
  month = nov,
  journal = {Pervasive and Mobile Computing},
  volume = {69},
  pages = {101289},
  issn = {15741192},
  doi = {10.1016/j.pmcj.2020.101289},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119220301267},
  urldate = {2023-07-12},
  abstract = {Recent advancements in wireless technologies enable pervasive and device free gesture recognition that enable assisted living utilizing off the shelf commercial Wi-Fi devices. This paper proposes a Device-Free Wi-Fi-based Sign Language Recognition (DF-WiSLR) for recognizing 30 static and 19 dynamic sign gestures. The raw Channel State Information (CSI) acquired from the Wi-Fi device for 49 sign gestures, with a volunteer performing the sign gestures in home and office environments. The proposed system adopts machine learning classifiers such as SVM, KNN, RF, NB, and a deep learning classifier CNN, for measuring the gesture recognition accuracy. To address the practical limitation of building a voluminous dataset, DF-WiSLR augments the originally acquired CSI values with Additive White Gaussian Noise (AWGN). Higher-order cumulant features of orders 2, 3, and 4 are extracted from the original and augmented data, as the machine learning classifiers demand manual feature extraction. To reduce the computational complexity of machine learning classifiers, an informative and reduced optimal feature subset is selected using MIFS. Whilst the pre-processed original and augmented CSI values directly fed as input to an 8-layer deep CNN, it performs auto feature extraction and selection. DF-WiSLR reported better recognition accuracies with SVM for static and dynamic gestures in both home and office environments. SVM achieved 93.4\% 98.8\% and 98.9\% accuracies in home and office environments respectively, for static gestures. For dynamic gestures, 92.3\% recognition accuracy achieved in home environment. On augmented data, the corresponding gesture recognition accuracy values reported are 97.1\%, 99.9\%, 99.9\%, and 98.5\%.},
  langid = {english},
  keywords = {app:sign-language,based-on-wifi,classes:49,model:cnn,model:knn,model:nb,model:random-forests,model:svm,movement:dynamic,movement:static,tech:wifi},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{ahmedRealtimeSignLanguage2021,
  title = {Real-Time Sign Language Framework Based on Wearable Device: Analysis of {{MSL}}, {{DataGlove}}, and Gesture Recognition},
  shorttitle = {Real-Time Sign Language Framework Based on Wearable Device},
  author = {Ahmed, M. A. and Zaidan, B. B. and Zaidan, A. A. and Alamoodi, A. H. and Albahri, O. S. and {Al-Qaysi}, Z. T. and Albahri, A. S. and Salih, Mahmood M.},
  year = {2021},
  month = aug,
  journal = {Soft Computing},
  volume = {25},
  number = {16},
  pages = {11101--11122},
  issn = {1432-7643, 1433-7479},
  doi = {10.1007/s00500-021-05855-6},
  url = {https://link.springer.com/10.1007/s00500-021-05855-6},
  urldate = {2023-03-07},
  abstract = {Researchers have been inspired to use technology to enable people with hearing and speech impairment to communicate and engage with others around them. Sensory approach to recognition facilitates real-time and accurate recognition of signs. Thus, this study proposes a Malaysian Sign Language (MSL) recognition framework. The framework consists of three sub-modules for the recognition of static isolated signs based on data collected from a DataGlove. The first module focuses on the characteristics of signs, yielding sign recognition system requirements. The second module describes the different steps required to develop a wearable sign-capture device. The third module discusses the real-time SL recognition approach, which uses a template-matching algorithm to recognize acquired data. The final design of the DataGlove with 65 data channel fulfils the requirement identified from an analysis of MSL. The DataGlove is able to record data for all of the signs (both dynamic and static) of MSL due to the wide range of captured hand features. As a result, the recognition engine can accurately recognize complex signs.},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{ahujaHandGestureRecognition2015,
  title = {Hand {{Gesture Recognition Using PCA}}},
  author = {Ahuja, M. and Singh, Amardeep},
  year = {2015},
  url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-PCA-Ahuja-Singh/43b7e88adbcdfcf5b25d337821c94a0f0ee525b3},
  urldate = {2023-07-12},
  abstract = {Interacting with physical world using expressive body movements is much easier and effective than just speaking. Gesture recognition turns up to be important field in the recent years. Communication through gestures has been used since early ages not only by physically challenged persons but nowadays for many other applications. As most predominantly hand is use to perform gestures, Hand Gesture Recognition have been widely accepted for numerous applications such as human computer interactions, robotics, sign language recognition, etc. This paper focuses on bare hand gesture recognition system by proposing a scheme using a database-driven hand gesture recognition based upon skin color model approach and thresholding approach along with an effective template matching with can be effectively used for human robotics applications and similar other applications.. Initially, hand region is segmented by applying skin color model in YCbCr color space. In the next stage otsu thresholding is applied to separate foreground and background. Finally, template based matching technique is developed using Principal Component Analysis (PCA) for recognition. The system is tested with the controlled and uncontrolled database and shows 100\% accuracy with controlled database and 91.43\% with low brightness images.},
  keywords = {app:hand-gesture-recognition,model:pca,tech:rgb},
  file = {/Users/brk/Zotero/storage/6NS5M2E7/Ahuja and Singh - 2015 - Hand Gesture Recognition Using PCA.pdf}
}

@article{aklAccelerometerbasedGestureRecognition2010,
  title = {Accelerometer-Based Gesture Recognition via Dynamic-Time Warping, Affinity Propagation, \&\#x00026; Compressive Sensing},
  author = {Akl, Ahmad and Valaee, Shahrokh},
  year = {2010},
  journal = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages = {2270--2273},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/ICASSP.2010.5495895},
  url = {http://ieeexplore.ieee.org/document/5495895/},
  urldate = {2023-03-07},
  abstract = {We propose a gesture recognition system based primarily on a single 3-axis accelerometer. The system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition. A dictionary of 18 gestures is defined and a database of over 3,700 repetitions is created from 7 users. Our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition, to the best of our knowledge. The proposed system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
  isbn = {9781424442959},
  keywords = {classes:10-29,participants:7,tech:accelerometer},
  annotation = {142 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/85QXVKVJ/Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf}
}

@article{aklNovelAccelerometerBasedGesture2011,
  title = {A {{Novel Accelerometer-Based Gesture Recognition System}}},
  author = {Akl, Ahmad and Feng, Chen and Valaee, Shahrokh},
  year = {2011},
  month = dec,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {12},
  pages = {6197--6205},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2011.2165707},
  url = {http://ieeexplore.ieee.org/document/5993550/},
  urldate = {2023-03-07},
  abstract = {In this paper, we address the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an {$\mathscr{l}$}1-minimization problem. The gesture recognition system operates primarily on data from a single 3-axis accelerometer and comprises two main stages: a training stage and a testing stage. For training, the system employs dynamic time warping as well as affinity propagation to create exemplars for each gesture while for testing, the system projects all candidate traces and also the unknown trace onto the same lower dimensional subspace for recognition. A dictionary of 18 gestures is defined and a database of over 3700 traces is created from seven subjects on which the system is tested and evaluated. To the best of our knowledge, our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition. The system achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
  keywords = {claims-to-be-first,classes:10-29,classes:18,from:cite.bib,model:dtw,msc,participants:7,read-priority-1,tech:accelerometer},
  annotation = {100 citations (Crossref) [2023-07-11] 170 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/SAQUJ2PY/Akl et al. - 2011 - A Novel Accelerometer-Based Gesture Recognition Sy.pdf}
}

@article{al-qanessWiGeRWiFiBasedGesture2016,
  title = {{{WiGeR}}: {{WiFi-Based Gesture Recognition System}}},
  shorttitle = {{{WiGeR}}},
  author = {{Al-qaness}, Mohammed and Li, Fangmin},
  year = {2016},
  month = jun,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {5},
  number = {6},
  pages = {92},
  issn = {2220-9964},
  doi = {10.3390/ijgi5060092},
  url = {http://www.mdpi.com/2220-9964/5/6/92},
  urldate = {2023-06-22},
  abstract = {Recently, researchers around the world have been striving to develop and modernize human\textendash computer interaction systems by exploiting advances in modern communication systems. The priority in this field involves exploiting radio signals so human\textendash computer interaction will require neither special devices nor vision-based technology. In this context, hand gesture recognition is one of the most important issues in human\textendash computer interfaces. In this paper, we present a novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions. We extract CSI from any common WiFi router and then filter out the noise to obtain the CSI fluctuation trends generated by hand motions. We design a novel and agile segmentation and windowing algorithm based on wavelet analysis and short-time energy to reveal the specific pattern associated with each hand gesture and detect duration of the hand motion. Furthermore, we design a fast dynamic time warping algorithm to classify our system's proposed hand gestures. We implement and test our system through experiments involving various scenarios. The results show that WiGeR can classify gestures with high accuracy, even in scenarios where the signal passes through multiple walls.},
  langid = {english},
  keywords = {based-on-wifi,classes:10-29,model:dtw,movement:dynamic,movement:static,tech:wifi},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/DV7B98FA/Al-qaness and Li - 2016 - WiGeR WiFi-Based Gesture Recognition System.pdf}
}

@article{al-shamaylehSystematicLiteratureReview2018,
  title = {A Systematic Literature Review on Vision Based Gesture Recognition Techniques},
  author = {{Al-Shamayleh}, Ahmad Sami and Ahmad, Rodina and Abushariah, Mohammad A. M. and Alam, Khubaib Amjad and Jomhari, Nazean},
  year = {2018},
  month = nov,
  journal = {Multimedia Tools and Applications},
  volume = {77},
  number = {21},
  pages = {28121--28184},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-018-5971-z},
  url = {http://link.springer.com/10.1007/s11042-018-5971-z},
  urldate = {2023-06-26},
  abstract = {Human Computer Interaction (HCI) technologies are rapidly evolving the way we interact with computing devices and adapting to the constantly increasing demands of modern paradigms. One of the most useful tools in this regard is the integration of Human-to-Human Interaction gestures to facilitate communication and expressing ideas. Gesture recognition requires the integration of postures, gestures, face expressions and movements for communicating or conveying certain messages. The aim of this study is to aggregate and synthesize experiences and accumulated knowledge about Vision-Based Recognition (VBR) techniques. The major objective of conducting this Systematic Literature Review (SLR) is to highlight the state-of-the-art in the context of vision-based gesture recognition with specific focus on hand gesture recognition (HGR) techniques and enabling technologies. After a careful systematic selection process, 100 studies relevant to the four research questions were selected. This process was followed by data collection, a detailed analysis, and a synthesis of the selected studies. The results reveal that among the VBR techniques, HGR is a predominant and highly focused area of research. Research focus is also found to be converging towards sign language recognition. Potential applications of HGR techniques include desktop applications, smart environments, entertainment, sign language interpretation, virtual reality and gamification. Although various experimental research efforts have been devoted to gestures recognition, there are still numerous open issues and research challenges in this field. Lastly, considering the results from this SLR, potential future research directions are suggested, including a much needed focus on grammatical interpretation, hybrid approaches, smartphone devices, normalization, and real-life systems.},
  langid = {english},
  keywords = {app:sign-language,read-priority-1,type:survey},
  annotation = {71 Citations},
  file = {/Users/brk/Zotero/storage/BL4SNP7J/Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf}
}

@article{alazraiDatasetWiFibasedHumantohuman2020,
  title = {A Dataset for {{Wi-Fi-based}} Human-to-Human Interaction Recognition},
  author = {Alazrai, Rami and Awad, Ali and Alsaify, Baha'A. and Hababeh, Mohammad and Daoud, Mohammad I.},
  year = {2020},
  month = aug,
  journal = {Data in Brief},
  volume = {31},
  pages = {105668},
  issn = {23523409},
  doi = {10.1016/j.dib.2020.105668},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S235234092030562X},
  urldate = {2023-07-12},
  abstract = {This paper presents a dataset for Wi-Fi-based human-tohuman interaction recognition that comprises twelve different interactions performed by 40 different pairs of subjects in an indoor environment. Each pair of subjects performed ten trials of each of the twelve interactions and the total number of trials recorded in our dataset for all the 40 pairs of subjects is 4800 trials (i.e., 40 pairs of subjects \texttimes{} 12 interactions \texttimes{} 10 trials). The publicly available CSI tool [1] is used to record the Wi-Fi signals transmitted from a commercial off-the-shelf access point, namely the Sagemcom 2704 access point, to a desktop computer that is equipped with an Intel 5300 network interface card. The recorded Wi-Fi signals consist of the Received Signal Strength Indicator (RSSI) values and the Channel State Information (CSI) values. Unlike the publicly available Wi-Fi-based human activity datasets, which mainly have focused on activities performed by a single human, our dataset provides a collection of Wi-Fi signals that are recorded for 40 different pairs of subjects while performing twelve two-person interactions. The presented dataset can be exploited to advance Wi-Fi-based human activity recognition in different aspects, such as the use of},
  langid = {english},
  keywords = {based-on-wifi,classes:12,participants:40,tech:wifi,type:dataset},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/TNBL8TTY/Alazrai et al. - 2020 - A dataset for Wi-Fi-based human-to-human interacti.pdf}
}

@article{alazraiEndtoEndDeepLearning2020,
  title = {An {{End-to-End Deep Learning Framework}} for {{Recognizing Human-to-Human Interactions Using Wi-Fi Signals}}},
  author = {Alazrai, Rami and Hababeh, Mohammad and Alsaify, Baha A. and Ali, Mostafa Z. and Daoud, Mohammad I.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {197695--197710},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3034849},
  url = {https://ieeexplore.ieee.org/document/9243938/},
  urldate = {2023-07-12},
  abstract = {Channel state information (CSI)-based human activity recognition plays an essential role in various application domains, such as security, healthcare, and Internet of Things. Most existing CSI-based activity recognition approaches rely on manually designed features that are classified using traditional classification methods. Furthermore, the use of deep learning methods for CSI-based activity recognition is still at its infancy with most of the existing approaches focus on recognizing single-human activities. The current study explores the feasibility of utilizing deep learning methods to recognize human-to-human interactions (HHIs) using CSI signals. Particularly, we introduce an end-to-end deep learning framework that comprises three phases, which are the input, feature extraction, and recognition phases. The input phase converts the raw CSI signals into CSI images that comprise time, frequency, and spatial information. In the feature extraction phase, a novel convolutional neural network (CNN) is designed to automatically extract deep features from the CSI images. Finally, the extracted features are fed to the recognition phase to identify the class of the HHI associated with each CSI image. The performance of our proposed framework is assessed using a publicly available CSI dataset that was acquired from 40 different pairs of subjects while performing 13 HHIs. Our proposed framework achieved an average recognition accuracy of 86.3\% across all HHIs. Moreover, the experiments indicate that our proposed framework enabled significant improvements over the results achieved using three state-of-the-art pre-trained CNNs as well as the results obtained using four different conventional classifiers that employs traditional handcrafted features.},
  keywords = {based-on-wifi,classes:13,model:cnn,participants:40,tech:wifi},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/JAT5WT5H/Alazrai et al. - 2020 - An End-to-End Deep Learning Framework for Recogniz.pdf}
}

@article{aliKeystrokeRecognitionUsing2015,
  title = {Keystroke {{Recognition Using WiFi Signals}}},
  author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {90--102},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790109},
  url = {https://dl.acm.org/doi/10.1145/2789168.2790109},
  urldate = {2023-07-12},
  abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of Channel State Information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal based keystroke recognition system called WiKey. WiKey consists of two Commercial Off-The-Shelf (COTS) WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves more than 97.5\textbackslash\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,classes:37,participants:10,read-priority-1,tech:wifi},
  annotation = {487 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/DGBM94AK/Ali et al. - 2015 - Keystroke Recognition Using WiFi Signals.pdf}
}

@article{aliRecognizingKeystrokesUsing2017,
  title = {Recognizing {{Keystrokes Using WiFi Devices}}},
  author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
  year = {2017},
  month = may,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {35},
  number = {5},
  pages = {1175--1190},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2017.2680998},
  url = {http://ieeexplore.ieee.org/document/7875144/},
  urldate = {2023-07-12},
  abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what is being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of channel state information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal-based keystroke recognition system called WiKey. WiKey consists of two commercial off-the-shelf WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves over 97.5\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%. WiKey can also recognize complete words inside a sentence with over 85\% accuracy.},
  keywords = {app:activity-inference,app:keystroke-inference,based-on-wifi,tech:wifi},
  annotation = {86 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{alsaediEfficientHandGestures2020,
  title = {An Efficient Hand Gestures Recognition System},
  author = {AlSaedi, Ahmed Kadem Hamed and AlAsadi, Abbas H. Hassin},
  year = {2020},
  month = feb,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {745},
  number = {1},
  pages = {012045},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/745/1/012045},
  url = {https://iopscience.iop.org/article/10.1088/1757-899X/745/1/012045},
  urldate = {2023-03-07},
  abstract = {Abstract             Talking about gestures make us return to the historical beginning of human communication because there is no language completely free of gestures. People cannot communicate without gestures. Any action or movement without gestures is free of real feelings and cannot express the thoughts. The purpose of any hand gesture recognition system is to recognize the hand gesture and used it to transfer a certain meaning or for computer control or/and a device. This paper introduced an efficient system to recognize hand gestures in real-time. Generally, the system is divided into five phases, first to image acquisition, second to pre-processing the image, third for detection and segmentation of the hand region, fourth to features extraction and fifth to count the numbers of fingers for gesture recognition. The system has been coded by Python language, PyAutoGUI library, OS Module of Python and the Open CV library.},
  keywords = {classes:10-29,tech:rgb},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/2NTXV8NA/AlSaedi and AlAsadi - 2020 - An efficient hand gestures recognition system.pdf}
}

@article{alviPakistanSignLanguage2007,
  title = {Pakistan {{Sign Language Recognition Using Statistical Template Matching}}},
  author = {Alvi, A. and Azhar, M. and Usman, M. and Mumtaz, Suleman and Rafiq, Sameer and Rehman, RaziUr and Ahmed, Israr},
  year = {2007},
  month = mar,
  journal = {World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering},
  url = {https://www.semanticscholar.org/paper/Pakistan-Sign-Language-Recognition-Using-Template-Alvi-Azhar/2ca46ccbde2346b336adc12948b234c75ed37686},
  urldate = {2023-07-12},
  abstract = {Sign language recognition has been a topic of research since the first data glove was developed. Many researchers have attempted to recognize sign language through various techniques. However none of them have ventured into the area of Pakistan Sign Language (PSL). The Boltay Haath project aims at recognizing PSL gestures using Statistical Template Matching. The primary input device is the DataGlove5 developed by 5DT. Alternative approaches use camera-based recognition which, being sensitive to environmental changes are not always a good choice. This paper explains the use of Statistical Template Matching for gesture recognition in Boltay Haath. The system recognizes one handed alphabet signs from PSL. Keywords\textemdash Gesture Recognition, Pakistan Sign Language, Data Glove, Human Computer Interaction, Template Matching, Boltay Haath},
  keywords = {app:american-sl,app:pakistan-sl,app:sign-language,based-on-gloves,from:cite.bib,hardware:dataglove,model:statistical-template-matching,movement:static,participants:6,tech:accelerometer,tech:flex},
  file = {/Users/brk/Zotero/storage/K5JIJI47/Alvi et al. - 2007 - Pakistan Sign Language Recognition Using Statistic.pdf}
}

@article{alySurveyMulticlassClassification2005,
  title = {Survey on Multiclass Classification Methods},
  author = {Aly, Mohamed},
  year = {2005},
  journal = {Neural Netw},
  volume = {19},
  number = {1-9},
  pages = {2},
  publisher = {{Citeseer}},
  keywords = {background}
}

@article{alzubaidiNovelAssistiveGlove2023,
  title = {A {{Novel Assistive Glove}} to {{Convert Arabic Sign Language}} into {{Speech}}},
  author = {Alzubaidi, Mohammad A. and Otoom, Mwaffaq and Abu Rwaq, Areen M.},
  year = {2023},
  month = mar,
  journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
  volume = {22},
  number = {2},
  pages = {1--16},
  issn = {2375-4699, 2375-4702},
  doi = {10.1145/3545113},
  url = {https://dl.acm.org/doi/10.1145/3545113},
  urldate = {2023-03-07},
  abstract = {People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98\%. A usability study was then conducted to determine the usefulness of the assistive device in real-time.},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{ammaAirwritingWearableHandwriting2014,
  title = {Airwriting: A Wearable Handwriting Recognition System},
  shorttitle = {Airwriting},
  author = {Amma, Christoph and Georgi, Marcus and Schultz, Tanja},
  year = {2014},
  month = jan,
  journal = {Personal and Ubiquitous Computing},
  volume = {18},
  number = {1},
  pages = {191--203},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-013-0637-3},
  url = {http://link.springer.com/10.1007/s00779-013-0637-3},
  urldate = {2023-07-11},
  abstract = {We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set.},
  langid = {english},
  keywords = {app:writing,model:hmm,participants:9,tech:accelerometer,tech:imu},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@misc{analogdevicesADXL335DatasheetProduct2010,
  title = {{{ADXL335 Datasheet}} and {{Product Info}} | {{Analog Devices}}},
  author = {{AnalogDevices}},
  year = {2010},
  journal = {ADXL335 Datasheet and Product Info | Analog Devices},
  publisher = {{www.analog.com}},
  url = {https://www.analog.com/en/products/adxl335.html#product-overview},
  keywords = {from:cite.bib,type:datasheet}
}

@article{anwarHandGestureRecognition2019,
  title = {Hand {{Gesture Recognition}}: {{A Survey}}},
  shorttitle = {Hand {{Gesture Recognition}}},
  author = {Anwar, Shamama and Sinha, Subham Kumar and Vivek, Snehanshu and Ashank, Vishal},
  editor = {Nath, Vijay and Mandal, Jyotsna Kumar},
  year = {2019},
  journal = {Nanoelectronics, Circuits and Communication Systems},
  volume = {511},
  pages = {365--371},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-0776-8\_33},
  url = {http://link.springer.com/10.1007/978-981-13-0776-8_33},
  urldate = {2023-03-07},
  abstract = {A human\textendash computer interaction is generally limited to taking input from the user using handheld devices like keyboard, mouse, or scanners. With the advancement in computers, the user interaction approaches have also advanced. Direct use of hands as an input device is an attractive method for providing natural Human\textendash Computer Interaction. It is also helpful for people who use sign language. The chapter aims to study the existing methods for Hand Gesture Recognition and provide a comparative analysis of the same. The entire process of hand gesture recognition is divided into three phases: hand detection, hand tracking, and recognition. The chapter includes a review of the different methods used for the hand gesture recognition. The recognition phase is classified based on the way the input is received as glove based or vision based. For recognition, various methods like Feature extraction, Hidden Markov Model (HMM), Principal Component Analysis (PCA) are compared along with the reported accuracy.},
  isbn = {9789811307751 9789811307768},
  langid = {english},
  keywords = {type:survey},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@misc{arduinoArduinoHome2005,
  title = {Arduino - {{Home}}},
  author = {{Arduino}},
  year = {2005},
  journal = {Arduino - Home},
  publisher = {{www.arduino.cc}},
  url = {https://www.arduino.cc/},
  keywords = {from:cite.bib,type:datasheet}
}

@misc{arduinoArduinoNano332016,
  title = {Arduino {{Nano}} 33 {{BLE}} \textemdash{} {{Arduino Official Store}}},
  author = {{Arduino}},
  year = {2016},
  journal = {Arduino Official Store},
  publisher = {{store.arduino.cc}},
  url = {https://store.arduino.cc/products/arduino-nano-33-ble},
  keywords = {from:cite.bib,type:datasheet}
}

@misc{arreguiSensorGlove2017,
  title = {Sensor {{Glove}}},
  author = {Arregui, Patxi Xabier Quintana},
  year = {2017},
  url = {https://sensorglove.wixsite.com/sensorglove/producto},
  keywords = {from:cite.bib,type:product}
}

@article{asadi-aghbolaghiSurveyDeepLearning2017,
  title = {A {{Survey}} on {{Deep Learning Based Approaches}} for {{Action}} and {{Gesture Recognition}} in {{Image Sequences}}},
  author = {{Asadi-Aghbolaghi}, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo Jair and {Ponce-Lopez}, Victor and Baro, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
  year = {2017},
  month = may,
  journal = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages = {476--483},
  publisher = {{IEEE}},
  address = {{Washington, DC, DC, USA}},
  doi = {10.1109/FG.2017.150},
  url = {http://ieeexplore.ieee.org/document/7961779/},
  urldate = {2023-07-12},
  abstract = {The interest in action and gesture recognition has grown considerably in the last years. In this paper, we present a survey on current deep learning methodologies for action and gesture recognition in image sequences. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. We review the details of the proposed architectures, fusion strategies, main datasets, and competitions. We summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, discussing their main features and identify opportunities and challenges for future research.},
  isbn = {9781509040230},
  keywords = {model:cnn,model:lstm,tech:rgb,type:survey},
  annotation = {160 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/4BDF37WP/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf;/Users/brk/Zotero/storage/Y9X75IZ5/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf}
}

@misc{atltvheadAtltvheadGestureRecognition,
  title = {Atltvhead {{Gesture Recognition Bracer}}},
  author = {{ATLTVHEAD}},
  abstract = {Atltvhead Gesture Recognition Bracer - A TensorflowLite gesture detector for the atltvhead project and exploration into Data Science This repository is my spin on Jennifer Wang's and Google Tensorflow's magic wand project, but for arm gestures},
  keywords = {tech:accelerometer}
}

@article{atzoriElectromyographyDataNoninvasive2014,
  title = {Electromyography Data for Non-Invasive Naturally-Controlled Robotic Hand Prostheses},
  author = {Atzori, Manfredo and Gijsberts, Arjan and Castellini, Claudio and Caputo, Barbara and Hager, Anne-Gabrielle Mittaz and Elsig, Simone and Giatsidis, Giorgio and Bassetto, Franco and M{\"u}ller, Henning},
  year = {2014},
  month = dec,
  journal = {Scientific Data},
  volume = {1},
  number = {1},
  pages = {140053},
  issn = {2052-4463},
  doi = {10.1038/sdata.2014.53},
  url = {https://www.nature.com/articles/sdata201453},
  urldate = {2023-07-02},
  abstract = {Abstract             Recent advances in rehabilitation robotics suggest that it may be possible for hand-amputated subjects to recover at least a significant part of the lost hand functionality. The control of robotic prosthetic hands using non-invasive techniques is still a challenge in real life: myoelectric prostheses give limited control capabilities, the control is often unnatural and must be learned through long training times. Meanwhile, scientific literature results are promising but they are still far from fulfilling real-life needs. This work aims to close this gap by allowing worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark scientific database. The database is targeted at studying the relationship between surface electromyography, hand kinematics and hand forces, with the final goal of developing non-invasive, naturally controlled, robotic hand prostheses. The validation section verifies that the data are similar to data acquired in real-life conditions, and that recognition of different hand tasks by applying state-of-the-art signal features and machine-learning algorithms is possible.},
  langid = {english},
  keywords = {app:robot-control,read-priority-1,tech:emg,type:dataset},
  annotation = {548 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/46J7VGGV/Atzori et al. - 2014 - Electromyography data for non-invasive naturally-c.pdf}
}

@article{atzoriNinaproDatabaseResource2015,
  title = {The {{Ninapro}} Database: {{A}} Resource for {{sEMG}} Naturally Controlled Robotic Hand Prosthetics},
  shorttitle = {The {{Ninapro}} Database},
  author = {Atzori, Manfredo and Muller, Henning},
  year = {2015},
  month = aug,
  journal = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages = {7151--7154},
  publisher = {{IEEE}},
  address = {{Milan}},
  doi = {10.1109/EMBC.2015.7320041},
  url = {http://ieeexplore.ieee.org/document/7320041/},
  urldate = {2023-07-02},
  abstract = {The dexterous natural control of robotic prosthetic hands with non-invasive techniques is still a challenge: surface electromyography gives some control capabilities but these are limited, often not natural and require long training times; the application of pattern recognition techniques recently started to be applied in practice. While results in the scientific literature are promising they have to be improved to reach the real needs. The Ninapro database aims to improve the field of naturally controlled robotic hand prosthetics by permitting to worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark database. Currently, the Ninapro database includes data from 67 intact subjects and 11 amputated subject performing approximately 50 different movements. The data are aimed at permitting the study of the relationships between surface electromyography, kinematics and dynamics. The Ninapro acquisition protocol was created in order to be easy to be reproduced. Currently, the number of datasets included in the database is increasing thanks to the collaboration of several research groups.},
  isbn = {9781424492718},
  keywords = {tech:emg,type:dataset},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{avolaExploitingRecurrentNeural2019,
  title = {Exploiting {{Recurrent Neural Networks}} and {{Leap Motion Controller}} for the {{Recognition}} of {{Sign Language}} and {{Semaphoric Hand Gestures}}},
  author = {Avola, Danilo and Bernardi, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Multimedia},
  volume = {21},
  number = {1},
  pages = {234--245},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2018.2856094},
  url = {https://ieeexplore.ieee.org/document/8410764/},
  urldate = {2023-07-02},
  abstract = {Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human\textendash human communication and human\textendash computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96\% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.},
  keywords = {app:american-sl,app:sign-language,model:rnn},
  annotation = {108 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/SQ8WF2YE/Avola et al. - 2019 - Exploiting Recurrent Neural Networks and Leap Moti.pdf}
}

@article{bagnallGreatTimeSeries2017,
  title = {The Great Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  shorttitle = {The Great Time Series Classification Bake Off},
  author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
  year = {2017},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {31},
  number = {3},
  pages = {606--660},
  issn = {1573-756X},
  doi = {10.1007/s10618-016-0483-9},
  url = {https://doi.org/10.1007/s10618-016-0483-9},
  urldate = {2023-05-13},
  abstract = {In the last 5~years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.},
  langid = {english},
  keywords = {background,type:survey},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-05-13]},
  file = {/Users/brk/Zotero/storage/SRFG34ZY/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf}
}

@article{bansalGestureRecognitionSurvey2016,
  title = {Gesture {{Recognition}}: {{A Survey}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Bansal, Bharti},
  year = {2016},
  month = apr,
  journal = {International Journal of Computer Applications},
  volume = {139},
  number = {2},
  pages = {8--10},
  issn = {09758887},
  doi = {10.5120/ijca2016909103},
  url = {http://www.ijcaonline.org/research/volume139/number2/bansal-2016-ijca-909103.pdf},
  urldate = {2023-03-07},
  abstract = {With increasing use of computers in our daily lives, lately there has been a rapid increase in the efforts to develop a better human computer interaction interface. The need of easy to use and advance types of human-computer interaction with natural interfaces is more than ever. In the present framework, the UI (User Interface) of a computer allows user to interact with electronic devices with graphical icons and visual indicators, which is still inconvenient and not suitable for working in virtual environments. An interface which allow user to communicate through gestures is the next step in the direction of advance human computer interface. In the present paper author explore different aspects of gesture recognition techniques.},
  keywords = {type:survey},
  annotation = {650 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/DXTI6K39/Bansal - 2016 - Gesture Recognition A Survey.pdf}
}

@article{baudelCharadeRemoteControl1993,
  title = {Charade: Remote Control of Objects Using Free-Hand Gestures},
  shorttitle = {Charade},
  author = {Baudel, Thomas and {Beaudouin-Lafon}, Michel},
  year = {1993},
  month = jul,
  journal = {Communications of the ACM},
  volume = {36},
  number = {7},
  pages = {28--35},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/159544.159562},
  url = {https://dl.acm.org/doi/10.1145/159544.159562},
  urldate = {2023-06-23},
  abstract = {This paper presents an application that uses hand gesture input to control a computer while giving a presentation. In order to develop a prototype of this application, we have defined an interaction model, a notation for gestures, and a set of guidelines to design gestural command sets. This works aims to define interaction styles that work in computerized reality environments. In our application, gestures are used for interacting with the computer as well as for communicating with other people or operating other devices.},
  langid = {english},
  keywords = {based-on-gloves,hardware:charade,hardware:dataglove,tech:accelerometer,tech:flex},
  annotation = {582 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/WRSWXDSK/Baudel and Beaudouin-Lafon - 1993 - Charade remote control of objects using free-hand.pdf}
}

@article{baumMaximizationTechniqueOccurring1970,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  month = feb,
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {1},
  pages = {164--171},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177697196},
  url = {http://projecteuclid.org/euclid.aoms/1177697196},
  urldate = {2023-07-11},
  abstract = {Semantic Scholar extracted view of "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains" by L. Baum et al.},
  langid = {english},
  keywords = {background,baum-welsch,model:hmm},
  annotation = {4728 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/888D4KE6/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf}
}

@article{baumStatisticalInferenceProbabilistic1966,
  title = {Statistical {{Inference}} for {{Probabilistic Functions}} of {{Finite State Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted},
  year = {1966},
  journal = {The Annals of Mathematical Statistics},
  volume = {37},
  number = {6},
  pages = {1554--1563},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/aoms/1177699147},
  url = {https://doi.org/10.1214/aoms/1177699147},
  keywords = {background,model:finite-state-markov-chains,model:hmm},
  annotation = {2933 citations (Semantic Scholar/DOI) [2023-06-19]},
  file = {/Users/brk/Zotero/storage/VMY4KUTL/Baum and Petrie - 1966 - Statistical Inference for Probabilistic Functions .pdf}
}

@article{bergstraRandomSearchHyperParameter2012,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  month = feb,
  journal = {J. Mach. Learn. Res.},
  volume = {13},
  number = {null},
  pages = {281--305},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  keywords = {deep learning,from:cite.bib,global optimization,model selection,neural networks,response surface modeling}
}

@article{berrarCaveatsPitfallsROC2012,
  title = {Caveats and Pitfalls of {{ROC}} Analysis in Clinical Microarray Research (and How to Avoid Them)},
  author = {Berrar, Daniel and Flach, Peter},
  year = {2012},
  month = jan,
  journal = {Briefings in Bioinformatics},
  volume = {13},
  number = {1},
  pages = {83--97},
  issn = {1467-5463},
  doi = {10.1093/bib/bbr008},
  url = {https://doi.org/10.1093/bib/bbr008},
  urldate = {2023-03-24},
  abstract = {The receiver operating characteristic (ROC) has emerged as the gold standard for assessing and comparing the performance of classifiers in a wide range of disciplines including the life sciences. ROC curves are frequently summarized in a single scalar, the area under the curve (AUC). This article discusses the caveats and pitfalls of ROC analysis in clinical microarray research, particularly in relation to (i) the interpretation of AUC (especially a value close to 0.5); (ii) model comparisons based on AUC; (iii) the differences between ranking and classification; (iv) effects due to multiple hypotheses testing; (v) the importance of confidence intervals for AUC; and (vi) the choice of the appropriate performance metric. With a discussion of illustrative examples and concrete real-world studies, this article highlights critical misconceptions that can profoundly impact the conclusions about the observed performance.},
  keywords = {background},
  annotation = {91 citations (Semantic Scholar/DOI) [2023-03-24]},
  file = {/Users/brk/Zotero/storage/RLN68QM9/Berrar and Flach - 2012 - Caveats and pitfalls of ROC analysis in clinical m.pdf}
}

@inproceedings{bevilacquaContinuousRealtimeGesture2010,
  title = {Continuous {{Realtime Gesture Following}} and {{Recognition}}},
  booktitle = {Gesture in {{Embodied Communication}} and {{Human-Computer Interaction}}},
  author = {Bevilacqua, Fr{\'e}d{\'e}ric and Zamborlin, Bruno and Sypniewski, Anthony and Schnell, Norbert and Gu{\'e}dy, Fabrice and Rasamimanana, Nicolas},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kopp, Stefan and Wachsmuth, Ipke},
  year = {2010},
  volume = {5934},
  pages = {73--84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12553-9\_7},
  url = {http://link.springer.com/10.1007/978-3-642-12553-9_7},
  urldate = {2023-03-07},
  abstract = {We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.},
  isbn = {978-3-642-12552-2 978-3-642-12553-9},
  keywords = {model:hmm},
  annotation = {207 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/9YT2MAGT/Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf}
}

@article{bhagatIndianSignLanguage2019,
  title = {Indian {{Sign Language Gesture Recognition}} Using {{Image Processing}} and {{Deep Learning}}},
  author = {Bhagat, Neel Kamal and Vishnusai, Y. and Rathna, G. N.},
  year = {2019},
  month = dec,
  journal = {2019 Digital Image Computing: Techniques and Applications (DICTA)},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Perth, Australia}},
  doi = {10.1109/DICTA47822.2019.8945850},
  url = {https://ieeexplore.ieee.org/document/8945850/},
  urldate = {2023-06-22},
  abstract = {Speech impaired people use hand based gestures to communicate. Unfortunately, the vast majority of the people are not aware of the semantics of these gestures. In a attempt to bridge the same, we propose a real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera. Given that there is no one to one mapping between the pixels of the depth and the RGB camera, we used computer vision techniques like 3D contruction and affine transformation. After achieving one to one mapping, segmentation of the hand gestures was done from the background noise. Convolutional Neural Networks (CNNs) were utilised for training 36 static gestures relating to Indian Sign Language (ISL) alphabets and numbers. The model achieved an accuracy of 98.81\% on training using 45,000 RGB images and 45,000 depth images. Further Convolutional LSTMs were used for training 10 ISL dynamic word gestures and an accuracy of 99.08\% was obtained by training 1080 videos. The model showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures. The model also showed competitive adaptability to American Sign Language (ASL) gestures when the ISL models weights were transfer learned to ASL and it resulted in giving 97.71\% accuracy.},
  isbn = {9781728138572},
  keywords = {app:sign-language},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{binhRealTimeHandTracking2005,
  title = {Real-{{Time Hand Tracking}} and {{Gesture Recognition System}}},
  author = {Binh, N. and Shuichi, Enokida and Ejima, T.},
  year = {2005},
  url = {https://www.semanticscholar.org/paper/Real-Time-Hand-Tracking-and-Gesture-Recognition-Binh-Shuichi/04cad56dd4fda01a3fa03a47dca2fa11d02695bf},
  urldate = {2023-07-11},
  abstract = {In this paper, we introduce a hand gesture recognition system to recognize real time gesture in unconstrained environments. The system consists of three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). We have used a Kalman filter and hand blobs analysis for hand tracking to obtain motion descriptors and hand region. It is fairy robust to background cluster and uses skin color for hand gesture tracking and recognition. Furthermore, there have been proposed to improve the overall performance of the approach: (1) Intelligent selection of training images and (2) Adaptive threshold gesture to remove non-gesture pattern that helps to qualify an input pattern as a gesture. A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed. In the experiments, we have tested our system to vocabulary of 36 gestures including the America sign language (ASL) letter spelling alphabet and digits, and results effectiveness of the approach.},
  keywords = {app:american-sl,app:sign-language,based-on-vision,classes:36,model:hmm,model:kalman-filter,tech:rgb}
}

@article{bloitShorttimeViterbiOnline2008,
  title = {Short-Time {{Viterbi}} for Online {{HMM}} Decoding: {{Evaluation}} on a Real-Time Phone Recognition Task},
  shorttitle = {Short-Time {{Viterbi}} for Online {{HMM}} Decoding},
  author = {Bloit, Julien and Rodet, Xavier},
  year = {2008},
  month = mar,
  journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages = {2121--2124},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2008.4518061},
  url = {http://ieeexplore.ieee.org/document/4518061/},
  urldate = {2023-03-07},
  abstract = {In this paper we present the implementation of an online HMM decoding process. The algorithm derives an online version of the Viterbi algorithm on successive variable length windows, iteratively storing portions of the optimal state path. We explicit the relation between the hidden layer's topology and the applicability and performance prediction of the algorithm. We evaluate the validity and performance of the algorithm on a phone recognition task on a database of continuous speech from a native French speaker. We specifically study the latency-accuracy performance of the algorithm.},
  isbn = {9781424414833 9781424414840},
  keywords = {background},
  annotation = {52 citations (Semantic Scholar/DOI) [2023-03-07]},
  file = {/Users/brk/Zotero/storage/BTJHRA6M/Bloit and Rodet - 2008 - Short-time Viterbi for online HMM decoding Evalua.pdf}
}

@inproceedings{boltPutthatthereVoiceGesture1980,
  title = {``{{Put-that-there}}'': {{Voice}} and Gesture at the Graphics Interface},
  shorttitle = {``{{Put-that-there}}''},
  booktitle = {Proceedings of the 7th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Bolt, Richard A.},
  year = {1980},
  month = jul,
  series = {{{SIGGRAPH}} '80},
  pages = {262--270},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800250.807503},
  url = {https://dl.acm.org/doi/10.1145/800250.807503},
  urldate = {2023-07-03},
  abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality. The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
  isbn = {978-0-89791-021-7},
  keywords = {Gesture,Graphics,Graphics interface,Man-machine interfaces,Space sensing,Spatial data management,Speech input,Voice input},
  annotation = {1893 citations (Semantic Scholar/DOI) [2023-07-03]},
  file = {/Users/brk/Zotero/storage/8U4B3N9X/Bolt - 1980 - Put-that-there Voice and gesture at the graphic.pdf}
}

@inproceedings{bowdenLinguisticFeatureVector2004,
  title = {A {{Linguistic Feature Vector}} for the {{Visual Interpretation}} of {{Sign Language}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2004},
  author = {Bowden, Richard and Windridge, David and Kadir, Timor and Zisserman, Andrew and Brady, Michael},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Pajdla, Tom{\'a}s and Matas, Ji{\v r}{\'i}},
  year = {2004},
  volume = {3021},
  pages = {390--401},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24670-1\_30},
  url = {http://link.springer.com/10.1007/978-3-540-24670-1_30},
  urldate = {2023-07-11},
  abstract = {This paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data. Key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion. This high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans. Moreover, such a description broadly generalises temporal activities naturally overcoming variability of people and environments. A second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of Markov chains combined with Independent Component Analysis. We demonstrate classification rates as high as 97.67\% for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required.},
  isbn = {978-3-540-21984-2 978-3-540-24670-1},
  langid = {english},
  keywords = {app:sign-language,based-on-vision,classes:43,model:hmm,tech:rgb},
  annotation = {182 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/RZMUR5ET/Bowden et al. - 2004 - A Linguistic Feature Vector for the Visual Interpr.pdf}
}

@inproceedings{bowdenVisionBasedInterpretation2003,
  title = {Vision Based {{Interpretation}} of {{Natural Sign Languages}}},
  author = {Bowden, R. and Zisserman, Andrew and Kadir, T. and Brady, M.},
  year = {2003},
  month = apr,
  url = {https://www.semanticscholar.org/paper/Vision-based-Interpretation-of-Natural-Sign-Bowden-Zisserman/86e9d0c4d14456932043fa0fb22f39e62f2de688},
  urldate = {2023-07-11},
  abstract = {This manuscript outlines our current demonstration system for translating visual Sign to written text. The system is based around a broad description of scene activity that naturally generalizes, reducing training requirements and allowing the knowledge base to be explicitly stated. This allows the same system to be used for different sign languages requiring only a change of the knowledge base.},
  keywords = {app:sign-language,based-on-vision,tech:rgb},
  file = {/Users/brk/Zotero/storage/BRKUR3W3/Bowden et al. - 2003 - Vision based Interpretation of Natural Sign Langua.pdf}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  url = {http://link.springer.com/10.1023/A:1010933404324},
  urldate = {2023-07-11},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  keywords = {background,model:random-forests,type:seminal},
  file = {/Users/brk/Zotero/storage/HSTITUV6/Breiman - 2001 - [No title found].pdf}
}

@article{brooksDataGloveManmachineInterface1989,
  title = {The {{DataGlove}} as a Man-Machine Interface for Robotics},
  author = {Brooks, Martin},
  year = {1989},
  journal = {The Second IARP Workshop on Medical and Healthcare Robotics},
  pages = {213--225}
}

@inproceedings{buchmannFingARtipsGestureBased2004,
  title = {{{FingARtips}}: Gesture Based Direct Manipulation in {{Augmented Reality}}},
  shorttitle = {{{FingARtips}}},
  booktitle = {Proceedings of the 2nd International Conference on {{Computer}} Graphics and Interactive Techniques in {{Australasia}} and {{South East Asia}}},
  author = {Buchmann, Volkert and Violich, Stephen and Billinghurst, Mark and Cockburn, Andy},
  year = {2004},
  month = jun,
  pages = {212--221},
  publisher = {{ACM}},
  address = {{Singapore}},
  doi = {10.1145/988834.988871},
  url = {https://dl.acm.org/doi/10.1145/988834.988871},
  urldate = {2023-07-02},
  abstract = {This paper presents a technique for natural, fingertip-based interaction with virtual objects in Augmented Reality (AR) environments. We use image processing software and finger- and hand-based fiducial markers to track gestures from the user, stencil buffering to enable the user to see their fingers at all times, and fingertip-based haptic feedback devices to enable the user to feel virtual objects. Unlike previous AR interfaces, this approach allows users to interact with virtual content using natural hand gestures. The paper describes how these techniques were applied in an urban planning interface, and also presents preliminary informal usability results.},
  isbn = {978-1-58113-883-2},
  langid = {english},
  keywords = {app:augmented-reality,based-on-vision,fiducials,tech:rgb},
  annotation = {290 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ZSNVZFEU/Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf}
}

@article{cabibihanSuitabilityOpenlyAccessible2021,
  title = {Suitability of the {{Openly Accessible 3D Printed Prosthetic Hands}} for {{War-Wounded Children}}},
  author = {Cabibihan, John-John and Alkhatib, Farah and Mudassir, Mohammed and Lambert, Laurent A. and {Al-Kwifi}, Osama S. and Diab, Khaled and Mahdi, Elsadig},
  year = {2021},
  month = jan,
  journal = {Frontiers in Robotics and AI},
  volume = {7},
  pages = {594196},
  issn = {2296-9144},
  doi = {10.3389/frobt.2020.594196},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2020.594196/full},
  urldate = {2023-07-12},
  abstract = {The field of rehabilitation and assistive devices is being disrupted by innovations in desktop 3D printers and open-source designs. For upper limb prosthetics, those technologies have demonstrated a strong potential to aid those with missing hands. However, there are basic interfacing issues that need to be addressed for long term usage. The functionality, durability, and the price need to be considered especially for those in difficult living conditions. We evaluated the most popular designs of body-powered, 3D printed prosthetic hands. We selected a representative sample and evaluated its suitability for its grasping postures, durability, and cost. The prosthetic hand can perform three grasping postures out of the 33 grasps that a human hand can do. This corresponds to grasping objects similar to a coin, a golf ball, and a credit card. Results showed that the material used in the hand and the cables can withstand a 22 N normal grasping force, which is acceptable based on standards for accessibility design. The cost model showed that a 3D printed hand could be produced for as low as \$19. For the benefit of children with congenital missing limbs and for the war-wounded, the results can serve as a baseline study to advance the development of prosthetic hands that are functional yet low-cost.},
  keywords = {medical},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/KXV4GLFK/Cabibihan et al. - 2021 - Suitability of the Openly Accessible 3D Printed Pr.pdf}
}

@article{caiRGBDDatasetsUsing2017,
  title = {{{RGB-D}} Datasets Using Microsoft Kinect or Similar Sensors: A Survey},
  shorttitle = {{{RGB-D}} Datasets Using Microsoft Kinect or Similar Sensors},
  author = {Cai, Ziyun and Han, Jungong and Liu, Li and Shao, Ling},
  year = {2017},
  month = feb,
  journal = {Multimedia Tools and Applications},
  volume = {76},
  number = {3},
  pages = {4313--4355},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-016-3374-6},
  url = {http://link.springer.com/10.1007/s11042-016-3374-6},
  urldate = {2023-06-22},
  abstract = {RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems. It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color, illumination, rotation angle and scale. With the invention of the low-cost Microsoft Kinect sensor, which was initially used for gaming and later became a popular device for computer vision, high quality RGB-D data can be acquired easily. In recent years, more and more RGB-D image/video datasets dedicated to various applications have become available, which are of great importance to benchmark the state-of-the-art. In this paper, we systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation. We provide the insights into the characteristics of each important dataset, and compare the popularity and the difficulty of those datasets. Overall, the main goal of this survey is to give a comprehensive description about the available RGB-D datasets and thus to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
  langid = {english},
  keywords = {hardware:kinect,read-priority-5,tech:rgbd,type:survey},
  annotation = {115 Citations},
  file = {/Users/brk/Zotero/storage/Q3HIZYJF/Cai et al. - 2017 - RGB-D datasets using microsoft kinect or similar s.pdf}
}

@inproceedings{chaiTwoStreamsRecurrent2016,
  title = {Two Streams {{Recurrent Neural Networks}} for {{Large-Scale Continuous Gesture Recognition}}},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Chai, Xiujuan and Liu, Zhipeng and Yin, Fang and Liu, Zhuang and Chen, Xilin},
  year = {2016},
  month = dec,
  pages = {31--36},
  doi = {10.1109/ICPR.2016.7899603},
  abstract = {In this paper, we tackle the continuous gesture recognition problem with a two streams Recurrent Neural Networks (2S-RNN) for the RGB-D data input. In our framework, the spotting-recognition strategy is used, that means the continuous gestures are first segmented into separated gestures, and then each isolated gesture is recognized by using the 2S-RNN. Concretely, the gesture segmentation is based on the accurate hand positions provided by the hand detector trained from Faster R-CNN. While in the recognition module, 2S-RNN is designed to efficiently fuse multi-modal features, i.e. the RGB and depth channels. The experimental results on both the validation and test sets of the Continuous Gesture Dataset (ConGD) have shown promising performance of the proposed framework. We ranked 1st in the ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge with the mean Jaccard Index of 0.286915.},
  keywords = {dataset:chalearn-lap,dataset:continuous-gesture-dataset,model:rnn,tech:rgbd},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{charayaphanImageProcessingSystem1992,
  title = {Image Processing System for Interpreting Motion in {{American Sign Language}}},
  author = {Charayaphan, C. and Marble, A.E.},
  year = {1992},
  month = sep,
  journal = {Journal of Biomedical Engineering},
  volume = {14},
  number = {5},
  pages = {419--425},
  issn = {01415425},
  doi = {10.1016/0141-5425(92)90088-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0141542592900883},
  urldate = {2023-07-15},
  langid = {english},
  keywords = {/unread},
  annotation = {99 citations (Semantic Scholar/DOI) [2023-07-15]}
}

@article{chatzisComprehensiveStudyDeep2020,
  title = {A {{Comprehensive Study}} on {{Deep Learning-Based 3D Hand Pose Estimation Methods}}},
  author = {Chatzis, Theocharis and Stergioulas, Andreas and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas and Daras, Petros},
  year = {2020},
  month = sep,
  journal = {Applied Sciences},
  volume = {10},
  number = {19},
  pages = {6850},
  issn = {2076-3417},
  doi = {10.3390/app10196850},
  url = {https://www.mdpi.com/2076-3417/10/19/6850},
  urldate = {2023-06-26},
  abstract = {The field of 3D hand pose estimation has been gaining a lot of attention recently, due to its significance in several applications that require human-computer interaction (HCI). The utilization of technological advances, such as cost-efficient depth cameras coupled with the explosive progress of Deep Neural Networks (DNNs), has led to a significant boost in the development of robust markerless 3D hand pose estimation methods. Nonetheless, finger occlusions and rapid motions still pose significant challenges to the accuracy of such methods. In this survey, we provide a comprehensive study of the most representative deep learning-based methods in literature and propose a new taxonomy heavily based on the input data modality, being RGB, depth, or multimodal information. Finally, we demonstrate results on the most popular RGB and depth-based datasets and discuss potential research directions in this rapidly growing field.},
  langid = {english},
  keywords = {model:deep-learning,read-priority-9,tech:rgbd,type:survey},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/5NQQ3Z7T/Chatzis et al. - 2020 - A Comprehensive Study on Deep Learning-Based 3D Ha.pdf}
}

@inproceedings{chaudharySurveyHandGesture2011,
  title = {A {{Survey}} on {{Hand Gesture Recognition}} in {{Context}} of {{Soft Computing}}},
  booktitle = {Advanced {{Computing}}},
  author = {Chaudhary, Ankit and Raheja, J. L. and Das, Karen and Raheja, Sonia},
  editor = {Meghanathan, Natarajan and Kaushik, Brajesh Kumar and Nagamalai, Dhinaharan},
  year = {2011},
  volume = {133},
  pages = {46--55},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17881-8\_5},
  url = {http://link.springer.com/10.1007/978-3-642-17881-8_5},
  urldate = {2023-07-11},
  abstract = {Hand gestures recognition is the natural way of Human Machine interaction and today many researchers in the academia and industry are interested in this direction. It enables human being to interact with machine very easily and conveniently without wearing any extra device. It can be applied from sign language recognition to robot control and from virtual reality to intelligent home systems. In this paper we are discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc. We also described hand detection methods in the preprocessed image for detecting the hand image. Most researchers used fingertips for hand detection in appearance based modeling. Finally we are comparing results given by different researchers after their implementation.},
  isbn = {978-3-642-17880-1 978-3-642-17881-8},
  keywords = {model:fuzzy,type:survey},
  annotation = {67 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{chenCrossDomainWiFiSensing2023,
  title = {Cross-{{Domain WiFi Sensing}} with {{Channel State Information}}: {{A Survey}}},
  shorttitle = {Cross-{{Domain WiFi Sensing}} with {{Channel State Information}}},
  author = {Chen, Chen and Zhou, Gang and Lin, Youfang},
  year = {2023},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {11},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3570325},
  url = {https://dl.acm.org/doi/10.1145/3570325},
  urldate = {2023-07-12},
  abstract = {The past years have witnessed the rapid conceptualization and development of wireless sensing based on               Channel State Information (CSI)               with commodity WiFi devices. Recent studies have demonstrated the vast potential of WiFi sensing in detection, recognition, and estimation applications. However, the widespread deployment of WiFi sensing systems still faces a significant challenge: how to ensure the sensing performance when exposing a pre-trained sensing system to new domains, such as new environments, different configurations, and unseen users, without data collection and system retraining. This survey provides a comprehensive review of recent research efforts on cross-domain WiFi Sensing. We first introduce the mathematical model of CSI and explore the impact of different domains on CSI. Then we present a general workflow of cross-domain WiFi sensing systems, which consists of signal processing and cross-domain sensing. Five cross-domain sensing algorithms, including domain-invariant feature extraction, virtual sample generation, transfer learning, few-shot learning and big data solution, are summarized to show how they achieve high sensing accuracy when encountering new domains. The advantages and limitations of each algorithm are also summarized and the performance comparison is made based on different applications. Finally, we discuss the remaining challenges to further promote the practical usability of cross-domain WiFi sensing systems.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi,type:survey},
  annotation = {115 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{chengSurvey3DHand2016,
  title = {Survey on {{3D Hand Gesture Recognition}}},
  author = {Cheng, Hong and Yang, Lu and Liu, Zicheng},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {26},
  number = {9},
  pages = {1659--1673},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2015.2469551},
  url = {http://ieeexplore.ieee.org/document/7208833/},
  urldate = {2023-06-22},
  abstract = {Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners.},
  keywords = {type:survey},
  annotation = {265 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{chenHandGestureRecognition2003,
  title = {Hand Gesture Recognition Using a Real-Time Tracking Method and Hidden {{Markov}} Models},
  author = {Chen, Feng-Sheng and Fu, Chih-Ming and Huang, Chung-Lin},
  year = {2003},
  month = aug,
  journal = {Image and Vision Computing},
  volume = {21},
  number = {8},
  pages = {745--758},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(03)00070-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885603000702},
  urldate = {2023-06-23},
  abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90\%.},
  langid = {english},
  keywords = {based-on-vision,model:hmm,tech:rgb},
  annotation = {528 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/7FR9BDWF/Chen et al. - 2003 - Hand gesture recognition using a real-time trackin.pdf;/Users/brk/Zotero/storage/9ZPXDS8C/S0262885603000702.html}
}

@article{chenSurveyHandGesture2013,
  title = {A {{Survey}} on {{Hand Gesture Recognition}}},
  author = {Chen, Lingchen and Wang, Feng and Deng, Hui and Ji, Kaifan},
  year = {2013},
  month = dec,
  journal = {2013 International Conference on Computer Sciences and Applications},
  pages = {313--316},
  publisher = {{IEEE}},
  address = {{Wuhan, China}},
  doi = {10.1109/CSA.2013.79},
  url = {http://ieeexplore.ieee.org/document/6835606/},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition has become one of the key techniques of human-computer interaction (HCI). Many researchers are devoted in this field. In this paper, firstly the history of hand gesture recognition is discussed and the technical difficulties are also enumerated. Then, we analyze the definition of hand gesture and introduce the basic principle of it. The approaches for hand gesture recognition, such as vision-based, glove-based and depth-based, are contrasted briefly in this paper. But the former two methods are too simple and not natural enough. Currently, the new finger identification and hand gesture recognition technique with Kinect depth data is the most popular research direction. Finally, we discuss the application prospective of hand gesture recognition based on Kinect.},
  isbn = {9780769551258},
  keywords = {hardware:kinect,type:survey},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{chenSurveyHandPose2020,
  title = {A {{Survey}} on {{Hand Pose Estimation}} with {{Wearable Sensors}} and {{Computer-Vision-Based Methods}}},
  author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
  year = {2020},
  month = feb,
  journal = {Sensors},
  volume = {20},
  number = {4},
  pages = {1074},
  issn = {1424-8220},
  doi = {10.3390/s20041074},
  url = {https://www.mdpi.com/1424-8220/20/4/1074},
  urldate = {2023-06-23},
  abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
  langid = {english},
  keywords = {have-read,type:survey},
  annotation = {54 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/Y67RWII7/Chen et al. - 2020 - A Survey on Hand Pose Estimation with Wearable Sen.pdf}
}

@article{chenWiFiCSIBased2019,
  title = {{{WiFi CSI Based Passive Human Activity Recognition Using Attention Based BLSTM}}},
  author = {Chen, Zhenghua and Zhang, Le and Jiang, Chaoyang and Cao, Zhiguang and Cui, Wei},
  year = {2019},
  month = nov,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {18},
  number = {11},
  pages = {2714--2724},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2018.2878233},
  url = {https://ieeexplore.ieee.org/document/8514811/},
  urldate = {2023-07-12},
  abstract = {Human activity recognition can benefit various applications including healthcare services and context awareness. Since human actions will influence WiFi signals, which can be captured by the channel state information (CSI) of WiFi, WiFi CSI based human activity recognition has gained more and more attention. Due to the complex relationship between human activities and WiFi CSI measurements, the accuracies of current recognition systems are far from satisfactory. In this paper, we propose a new deep learning based approach, i.e., attention based bi-directional long short-term memory (ABLSTM), for passive human activity recognition using WiFi CSI signals. The BLSTM is employed to learn representative features in two directions from raw sequential CSI measurements. Since the learned features may have different contributions for final activity recognition, we leverage on an attention mechanism to assign different weights for all the learned features. Real experiments have been carried out to evaluate the performance of the proposed ABLSTM for human activity recognition. The experimental results show that our proposed ABLSTM is able to achieve the best recognition performance for all activities when compared with some benchmark approaches.},
  keywords = {based-on-wifi,model:lstm,tech:wifi},
  annotation = {192 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{cheokReviewHandGesture2019,
  title = {A Review of Hand Gesture and Sign Language Recognition Techniques},
  author = {Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
  year = {2019},
  month = jan,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {10},
  number = {1},
  pages = {131--153},
  issn = {1868-8071, 1868-808X},
  doi = {10.1007/s13042-017-0705-5},
  url = {http://link.springer.com/10.1007/s13042-017-0705-5},
  urldate = {2023-06-26},
  abstract = {Hand gesture recognition serves as a key for overcoming many difficulties and providing convenience for human life. The ability of machines to understand human activities and their meaning can be utilized in a vast array of applications. One specific field of interest is sign language recognition. This paper provides a thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research. The techniques reviewed are suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification, where the various algorithms at each stage are elaborated and their merits compared. Further, we also discuss the challenges and limitations faced by gesture recognition research in general, as well as those exclusive to sign language recognition. Overall, it is hoped that the study may provide readers with a comprehensive introduction into the field of automated gesture and sign language recognition, and further facilitate future research efforts in this area.},
  langid = {english},
  keywords = {type:survey},
  annotation = {301 Citations},
  file = {/Users/brk/Zotero/storage/62H2TEM2/Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf}
}

@inproceedings{chunliRealTimeLargeVocabulary2002,
  title = {A {{Real-Time Large Vocabulary Recognition System}} for {{Chinese Sign Language}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {Chunli, Wang and Wen, Gao and Jiyong, Ma},
  editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Sowa, Timo},
  year = {2002},
  volume = {2298},
  pages = {86--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-47873-6\_9},
  url = {http://link.springer.com/10.1007/3-540-47873-6_9},
  urldate = {2023-07-02},
  abstract = {The major challenge that faces Sign Language recognition now is to develop methods that will scale well with increasing vocabulary size. In this paper, a real-time system designed for recognizing Chinese Sign Language (CSL) signs with a 5100 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. An algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed. Then the worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including modifying the transferring probability, clustering the Gaussians and fast matching algorithm. Experiments show that techniques proposed in this paper are efficient on either recognition speed or recognition performance.},
  isbn = {978-3-540-43678-2 978-3-540-47873-7},
  keywords = {app:chinese-sl,app:sign-language,based-on-gloves,classes:5100,contains-more-references,hardware:cyberglove,have-read,model:hmm},
  annotation = {38 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/QZ3Y7L3J/Chunli et al. - 2002 - A Real-Time Large Vocabulary Recognition System fo.pdf}
}

@article{chuSensorBasedHandGesture2021,
  title = {A {{Sensor-Based Hand Gesture Recognition System}} for {{Japanese Sign Language}}},
  author = {Chu, Xianzhi and Liu, Jiang and Shimamoto, Shigeru},
  year = {2021},
  month = mar,
  journal = {2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)},
  pages = {311--312},
  publisher = {{IEEE}},
  address = {{Nara, Japan}},
  doi = {10.1109/LifeTech52111.2021.9391981},
  url = {https://ieeexplore.ieee.org/document/9391981/},
  urldate = {2023-03-07},
  abstract = {In this paper, we propose a sensor-based data acquisition glove for Japanese Sign Language (JSL) hand gesture recognition. Five flex sensors, an Inertial Measurement Unit (IMU), and three Force Sensing Resistors (FSRs) are used to detect the bending degree of fingers and hand movement information. The detected data are transmitted to the computer by an Arduino Micro. The average accuracy of the hand gesture recognition for a single subject, using the Support Vector Machine (SVM) based and the Dynamic Time Wrapping (DTW) based algorithm are 96.9\% and 94.5\%, respectively. Our proposed system also achieves an average recognition accuracy of about 82.5\% for the cross-recognition among three subjects. The experimental results indicate that our proposed system has great potential for JSL hand gesture recognition.},
  isbn = {9781665418751},
  keywords = {app:sign-language},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{collialfaroUserIndependentHandGesture2022,
  title = {User-{{Independent Hand Gesture Recognition Classification Models Using Sensor Fusion}}},
  author = {Colli Alfaro, Jose Guillermo and Trejos, Ana Luisa},
  year = {2022},
  month = feb,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1321},
  issn = {1424-8220},
  doi = {10.3390/s22041321},
  url = {https://www.mdpi.com/1424-8220/22/4/1321},
  urldate = {2023-07-11},
  abstract = {Recently, it has been proven that targeting motor impairments as early as possible while using wearable mechatronic devices for assisted therapy can improve rehabilitation outcomes. However, despite the advanced progress on control methods for wearable mechatronic devices, the need for a more natural interface that allows for better control remains. To address this issue, electromyography (EMG)-based gesture recognition systems have been studied as a potential solution for human\textendash machine interface applications. Recent studies have focused on developing user-independent gesture recognition interfaces to reduce calibration times for new users. Unfortunately, given the stochastic nature of EMG signals, the performance of these interfaces is negatively impacted. To address this issue, this work presents a user-independent gesture classification method based on a sensor fusion technique that combines EMG data and inertial measurement unit (IMU) data. The Myo Armband was used to measure muscle activity and motion data from healthy subjects. Participants were asked to perform seven types of gestures in four different arm positions while using the Myo on their dominant limb. Data obtained from 22 participants were used to classify the gestures using three different classification methods. Overall, average classification accuracies in the range of 67.5\textendash 84.6\% were obtained, with the Adaptive Least-Squares Support Vector Machine model obtaining accuracies as high as 92.9\%. These results suggest that by using the proposed sensor fusion approach, it is possible to achieve a more natural interface that allows better control of wearable mechatronic devices during robot assisted therapies.},
  langid = {english},
  keywords = {app:medical,model:svm,participants:22,tech:emg,tech:imu},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/BBADBX3U/Colli Alfaro and Trejos - 2022 - User-Independent Hand Gesture Recognition Classifi.pdf}
}

@inproceedings{cooperSignLanguageRecognition2011,
  title = {Sign {{Language Recognition}}},
  booktitle = {Visual {{Analysis}} of {{Humans}}},
  author = {Cooper, Helen and Holt, Brian and Bowden, Richard},
  editor = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker and Sigal, Leonid},
  year = {2011},
  pages = {539--562},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-0-85729-997-0\_27},
  url = {https://link.springer.com/10.1007/978-0-85729-997-0_27},
  urldate = {2023-03-07},
  abstract = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a precis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
  isbn = {978-0-85729-996-3 978-0-85729-997-0},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {293 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@misc{cybergloveincWirelessCyberGloveII,
  title = {Wireless {{CyberGlove II Motion Capture Data Glove}}},
  author = {{CyberGlove Inc}},
  url = {http://www.cyberglovesystems.com/cyberglove-ii/},
  keywords = {from:cite.bib,type:product}
}

@article{damasioAnimatingVirtualHumans2002,
  title = {Animating Virtual Humans Using Hand Postures},
  author = {Damasio, F.W. and Musse, S.R.},
  year = {2002},
  journal = {Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing},
  pages = {437},
  publisher = {{IEEE Comput. Soc}},
  address = {{Fortaleza-CE, Brazil}},
  doi = {10.1109/SIBGRA.2002.1167207},
  url = {http://ieeexplore.ieee.org/document/1167207/},
  urldate = {2023-07-02},
  abstract = {Interaction between human and computer has been used in a large scale in computer graphics and virtual reality. This paper presents a system to provide interaction between user and virtual humans. The system uses a data glove and an artificial neural network system responsible for the recognition of hand postures.},
  isbn = {9780769518466},
  keywords = {based-on-gloves,model:ffnn,movement:static,pdf:paywalled},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{dangAirGestureRecognition2020,
  title = {Air {{Gesture Recognition Using WLAN Physical Layer Information}}},
  author = {Dang, Xiaochao and Liu, Yang and Hao, Zhanjun and Tang, Xuhao and Shao, Chenguang},
  year = {2020},
  month = aug,
  journal = {Wireless Communications and Mobile Computing},
  volume = {2020},
  pages = {1--14},
  issn = {1530-8669, 1530-8677},
  doi = {10.1155/2020/8546237},
  url = {https://www.hindawi.com/journals/wcmc/2020/8546237/},
  urldate = {2023-07-12},
  abstract = {In recent years, the researchers have witnessed the important role of air gesture recognition in human-computer interactive (HCI), smart home, and virtual reality (VR). The traditional air gesture recognition method mainly depends on external equipment (such as special sensors and cameras) whose costs are high and also with a limited application scene. In this paper, we attempt to utilize channel state information (CSI) derived from a WLAN physical layer, a Wi-Fibased air gesture recognition system, namely, WiNum, which solves the problems of users' privacy and energy consumption compared with the approaches using wearable sensors and depth cameras. In the process of recognizing the WiNum method, the collected raw data of CSI should be screened, among which can reflect the gesture motion. Meanwhile, the screened data should be preprocessed by noise reduction and linear transformation. After preprocessing, the joint of amplitude information and phase information is extracted, to match and recognize different air gestures by using the S-DTW algorithm which combines dynamic time warping algorithm (DTW) and support vector machine (SVM) properties. Comprehensive experiments demonstrate that under two different indoor scenes, WiNum can achieve higher recognition accuracy for air number gestures; the average recognition accuracy of each motion reached more than 93\%, in order to achieve effective recognition of air gestures.},
  langid = {english},
  keywords = {based-on-wifi,classes:10,model:dtw,model:svm,tech:wifi},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/9WJ72HZQ/Dang et al. - 2020 - Air Gesture Recognition Using WLAN Physical Layer .pdf}
}

@inproceedings{davisVisualGestureRecognition1994,
  title = {Visual Gesture Recognition},
  booktitle = {{{IEE Proceedings}} - {{Vision}}, {{Image}}, and {{Signal Processing}}},
  author = {Davis, J.},
  year = {1994},
  volume = {141},
  pages = {101},
  issn = {1350245X},
  doi = {10.1049/ip-vis:19941058},
  url = {https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941058},
  urldate = {2023-07-11},
  abstract = {Presents a method for recognising human-hand gestures using a model based approach. A finite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of left, right, up, down, grab, rotate, and stop.},
  langid = {english},
  keywords = {based-on-vision,model:fsm,tech:rgb,type:seminal},
  annotation = {272 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{dept.ofbiomedicalengineeringkyungheeuniversityrepublicofkoreaRecognitionHumanHand2017,
  title = {Recognition of {{Human Hand Activities Based}} on a {{Single Wrist IMU Using Recurrent Neural Networks}}},
  booktitle = {International {{Journal}} of {{Pharma Medicine}} and {{Biological Sciences}}},
  author = {{Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea} and Rivera, Patricio and Valarezo, Edwin and Choi, Mun-Taek and Kim, Tae-Seong},
  year = {2017},
  volume = {6},
  pages = {114--118},
  issn = {22785221},
  doi = {10.18178/ijpmbs.6.4.114-118},
  url = {http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf},
  urldate = {2023-07-11},
  abstract = {Recognition of hand activities could provide new information towards daily human activity logging and gesture interface applications. However, there is a technical challenge due to delicate hand motions and complex movement contexts. In this work, we proposed hand activity recognition (HAR) based on a single inertial measurement unit (IMU) sensor at one wrist via deep learning recurrent neural network. The proposed HAR works directly with signals from a tri-axial accelerometer, gyroscope, and magnetometer sensors within one IMU. We evaluated the performance of our HAR with a public human hand activity database for six hand activities including Open Door, Close Door, Open Fridge, Close Fridge, Clean Table and Drink from Cup. Our results show an overall recognition accuracy of 80.09\% with discrete standard epochs and 74.92\% with noise-added epochs. With continuous time series epochs, the accuracy of 71.75\% was obtained. },
  keywords = {classes:6,model:rnn,tech:imu},
  annotation = {19 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/38I9KL9J/Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea et al. - 2017 - Recognition of Human Hand Activities Based on a Si.pdf}
}

@article{dipietroSurveyGloveBasedSystems2008,
  title = {A {{Survey}} of {{Glove-Based Systems}} and {{Their Applications}}},
  author = {Dipietro, L. and Sabatini, A.M. and Dario, P.},
  year = {2008},
  month = jul,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {4},
  pages = {461--482},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2008.923862},
  url = {http://ieeexplore.ieee.org/document/4539650/},
  urldate = {2023-06-23},
  abstract = {Hand movement data acquisition is used in many engineering applications ranging from the analysis of gestures to the biomedical sciences. Glove-based systems represent one of the most important efforts aimed at acquiring hand movement data. While they have been around for over three decades, they keep attracting the interest of researchers from increasingly diverse fields. This paper surveys such glove systems and their applications. It also analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research. A foremost goal of this paper is to provide readers who are new to the area with a basis for understanding glove systems technology and how it can be applied, while offering specialists an updated picture of the breadth of applications in several engineering and biomedical sciences areas.},
  keywords = {based-on-gloves,based-on-vision,have-read,type:survey},
  annotation = {652 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/RWUPUG9F/Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf}
}

@incollection{DynamicTimeWarping2007,
  title = {Dynamic {{Time Warping}}},
  booktitle = {Information {{Retrieval}} for {{Music}} and {{Motion}}},
  year = {2007},
  pages = {69--84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74048-3\_4},
  url = {https://doi.org/10.1007/978-3-540-74048-3_4},
  abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
  isbn = {978-3-540-74048-3},
  keywords = {background,model:dtw}
}

@article{eickelerHiddenMarkovModel1998,
  title = {Hidden {{Markov}} Model Based Continuous Online Gesture Recognition},
  author = {Eickeler, S. and Kosmala, A. and Rigoll, G.},
  year = {1998},
  journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
  volume = {2},
  pages = {1206--1208},
  publisher = {{IEEE Comput. Soc}},
  address = {{Brisbane, Qld., Australia}},
  doi = {10.1109/ICPR.1998.711914},
  url = {http://ieeexplore.ieee.org/document/711914/},
  urldate = {2023-07-11},
  abstract = {Presents the extension of an existing vision-based gesture recognition system using hidden Markov models(HMMs). Several improvements have been carried out in order to increase the capabilities and the functionality of the system. These improvements include position independent recognition, rejection of unknown gestures, and continuous online recognition of spontaneous gestures. We show that especially the latter requirement is highly complicated and demanding, if we allow the user to move in front of the camera without any restrictions and to perform the gestures spontaneously at any arbitrary moment. We present solutions to this problem by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.},
  isbn = {9780818685125},
  keywords = {based-on-vision,from:cite.bib,model:hmm,tech:rgb},
  annotation = {39 citations (Crossref) [2023-07-11] 140 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{elbadawyArabicSignLanguage2017,
  title = {Arabic Sign Language Recognition with {{3D}} Convolutional Neural Networks},
  author = {ElBadawy, Menna and Elons, A. S. and Shedeed, Howida A. and Tolba, M. F.},
  year = {2017},
  month = dec,
  journal = {2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)},
  pages = {66--71},
  publisher = {{IEEE}},
  address = {{Cairo}},
  doi = {10.1109/INTELCIS.2017.8260028},
  url = {http://ieeexplore.ieee.org/document/8260028/},
  urldate = {2023-06-22},
  abstract = {Sign Language recognition is very important for communication purposes between Hearing Impaired (HI) people and hearing ones. Arabic Sign Language Recognition field became widespread because of its difficult nature and numerous details. Most researchers employed different input sensors, features extractors, and classifiers on static and dynamic data. These different ways were customized and employed in our previous work in the Arabic Sign Language Recognition field. In this paper, features extractor with deep behavior was used to deal with the minor details of Arabic Sign Language. 3D Convolutional Neural Network (CNN) was used to recognize 25 gestures from Arabic sign language dictionary. The recognition system was fed with data from depth maps. The system achieved 98\% accuracy for observed data and 85\% average accuracy for new data. The results could be improved as more data from more different signers are included.},
  isbn = {9781538608210},
  keywords = {app:sign-language},
  annotation = {43 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{elmanDistributedRepresentationsSimple2004,
  title = {Distributed Representations, Simple Recurrent Networks, and Grammatical Structure},
  author = {Elman, Jeffrey L.},
  year = {2004},
  journal = {Machine Learning},
  volume = {7},
  pages = {195--225},
  keywords = {background,from:cite.bib,model:elman-nn,type:seminal}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = mar,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {03640213},
  doi = {10.1207/s15516709cog1402\_1},
  url = {http://doi.wiley.com/10.1207/s15516709cog1402_1},
  urldate = {2023-07-13},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  keywords = {background,model:elman-rnn,model:rnn,type:seminal},
  annotation = {9998 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@article{elmezainGestureRecognitionAlphabets2007,
  title = {Gesture {{Recognition}} for {{Alphabets}} from {{Hand Motion Trajectory Using Hidden Markov Models}}},
  author = {Elmezain, Mahmoud and {Al-Hamadi}, Ayoub and Krell, Gerald and {El-Etriby}, Sherif and Michaelis, Bernd},
  year = {2007},
  month = dec,
  journal = {2007 IEEE International Symposium on Signal Processing and Information Technology},
  pages = {1192--1197},
  publisher = {{IEEE}},
  address = {{Giza, Egypt}},
  doi = {10.1109/ISSPIT.2007.4458209},
  url = {http://ieeexplore.ieee.org/document/4458209/},
  urldate = {2023-07-11},
  abstract = {This paper describes a method to recognize the alphabets from a single hand motion using Hidden Markov Models (HMM). In our method, gesture recognition for alphabets is based on three main stages; preprocessing, feature extraction and classification. In preprocessing stage, color and depth information are used to detect both hands and face in connection with morphological operation. After the detection of the hand, the tracking will take place in further step in order to determine the motion trajectory so-called gesture path. The second stage, feature extraction enhances the gesture path which gives us a pure path and also determines the orientation between the center of gravity and each point in a pure path. Thereby, the orientation is quantized to give a discrete vector that used as input to HMM. In the final stage, the gesture of alphabets is recognized by using Left-Right Banded model (LRB) in conjunction with Baum-Welch algorithm (BW) for training the parameters of HMM. Therefore, the best path is obtained by Viterbi algorithm using a gesture database. In our experiment, 520 trained gestures are used for training and also 260 tested gestures for testing. Our method recognizes the alphabets from A to Z and achieves an average recognition rate of 92.3\%.},
  isbn = {9781424418343 9781424418350},
  keywords = {based-on-vision,classes:26,model:hmm,tech:rgbd},
  annotation = {49 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{elmezainHandGestureRecognition2009,
  title = {Hand {{Gesture Recognition Based}} on {{Combined Features Extraction}}},
  author = {Elmezain, M. and {Al-Hamadi}, A. and Michaelis, B.},
  year = {2009},
  month = dec,
  journal = {International Journal of Electrical and Computer Engineering},
  url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Based-on-Combined-Features-Elmezain-Al-Hamadi/92a80f36e199e0ddf73ba06f81b31ef60efcf5f8},
  urldate = {2023-07-11},
  abstract = {Hand gesture is an active area of research in the vision community, mainly for the purpose of sign language recognition and Human Computer Interaction. In this paper, we propose a system to recognize alphabet characters (A-Z) and numbers (0-9) in real-time from stereo color image sequences using Hidden Markov Models (HMMs). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, color and 3D depth map are used to detect hands where the hand trajectory will take place in further step using Mean-shift algorithm and Kalman filter. In the feature extraction stage, 3D combined features of location, orientation and velocity with respected to Cartesian systems are used. And then, k-means clustering is employed for HMMs codeword. The final stage so-called classification, BaumWelch algorithm is used to do a full train for HMMs parameters. The gesture of alphabets and numbers is recognized using Left-Right Banded model in conjunction with Viterbi algorithm. Experimental results demonstrate that, our system can successfully recognize hand gestures with 98.33\% recognition rate. Keywords\textemdash Gesture Recognition, Computer Vision \& Image Processing, Pattern Recognition.},
  keywords = {based-on-vision,model:hmm,model:kalman-filter,model:kmeans,tech:rgb},
  file = {/Users/brk/Zotero/storage/GP7SDB8D/Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf}
}

@article{elmezainRealTimeCapableSystem2008,
  title = {Real-{{Time Capable System}} for {{Hand Gesture Recognition Using Hidden Markov Models}} in {{Stereo Color Image Sequences}}},
  author = {Elmezain, M. and {Al-Hamadi}, A. and Michaelis, B.},
  year = {2008},
  journal = {J. WSCG},
  url = {https://www.semanticscholar.org/paper/Real-Time-Capable-System-for-Hand-Gesture-Using-in-Elmezain-Al-Hamadi/aa17055855518ac47031bc509f5b7aa1b82fdcff},
  urldate = {2023-07-11},
  abstract = {This paper proposes a system to recognize the alphabets and numbers in real time from color image sequences by the motion trajectory of a single hand using Hidden Markov Models (HMM). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, YCbCr color space and depth information are used to detect hands and face in connection with morphological operation where Gaussian Mixture Model (GMM) is used for computing the skin probability. After the hand is detected and the centroid point of the hand region is determined, the tracking will take place in the further steps to determine the hand motion trajectory by using a search area around the hand region. In the feature extraction stage, the orientation is determined between two consecutive points from hand motion trajectory and then it is quantized to give a discrete vector that is used as input to HMM. The final stage so-called classification, Baum-Welch algorithm (BW) is used to do a full train for HMM parameters. The gesture of alphabets and numbers is recognized by using Left-Right Banded model (LRB) in conjunction with Forward algorithm. In our experiment, 720 trained gestures are used for training and also 360 tested gestures for testing. Our system recognizes the alphabets from A to Z and numbers from 0 to 9 and achieves an average recognition rate of 94.72\%.},
  keywords = {based-on-vision,classes:36,model:gmm,model:hmm,tech:rgb}
}

@article{erolVisionbasedHandPose2007,
  title = {Vision-Based Hand Pose Estimation: {{A}} Review},
  shorttitle = {Vision-Based Hand Pose Estimation},
  author = {Erol, Ali and Bebis, George and Nicolescu, Mircea and Boyle, Richard D. and Twombly, Xander},
  year = {2007},
  month = oct,
  journal = {Computer Vision and Image Understanding},
  volume = {108},
  number = {1-2},
  pages = {52--73},
  issn = {10773142},
  doi = {10.1016/j.cviu.2006.10.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206002281},
  urldate = {2023-06-23},
  abstract = {Semantic Scholar extracted view of "Vision-based hand pose estimation: A review" by A. Erol et al.},
  langid = {english},
  keywords = {based-on-vision,tech:rgb,type:survey},
  annotation = {885 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/33ZZZF27/Erol et al. - 2007 - Vision-based hand pose estimation A review.pdf}
}

@misc{experimentaltelevisioncenterComputerImageCorporation1969,
  title = {Computer {{Image Corporation Archive}}},
  author = {{\{Experimental Television Center\}}},
  year = {1969},
  journal = {Computer Image Corporation Archive},
  url = {https://www.experimentaltvcenter.org/computer-image-corporation-archive/},
  abstract = {The Computer Image Corporation Archive contains an extensive film and video collection documenting work produced using machines developed by Computer Image Corporation of Denver, Colorado (1960-1985).},
  keywords = {hardware:animac}
}

@inproceedings{fangRealTimeHandGesture2007,
  title = {A {{Real-Time Hand Gesture Recognition Method}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Fang, Yikai and Wang, Kongqiao and Cheng, Jian and Lu, Hanqing},
  year = {2007},
  month = jul,
  pages = {995--998},
  issn = {1945-788X},
  doi = {10.1109/ICME.2007.4284820},
  abstract = {Compared with the traditional interaction approaches, such as keyboard, mouse, pen, etc, vision based hand interaction is more natural and efficient. In this paper, we proposed a robust real-time hand gesture recognition method. In our method, firstly, a specific gesture is required to trigger the hand detection followed by tracking; then hand is segmented using motion and color cues; finally, in order to break the limitation of aspect ratio encountered in most of learning based hand gesture methods, the scale-space feature detection is integrated into gesture recognition. Applying the proposed method to navigation of image browsing, experimental results show that our method achieves satisfactory performance.},
  keywords = {based-on-vision,tech:rgb,tech:rgbd},
  annotation = {227 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/4T92MUB7/4284820.html}
}

@article{fatmiComparingANNSVM2019,
  title = {Comparing {{ANN}}, {{SVM}}, and {{HMM}} Based {{Machine Learning Methods}} for {{American Sign Language Recognition}} Using {{Wearable Motion Sensors}}},
  author = {Fatmi, Rabeet and Rashad, Sherif and Integlia, Ryan},
  year = {2019},
  month = jan,
  journal = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
  pages = {0290--0297},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CCWC.2019.8666491},
  url = {https://ieeexplore.ieee.org/document/8666491/},
  urldate = {2023-07-02},
  abstract = {Millions of people with speech and hearing impairments, worldwide, communicate through sign languages every day. In the same way that voice recognition provides a simple communication platform for most users, gesture recognition is a natural means of correspondence for the hearing-impaired. In this paper, we explore the problem of translating/converting sign language to speech, and propose an improved solution using different machine learning techniques. We seek to build a system that can be employed in the daily lives of people with hearing impairments, in order to enhance communication and collaboration between the hearing-impaired community and those untrained in American Sign Language (ASL). The system architecture is based on using wearable motion sensors and machine learning techniques. In this study, we propose a solution using Artificial Neural Networks (ANN) and Support Vector Machines (SVM), and compare their accuracy with the Hidden Markov Model (HMM) results from our previous work to recognize ASL words. Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.},
  isbn = {9781728105543},
  keywords = {app:american-sl,app:sign-language,model:ffnn,model:hmm,model:svm},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{feinerVisualizingDimensionalVirtual1990,
  title = {Visualizing {\emph{n}} -Dimensional Virtual Worlds with {\emph{n}} -Vision},
  booktitle = {Proceedings of the 1990 Symposium on {{Interactive 3D}} Graphics  - {{SI3D}} '90},
  author = {Feiner, S. K. and Beshers, Clifford},
  year = {1990},
  pages = {37--38},
  publisher = {{ACM Press}},
  address = {{Snowbird, Utah, United States}},
  doi = {10.1145/91385.91412},
  url = {http://portal.acm.org/citation.cfm?doid=91385.91412},
  urldate = {2023-07-13},
  abstract = {There are many applications in science, mathematics , statistics, and business, in which it is important to explore an d manipulate clam in more than three dimensions . In thes e applications, data can be defined by points in Euclidean n-space . A point's position is then specified with it coordinates . each o f which determines its position relative to one of It mutuall y perpendicular axes . We describe here research that has as its goa l the development of interaction techniques and metaphors for th e 4D and higher-dimensional worlds that this data represents .},
  isbn = {978-0-89791-351-5},
  langid = {english},
  keywords = {based-on-gloves,hardware:dataglove,referenced},
  annotation = {133 citations (Semantic Scholar/DOI) [2023-07-13]},
  file = {/Users/brk/Zotero/storage/HJJIL3ZW/Feiner and Beshers - 1990 - Visualizing n -dimensional virtual worlds w.pdf}
}

@inproceedings{felsBuildingAdaptiveInterfaces1990,
  title = {Building Adaptive Interfaces with Neural Networks: {{The}} Glove-Talk Pilot Study},
  shorttitle = {Building Adaptive Interfaces with Neural Networks},
  booktitle = {{{IFIP TC13 International Conference}} on {{Human-Computer Interaction}}},
  author = {Fels, S. and Hinton, Geoffrey E.},
  year = {1990},
  month = aug,
  url = {https://www.semanticscholar.org/paper/Building-adaptive-interfaces-with-neural-networks%3A-Fels-Hinton/4886fa9820ada07ca905cfd6a9ee973ce070b10b},
  urldate = {2023-07-13},
  abstract = {Semantic Scholar extracted view of "Building adaptive interfaces with neural networks: The glove-talk pilot study" by S. Fels et al.},
  keywords = {based-on-gloves,hardware:dataglove,model:nn,referenced}
}

@article{felsGloveTalkIIaNeuralnetworkInterface1998,
  title = {Glove-{{TalkII-a}} Neural-Network Interface Which Maps Gestures to Parallel Formant Speech Synthesizer Controls},
  author = {Fels, S.S. and Hinton, G.E.},
  year = {Jan./1998},
  journal = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {1},
  pages = {205--212},
  issn = {10459227},
  doi = {10.1109/72.655042},
  url = {http://ieeexplore.ieee.org/document/655042/},
  urldate = {2023-07-02},
  abstract = {Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.},
  keywords = {based-on-gloves,model:ffnn,tech:accelerometer,tech:flex},
  annotation = {73 citations (Crossref) [2023-07-02]}
}

@article{fengWiMultiThreePhaseSystem2019,
  title = {Wi-{{Multi}}: {{A Three-Phase System}} for {{Multiple Human Activity Recognition With Commercial WiFi Devices}}},
  shorttitle = {Wi-{{Multi}}},
  author = {Feng, Chunhai and Arshad, Sheheryar and Zhou, Siwang and Cao, Dun and Liu, Yonghe},
  year = {2019},
  month = aug,
  journal = {IEEE Internet of Things Journal},
  volume = {6},
  number = {4},
  pages = {7293--7304},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2019.2915989},
  url = {https://ieeexplore.ieee.org/document/8710328/},
  urldate = {2023-07-12},
  abstract = {Channel state information-based activity recognition has gathered immense attention over recent years. Many existing works achieved desirable performance in various applications, including healthcare, security, and Internet of Things, with different machine learning algorithms. However, they usually fail to consider the availability of enough samples to be trained. Besides, many applications only focus on the scenario where only single subject presents. To address these challenges, in this paper, we propose a three-phase system Wi-multi that targets at recognizing multiple human activities in a wireless environment. Different system phases are applied according to the size of available collected samples. Specifically, distance-based classification using dynamic time warping is applied when there are few samples in the profile. Then, support vector machine is employed when representative features can be extracted from training samples. Lastly, recurrent neural networks is exploited when a large number of samples are available. Extensive experiments results show that Wi-multi achieves an accuracy of 96.1\% on average. It is also able to achieve a desirable tradeoff between accuracy and efficiency in different phases.},
  keywords = {based-on-wifi,classes:4,classes:5,model:dtw,tech:wifi},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@misc{fifthdimensiontechnologiesDataGlove52005,
  title = {{{DataGlove5}}},
  author = {{Fifth Dimension Technologies}},
  year = {2005},
  url = {https://5dt.com/5dt-data-glove-ultra/},
  keywords = {based-on-gloves,from:cite.bib,type:product}
}

@inproceedings{fisherTelepresenceMasterGlove1987,
  title = {Telepresence {{Master Glove Controller For Dexterous Robotic End-Effectors}}},
  booktitle = {Cambridge {{Symposium}}\_{{Intelligent Robotics Systems}}},
  author = {Fisher, Scott S.},
  editor = {Casasent, David P.},
  year = {1987},
  month = mar,
  pages = {396},
  address = {{Cambridge, MA}},
  doi = {10.1117/12.937753},
  url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.937753},
  urldate = {2023-07-12},
  abstract = {This paper describes recent research in the Aerospace Human Factors Research Division at NASA's Ames Research Center to develop a glove-like, control and data-recording device (DataGlove) that records and transmits to a host computerin real time, and at appropriate resolution, a numeric data-record of a user's hand/finger shape and dynamics. System configuration and performance specifications are detailed, and current research is discussed investigating its applications in operator control of dexterous robotic end-effectors and for use as a human factors research tool in evaluation of operator hand function requirements and performance in other specialized task environments.},
  keywords = {hardware:dataglove,tech:accelerometer,type:seminal},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/WEK8VIH3/Fisher - 1987 - Telepresence Master Glove Controller For Dexterous.pdf}
}

@inproceedings{fisherVirtualEnvironmentDisplay1987,
  title = {Virtual Environment Display System},
  booktitle = {Proceedings of the 1986 Workshop on {{Interactive 3D}} Graphics  - {{SI3D}} '86},
  author = {Fisher, S. S. and McGreevy, M. and Humphries, J. and Robinett, W.},
  year = {1987},
  pages = {77--87},
  publisher = {{ACM Press}},
  address = {{Chapel Hill, North Carolina, United States}},
  doi = {10.1145/319120.319127},
  url = {http://portal.acm.org/citation.cfm?doid=319120.319127},
  urldate = {2023-07-13},
  abstract = {A head-mounted, wide-angle, stereoscopic display system controlled by operator position, voice and gesture has been developed for use as a multipurpose interface environment. The system provides a multisensory, interactive display environment in which a user can virtually explore a 360-degree synthesized or remotely sensed environment and can viscerally interact with its components. Primary applications of the system are in telerobotics, management of large-scale integrated information systems, and human factors research. System configuration, application scenarios, and research directions are described.},
  isbn = {978-0-89791-228-0},
  langid = {english},
  keywords = {based-on-gloves},
  annotation = {396 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@inproceedings{flachPrecisionRecallGainCurvesPR2015,
  title = {Precision-{{Recall-Gain Curves}}: {{PR Analysis Done Right}}},
  shorttitle = {Precision-{{Recall-Gain Curves}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Flach, Peter and Kull, Meelis},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
  urldate = {2023-03-24},
  keywords = {background},
  file = {/Users/brk/Zotero/storage/5B3MNYUA/Flach and Kull - 2015 - Precision-Recall-Gain Curves PR Analysis Done Rig.pdf}
}

@misc{frankhofmannSensorGloveAnthropomorphicRobot1995,
  title = {{{SensorGlove}}: {{Anthropomorphic Robot Hand}}},
  author = {{Frank Hofmann} and {J\"urgen Henz}},
  year = {1995},
  url = {https://pdv.cs.tu-berlin.de/forschung/SensorGlove2_engl.html},
  abstract = {The TU Berlin SensorGlove was developed at the institute for Real-Time Systems and Robotics of the computer science department at the Technical University of Berlin. It was part of the project "Anthropomorphic Robot Hand", in which it was developed in the framework of a student and diploma thesis. It was shown at the Hanover Industrial Fair in 1993, arousing widespread interest. The improved second prototype was presented at the CeBIT 1995. The patented glove is used as a computer input device for human grasping movements and hand gestures. It is equipped with different types of sensors enabling highly precise measurements (cf. Technical Data, Section 5). A closer description follows below.},
  keywords = {based-on-gloves,from:cite.bib,type:product}
}

@inproceedings{freemanOrientationHistogramsHand1995,
  title = {Orientation {{Histograms}} for {{Hand Gesture Recognition}}},
  author = {Freeman, W. and Roth, Michal},
  year = {1995},
  url = {https://www.semanticscholar.org/paper/Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth/2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2},
  urldate = {2023-07-03},
  abstract = {We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or \textbackslash dynamic gestures", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,},
  keywords = {app:gaming,based-on-vision,classes:10,technique:histogram-oriented-gradients,type:seminal},
  file = {/Users/brk/Zotero/storage/N6NR3A34/Freeman and Roth - 1995 - Orientation Histograms for Hand Gesture Recognitio.pdf}
}

@inproceedings{frieslaarRobustSouthAfrican2014,
  title = {Robust {{South African}} Sign Language Gesture Recognition Using Hand Motion and Shape},
  author = {Frieslaar, I.},
  year = {2014},
  url = {https://www.semanticscholar.org/paper/Robust-South-African-sign-language-gesture-using-Frieslaar/af10d1bb6ef67ab9c8b624b5fcafd9dd2e2daf69},
  urldate = {2023-07-02},
  abstract = {Research has shown that five fundamental parameters are required to recognize any sign language gesture: hand shape, hand motion, hand location, hand orientation and facial expressions. The South African Sign Language (SASL) research group at the University of the Western Cape (UWC) has created several systems to recognize sign language gestures using single parameters. These systems are, however, limited to a vocabulary size of 20 \textendash{} 23 signs, beyond which the recognition accuracy is expected to decrease. The first aim of this research is to investigate the use of two parameters \textendash{} hand motion and hand shape \textendash{} to recognise a larger vocabulary of SASL gestures at a high accuracy. Also, the majority of related work in the field of sign language gesture recognition using these two parameters makes use of Hidden Markov Models (HMMs) to classify gestures. Hidden Markov Support Vector Machines (HM-SVMs) are a relatively new technique that make use of Support Vector Machines (SVMs) to simulate the functions of HMMs. Research indicates that HM-SVMs may perform better than HMMs in some applications. To our knowledge, they have not been applied to the field of sign language gesture recognition. This research compares the use of these two techniques in the context of SASL gesture recognition. The results indicate that, using two parameters results in a 15\% increase in accuracy over the use of a single parameter. Also, it is shown that HM-SVMs are a more accurate technique than HMMs, generally performing better or at least as good as HMMs.},
  keywords = {model:hmm,model:svm},
  annotation = {6 Citations},
  file = {/Users/brk/Zotero/storage/U9U3CGTS/Frieslaar - 2014 - Robust South African sign language gesture recogni.pdf}
}

@inproceedings{funkeUsing3DConvolutional2019,
  title = {Using {{3D Convolutional Neural Networks}} to {{Learn Spatiotemporal Features}} for {{Automatic Surgical Gesture Recognition}} in {{Video}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2019},
  author = {Funke, Isabel and Bodenstedt, Sebastian and Oehme, Florian and Von Bechtolsheim, Felix and Weitz, J{\"u}rgen and Speidel, Stefanie},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  volume = {11768},
  pages = {467--475},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32254-0\_52},
  url = {http://link.springer.com/10.1007/978-3-030-32254-0_52},
  urldate = {2023-06-22},
  abstract = {Automatically recognizing surgical gestures is a crucial step towards a thorough understanding of surgical skill. Possible areas of application include automatic skill assessment, intra-operative monitoring of critical surgical steps, and semi-automation of surgical tasks. Solutions that rely only on the laparoscopic video and do not require additional sensor hardware are especially attractive as they can be implemented at low cost in many scenarios. However, surgical gesture recognition based only on video is a challenging problem that requires effective means to extract both visual and temporal information from the video. Previous approaches mainly rely on frame-wise feature extractors, either handcrafted or learned, which fail to capture the dynamics in surgical video. To address this issue, we propose to use a 3D Convolutional Neural Network (CNN) to learn spatiotemporal features from consecutive video frames. We evaluate our approach on recordings of robot-assisted suturing on a bench-top model, which are taken from the publicly available JIGSAWS dataset. Our approach achieves high frame-wise surgical gesture recognition accuracies of more than 84\%, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately. For the first time, these results demonstrate the benefit of spatiotemporal CNNs for video-based surgical gesture recognition.},
  isbn = {978-3-030-32253-3 978-3-030-32254-0},
  langid = {english},
  keywords = {model:cnn,tech:rgb},
  annotation = {61 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/2P6JU4JY/Funke et al. - 2019 - Using 3D Convolutional Neural Networks to Learn Sp.pdf}
}

@article{galkaInertialMotionSensing2016,
  title = {Inertial {{Motion Sensing Glove}} for {{Sign Language Gesture Acquisition}} and {{Recognition}}},
  author = {Galka, Jakub and Masior, Mariusz and Zaborski, Mateusz and Barczewska, Katarzyna},
  year = {2016},
  month = aug,
  journal = {IEEE Sensors Journal},
  volume = {16},
  number = {16},
  pages = {6310--6316},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2016.2583542},
  url = {http://ieeexplore.ieee.org/document/7497574/},
  urldate = {2023-03-07},
  abstract = {The most popular systems for automatic sign language recognition are based on vision. They are user-friendly, but very sensitive to changes in regard to recording conditions. This paper presents a description of the construction of a more robust system-an accelerometer glove-as well as its application in the recognition of sign language gestures. The basic data regarding inertial motion sensors and the design of the gesture acquisition system as well as project proposals are presented. The evaluation of the solution presents the results of the gesture recognition attempt by using a selected set of sign language gestures with a described method based on Hidden Markov Model (HMM) and parallel HMM approaches. The proposed usage of parallel HMM for sensor-fusion modeling reduced the equal error rate by more than 60\%, while preserving 99.75\% recognition accuracy.},
  keywords = {app:sign-language,from:cite.bib,model:hmm,read-priority-1,tech:accelerometer},
  annotation = {81 citations (Crossref) [2023-07-11] 92 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/AI4LXML7/Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf}
}

@article{gaoChineseSignLanguage2004,
  title = {A {{Chinese}} Sign Language Recognition System Based on {{SOFM}}/{{SRN}}/{{HMM}}},
  author = {Gao, W and Fang, G and Zhao, D and Chen, Y},
  year = {2004},
  month = dec,
  journal = {Pattern Recognition},
  volume = {37},
  number = {12},
  pages = {2389--2402},
  issn = {00313203},
  doi = {10.1016/S0031-3203(04)00165-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320304001657},
  urldate = {2023-07-02},
  abstract = {Semantic Scholar extracted view of "A Chinese sign language recognition system based on SOFM/SRN/HMM" by Wen Gao et al.},
  langid = {english},
  keywords = {app:chinese-sl,app:sign-language,based-on-gloves,classes:5113,contains-more-references,hardware:cyberglove,model:hmm,model:som,participants:6},
  annotation = {128 citations},
  file = {/Users/brk/Zotero/storage/X6ZWMRXU/Gao et al. - 2004 - A Chinese sign language recognition system based o.pdf}
}

@article{gaoSIGNLANGUAGERECOGNITION2000,
  title = {{{SIGN LANGUAGE RECOGNITION BASED ON HMM}}/{{ANN}}/{{DP}}},
  author = {Gao, Wen and Ma, Jiyong and Wu, Jiangqin and Wang, Chunli},
  year = {2000},
  month = aug,
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  volume = {14},
  number = {05},
  pages = {587--602},
  issn = {0218-0014, 1793-6381},
  doi = {10.1142/S0218001400000386},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001400000386},
  urldate = {2023-07-02},
  abstract = {In this paper, a system designed for helping the deaf to communicate with others is presented. Some useful new ideas are proposed in design and implementation. An algorithm based on geometrical analysis for the purpose of extracting invariant feature to signer position is presented. An ANN\textendash DP combined approach is employed for segmenting subwords automatically from the data stream of sign signals. To tackle the epenthesis movement problem, a DP-based method has been used to obtain the context-dependent models. Some techniques for system implementation are also given, including fast matching, frame prediction and search algorithms. The implemented system is able to recognize continuous large vocabulary Chinese Sign Language. Experiments show that proposed techniques in this paper are efficient on either recognition speed or recognition performance.},
  langid = {english},
  keywords = {app:sign-language,model:ffnn,model:hmm,pdf:paywalled},
  annotation = {99 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@misc{garyj.grimesUSPatentDigital1981,
  title = {{{US Patent}} for {{Digital}} Data Entry Glove Interface Device {{Patent}} ({{Patent}} \# 4,414,537 Issued {{November}} 8, 1983) - {{Justia Patents Search}}},
  author = {{Gary J. Grimes}},
  year = {1981},
  url = {https://patents.justia.com/patent/4414537},
  urldate = {2023-07-13},
  abstract = {A man-machine interface is disclosed for translating discrete hand positions into electrical signals representing alpha-numeric characters. The interface comprises a glove having sensors positioned with respect to the hand for detecting the flex of finger joints and sensors for detecting the contact between various portions of the hand. Additional sensors detect the movement of the hand with respect to a gravitational vector and a horizontal plane of reference. Further additional sensors detect the twisting and flexing of the wrist. The additional sensors are associated with prescribed mode signals which determine whether subsequently formed or priorly formed character specifying hand positions are to be entered for transmission. The alpha-numeric characters associated with the formed character specifying hand positions are transmitted only when the appropriate mode signal results. The forming and moving of the hand actuates various combinations of sensors so that electrical signals representing the specified characters are generated and transmitted.},
  keywords = {app:sign-language,based-on-gloves,tech:flex,tech:touch,type:patent},
  file = {/Users/brk/Zotero/storage/J9WZWAX2/US Patent for Digital data entry glove interface d.pdf;/Users/brk/Zotero/storage/FQNIK68R/4414537.html}
}

@misc{geoffreyhintonCourseraNeuralNetworks2012,
  title = {Coursera: {{Neural Networks}} for {{Machine Learning}}},
  author = {{Geoffrey Hinton}},
  year = {2012},
  url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  keywords = {background,technique:rmsprop}
}

@article{ghotkarDynamicHandGesture2016,
  title = {Dynamic {{Hand Gesture Recognition}} Using {{Hidden Markov Model}} by {{Microsoft Kinect Sensor}}},
  author = {Ghotkar, Archana and Vidap, Pujashree and Deo, Kshitish},
  year = {2016},
  month = sep,
  journal = {International Journal of Computer Applications},
  volume = {150},
  number = {5},
  pages = {5--9},
  issn = {09758887},
  doi = {10.5120/ijca2016911498},
  url = {http://www.ijcaonline.org/archives/volume150/number5/ghotkar-2016-ijca-911498.pdf},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is one of the leading applications of human computer interaction. With diversity of applications of hand gesture recognition, sign language interpretation is the most demanding application. In this paper, dynamic hand gesture recognition for few subset of Indian sign language recognition was considered. The use of depth camera such as Kinect sensor gave skeleton information of signer body. After detailed study of dynamic ISL vocabulary with reference to skeleton joint information, angle has identified as a feature with reference to two moving hand. Here, real time video has been captured and gesture was recognized using Hidden Markov Model (HMM). Ten state HMM model was designed and normalized angle feature of dynamic sign was being observed. Maximum likelihood probability symbol was considered as a recognized gesture. Algorithm has been tested on ISL 20 dynamic signs of total 800 training set of four persons and achieved 89.25\% average accuracy. General Terms Human Computer Interaction, Pattern Recognition, Machine Learning},
  keywords = {app:indian-sl,app:sign-language,classes:20,hardware:kinect,model:hmm,participants:4},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/7RH4TGA8/Ghotkar et al. - 2016 - Dynamic Hand Gesture Recognition using Hidden Mark.pdf}
}

@inproceedings{gillianGestureRecognitionToolkit2014,
  title = {The Gesture Recognition Toolkit},
  booktitle = {J. {{Mach}}. {{Learn}}. {{Res}}.},
  author = {Gillian, Nicholas Edward and Paradiso, Joseph A.},
  year = {2014},
  keywords = {from:cite.bib,type:software-lib}
}

@article{guptaDefocusBasedNovel2019,
  title = {A {{Defocus Based Novel Keyboard Design}}},
  author = {Gupta, Priyanshu and Goswamy, Tushar and Kumar, Himanshu and Venkatesh, K.S.},
  year = {2019},
  month = dec,
  journal = {2019 IEEE Conference on Information and Communication Technology},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Allahabad, India}},
  doi = {10.1109/CICT48419.2019.9066224},
  url = {https://ieeexplore.ieee.org/document/9066224/},
  urldate = {2023-07-11},
  abstract = {Depth map estimation from Defocus is a computer vision technique which has wide applications such as constructing the \$3D\$ setup from \$2D\$ image(s), image refocusing and reconstructing \$3D\$ scenes. In this paper, we propose an application of Depth from Defocus to a novel keyboard design for detecting keystrokes. The proposed keyboard can be integrated with devices such as mobile, PC and tablets and can be generated by either printing on plain paper or by projection on a flat surface. The proposed design utilizes measured defocus together with a precalibrated relation between the defocus amount and the keyboard pattern to infer the depth, which, along with the azimuth position of the stroke identifies the key. As the proposed design does not require any other hardware besides a monocular camera, this makes the proposed approach a cost effective and feasible solution for a portable keyboard.},
  isbn = {9781728153988},
  keywords = {read-priority-3,tech:rgb},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{guyonChaLearnGestureDataset2014,
  title = {The {{ChaLearn}} Gesture Dataset ({{CGD}} 2011)},
  author = {Guyon, Isabelle and Athitsos, Vassilis and Jangyodsuk, Pat and Escalante, Hugo Jair},
  year = {2014},
  month = nov,
  journal = {Machine Vision and Applications},
  volume = {25},
  number = {8},
  pages = {1929--1951},
  issn = {1432-1769},
  doi = {10.1007/s00138-014-0596-3},
  url = {https://doi.org/10.1007/s00138-014-0596-3},
  urldate = {2023-06-23},
  abstract = {This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shot-learning. The data include 54,000 hand and arm gestures recorded with an RGB-D \$\$\textbackslash hbox \{Kinect\}\^\textbackslash mathrm\{TM\}\$\$camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8\textendash 12 gestures, recorded by the same user. Short continuous sequences of 1\textendash 5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user's horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, datacollection software and the gesture vocabularies are downloadable from http://gesture.chalearn.org. We set up a forum for researchers working on these data http://groups.google.com/group/gesturechallenge.},
  langid = {english},
  keywords = {type:dataset},
  annotation = {70 Citations}
}

@article{hahnloserDigitalSelectionAnalogue2000,
  title = {Digital Selection and Analogue Amplification Coexist in a Cortex-Inspired Silicon Circuit},
  author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
  year = {2000},
  month = jun,
  journal = {Nature},
  volume = {405},
  number = {6789},
  pages = {947--951},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/35016072},
  url = {https://www.nature.com/articles/35016072},
  urldate = {2023-07-11},
  abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
  langid = {english},
  keywords = {background,relu},
  annotation = {1114 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{hakimDynamicHandGesture2019,
  title = {Dynamic {{Hand Gesture Recognition Using 3DCNN}} and {{LSTM}} with {{FSM Context-Aware Model}}},
  author = {Hakim, Noorkholis Luthfil and Shih, Timothy K. and Kasthuri Arachchi, Sandeli Priyanwada and Aditya, Wisnu and Chen, Yi-Cheng and Lin, Chih-Yang},
  year = {2019},
  month = dec,
  journal = {Sensors},
  volume = {19},
  number = {24},
  pages = {5429},
  issn = {1424-8220},
  doi = {10.3390/s19245429},
  url = {https://www.mdpi.com/1424-8220/19/24/5429},
  urldate = {2023-06-22},
  abstract = {With the recent growth of Smart TV technology, the demand for unique and beneficial applications motivates the study of a unique gesture-based system for a smart TV-like environment. Combining movie recommendation, social media platform, call a friend application, weather updates, chatting app, and tourism platform into a single system regulated by natural-like gesture controller is proposed to allow the ease of use and natural interaction. Gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment. Dataset of a sequence of RGB and depth images were collected, preprocessed, and trained in the proposed deep learning architecture. Combination of three-dimensional Convolutional Neural Network (3DCNN) followed by Long Short-Term Memory (LSTM) model was used to extract the spatio-temporal features. At the end of the classification, Finite State Machine (FSM) communicates the model to control the class decision results based on application context. The result suggested the combination data of depth and RGB to hold 97.8\% of accuracy rate on eight selected gestures, while the FSM has improved the recognition rate from 89\% to 91\% in a real-time performance.},
  langid = {english},
  keywords = {classes:10-29,model:cnn,model:fsm,model:lstm,movement:dynamic,movement:static,tech:rgbd},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/W6XRMHDD/Hakim et al. - 2019 - Dynamic Hand Gesture Recognition Using 3DCNN and L.pdf}
}

@article{halperinToolReleaseGathering2011,
  title = {Tool Release: Gathering 802.11n Traces with Channel State Information},
  shorttitle = {Tool Release},
  author = {Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David},
  year = {2011},
  month = jan,
  journal = {ACM SIGCOMM Computer Communication Review},
  volume = {41},
  number = {1},
  pages = {53--53},
  issn = {0146-4833},
  doi = {10.1145/1925861.1925870},
  url = {https://dl.acm.org/doi/10.1145/1925861.1925870},
  urldate = {2023-07-12},
  abstract = {We are pleased to announce the release of a tool that records detailed measurements of the wireless channel along with received 802.11 packet traces. It runs on a commodity 802.11n NIC, and records Channel State Information (CSI) based on the 802.11 standard. Unlike Receive Signal Strength Indicator (RSSI) values, which merely capture the total power received at the listener, the CSI contains information about the channel between sender and receiver at the level of individual data subcarriers, for each pair of transmit and receive antennas.             Our toolkit uses the Intel WiFi Link 5300 wireless NIC with 3 antennas. It works on up-to-date Linux operating systems: in our testbed we use Ubuntu 10.04 LTS with the 2.6.36 kernel. The measurement setup comprises our customized versions of Intel's close-source firmware and open-source iwlwifi wireless driver, userspace tools to enable these measurements, access point functionality for controlling both ends of the link, and Matlab (or Octave) scripts for data analysis. We are releasing the binary of the modified firmware, and the source code to all the other components.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi,type:seminal},
  annotation = {1239 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{hamdyaliComparativeStudyUser2014,
  title = {A {{Comparative Study}} of {{User Dependent}} and {{Independent Accelerometer-Based Gesture Recognition Algorithms}}},
  booktitle = {Distributed, {{Ambient}}, and {{Pervasive Interactions}}},
  author = {Hamdy Ali, Aya and Atia, Ayman and Sami, Mostafa},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Streitz, Norbert and Markopoulos, Panos},
  year = {2014},
  volume = {8530},
  pages = {119--129},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07788-8\_12},
  url = {http://link.springer.com/10.1007/978-3-319-07788-8_12},
  urldate = {2023-03-07},
  abstract = {In this paper, we introduce an evaluation of accelerometer-based gesture recognition algorithms in user dependent and independent cases. Gesture recognition has many algorithms and this evaluation includes Hidden Markov Models, Support Vector Machine, K-nearest neighbor, Artificial Neural Net-work and Dynamic Time Warping. Recognition results are based on acceleration data collected from 12 users. We evaluated the algorithms based on the recognition accuracy related to different number of gestures from two datasets. Evaluation results show that the best accuracy for 8 and 18 gestures is achieved with dynamic time warping and K-nearest neighbor algorithms.},
  isbn = {978-3-319-07787-1 978-3-319-07788-8},
  langid = {english},
  keywords = {classes:{$<$}10,classes:10-29,model:dtw,model:ffnn,model:hmm,model:knn,model:svm,tech:accelerometer},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/CXIM8DZI/Hamdy Ali et al. - 2014 - A Comparative Study of User Dependent and Independ.pdf}
}

@article{haoWiSLContactlessFineGrained2020,
  title = {Wi-{{SL}}: {{Contactless Fine-Grained Gesture Recognition Uses Channel State Information}}},
  shorttitle = {Wi-{{SL}}},
  author = {Hao, Zhanjun and Duan, Yu and Dang, Xiaochao and Liu, Yang and Zhang, Daiyang},
  year = {2020},
  month = jul,
  journal = {Sensors},
  volume = {20},
  number = {14},
  pages = {4025},
  issn = {1424-8220},
  doi = {10.3390/s20144025},
  url = {https://www.mdpi.com/1424-8220/20/14/4025},
  urldate = {2023-07-12},
  abstract = {In recent years, with the development of wireless sensing technology and the widespread popularity of WiFi devices, human perception based on WiFi has become possible, and gesture recognition has become an active topic in the field of human-computer interaction. As a kind of gesture, sign language is widely used in life. The establishment of an effective sign language recognition system can help people with aphasia and hearing impairment to better interact with the computer and facilitate their daily life. For this reason, this paper proposes a contactless fine-grained gesture recognition method using Channel State Information (CSI), namely Wi-SL. This method uses a commercial WiFi device to establish the correlation mapping between the amplitude and phase difference information of the subcarrier level in the wireless signal and the sign language action, without requiring the user to wear any device. We combine an efficient denoising method to filter environmental interference with an effective selection of optimal subcarriers to reduce the computational cost of the system. We also use K-means combined with a Bagging algorithm to optimize the Support Vector Machine (SVM) classification (KSB) model to enhance the classification of sign language action data. We implemented the algorithms and evaluated them for three different scenarios. The experimental results show that the average accuracy of Wi-SL gesture recognition can reach 95.8\%, which realizes device-free, non-invasive, high-precision sign language gesture recognition.},
  langid = {english},
  keywords = {app:sign-language,based-on-wifi,model:k-means,model:svm,tech:wifi},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/IM4P4L7G/Hao et al. - 2020 - Wi-SL Contactless Fine-Grained Gesture Recognitio.pdf}
}

@article{harrisArrayProgrammingNumPy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del R{\'i}o, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/s41586-020-2649-2},
  url = {https://doi.org/10.1038/s41586-020-2649-2},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {7070 citations (Crossref) [2023-07-11]}
}

@article{harrisonOmniTouchWearableMultitouch2011,
  title = {{{OmniTouch}}: Wearable Multitouch Interaction Everywhere},
  shorttitle = {{{OmniTouch}}},
  author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
  year = {2011},
  month = oct,
  journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
  pages = {441--450},
  publisher = {{ACM}},
  address = {{Santa Barbara California USA}},
  doi = {10.1145/2047196.2047255},
  url = {https://dl.acm.org/doi/10.1145/2047196.2047255},
  urldate = {2023-06-23},
  abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
  isbn = {9781450307161},
  langid = {english},
  keywords = {hardware:unique,tech:rgb},
  annotation = {587 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{harshithSurveyVariousGesture2010,
  title = {Survey on {{Various Gesture Recognition Techniques}} for {{Interfacing Machines Based}} on {{Ambient Intelligence}}},
  author = {Harshith, C and Shastry, {\relax Karthik.R}. and Ravindran, Manoj and Srikanth, M.V.V.N.S and Lakshmikhanth, Naveen},
  year = {2010},
  month = nov,
  journal = {International Journal of Computer Science \& Engineering Survey},
  volume = {1},
  number = {2},
  pages = {31--42},
  issn = {09763252},
  doi = {10.5121/ijcses.2010.1203},
  url = {http://www.airccse.org/journal/ijcses/papers/1110ijcses03.pdf},
  urldate = {2023-06-22},
  abstract = {Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions. Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods, appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.},
  keywords = {type:survey},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UDQVJPM7/Harshith et al. - 2010 - Survey on Various Gesture Recognition Techniques f.pdf}
}

@inproceedings{hassanpourVisionBasedHand2008,
  title = {{{Vision}}\-{{Based Hand Gesture Recognition}} for {{Human Computer Interaction}}: {{A Review}}},
  shorttitle = {{{Vision}}\-{{Based Hand Gesture Recognition}} for {{Human Computer Interaction}}},
  author = {Hassanpour, R. and Wong, Stephan and Shahbahrami, A. and Wong, J.},
  year = {2008},
  url = {https://www.semanticscholar.org/paper/Vision%C2%ADBased-Hand-Gesture-Recognition-for-Human-A-Hassanpour-Wong/1285c27b3a3b2304274b88676fc20086bf5cc3dd},
  urldate = {2023-07-11},
  abstract = {Evolution of user interfaces shapes the change in the human\-computer interaction. With the rapid emergence of three\-dimensional (3\-D) applications; the need for a new type of interaction device arises as traditional devices such as mouse, keyboard, and joystick become inefficient and cumbersome within these~virtual environments. Intuitive and naturalness characteristics of ``Ha nd Gestures'' in human computer interaction have been the driving force and motivation to develop an interaction device which can replace current unwieldy tools. This study is a survey on the methods of analyzing, modeling and recognizing hand gestures in the context of human\- computer interaction. Taxonomy of the methods based on the applications that they have been developed for and the approaches that they have used to represent gestures is presented. Direction of future developments is also discussed.},
  keywords = {based-on-vision,tech:rgb,type:survey},
  file = {/Users/brk/Zotero/storage/79Q9UGWM/Hassanpour et al. - 2008 - VisionBased Hand Gesture Recognition for Human Co.pdf}
}

@inproceedings{hernandez-rebollarAcceleGloveWholehandInput2002,
  title = {The {{AcceleGlove}}: A Whole-Hand Input Device for Virtual Reality},
  shorttitle = {The {{AcceleGlove}}},
  booktitle = {{{ACM SIGGRAPH}} 2002 Conference Abstracts and Applications},
  author = {{Hernandez-Rebollar}, Jose L. and Kyriakopoulos, Nicholas and Lindeman, Robert W.},
  year = {2002},
  month = jul,
  pages = {259--259},
  publisher = {{ACM}},
  address = {{San Antonio Texas}},
  doi = {10.1145/1242073.1242272},
  url = {https://dl.acm.org/doi/10.1145/1242073.1242272},
  urldate = {2023-07-02},
  abstract = {We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.},
  isbn = {978-1-58113-525-1},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on-gloves,classes:26,hardware:accelerometers,hardware:fabricless,have-read,movement:static},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/GPITAK45/Hernandez-Rebollar et al. - 2002 - The AcceleGlove a whole-hand input device for vir.pdf}
}

@article{heumerGraspRecognitionUncalibrated2007,
  title = {Grasp {{Recognition}} with {{Uncalibrated Data Gloves}} - {{A Comparison}} of {{Classification Methods}}},
  author = {Heumer, Guido and Amor, Heni Ben and Weber, Matthias and Jung, Bernhard},
  year = {2007},
  month = mar,
  journal = {2007 IEEE Virtual Reality Conference},
  pages = {19--26},
  publisher = {{IEEE}},
  address = {{Charlotte, NC, USA}},
  doi = {10.1109/VR.2007.352459},
  url = {https://ieeexplore.ieee.org/document/4161001/},
  urldate = {2023-07-11},
  abstract = {This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out, where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was analyzed with 28 classifiers including different types of neural networks, decision trees, Bayes nets, and lazy learners. Each classifier was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.},
  isbn = {9781424409051},
  keywords = {based-on-gloves,hardware:dataglove,model:dt,model:nn,tech:accelerometer,tech:flex},
  annotation = {64 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/3422NZ4Q/Heumer et al. - 2007 - Grasp Recognition with Uncalibrated Data Gloves - .pdf}
}

@article{heWiGWiFiBasedGesture2015,
  title = {{{WiG}}: {{WiFi-Based Gesture Recognition System}}},
  shorttitle = {{{WiG}}},
  author = {He, Wenfeng and Wu, Kaishun and Zou, Yongpan and Ming, Zhong},
  year = {2015},
  month = aug,
  journal = {2015 24th International Conference on Computer Communication and Networks (ICCCN)},
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/ICCCN.2015.7288485},
  url = {http://ieeexplore.ieee.org/document/7288485/},
  urldate = {2023-06-22},
  abstract = {Most recently, gesture recognition has increasingly attracted intense academic and industrial interest due to its various applications in daily life, such as home automation, mobile games. Present approaches for gesture recognition, mainly including vision-based, sensor-based and RF-based, all have certain limitations which hinder their practical use in some scenarios. For example, the vision-based approaches fail to work well in poor light conditions and the sensor-based ones require users to wear devices. To address these, we propose WiG in this paper, a device-free gesture recognition system based solely on Commercial Off-The-Shelf (COTS) WiFi infrastructures and devices. Compared with existing Radio Frequency (RF)-based systems, WiG stands out for its systematic simplicity, extremely low cost and high practicability. We implemented WiG in indoor environment and conducted experiments to evaluate its performance in two typical scenarios. The results demonstrate that WiG can achieve an average recognition accuracy of 92\% in line-of-sight scenario and average accuracy of 88\% in the none-line-of sight scenario.},
  isbn = {9781479999644},
  keywords = {based-on-wifi,tech:wifi},
  annotation = {132 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{hochreiterGradientFlowRecurrent2001,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: The {{Difficulty}} of {{Learning Long-Term Dependencies}}},
  shorttitle = {Gradient {{Flow}} in {{Recurrent Nets}}},
  author = {Hochreiter, S. and Bengio, Yoshua},
  year = {2001},
  url = {https://www.semanticscholar.org/paper/Gradient-Flow-in-Recurrent-Nets%3A-the-Difficulty-of-Hochreiter-Bengio/aed054834e2c696807cc8b227ac7a4197196e211},
  urldate = {2023-07-11},
  abstract = {D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\textbackslash M[ X N@]\_\^O\textbackslash `JaNcb V RcQ W d EGKeL(\^(QgfhKeLOE?i)\^(QSj ETNPfPQkRl[ V R)m"[ X \^(KeLOEG\^ npo qarpo m"[ X \^(KeLOEG\^tsAu EGNPb V \^ v wyx zlwO\{(|(\}{$<\sptilde$}OC\vphantom\{\}((xp\{ay.\textasciitilde A\}\_\textasciitilde{} Cl3\#|{$<$}Azw\#|l6 (|  JpfhL XV EG\^O QgJ  ETFOR] \^O\textbackslash JNPb V RcQ X E)ETR 6EGKeLOETNcKMLOE F ETN V RcQgJp\^(\^OE ZgZ E i \^(Qkj EGNPfhQSRO E OE2m1Jp\^ RcNY E VZ sO! \textexclamdown{} q.n sCD X KGKa8\textcent EG\^ RPNhE\textcurrency\textsterling{} \textyen\textbrokenbar Q ZgZ Es m\textsection J\^ RPNO E VZ s({\" } X  EG\textcopyright\#EKas\# V \^ V  V s(H a \guillemotleft a{$\lnot$}3\- \textregistered\#|.Y{\= }y\vphantom\{\} xa\textdegree OC\vphantom\{\}l\{x  yxlY\textasciitilde 3\{|  {$\pm$}2Pz   V J Z J U N V fhKTJp\^(Q  ETFOR J\textbackslash{} D vYf3RPEGb{\' }f V \^(\textsection JpbF X RPETN@D KTQEG\^(KTE i \^(QSjpEGNPfhQSR4v{$\mu$}J\textbackslash{} U\textparagraph Z JaNPEG\^(K{$\cdot$}E jYQ V (Q{\c  }D V \^ R V m V N3R V aOs\#1 o \textexclamdown Ga r U QNhE\^OoTE1{$\fracslash$}4\guillemotright,] R VZ vC1{$\fracslash$}2 3{$\fracslash$}4  x {$\pm$} x  \#\textquestiondown{} \}\`A 3t\}lC\vphantom\{\}2P\vphantom\{\}{$<\sptilde$} {$\lnot$}t[ X NPE\^\textsection D KeL(b{\' }Qg(L X \textcopyright yETN ]  DY]\_\'A JNPfhJ\~A\^A Z j EToQ V a rpopo2\"A X  V \^(J(sCD \AA )QSRPoTEGN ZgV \^( \AE{} \#|\{3{\= }|.(C\}.C\textquestiondown Y\vphantom\{\}p Pzw},
  keywords = {background,vanishing-gradient}
}

@inproceedings{hofmannVelocityProfileBased1998,
  title = {Velocity Profile Based Recognition of Dynamic Gestures with Discrete {{Hidden Markov Models}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {Hofmann, Frank G. and Heyer, Peter and Hommel, G{\"u}nter},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Fr{\"o}hlich, Martin},
  year = {1998},
  volume = {1371},
  pages = {81--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052991},
  url = {http://link.springer.com/10.1007/BFb0052991},
  urldate = {2023-07-12},
  abstract = {In this paper we present a method for the recognition of dynamic gestures with discrete Hidden Markov Models (HMMs) from a continuous stream of gesture input data. The segmentation problem is addressed by extracting two velocity profiles from the gesture data and using their extrema as segmentation cues. Gestures are captured with a TUB-SensorGlove. The paper focuses on the description of the gesture recognition method (including data preprocessing) and describes experiments for the evaluation of the performance of the recognition method. The paper combines and further develops ideas from some of our previous work.},
  isbn = {978-3-540-64424-8 978-3-540-69782-4},
  keywords = {based-on-vision,from:cite.bib,model:adaboost,model:hmm,tech:rgb,technique:histogram-oriented-gradients},
  annotation = {108 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/7QFBJZY3/Hofmann et al. - 1998 - Velocity profile based recognition of dynamic gest.pdf}
}

@article{hongCalibratingVPLDataGlove1989,
  title = {Calibrating a {{VPL DataGlove}} for Teleoperating the {{Utah}}/{{MIT}} Hand},
  author = {Hong, J. and Tan, X.},
  year = {1989},
  journal = {Proceedings, 1989 International Conference on Robotics and Automation},
  pages = {1752--1757},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Scottsdale, AZ, USA}},
  doi = {10.1109/ROBOT.1989.100228},
  url = {http://ieeexplore.ieee.org/document/100228/},
  urldate = {2023-07-13},
  abstract = {A system able to control the Utah/MIT hand with the VPL DataGlove has been developed. To get the actual joint angles from the DataGlove sensor values, a least-squares fit is used to find the best-fit exponential curve for each sensor, and then the correlation between the sensors is reduced by the iterative correlation elimination procedure. The calibration depends both on the wearer and the particular DataGlove being used. The first level calibration is simple and can be done in under 15 minutes with a little experience. The second level is fixed and requires no adjustments. To control the hand, a mapping from the DataGlove angles to the hand angles is applied, making the hand fingertips follow the DataGlove fingertips. The hand can successfully implement various high-level tasks under the DataGlove wearer's control.{$<<$}ETX{$>>$}},
  isbn = {9780818619380},
  keywords = {based-on-gloves,hardware:dataglove},
  annotation = {74 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
  urldate = {2023-05-30},
  abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
  langid = {english},
  keywords = {background,model:ffnn,universal-approximators},
  annotation = {9990 citations (Semantic Scholar/DOI) [2023-05-30]}
}

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {17522 citations (Crossref) [2023-07-11]}
}

@article{hurrooSignLanguageRecognition2020,
  title = {Sign {{Language Recognition System}} Using {{Convolutional Neural Network}} and {{Computer Vision}}},
  author = {Hurroo, Mehreen and Elham, M.},
  year = {2020},
  month = dec,
  journal = {International journal of engineering research and technology},
  url = {https://www.semanticscholar.org/paper/Sign-Language-Recognition-System-using-Neural-and-Hurroo-Elham/5ecc9d2073755a1364433d83df2a4007e42b16e1},
  urldate = {2023-07-11},
  abstract = {Conversing to a person with hearing disability is always a major challenge. Sign language has indelibly become the ultimate panacea and is a very powerful tool for individuals with hearing and speech disability to communicate their feelings and opinions to the world. It makes the integration process between them and others smooth and less complex. However, the invention of sign language alone, is not enough . There are many strings attached to this boon.The sign gestures often get mixed and confused for someone who has never learnt it or knows it in a different language. However, this communication gap which has existed for years can now be narrowed with the introduction of various techniques to automate the detection of sign gestures . In this paper, we introduce a Sign Language recognition using American Sign Language. In this study, the user must be able to capture images of the hand gesture using web camera and the system shall predict and display the name of the captured image. We use the HSV colour algorithm to detect the hand gesture and set the background to black. The images undergo a series of processing steps which include various Computer vision techniques such as the conversion to grayscale, dilation and mask operation. And the region of interest which, in our case is the hand gesture is segmented. The features extracted are the binary pixels of the images. We make use of Convolutional Neural Network(CNN) for training and to classify the images. We are able to recognise 10 American Sign gesture alphabets with high accuracy. Our model has achieved a remarkable accuracy of above 90\%.},
  keywords = {app:american-sl,app:sign-language,claims-to-be-first,classes:10,from:cite.bib,model:cnn},
  file = {/Users/brk/Zotero/storage/IQGA9ZVK/Hurroo and Elham - 2020 - Sign Language Recognition System using Convolution.pdf}
}

@article{hussainReviewCategorizationTechniques2020,
  title = {A Review and Categorization of Techniques on Device-Free Human Activity Recognition},
  author = {Hussain, Zawar and Sheng, Quan Z. and Zhang, Wei Emma},
  year = {2020},
  month = oct,
  journal = {Journal of Network and Computer Applications},
  volume = {167},
  pages = {102738},
  issn = {10848045},
  doi = {10.1016/j.jnca.2020.102738},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804520302125},
  urldate = {2023-07-12},
  abstract = {Human activity recognition has gained importance in recent years due to its applications in various fields such as health, security and surveillance, entertainment, and intelligent environments. A significant amount of work has been done on human activity recognition and researchers have leveraged different approaches, such as wearable, object-tagged, and devicefree, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the period 2010-2018 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything, instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the field of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interactionbased. We further divide these areas into ten different sub-topics and present the latest research work in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these sub-areas. Specifically, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to give the reader, a complete overview of the state-of-the-art techniques and trends in different sub-areas of human activity recognition. In the end, we discuss open research issues and provide future research directions in the field of human activity recognition.},
  langid = {english},
  keywords = {based-on-wifi,read-priority-1,survey-finsh:2018,survey-start:2010,tech:wifi,type:survey},
  annotation = {57 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/5M8W2RNX/Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf}
}

@article{hyeon-kyuleeHMMbasedThresholdModel1999,
  title = {An {{HMM-based}} Threshold Model Approach for Gesture Recognition},
  author = {{Hyeon-Kyu Lee} and Kim, J.H.},
  year = {Oct./1999},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {10},
  pages = {961--973},
  issn = {01628828},
  doi = {10.1109/34.799904},
  url = {http://ieeexplore.ieee.org/document/799904/},
  urldate = {2023-07-12},
  abstract = {A new method is developed using the hidden Markov model (HMM) based technique. To handle nongesture patterns, we introduce the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures in the sense that its likelihood is smaller than that of the dedicated gesture model for a given gesture. Consequently, the likelihood can be used as an adaptive threshold for selecting proper gesture model. It has, however, a large number of states and needs to be reduced because the threshold model is constructed by collecting the states of all gesture models in the system. To overcome this problem, the states with similar probability distributions are merged, utilizing the relative entropy measure. Experimental results show that the proposed method can successfully extract trained gestures from continuous hand motion with 93.14\% reliability.},
  keywords = {based-on-vision,from:cite.bib,model:hmm},
  annotation = {718 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@misc{immersioncorporationCyberGlove2001,
  title = {Cyber {{Glove}}},
  author = {{Immersion Corporation}},
  year = {2001},
  url = {https://web.archive.org/web/20011005155130/https://www.immersion.com/products/3d/interaction/cyberglove.shtml},
  abstract = {Immersion 3D's award-winning instrumented glove CyberGlove\textregistered{} The CyberGlove\textregistered{} is a fully instrumented glove that provides up to 22 high-accuracy joint-angle measurements. It uses proprietary resistive bend-sensing technology to accurately transform hand and finger motions into real-time digital joint-angle data. Our VirtualHand\textregistered{} Studio software converts the data into a graphical hand which mirrors the subtle movements of the physical hand. It is available in two models and for either hand.},
  keywords = {based-on-gloves,hardware:cyberglove,tech:flex,type:product}
}

@misc{immersionincImmersionIncCyberGlove2005,
  title = {Immersion {{Inc CyberGlove II}}},
  author = {{Immersion Inc}},
  year = {2005},
  url = {https://www.immersion.fr/en/cyberglove-ii/},
  keywords = {based-on-gloves,from:cite.bib,type:product}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  url = {https://www.semanticscholar.org/paper/Batch-Normalization%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3},
  urldate = {2023-07-11},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  keywords = {background,batch-normalization},
  file = {/Users/brk/Zotero/storage/ST7LJZZJ/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf}
}

@article{ismailDynamicHandGesture2022,
  title = {Dynamic Hand Gesture Recognition of {{Arabic}} Sign Language by Using Deep Convolutional Neural Networks},
  author = {Ismail, Mohammad H. and Dawwd, Shefa A. and Ali, Fakhradeen H.},
  year = {2022},
  month = feb,
  journal = {Indonesian Journal of Electrical Engineering and Computer Science},
  volume = {25},
  number = {2},
  pages = {952},
  issn = {2502-4760, 2502-4752},
  doi = {10.11591/ijeecs.v25.i2.pp952-962},
  url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/26823},
  urldate = {2023-07-02},
  abstract = {{$<$}p{$>$}In computer vision, one of the most difficult problems is human gestures in videos recognition Because of certain irrelevant environmental variables. This issue has been solved by using single deep networks to learn spatiotemporal characteristics from video data, and this approach is still insufficient to handle both problems at the same time. As a result, the researchers fused various models to allow for the effective collection of important shape information as well as precise spatiotemporal variation of gestures. In this study, we collected the dynamic dataset for twenty meaningful words of Arabic sign language (ArSL) using a Microsoft Kinect v2 camera. The recorded data included 7350 red, green, and blue (RGB) videos and 7350 depth videos. We proposed four deep neural networks models using 2D and 3D convolutional neural network (CNN) to cover all feature extraction methods and then passing these features to the recurrent neural network (RNN) for sequence classification. Long short-term memory (LSTM) and gated recurrent unit (GRU) are two types of using RNN. Also, the research included evaluation fusion techniques for several types of multiple models. The experiment results show the best multi-model for the dynamic dataset of the ArSL recognition achieved 100\% accuracy.{$<$}/p{$>$}},
  keywords = {app:arabic-sl,app:sign-language,model:cnn,model:gru,model:lstm,model:rnn},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UKX6R46H/Ismail et al. - 2022 - Dynamic hand gesture recognition of Arabic sign la.pdf}
}

@article{izotovRecognitionHandwrittenMNIST2021,
  title = {Recognition of Handwritten {{MNIST}} Digits on Low-Memory 2 {{Kb RAM Arduino}} Board Using {{LogNNet}} Reservoir Neural Network},
  author = {Izotov, Y A and Velichko, A A and Ivshin, A A and Novitskiy, R E},
  year = {2021},
  month = jun,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {1155},
  number = {1},
  pages = {012056},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/1155/1/012056},
  url = {https://iopscience.iop.org/article/10.1088/1757-899X/1155/1/012056},
  urldate = {2023-07-11},
  abstract = {Abstract             The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82\%. The algorithm was tested on a low-memory Arduino board with 2 Kb static RAM low-power microcontroller. The dependences of the accuracy and time of image recognition on the number of neurons in the reservoir have been investigated. The memory allocation demonstrates that the algorithm stores all the necessary information in RAM without using additional data storage, and operates with original images without preliminary processing. The simple structure of the algorithm, with appropriate training, can be adapted for wide practical application, for example, for creating mobile biosensors for early diagnosis of adverse events in medicine. The study results are important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing.},
  keywords = {app:resource-constrained,dataset:mnist,from:cite.bib,hardware:arduino,hardware:iot,model:nn},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/ZZ94MAW3/Izotov et al. - 2021 - Recognition of handwritten MNIST digits on low-mem.pdf}
}

@article{jacobsenDesignUtahDextrous1986,
  title = {Design of the {{Utah}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}},
  author = {Jacobsen, S. and Iversen, E. and Knutti, D. and Johnson, R. and Biggers, K.},
  year = {1986},
  journal = {Proceedings. 1986 IEEE International Conference on Robotics and Automation},
  volume = {3},
  pages = {1520--1532},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/ROBOT.1986.1087395},
  url = {http://ieeexplore.ieee.org/document/1087395/},
  urldate = {2023-07-13},
  abstract = {The Center for Engineering Design at the University of Utah, and the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology have developed a robotic end effector intended to function as a general purpose research tool for the study of machine dexterity. The high performance, multifingered hand will provide two important capabilities. First, it will permit the experimental investigation of basic concepts in manipulation theory, control system design and tactile sensing. Second, it will expand understanding required for the future design of physical machinery and will serve as a "test bed" for the development of tactile sensing systems. The paper includes: 1) a discussion of issues important to the development of manipulation machines; 2) general comments regarding design of the Utah/M.I.T. Dextrous Hand; and, 3) a detailed discussion of specific subsystems of the hand.},
  keywords = {based-on-gloves,tech:hall-effect},
  annotation = {557 citations (Semantic Scholar/DOI) [2023-07-13]},
  file = {/Users/brk/Zotero/storage/7U79YIGP/Jacobsen et al. - DESIGN OF THE UTAHh4.I.T. DEXTROUSHAND.pdf}
}

@article{jacobsenUTAHDextrousHand1984,
  title = {The {{UTAH}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}: {{Work}} in {{Progress}}},
  shorttitle = {The {{UTAH}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}},
  author = {Jacobsen, S.C. and Wood, J.E. and Knutti, D.F. and Biggers, K.B.},
  year = {1984},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {3},
  number = {4},
  pages = {21--50},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/027836498400300402},
  url = {http://journals.sagepub.com/doi/10.1177/027836498400300402},
  urldate = {2023-07-13},
  abstract = {The Center for Biomedical Design at the University of Utah and the Artificial Intelligence Laboratory at the Massachu setts Institute of Technology are developing a tendon-oper ated multiple-degree-of-freedom dextrous hand (DH) with multichannel touch-sensing capability. Our goal is the design and fabrication of a high-performance yet well-behaved sys tem that is fast and stable and that includes considerable operational flexibility as a research tool. The paper reviews progress to date on project subtasks and discusses design issues important to hardware and control systems develop ment in terms of (1) structures that contain tendons, actua tors, joints, and sensors; (2) both pneumatic and electric tendon actuation systems; (3) optically based sensors that detect touch; (4) subcontrol systems that provide internal management of the DH; and (5) preliminary higher control systems that supervise general operation of the hand during execution of tasks and that provide integration of vision and tactile information.},
  langid = {english},
  keywords = {based-on-gloves,tech:hall-effect},
  annotation = {630 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@article{jiangDevelopmentRealtimeHand2016,
  title = {Development of a Real-Time Hand Gesture Recognition Wristband Based on {{sEMG}} and {{IMU}} Sensing},
  author = {Jiang, Shuo and Lv, Bo and Sheng, Xinjun and Zhang, Chao and Wang, Haitao and Shull, Peter B.},
  year = {2016},
  month = dec,
  journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  pages = {1256--1261},
  publisher = {{IEEE}},
  address = {{Qingdao, China}},
  doi = {10.1109/ROBIO.2016.7866498},
  url = {http://ieeexplore.ieee.org/document/7866498/},
  urldate = {2023-03-07},
  abstract = {Human computer interaction is becoming more integrated in daily life with the proliferation of mobile devices and virtual reality technology. Hand gesture recognition is a potentially promising mechanism to facilitate human computer interaction, however wrist-mounted surface electromyography (sEMG) hand gesture classification is particularly challenging given the relatively small sEMG signals as compared to traditional forearm-based sEMG sensing. This paper introduces the development of a wristband for detecting eight air gestures and four surface gestures at two different force levels through sEMG and inertial measurement unit (IMU) sensor fusion. To validate the wrist-worn device, ten healthy subjects performed hand gesture recognition experiments resulting in a total average recognition rate of 92.6\% for air gestures and 88.8\% for surface gestures. This paper demonstrates the potential of wrist-worn devices for accurate hand gesture recognition applications.},
  isbn = {9781509043644},
  keywords = {classes:10-29,tech:emg},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{jiehuangSignLanguageRecognition2015,
  title = {Sign {{Language Recognition}} Using {{3D}} Convolutional Neural Networks},
  author = {{Jie Huang} and {Wengang Zhou} and {Houqiang Li} and {Weiping Li}},
  year = {2015},
  month = jun,
  journal = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Turin, Italy}},
  doi = {10.1109/ICME.2015.7177428},
  url = {https://ieeexplore.ieee.org/document/7177428},
  urldate = {2023-07-11},
  abstract = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
  isbn = {9781479970827},
  keywords = {app:sign-language,claims-to-be-first,hardware:kinect,model:cnn,tech:rgbd},
  annotation = {199 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{jong-sungkimDynamicGestureRecognition1996,
  title = {A Dynamic Gesture Recognition System for the {{Korean}} Sign Language ({{KSL}})},
  author = {{Jong-Sung Kim} and {Won Jang} and {Zeungnam Bien}},
  year = {1996},
  month = apr,
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
  volume = {26},
  number = {2},
  pages = {354--359},
  issn = {10834419},
  doi = {10.1109/3477.485888},
  url = {http://ieeexplore.ieee.org/document/485888/},
  urldate = {2023-07-02},
  abstract = {The sign language is a method of communication for the deaf-mute. Articulated gestures and postures of hands and fingers are commonly used for the sign language. This paper presents a system which recognizes the Korean sign language (KSL) and translates into a normal Korean text. A pair of data-gloves are used as the sensing device for detecting motions of hands and fingers. For efficient recognition of gestures and postures, a technique of efficient classification of motions is proposed and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
  keywords = {app:korean-sl,app:sign-language,based-on-gloves,model:ffnn,model:hmm,movement:dynamic},
  annotation = {192 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{kadirMinimalTrainingLarge2004,
  title = {Minimal {{Training}}, {{Large Lexicon}}, {{Unconstrained Sign Language Recognition}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2004},
  author = {Kadir, T. and Bowden, R. and Ong, E. J. and Zisserman, A.},
  year = {2004},
  pages = {96.1-96.10},
  publisher = {{British Machine Vision Association}},
  address = {{Kingston}},
  doi = {10.5244/C.18.96},
  url = {http://www.bmva.org/bmvc/2004/papers/paper_265.html},
  urldate = {2023-07-11},
  abstract = {This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92\% for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required.},
  isbn = {978-1-901725-25-4},
  langid = {english},
  keywords = {app:sign-language,based-on-vision,classes:164,tech:rgb},
  annotation = {105 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/MYCSBHHF/Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf}
}

@article{kadousGRASPRecognitionAustralian1995,
  title = {{{GRASP}}: {{Recognition}} of {{Australian Sign Language Using Instrumented Gloves}}},
  shorttitle = {{{GRASP}}},
  author = {Kadous, Mohammed},
  year = {1995},
  month = dec,
  abstract = {Instrumented gloves -- gloves equipped with sensors for detecting finger bend, hand position and orientation -- were conceived to allow a more natural interface to computers. However, the extension of their use for recognising sign language, and in this case Auslan (Australian Sign Language), is possible. Several researchers have already explored these possibilities and have successfully achieved finger-spelling recognition with high levels of accuracy, but progress in the recognition of sign language as a whole has been limited.},
  keywords = {app:australian-sl,app:sign-language,based-on-gloves,classes:95,from:cite.bib,model:hmm,model:instance-based-learning,model:nn,participants:5},
  file = {/Users/brk/Zotero/storage/ZVYPR4WQ/Kadous - 1995 - GRASP Recognition of Australian Sign Language Usi.pdf}
}

@inproceedings{kadousMachineRecognitionAuslan1996,
  title = {Machine {{Recognition}} of {{Auslan Signs Using PowerGloves}}: {{Towards Large-Lexicon Recognition}} of {{Sign Lan}}},
  shorttitle = {Machine {{Recognition}} of {{Auslan Signs Using PowerGloves}}},
  author = {Kadous, M. W.},
  year = {1996},
  url = {https://www.semanticscholar.org/paper/Machine-Recognition-of-Auslan-Signs-Using-Towards-Kadous/1ed3e20bbcf12c60e766189987074ac2935d7140},
  urldate = {2023-03-07},
  abstract = {Instrumented gloves use a variety of sensors to provide information about the user's hand. They can be used for recognition of gestures; especially well-deened gesture sets such as sign languages. However, recognising gestures is a diicult task, due to intrapersonal and inter-personal variations in performing them. One approach to solving this problem is to use machine learning. In this case, samples of 95 discrete Australian Sign Language (Auslan) signs were collected using a Power-Glove. Two machine learning techniques were applied \{ instance-based learning (IBL) and decision-tree learning \{ to the data after some simple features were extracted. Accuracy of approximately 80 per cent was achieved using IBL, despite the severe limitations of the glove.\vphantom{\}\}}},
  keywords = {app:australian-sl,app:sign-language,based-on-gloves,classes:50-100,model:decision-tree,model:instance-based-learning},
  annotation = {115 Citations},
  file = {/Users/brk/Zotero/storage/3FZ4HCVJ/Kadous - 1996 - Machine Recognition of Auslan Signs Using PowerGlo.pdf}
}

@article{karantonisImplementationRealTimeHuman2006,
  title = {Implementation of a {{Real-Time Human Movement Classifier Using}} a {{Triaxial Accelerometer}} for {{Ambulatory Monitoring}}},
  author = {Karantonis, D.M. and Narayanan, M.R. and Mathie, M. and Lovell, N.H. and Celler, B.G.},
  year = {2006},
  month = jan,
  journal = {IEEE Transactions on Information Technology in Biomedicine},
  volume = {10},
  number = {1},
  pages = {156--167},
  issn = {1089-7771},
  doi = {10.1109/TITB.2005.856864},
  url = {http://ieeexplore.ieee.org/document/1573717/},
  urldate = {2023-07-11},
  abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence},
  langid = {english},
  keywords = {based-on-gloves,classes:12,contains-more-references,model:decision-tree,participants:6,tech:accelerometer,tech:imu},
  annotation = {1273 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/4X8QFWXU/Karantonis et al. - 2006 - Implementation of a Real-Time Human Movement Class.pdf}
}

@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2019},
  journal = {Andrej Karpathy Blog},
  publisher = {{https://karpathy.github.io}},
  url = {https://karpathy.github.io/2019/04/25/recipe/},
  keywords = {from:cite.bib,model:nn,type:blog}
}

@inproceedings{kaufmanToolsInteractionThree1989,
  title = {Tools for Interaction in Three Dimensions},
  author = {Kaufman, A. and Yagel, R.},
  year = {1989},
  month = sep,
  url = {https://www.semanticscholar.org/paper/Tools-for-interaction-in-three-dimensions-Kaufman-Yagel/4bdf137539c2b6955a9cd7251b0b5839c96f6ecf},
  urldate = {2023-07-13},
  abstract = {Bodies, or frames, of use in the formation of modular interconnect terminals, or terminal modules are shown. The bodies are formed, characteristically, from single pieces of material to include pockets or receptacles into which electrical contact elements may be fitted. Each body, or frame, includes regions about which it may be folded to provide a four-sided box-like structure. Before being folded the body is equipped with electrical contact elements which it supports in such a way that first terminals of the contact elements are accessible from outside the folded structure. A flexible printed circuit board having conductors arranged in patterns on at least one face is folded within the box-like structure and its conductors are connected to second terminals of the contact elements to provide selected interconnections between the electrical contact elements.},
  keywords = {based-on-gloves,hardware:dataglove}
}

@article{kelaAccelerometerbasedGestureControl2006,
  title = {Accelerometer-Based Gesture Control for a Design Environment},
  author = {Kela, Juha and Korpip{\"a}{\"a}, Panu and M{\"a}ntyj{\"a}rvi, Jani and Kallio, Sanna and Savino, Giuseppe and Jozzo, Luca and Marca, Sergio Di},
  year = {2006},
  month = aug,
  journal = {Personal and Ubiquitous Computing},
  volume = {10},
  number = {5},
  pages = {285--299},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-005-0033-8},
  url = {http://link.springer.com/10.1007/s00779-005-0033-8},
  urldate = {2023-06-23},
  abstract = {Accelerometer-based gesture control is studied as a supplementary or an alternative interaction modality. Gesture commands freely trainable by the user can be used for controlling external devices with handheld wireless sensor unit. Two user studies are presented. The first study concerns finding gestures for controlling a design environment (Smart Design Studio), TV, VCR, and lighting. The results indicate that different people usually prefer different gestures for the same task, and hence it should be possible to personalise them. The second user study concerns evaluating the usefulness of the gesture modality compared to other interaction modalities for controlling a design environment. The other modalities were speech, RFID-based physical tangible objects, laser-tracked pen, and PDA stylus. The results suggest that gestures are a natural modality for certain tasks, and can augment other modalities. Gesture commands were found to be natural, especially for commands with spatial association in design environment control.},
  langid = {english},
  keywords = {based-on-gloves,from:cite.bib,read-priority-7,tech:accelerometer},
  annotation = {201 citations (Crossref) [2023-07-11] 333 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/L5WTBMWR/Kela et al. - 2006 - Accelerometer-based gesture control for a design e.pdf}
}

@inproceedings{kelloggBringingGestureRecognition2014,
  title = {Bringing {{Gesture Recognition}} to {{All Devices}}},
  booktitle = {Symposium on {{Networked Systems Design}} and {{Implementation}}},
  author = {Kellogg, Bryce and Talla, V. and Gollakota, Shyamnath},
  year = {2014},
  month = apr,
  url = {https://www.semanticscholar.org/paper/Bringing-Gesture-Recognition-to-All-Devices-Kellogg-Talla/c17ac67f8c74c767073e5ef09516a70ede65adcf},
  urldate = {2023-07-12},
  abstract = {Existing gesture-recognition systems consume significant power and computational resources that limit how they may be used in low-end devices. We introduce AllSee, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries. AllSee consumes three to four orders of magnitude lower power than state-of-the-art systems and can enable always-on gesture recognition for smartphones and tablets. It extracts gesture information from existing wireless signals (e.g., TV transmissions), but does not incur the power and computational overheads of prior wireless approaches. We build AllSee prototypes that can recognize gestures on RFID tags and power-harvesting sensors. We also integrate our hardware with an off-the-shelf Nexus S phone and demonstrate gesture recognition in through-the-pocket scenarios. Our results show that AllSee achieves classification accuracies as high as 97\% over a set of eight gestures.},
  keywords = {app:activity-inference,based-on-wifi,classes:8,tech:wifi}
}

@article{keskinRealTimeHand2003,
  title = {Real Time Hand Tracking and {{3D}} Gesture Recognition for Interactive Interfaces Using {{HMM}}},
  author = {Keskin, Cem and Erkan, A.N. and Akarun, L},
  year = {2003},
  month = jan,
  journal = {Proceedings of the joint international conference ICANN/ICONIP},
  keywords = {based-on-vision,classes:8,from:cite.bib,model:hmm,movement:dynamic,tech:rgb}
}

@article{kesslerEvaluationCyberGloveWholehand1995,
  title = {Evaluation of the {{CyberGlove}} as a Whole-Hand Input Device},
  author = {Kessler, G. Drew and Hodges, Larry F. and Walker, Neff},
  year = {1995},
  month = dec,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {2},
  number = {4},
  pages = {263--283},
  issn = {1073-0516, 1557-7325},
  doi = {10.1145/212430.212431},
  url = {https://dl.acm.org/doi/10.1145/212430.212431},
  urldate = {2023-06-23},
  abstract = {We present a careful evaluation of the sensory characteristics of the CyberGlove model CG1801 whole-hand input device. In particular, we conducted an experimental study that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements. Among our results, we show that hand size differences among the subjects of the study did not have a statistical effect on the accuracy of the device. We also analyzed the effect of different software calibration approaches on accuracy of the sensors.},
  langid = {english},
  keywords = {based-on-gloves,hardware:cyberglove},
  annotation = {218 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/SDEHCHUH/Kessler et al. - 1995 - Evaluation of the CyberGlove as a whole-hand input.pdf}
}

@article{khanSurveyGestureRecognition2012,
  title = {Survey on {{Gesture Recognition}} for {{Hand Image Postures}}},
  author = {Khan, Rafiqul Zaman and Ibraheem, Noor Adnan},
  year = {2012},
  month = apr,
  journal = {Computer and Information Science},
  volume = {5},
  number = {3},
  pages = {p110},
  issn = {1913-8997, 1913-8989},
  doi = {10.5539/cis.v5n3p110},
  url = {http://www.ccsenet.org/journal/index.php/cis/article/view/16598},
  urldate = {2023-03-07},
  abstract = {One of the attractive methods for providing natural human-computer interaction is the use of the hand as an input device rather than the cumbersome devices such as keyboards and mice, which need the user to be located in a specific location to use these devices. Since human hand is an articulated object, it is an open issue to discuss. The most important thing in hand gesture recognition system is the input features, and the selection of good features representation. This paper presents a review study on the hand postures and gesture recognition methods, which is considered to be a challenging problem in the human-computer interaction context and promising as well. Many applications and techniques were discussed here with the explanation of system recognition framework and its main phases.},
  keywords = {type:survey},
  annotation = {94 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/J2T7FEZP/Khan and Ibraheem - 2012 - Survey on Gesture Recognition for Hand Image Postu.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8},
  urldate = {2023-06-07},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {background},
  file = {/Users/brk/Zotero/storage/SB7A4J49/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf}
}

@inproceedings{klingmannAccelerometerBasedGestureRecognition2009,
  title = {Accelerometer-{{Based Gesture Recognition}} with the {{iPhone}}},
  author = {Klingmann, Marco},
  year = {2009},
  url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Gesture-Recognition-with-the-Klingmann/cbe0d68e8e9ec8d6b3d4129366214e98437c896b},
  urldate = {2023-03-07},
  abstract = {The growing number of small sensors built into consumer electronic devices, such as mobile phones, allow experiments with alternative interaction methods in favour of more physical, intuitive and pervasive human computer interaction. This paper examines hand gestures as an alternative or supplementary input modality for mobile devices. The iPhone is chosen as sensing and processing device. Based on its built-in accelerometer, hand movements are detected and classified into previously trained gestures. A software library for accelerometer-based gesture recognition and a demonstration iPhone application have been developed. The system allows the training and recognition of free-from hand gestures. Discrete hidden Markov models form the core part of the gesture recognition apparatus. Five test gestures have been defined and used to evaluate the performance of the application. The evaluation shows that with 10 training repetitions, an average recognition rate of over 90 percent can be achieved.},
  keywords = {based-on-gloves,classes:{$<$}10,model:hmm,tech:accelerometer},
  annotation = {22 Citations}
}

@article{knuthTwoNotesNotation1992,
  title = {Two {{Notes}} on {{Notation}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = may,
  journal = {The American Mathematical Monthly},
  volume = {99},
  number = {5},
  eprint = {2325085},
  eprinttype = {jstor},
  pages = {403},
  issn = {00029890},
  doi = {10.2307/2325085},
  url = {https://www.jstor.org/stable/2325085?origin=crossref},
  urldate = {2023-07-11},
  abstract = {Mathematical notation evolves like all languages do. As new experiments are made, we sometimes witness the survival of the fittest, sometimes the survival of the most familiar. A healthy conservatism keeps things from changing too rapidly; a healthy radicalism keeps things in tune with new theoretical emphases. Our mathematical language continues to improve, just as "the d-ism of Leibniz overtook the dotage of Newton" in past centuries [4, Chapter 4]. In 1970 I began teaching a class at Stanford University entitled Concrete Mathematics. The students and I studied how to manipulate formulas in continuous and discrete mathematics, and the problems we investigated were often inspired by new developments in computer science. As the years went by we began to see that a few changes in notational traditions would greatly facilitate our work. The notes from that class have recently been published in a book [15], and as I wrote the final drafts of that book I learned to my surprise that two of the notations we had been using were considerably more useful than I had previously realized. The ideas "clicked" so well, in fact, that I've decided to write this article, blatantly attempting to promote these notations among the mathematicians who have no use for [15]. I hope that within five years everybody will be able to use these notations in published papers without needing to explain what they mean. The notations I'm talking about are (1) Iverson's convention for characteristic functions; and (2) the "right" notation for Stirling numbers, at last.},
  keywords = {background},
  annotation = {458 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/QUCL5Y47/Knuth - 1992 - Two Notes on Notation.pdf}
}

@article{kochRecurrentNeuralNetwork2019,
  title = {A {{Recurrent Neural Network}} for {{Hand Gesture Recognition}} Based on {{Accelerometer Data}}},
  author = {Koch, Philipp and Dreier, Mark and Maass, Marco and Bohme, Martina and Phan, Huy and Mertins, Alfred},
  year = {2019},
  month = jul,
  journal = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages = {5088--5091},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856844},
  url = {https://ieeexplore.ieee.org/document/8856844/},
  urldate = {2023-06-22},
  abstract = {For many applications, hand gesture recognition systems that rely on biosignal data exclusively are mandatory. Usually, theses systems have to be affordable, reliable as well as mobile. The hand is moved due to muscle contractions that cause motions of the forearm skin. Theses motions can be captured with cheap and reliable accelerometers placed around the forearm. Since accelerometers can also be integrated into mobile systems easily, the possibility of a robust hand gesture recognition based on accelerometer signals is evaluated in this work. For this, a neural network architecture consisting of two different kinds of recurrent neural network (RNN) cells is proposed. Experiments on three databases reveal that this relatively small network outperforms by far state-of-the-art hand gesture recognition approaches that rely on multi-modal data. The combination of accelerometer data and an RNN forms a robust hand gesture classification system, i.e., the performance of the network does not vary a lot between subjects and it is outstanding for amputees. Furthermore, the proposed network uses only 5 ms short windows to classify the hand gestures. Consequently, this approach allows for a quick, and potentially delay-free hand gesture detection.},
  isbn = {9781538613115},
  keywords = {model:rnn,tech:accelerometer,tech:emg},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ZF224WN7/Koch et al. - 2019 - A Recurrent Neural Network for Hand Gesture Recogn.pdf}
}

@article{kohonenSelforganizedFormationTopologically1982,
  title = {Self-Organized Formation of Topologically Correct Feature Maps},
  author = {Kohonen, Teuvo},
  year = {1982},
  journal = {Biological Cybernetics},
  volume = {43},
  number = {1},
  pages = {59--69},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337288},
  url = {http://link.springer.com/10.1007/BF00337288},
  urldate = {2023-07-13},
  langid = {english},
  keywords = {background},
  annotation = {7573 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@inproceedings{kolschKeyboardsKeyboardsSurvey2002,
  title = {Keyboards without {{Keyboards}}: {{A Survey}} of {{Virtual Keyboards}}},
  shorttitle = {Keyboards without {{Keyboards}}},
  author = {K{\"o}lsch, M. and Turk, M.},
  year = {2002},
  url = {https://www.semanticscholar.org/paper/Keyboards-without-Keyboards%3A-A-Survey-of-Virtual-K%C3%B6lsch-Turk/44355bea0b025ba8719a946ba6c009e0eba133f3},
  urldate = {2023-06-29},
  abstract = {Input to small devices is becoming an increasingly crucial factor in development for the ever-more powerful embedded market. Speech input promises to become a feasible alternative to tiny keypads, yet its limited reliability, robustness, and flexibility render it unsuitable for certain tasks and/or environments. Various attempts have been made to provide the common keyboard metaphor without the physical keyboard, to build ``virtual keyboards''. This promises to leverage our familiarity with the device without incurring the constraints of the bulky physics. This paper surveys technologies for alphanumeric input devices and methods with a strong focus on touch-typing. We analyze the characteristics of the keyboard modality and show how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
  keywords = {based-on-gloves,based-on-vision,contains-more-references,have-read,read-priority-1,type:survey},
  file = {/Users/brk/Zotero/storage/MB8CKX7D/Klsch and Turk - 2002 - Keyboards without Keyboards A Survey of Virtual K.pdf}
}

@article{kongGestureRecognitionModel2009,
  title = {Gesture Recognition Model Based on {{3D}} Accelerations},
  author = {Kong, Jun-qi and Wang, Hui and Zhang, Guang-quan},
  year = {2009},
  month = jul,
  journal = {2009 4th International Conference on Computer Science \& Education},
  pages = {66--70},
  publisher = {{IEEE}},
  address = {{Nanning}},
  doi = {10.1109/ICCSE.2009.5228524},
  url = {https://ieeexplore.ieee.org/document/5228524/},
  urldate = {2023-03-07},
  abstract = {This paper presents a recognition model on gesture interaction, gesture recognition based on accererations, which are collected by 3D accelerometer. Unlike most other vision-based gesture recognition, our method resorting to accelerations of the gesture in three directions. Gesture is divided into small units, and each unit is modeled by an HMM, the HMM classifies the gesture unit after being well trained, the gesture identified when all of the corresponding gesture units recognized successfully, then the instruction will be triggered, and the Human-Computer Interaction achieved. Through experiments, the method proved to be effective. This recognition model can be wildly used in the field of mobile computing and remote control, and people could use computer more friendly and naturally.},
  isbn = {9781424435203},
  keywords = {based-on-gloves,model:hmm,tech:accelerometer},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{kopukluRealtimeHandGesture2019,
  title = {Real-Time {{Hand Gesture Detection}} and {{Classification Using Convolutional Neural Networks}}},
  author = {Kopuklu, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
  year = {2019},
  month = may,
  journal = {2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Lille, France}},
  doi = {10.1109/FG.2019.8756576},
  url = {https://ieeexplore.ieee.org/document/8756576/},
  urldate = {2023-07-11},
  abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04\% and 83.82\% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available1.},
  isbn = {9781728100890},
  keywords = {dataset:egogesture,dataset:nvidia-dynamic-hand-gesture,model:cnn,tech:rgb},
  annotation = {112 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/GE2CVBAG/Kopuklu et al. - 2019 - Real-time Hand Gesture Detection and Classificatio.pdf}
}

@article{kotaruSpotFiDecimeterLevel2015,
  title = {{{SpotFi}}: {{Decimeter Level Localization Using WiFi}}},
  shorttitle = {{{SpotFi}}},
  author = {Kotaru, Manikanta and Joshi, Kiran and Bharadia, Dinesh and Katti, Sachin},
  year = {2015},
  month = aug,
  journal = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
  pages = {269--282},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/2785956.2787487},
  url = {https://dl.acm.org/doi/10.1145/2785956.2787487},
  urldate = {2023-07-12},
  abstract = {This paper presents the design and implementation of SpotFi, an accurate indoor localization system that can be deployed on commodity WiFi infrastructure. SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems. SpotFi makes two key technical contributions. First, SpotFi incorporates super-resolution algorithms that can accurately compute the angle of arrival (AoA) of multipath components even when the access point (AP) has only three antennas. Second, it incorporates novel filtering and estimation techniques to identify AoA of direct path between the localization target and AP by assigning values for each path depending on how likely the particular path is the direct path. Our experiments in a multipath rich indoor environment show that SpotFi achieves a median accuracy of 40 cm and is robust to indoor hindrances such as obstacles and multipath.},
  isbn = {9781450335423},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {1082 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{kramerTalkingGlove1988,
  title = {The Talking Glove},
  author = {Kramer, James and Leifer, Larry},
  year = {1988},
  month = apr,
  journal = {ACM SIGCAPH Computers and the Physically Handicapped},
  number = {39},
  pages = {12--16},
  issn = {0163-5727},
  doi = {10.1145/47937.47938},
  url = {https://dl.acm.org/doi/10.1145/47937.47938},
  urldate = {2023-07-13},
  abstract = {A new bi-directional communication aid is being developed which allows deaf, deaf-blind, or nonvocal individuals to interact verbally with others. The device analyzes a nonvocal person's fingerspelling hand formations and outputs the spelled words as synthesized speech. In effect, this component of the communication aid is a "talking glove". In addition, by using state-of-the-art voice recognition equipment, a deaf user is able to read incoming speech on the miniature LCD screen of a modified digital wristwatch. Similarly, a deaf-blind individual may read incoming speech on a portable braille display module.},
  langid = {english},
  keywords = {based-on-gloves,hardware:cyberglove},
  annotation = {22 citations (Semantic Scholar/DOI) [2023-07-13]},
  file = {/Users/brk/Zotero/storage/E358BBT2/Kramer and Leifer - 1988 - The talking glove.pdf}
}

@inproceedings{kratzWiizards3DGesture2007,
  title = {Wiizards: {{3D}} Gesture Recognition for Game Play Input},
  shorttitle = {Wiizards},
  booktitle = {Proceedings of the 2007 Conference on {{Future Play}}  - {{Future Play}} '07},
  author = {Kratz, Louis and Smith, Matthew and Lee, Frank J.},
  year = {2007},
  pages = {209},
  publisher = {{ACM Press}},
  address = {{Toronto, Canada}},
  doi = {10.1145/1328202.1328241},
  url = {http://portal.acm.org/citation.cfm?doid=1328202.1328241},
  urldate = {2023-07-11},
  abstract = {Gesture based input is an emerging technology gaining widespread popularity in interactive entertainment. The use of gestures provides intuitive and natural input mechanics for games, presenting an easy to learn yet richly immersive experience. In Wiizards, we explore the use of 3D accelerometer gestures in a multiplayer, zero sum game. Hidden Markov models are constructed for gesture recognition, providing increased flexibility and fluid tolerance. Users can strategically effect the outcome via combinations of gestures with limitless scalability.},
  isbn = {978-1-59593-943-2},
  langid = {english},
  keywords = {app:gaming,based-on-gloves,contains-more-references,model:hmm,participants:7,tech:accelerometer},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/RT6LR5A7/Kratz et al. - 2007 - Wiizards 3D gesture recognition for game play inp.pdf}
}

@article{kudrinkoWearableSensorBasedSign2021,
  title = {Wearable {{Sensor-Based Sign Language Recognition}}: {{A Comprehensive Review}}},
  shorttitle = {Wearable {{Sensor-Based Sign Language Recognition}}},
  author = {Kudrinko, Karly and Flavin, Emile and Zhu, Xiaodan and Li, Qingguo},
  year = {2021},
  journal = {IEEE Reviews in Biomedical Engineering},
  volume = {14},
  pages = {82--97},
  issn = {1937-3333, 1941-1189},
  doi = {10.1109/RBME.2020.3019769},
  url = {https://ieeexplore.ieee.org/document/9178440/},
  urldate = {2023-03-07},
  abstract = {Sign language is used as a primary form of communication by many people who are Deaf, deafened, hard of hearing, and non-verbal. Communication barriers exist for members of these populations during daily interactions with those who are unable to understand or use sign language. Advancements in technology and machine learning techniques have led to the development of innovative approaches for gesture recognition. This literature review focuses on analyzing studies that use wearable sensor-based systems to classify sign language gestures. A review of 72 studies from 1991 to 2019 was performed to identify trends, best practices, and common challenges. Attributes including sign language variation, sensor configuration, classification method, study design, and performance metrics were analyzed and compared. Results from this literature review could aid in the development of user-centred and robust wearable sensor-based systems for sign language recognition.},
  keywords = {app:sign-language,type:survey},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{kunduHandGestureRecognition2018,
  title = {Hand {{Gesture Recognition Based Omnidirectional Wheelchair Control Using IMU}} and {{EMG Sensors}}},
  author = {Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis},
  year = {2018},
  month = sep,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {91},
  number = {3-4},
  pages = {529--541},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-017-0725-0},
  url = {http://link.springer.com/10.1007/s10846-017-0725-0},
  urldate = {2023-06-22},
  abstract = {This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors. Seven common gestures are recognized and classified using shape based feature extraction and Dendogram Support Vector Machine (DSVM) classifier. The dynamic gestures are mapped to the omnidirectional motion commands to navigate the wheelchair. A single IMU is used to measure the wrist tilt angle and acceleration in three axis. EMG signals are extracted from two forearm muscles namely Extensor Carpi Radialis and Flexor Carpi Radialis and processed to provide Root Mean Square (RMS) signal. Initiation and termination of dynamic activities are based on autonomous identification of static to dynamic or dynamic to static transition by setting static thresholds on processed IMU and myoelectric sensor data. Classification involves recognizing the activity pattern based on periodic shape of trajectories of the triaxial wrist tilt angle and EMG-RMS from the two selected muscles. Second order Polynomial coefficients extracted from the sensor trajectory templates during specific dynamic activity cycles are used as features to classify dynamic activities. Classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested by five healthy subjects. Classification accuracy of 94\% was achieved by DSVM classifier on `k' fold cross validation data of 5 users. Classification accuracy while operating the wheelchair was 90.5\%.},
  langid = {english},
  keywords = {model:svm,tech:accelerometer,tech:emg},
  annotation = {70 Citations}
}

@inproceedings{laviolaSurveyHandPosture1999,
  title = {A {{Survey}} of {{Hand Posture}} and {{Gesture Recognition Techniques}} and {{Technology}}},
  author = {LaViola, Jr Joseph J.},
  year = {1999},
  month = jun,
  url = {https://www.semanticscholar.org/paper/A-Survey-of-Hand-Posture-and-Gesture-Recognition-LaViola/856d4bf0f1f5d4480ce3115d828f34d4b2782e1c},
  urldate = {2023-06-23},
  abstract = {This paper surveys the use of hand postures and gestures as a mechanism for interaction with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based interaction. First, the technological requirements and limitations for using hand postures and gestures are described by discussing both glove-based and vision-based recognition systems along with advantages and disadvantages of each. Second, the various types of techniques used in recognizing hand postures and gestures are compared and contrasted. Third, the applications that have used hand posture and gesture interfaces are examined. The survey concludes with a summary and a discussion of future research directions.},
  keywords = {based-on-gloves,based-on-vision,hardware:cyberglove,hardware:dataglove,hardware:powerglove,read-priority-3,tech:flex,tech:rgb,type:survey},
  annotation = {253 Citations},
  file = {/Users/brk/Zotero/storage/M8XQ3QQL/LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf}
}

@article{leeDeepLearningBased2020,
  title = {Deep {{Learning Based Real-Time Recognition}} of {{Dynamic Finger Gestures Using}} a {{Data Glove}}},
  author = {Lee, Minhyuk and Bae, Joonbum},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {219923--219933},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3039401},
  url = {https://ieeexplore.ieee.org/document/9264164/},
  urldate = {2023-03-07},
  abstract = {In this article, a real-time dynamic finger gesture recognition using a soft sensor embedded data glove is presented, which measures the metacarpophalangeal (MCP) and proximal interphalangeal (PIP) joint angles of five fingers. In the gesture recognition field, a challenging problem is that of separating meaningful dynamic gestures from a continuous data stream. Unconscious hand motions or sudden tremors, which can easily lead to segmentation ambiguity, makes this problem difficult. Furthermore, the hand shapes and speeds of users differ when performing the same dynamic gesture, and even those made by one user often vary. To solve the problem of separating meaningful dynamic gestures, we propose a deep learning-based gesture spotting algorithm that detects the start/end of a gesture sequence in a continuous data stream. The gesture spotting algorithm takes window data and estimates a scalar value named gesture progress sequence (GPS). GPS is a quantity that represents gesture progress. Moreover, to solve the gesture variation problem, we propose a sequence simplification algorithm and a deep learning-based gesture recognition algorithm. The proposed three algorithms (gesture spotting algorithm, sequence simplification algorithm, and gesture recognition algorithm) are unified into the real-time gesture recognition system and the system was tested with 11 dynamic finger gestures in real-time. The proposed system took only 6 ms to estimate a GPS and no more than 12 ms to recognize the completed gesture in real-time.},
  keywords = {classes:10-29,movement:dynamic,tech:flex},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/NRM9LBZL/Lee and Bae - 2020 - Deep Learning Based Real-Time Recognition of Dynam.pdf}
}

@article{leeSmartWearableHand2018,
  title = {Smart {{Wearable Hand Device}} for {{Sign Language Interpretation System With Sensors Fusion}}},
  author = {Lee, Boon Giin and Lee, Su Min},
  year = {2018},
  month = feb,
  journal = {IEEE Sensors Journal},
  volume = {18},
  number = {3},
  pages = {1224--1232},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2017.2779466},
  url = {http://ieeexplore.ieee.org/document/8126796/},
  urldate = {2023-03-07},
  abstract = {Gesturing is an instinctive way of communicating to present a specific meaning or intent. Therefore, research into sign language interpretation using gestures has been explored progressively during recent decades to serve as an auxiliary tool for deaf and mute people to blend into society without barriers. In this paper, a smart sign language interpretation system using a wearable hand device is proposed to meet this purpose. This wearable system utilizes five flex-sensors, two pressure sensors, and a three-axis inertial motion sensor to distinguish the characters in the American sign language alphabet. The entire system mainly consists of three modules: 1) a wearable device with a sensor module; 2) a processing module; and 3) a display unit mobile application module. Sensor data are collected and analyzed using a built-in embedded support vector machine classifier. Subsequently, the recognized alphabet is further transmitted to a mobile device through Bluetooth low energy wireless communication. An Android-based mobile application was developed with a text-to-speech function that converts the received textinto audible voice output. Experiment results indicate that a true sign language recognition accuracy rate of 65.7\% can be achieved on average in the first version without pressure sensors. A second version of the proposed wearable system with the fusion of pressure sensors on the middle finger increased the recognition accuracy rate dramatically to 98.2\%. The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
  keywords = {app:sign-language},
  annotation = {102 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/CQT634W2/Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf}
}

@article{lepetitMonocularModelBased3D2005,
  title = {Monocular {{Model-Based 3D Tracking}} of {{Rigid Objects}}: {{A Survey}}},
  shorttitle = {Monocular {{Model-Based 3D Tracking}} of {{Rigid Objects}}},
  author = {Lepetit, Vincent and Fua, Pascal},
  year = {2005},
  journal = {Foundations and Trends\textregistered{} in Computer Graphics and Vision},
  volume = {1},
  number = {1},
  pages = {1--89},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000001},
  url = {http://www.nowpublishers.com/article/Details/CGV-001},
  urldate = {2023-07-11},
  abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Augmented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated interfaces can use. Computer Vision offers solutions that are cheap, practical and non-invasive.This survey reviews the different techniques and approaches that have been developed by industry and research. First, important mathematical tools are introduced: Camera representation, robust estimation and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery concludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking.Because it encompasses many computer vision techniques from low-level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.},
  langid = {english},
  keywords = {app:robotics,based-on-vision,tech:rgb,type:survey},
  annotation = {747 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/8KHLPAGM/Lepetit and Fua - 2005 - Monocular Model-Based 3D Tracking of Rigid Objects.pdf}
}

@article{liang3DConvolutionalNeural2018,
  title = {{{3D Convolutional Neural Networks}} for {{Dynamic Sign Language Recognition}}},
  author = {Liang, Zhi-jie and Liao, Sheng-bin and Hu, Bing-zhang},
  editor = {Manolopoulos, Yannis},
  year = {2018},
  month = nov,
  journal = {The Computer Journal},
  volume = {61},
  number = {11},
  pages = {1724--1736},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/bxy049},
  url = {https://academic.oup.com/comjnl/article/61/11/1724/4995616},
  urldate = {2023-06-22},
  abstract = {Automatic dynamic sign language recognition is even more challenging than gesture recognition due to the fact that the vocabularies are large and signs are context dependent. Previous works in this direction tend to build classifiers based on complex hand-crafted features computed from the raw inputs. As a type of deep learning model, convolutional neural networks (CNNs) have significantly advanced the accuracy of human gesture classification. However, such methods are currently used to treat video frames as 2D images and recognize gestures at the individual frame level. In this paper, we present a data driven system in which 3D-CNNs are applied to extract spatial and temporal features from video streams, and the motion information is captured by noting the variation in depth between each pair of consecutive frames. To further boost the performance, multi-modal of video streams, including infrared, contour and skeleton are used as input for the architecture and the prediction results estimated from the different sub-networks were fused together. In order to validate our method, we introduce a new challenging multi-modal dynamic sign language dataset captured with Kinect sensors. We evaluate the proposed approach on the collected dataset and achieve superior performance. Moreover, our method achieves a mean Jaccard Index score of 0.836 on the ChaLearn Looking at People Gesture datasets.},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {47 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{liangSignLanguageRecognition1996,
  title = {A Sign Language Recognition System Using Hidden Markov Model and Context Sensitive Search},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}} - {{VRST}} '96},
  author = {Liang, Rung-Huei and Ouhyoung, Ming},
  year = {1996},
  pages = {59--66},
  publisher = {{ACM Press}},
  address = {{Hong Kong}},
  doi = {10.1145/3304181.3304194},
  url = {http://dl.acm.org/citation.cfm?doid=3304181.3304194},
  urldate = {2023-07-02},
  abstract = {Hand gesture is one of the most natural and expressive ways for the hearing impaired. However, because of the complexity of dynamic gestures, most researches are focused either on static gestures, postures, or a small set of dynamic gestures. As real-time recognition of a large set of dynamic gestures is considered, some efficient algorithms and models are needed. To solve this problem in Taiwanese Sign Language, a statistics based context sensitive model is presented and both gestures and postures can be successfully recognized. A gesture is decomposed as a sequence of postures and the postures can be quickly recognized using hidden Markov model. With the probability resulted from hidden Markov model and the probability of each gesture in a lexicon, a gesture can be easily recognized in a linguistic way in real-time.},
  isbn = {978-0-89791-825-1},
  langid = {english},
  keywords = {app:sign-language,app:taiwanese-sl,based-on-gloves,classes:50,hardware:dataglove,model:hmm},
  annotation = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/LU3L7VBE/Liang and Ouhyoung - 1996 - A sign language recognition system using hidden ma.pdf}
}

@article{liHandGestureRecognition2018,
  title = {Hand {{Gesture Recognition}} and {{Real-time Game Control Based}} on {{A Wearable Band}} with 6-Axis {{Sensors}}},
  author = {Li, Yande and Wang, Taiqian and {khan}, Aamir and Li, Lian and Li, Caihong and Yang, Yi and Liu, Li},
  year = {2018},
  month = jul,
  journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro}},
  doi = {10.1109/IJCNN.2018.8489743},
  url = {https://ieeexplore.ieee.org/document/8489743/},
  urldate = {2023-03-07},
  abstract = {Human-computer interaction introduces critical open door with the proceeds with improvement of wearable gadgets. Gesture recognition through smart devices is becoming a popular research direction. This paper proposes a hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband. We utilize three-axis accelerator and gyroscope sensors embedded in smart band to collect hand motion information and use Kinect camera capture video information for manual segmentation during the training model phase. A continuous gesture segmentation algorithm based on sliding window and DTW algorithm is developed to detect meaningful gestures in the real-time game control stage. In addition, an android game named Fly Birds which is controlled by gesture recognition result is presented to simulate real-time human-computer interaction. Then, we classify the data in the window using common classifiers. Finally, our experimental results show that, we can accurately identify the designed gestures during the stage of static gesture recognition, and we also achieve a perfect interactive effect in the process of dynamic real-time game control. The experiment outcomes will advance the ascent of human-PC cooperation in view of hand gesture recognition and related applications will rise in vast numbers.},
  isbn = {9781509060146},
  keywords = {app:gaming,model:dtw,tech:accelerometer,tech:rgbd},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{liIndoTrackDeviceFreeIndoor2017,
  title = {{{IndoTrack}}: {{Device-Free Indoor Human Tracking}} with {{Commodity Wi-Fi}}},
  shorttitle = {{{IndoTrack}}},
  author = {Li, Xiang and Zhang, Daqing and Lv, Qin and Xiong, Jie and Li, Shengjie and Zhang, Yue and Mei, Hong},
  year = {2017},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {1},
  number = {3},
  pages = {1--22},
  issn = {2474-9567},
  doi = {10.1145/3130940},
  url = {https://dl.acm.org/doi/10.1145/3130940},
  urldate = {2023-07-12},
  abstract = {Indoor human tracking is fundamental to many real-world applications such as security surveillance, behavioral analysis, and elderly care. Previous solutions usually require dedicated device being carried by the human target, which is inconvenient or even infeasible in scenarios such as elderly care and break-ins. However, compared with device-based tracking, device-free tracking is particularly challenging because the much weaker reflection signals are employed for tracking. The problem becomes even more difficult with commodity Wi-Fi devices, which have limited number of antennas, small bandwidth size, and severe hardware noise.             In this work, we propose IndoTrack, a device-free indoor human tracking system that utilizes only commodity Wi-Fi devices. IndoTrack is composed of two innovative methods: (1) Doppler-MUSIC is able to extract accurate Doppler velocity information from noisy Wi-Fi Channel State Information (CSI) samples; and (2) Doppler-AoA is able to determine the absolute trajectory of the target by jointly estimating target velocity and location via probabilistic co-modeling of spatial-temporal Doppler and AoA information. Extensive experiments demonstrate that IndoTrack can achieve a 35cm median error in human trajectory estimation, outperforming the state-of-the-art systems and provide accurate location and velocity information for indoor human mobility and behavioral analysis.},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{limingchenSensorBasedActivityRecognition2012,
  title = {Sensor-{{Based Activity Recognition}}},
  author = {{Liming Chen} and Hoey, J. and Nugent, C. D. and Cook, D. J. and {Zhiwen Yu}},
  year = {2012},
  month = nov,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {42},
  number = {6},
  pages = {790--808},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2012.2198883},
  url = {http://ieeexplore.ieee.org/document/6208895/},
  urldate = {2023-07-12},
  abstract = {Research on sensor-based activity recognition has, recently, made significant progress and is attracting growing attention in a number of disciplines and application domains. However, there is a lack of high-level overview on this topic that can inform related communities of the research state of the art. In this paper, we present a comprehensive survey to examine the development and current status of various aspects of sensor-based activity recognition. We first discuss the general rationale and distinctions of vision-based and sensor-based activity recognition. Then, we review the major approaches and methods associated with sensor-based activity monitoring, modeling, and recognition from which strengths and weaknesses of those approaches are highlighted. We make a primary distinction in this paper between data-driven and knowledge-driven approaches, and use this distinction to structure our survey. We also discuss some promising directions for future research.},
  keywords = {type:survey},
  annotation = {910 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{linnainmaaTaylorExpansionAccumulated1976,
  title = {Taylor Expansion of the Accumulated Rounding Error},
  author = {Linnainmaa, Seppo},
  year = {1976},
  month = jun,
  journal = {BIT},
  volume = {16},
  number = {2},
  pages = {146--160},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01931367},
  url = {http://link.springer.com/10.1007/BF01931367},
  urldate = {2023-07-11},
  abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
  langid = {english},
  keywords = {background,backprop},
  annotation = {247 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{liSurvey3DHand2019,
  title = {A Survey on {{3D}} Hand Pose Estimation: {{Cameras}}, Methods, and Datasets},
  shorttitle = {A Survey on {{3D}} Hand Pose Estimation},
  author = {Li, Rui and Liu, Zhenyu and Tan, Jianrong},
  year = {2019},
  month = sep,
  journal = {Pattern Recognition},
  volume = {93},
  pages = {251--272},
  issn = {00313203},
  doi = {10.1016/j.patcog.2019.04.026},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320319301724},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "A survey on 3D hand pose estimation: Cameras, methods, and datasets" by Rui Li et al.},
  langid = {english},
  keywords = {type:survey},
  annotation = {51 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{liuDynamicGestureRecognition2021,
  title = {Dynamic {{Gesture Recognition Algorithm Based}} on {{3D Convolutional Neural Network}}},
  author = {Liu, Yuting and Jiang, Du and Duan, Haojie and Sun, Ying and Li, Gongfa and Tao, Bo and Yun, Juntong and Liu, Ying and Chen, Baojia},
  editor = {Ahmed, Syed Hassan},
  year = {2021},
  month = aug,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2021},
  pages = {1--12},
  issn = {1687-5273, 1687-5265},
  doi = {10.1155/2021/4828102},
  url = {https://www.hindawi.com/journals/cin/2021/4828102/},
  urldate = {2023-03-07},
  abstract = {Gesture recognition is one of the important ways of human-computer interaction, which is mainly detected by visual technology. The temporal and spatial features are extracted by convolution of the video containing gesture. However, compared with the convolution calculation of a single image, multiframe image of dynamic gestures has more computation, more complex feature extraction, and more network parameters, which affects the recognition efficiency and real-time performance of the model. To solve above problems, a dynamic gesture recognition model based on CBAM-C3D is proposed. Key frame extraction technology, multimodal joint training, and network optimization with BN layer are used for making the network performance better. The experiments show that the recognition accuracy of the proposed 3D convolutional neural network combined with attention mechanism reaches 72.4\% on EgoGesture dataset, which is improved greatly compared with the current main dynamic gesture recognition methods, and the effectiveness of the proposed algorithm is verified.},
  langid = {english},
  keywords = {model:cnn,tech:rgb},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ELPL8EWG/Liu et al. - 2021 - Dynamic Gesture Recognition Algorithm Based on 3D .pdf}
}

@inproceedings{liuHandGestureRecognition2004,
  title = {Hand Gesture Recognition Using Depth Data},
  booktitle = {Sixth {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}}, 2004. {{Proceedings}}.},
  author = {Liu, Xia and Fujimura, K.},
  year = {2004},
  month = may,
  pages = {529--534},
  doi = {10.1109/AFGR.2004.1301587},
  abstract = {A method is presented for recognizing hand gestures by using a sequence of real-time depth image data acquired by an active sensing hardware. Hand posture and motion information extracted from a video is represented in a gesture space which consists of a number of aspects including hand shape, location and motion information. In this space, it is shown to be possible to recognize many types of gestures. Experimental results are shown to validate our approach and characteristics of our approach are discussed.},
  keywords = {based-on-vision,tech:rgb,tech:rgbd},
  annotation = {243 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/R7WNTKII/1301587.html}
}

@article{liuTrackingVitalSigns2015,
  title = {Tracking {{Vital Signs During Sleep Leveraging Off-the-shelf WiFi}}},
  author = {Liu, Jian and Wang, Yan and Chen, Yingying and Yang, Jie and Chen, Xu and Cheng, Jerry},
  year = {2015},
  month = jun,
  journal = {Proceedings of the 16th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {267--276},
  publisher = {{ACM}},
  address = {{Hangzhou China}},
  doi = {10.1145/2746285.2746303},
  url = {https://dl.acm.org/doi/10.1145/2746285.2746303},
  urldate = {2023-07-12},
  abstract = {Tracking human vital signs of breathing and heart rates during sleep is important as it can help to assess the general physical health of a person and provide useful clues for diagnosing possible diseases. Traditional approaches (e.g., Polysomnography (PSG)) are limited to clinic usage. Recent radio frequency (RF) based approaches require specialized devices or dedicated wireless sensors and are only able to track breathing rate. In this work, we propose to track the vital signs of both breathing rate and heart rate during sleep by using off-the-shelf WiFi without any wearable or dedicated devices. Our system re-uses existing WiFi network and exploits the fine-grained channel information to capture the minute movements caused by breathing and heart beats. Our system thus has the potential to be widely deployed and perform continuous long-term monitoring. The developed algorithm makes use of the channel information in both time and frequency domain to estimate breathing and heart rates, and it works well when either individual or two persons are in bed. Our extensive experiments demonstrate that our system can accurately capture vital signs during sleep under realistic settings, and achieve comparable or even better performance comparing to traditional and existing approaches, which is a strong indication of providing non-invasive, continuous fine-grained vital signs monitoring without any additional cost.},
  isbn = {9781450334891},
  langid = {english},
  keywords = {app:activity-inference,app:vital-signs,based-on-wifi,tech:wifi},
  annotation = {390 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{liuUWaveAccelerometerbasedPersonalized2009,
  title = {{{uWave}}: {{Accelerometer-based}} Personalized Gesture Recognition and Its Applications},
  shorttitle = {{{uWave}}},
  author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
  year = {2009},
  month = dec,
  journal = {Pervasive and Mobile Computing},
  volume = {5},
  number = {6},
  pages = {657--675},
  issn = {15741192},
  doi = {10.1016/j.pmcj.2009.07.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119209000674},
  urldate = {2023-07-02},
  abstract = {Semantic Scholar extracted view of "uWave: Accelerometer-based personalized gesture recognition and its applications" by Jiayang Liu et al.},
  langid = {english},
  keywords = {based-on-gloves,model:uwave,participants:8,tech:accelerometer},
  annotation = {739 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UMP9M3LQ/Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf;/Users/brk/Zotero/storage/H47GNEKC/4912759.html}
}

@article{liuWiSleepContactlessSleep2014,
  title = {Wi-{{Sleep}}: {{Contactless Sleep Monitoring}} via {{WiFi Signals}}},
  shorttitle = {Wi-{{Sleep}}},
  author = {Liu, Xuefeng and Cao, Jiannong and Tang, Shaojie and Wen, Jiaqi},
  year = {2014},
  month = dec,
  journal = {2014 IEEE Real-Time Systems Symposium},
  pages = {346--355},
  publisher = {{IEEE}},
  address = {{Rome}},
  doi = {10.1109/RTSS.2014.30},
  url = {https://ieeexplore.ieee.org/document/7010501/},
  urldate = {2023-07-12},
  abstract = {Is it possible to leverage WiFi signals collected in bedrooms to monitor a person's sleep? In this paper, we show that with off-the-shelf WiFi devices, fine-grained sleep information like a person's respiration, sleeping postures and rollovers can be successfully extracted. We do this by introducing Wi-Sleep, the first sleep monitoring system based on WiFi signals. Wi-Sleep adopts off-the-shelf WiFi devices to continuously collect the fine-grained wireless channel state information (CSI) around a person. From the CSI, Wi-Sleep extracts rhythmic patterns associated with respiration and abrupt changes due to the body movement. Compared to existing sleep monitoring systems that usually require special devices attached to human body (i.e. Probes, head belt, and wrist band), Wi-Sleep is completely contact less. In addition, different from many vision-based sleep monitoring systems, Wi-Sleep is robust to low-light environments and does not raise privacy concerns. Preliminary testing results show that the Wi-Sleep can reliably track a person's respiration and sleeping postures in different conditions.},
  isbn = {9781479972883},
  keywords = {app:activity-inference,app:breathing-detection,app:sleep-detection,based-on-wifi,tech:wifi},
  annotation = {243 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{liWhenCSIMeets2016,
  title = {When {{CSI Meets Public WiFi}}: {{Inferring Your Mobile Phone Password}} via {{WiFi Signals}}},
  shorttitle = {When {{CSI Meets Public WiFi}}},
  author = {Li, Mengyuan and Meng, Yan and Liu, Junyi and Zhu, Haojin and Liang, Xiaohui and Liu, Yao and Ruan, Na},
  year = {2016},
  month = oct,
  journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {1068--1079},
  publisher = {{ACM}},
  address = {{Vienna Austria}},
  doi = {10.1145/2976749.2978397},
  url = {https://dl.acm.org/doi/10.1145/2976749.2978397},
  urldate = {2023-07-12},
  abstract = {In this study, we present WindTalker, a novel and practical keystroke inference framework that allows an attacker to infer the sensitive keystrokes on a mobile device through WiFi-based side-channel information. WindTalker is motivated from the observation that keystrokes on mobile devices will lead to different hand coverage and the finger motions, which will introduce a unique interference to the multi-path signals and can be reflected by the channel state information (CSI). The adversary can exploit the strong correlation between the CSI fluctuation and the keystrokes to infer the user's number input. WindTalker presents a novel approach to collect the target's CSI data by deploying a public WiFi hotspot. Compared with the previous keystroke inference approach, WindTalker neither deploys external devices close to the target device nor compromises the target device. Instead, it utilizes the public WiFi to collect user's CSI data, which is easy-to-deploy and difficult-to-detect. In addition, it jointly analyzes the traffic and the CSI to launch the keystroke inference only for the sensitive period where password entering occurs. WindTalker can be launched without the requirement of visually seeing the smart phone user's input process, backside motion, or installing any malware on the tablet. We implemented Windtalker on several mobile phones and performed a detailed case study to evaluate the practicality of the password inference towards Alipay, the largest mobile payment platform in the world. The evaluation results show that the attacker can recover the key with a high successful rate.},
  isbn = {9781450341394},
  langid = {english},
  keywords = {app:activity-inference,app:password-inference,based-on-wifi,classes:10,tech:wifi},
  annotation = {159 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{liWiFingerTalkYour2016,
  title = {{{WiFinger}}: Talk to Your Smart Devices with Finger-Grained Gesture},
  shorttitle = {{{WiFinger}}},
  author = {Li, Hong and Yang, Wei and Wang, Jianxin and Xu, Yang and Huang, Liusheng},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {250--261},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971738},
  url = {https://dl.acm.org/doi/10.1145/2971648.2971738},
  urldate = {2023-06-23},
  abstract = {In recent literatures, WiFi signals have been widely used to "sense" people's locations and activities. Researchers have exploited the characteristics of wireless signals to "hear" people's talk and "see" keystrokes by human users. Inspired by the excellent work of relevant scholars, we turn to explore the field of human-computer interaction using finger-grained gestures under WiFi environment. In this paper, we present Wi-Finger - the first solution using ubiquitous wireless signals to achieve number text input in WiFi devices. We implement a prototype of WiFinger on a commercial Wi-Fi infrastructure. Our scheme is based on the key intuition that while performing a certain gesture, the fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time series of Channel State Information (CSI) values. WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices. As the results show, WiFinger achieves up to 90.4\% average classification accuracy for recognizing 9 digits finger-grained gestures from American Sign Language (ASL), and its average accuracy for single individual number text input in desktop reaches 82.67\% within 90 digits.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on-wifi,read-priority-3,tech:wifi},
  annotation = {230 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/DT2Z2XQS/Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf}
}

@inproceedings{longSaliencyDetectionVideos2015,
  title = {Saliency Detection for Videos Using {{3D FFT}} Local Spectra},
  booktitle = {{{IS}}\&{{T}}/{{SPIE Electronic Imaging}}},
  author = {Long, Zhiling and AlRegib, Ghassan},
  editor = {Rogowitz, Bernice E. and Pappas, Thrasyvoulos N. and De Ridder, Huib},
  year = {2015},
  month = mar,
  pages = {93941G},
  address = {{San Francisco, California, United States}},
  doi = {10.1117/12.2077762},
  url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2077762},
  urldate = {2023-07-11},
  abstract = {Bottom-up spatio-temporal saliency detection identifies perceptually important regions of interest in video sequences. The center-surround model proves to be useful for visual saliency detection. In this work, we explore using 3D FFT local spectra as features for saliency detection within the center-surround framework. We develop a spectral location based decomposition scheme to divide a 3D FFT cube into two components, one related to temporal changes and the other related to spatial changes. Temporal saliency and spatial saliency are detected separately using features derived from each spectral component through a simple center-surround comparison method. The two detection results are then combined to yield a saliency map. We apply the same detection algorithm to different color channels (YIQ) and incorporate the results into the final saliency determination. The proposed technique is tested with the public CRCNS database. Both visual and numerical evaluations verify the promising performance of our technique.},
  keywords = {dataset:crcns,model:fft,read-priority-3,tech:rgb},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@misc{LucidVR,
  title = {{{LucidVR}}},
  url = {https://github.com/LucidVR/lucidgloves},
  keywords = {hardware:unique}
}

@article{luOneshotLearningHand2019,
  title = {One-Shot Learning Hand Gesture Recognition Based on Modified 3d Convolutional Neural Networks},
  author = {Lu, Zhi and Qin, Shiyin and Li, Xiaojie and Li, Lianwei and Zhang, Dinghao},
  year = {2019},
  month = oct,
  journal = {Machine Vision and Applications},
  volume = {30},
  number = {7-8},
  pages = {1157--1180},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-019-01043-7},
  url = {http://link.springer.com/10.1007/s00138-019-01043-7},
  urldate = {2023-07-11},
  abstract = {Though deep neural networks have played a very important role in the field of vision-based hand gesture recognition, however, it is challenging to acquire large numbers of annotated samples to support its deep learning or training. Furthermore, in practical applications it often encounters some case with only one single sample for a new gesture class so that conventional recognition method cannot be qualified with a satisfactory classification performance. In this paper, the methodology of transfer learning is employed to build an effective network architecture of one-shot learning so as to deal with such intractable problem. Then some useful knowledge from deep training with big dataset of relative objects can be transferred and utilized to strengthen one-shot learning hand gesture recognition (OSLHGR) rather than to train a network from scratch. According to this idea a well-designed convolutional network architecture with deeper layers, C3D (Tran et al. in: ICCV, pp 4489\textendash 4497, 2015), is modified as an effective tool to extract spatiotemporal feature by deep learning. Then continuous fine-tune training is performed on a sample of new classes to complete one-shot learning. Moreover, the test of classification is carried out by Softmax classifier and geometrical classification based on Euclidean distance. Finally, a series of experiments and tests on two benchmark datasets, VIVA (Vision for Intelligent Vehicles and Applications) and SKIG (Sheffield Kinect Gesture) are conducted to demonstrate its state-of-the-art recognition accuracy of our proposed method. Meanwhile, a special dataset of gestures, BSG, is built using SoftKinetic DS325 for the test of OSLHGR, and a series of test results verify and validate its well classification performance and real-time response speed.},
  langid = {english},
  keywords = {dataset:skig,dataset:viva,model:cnn,model:nn,technique:one-shot},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{madadiOcclusionAwareHand2017,
  title = {Occlusion {{Aware Hand Pose Recovery}} from {{Sequences}} of {{Depth Images}}},
  booktitle = {2017 12th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2017)},
  author = {Madadi, Meysam and Escalera, Sergio and Carruesco, Alex and Andujar, Carlos and Bar{\'o}, Xavier and Gonz{\`a}lez, Jordi},
  year = {2017},
  month = may,
  pages = {230--237},
  doi = {10.1109/FG.2017.37},
  abstract = {State-of-the-art approaches on hand pose estimation from depth images have reported promising results under quite controlled considerations. In this paper we propose a two-step pipeline for recovering the hand pose from a sequence of depth images. The pipeline has been designed to deal with images taken from any viewpoint and exhibiting a high degree of finger occlusion. In a first step we initialize the hand pose using a part-based model, fitting a set of hand components in the depth images. In a second step we consider temporal data and estimate the parameters of a trained bilinear model consisting of shape and trajectory bases. Results on a synthetic, highly-occluded dataset demonstrate that the proposed method outperforms most recent pose recovering approaches, including those based on CNNs.},
  keywords = {tech:rgb,tech:rgbd},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/Y54S29Q8/Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf;/Users/brk/Zotero/storage/MKR4QCS5/7961746.html}
}

@article{maHandGestureRecognition2017,
  title = {Hand Gesture Recognition with Convolutional Neural Networks for the Multimodal {{UAV}} Control},
  author = {Ma, Yuntao and Liu, Yuxuan and Jin, Ruiyang and Yuan, Xingyang and Sekha, Raza and Wilson, Samuel and Vaidyanathan, Ravi},
  year = {2017},
  month = oct,
  journal = {2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)},
  pages = {198--203},
  publisher = {{IEEE}},
  address = {{Linkoping}},
  doi = {10.1109/RED-UAS.2017.8101666},
  url = {http://ieeexplore.ieee.org/document/8101666/},
  urldate = {2023-06-22},
  abstract = {We introduce a robust wearable sensor suite fusing arm motion and hand gesture recognition for operator control of UAVs. The sensor suite fuses mechanomyography (MMG) and an inertial measurement unit (IMU) to capture multi-modal (arm movement and hand gesture) command signals simultaneously. The IMU produces world referenced orientation and acceleration data while concomitant MMG tracks muscle activation through surface vibration. The use of surface muscle vibration for gesture recognition removes the need for electrical contact with the skin, which has impeded other forms of muscle measurement for gesture recognition in the field. This investigation presents hardware design, inertial recognition of arm movement, and the detailed structure of a convolutional neural network (CNN) system used for real-time hand gesture recognition based on MMG signals. The system achieved 94\% accuracy for five gestures with simple calibration for each user, thereby providing an intuitive gesture based UAV control system. To our knowledge this is the first wearable system enabling multimodal control of UAVs through intuitive gestures that does not require electrical skin contact. Future work involves testing the system with larger UAV swarms.},
  isbn = {9781538609392},
  keywords = {app:robot-control,classes:{$<$}10,model:cnn,tech:imu,tech:mechanomyography},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{mahmoudConvolutionalNeuralNetworks2021,
  title = {Convolutional Neural Networks Framework for Human Hand Gesture Recognition},
  booktitle = {Bulletin of {{Electrical Engineering}} and {{Informatics}}},
  author = {Mahmoud, Aseel Ghazi and Hasan, Ahmed Mudheher and Hassan, Nadia Moqbel},
  year = {2021},
  month = aug,
  volume = {10},
  pages = {2223--2230},
  issn = {2302-9285, 2089-3191},
  doi = {10.11591/eei.v10i4.2926},
  url = {https://beei.org/index.php/EEI/article/view/2926},
  urldate = {2023-07-11},
  abstract = {Recently, the recognition of human hand gestures is becoming a valuable technology for various applications like sign language recognition, virtual games and robotics control, video surveillance, and home automation. Owing to the recent development of deep learning and its excellent performance, deep learning-based hand gesture recognition systems can provide promising results. However, accurate recognition of hand gestures remains a substantial challenge that faces most of the recently existing recognition systems. In this paper, convolutional neural networks (CNN) framework with multiple layers for accurate, effective, and less complex human hand gesture recognition has been proposed. Since the images of the infrared hand gestures can provide accurate gesture information through the low illumination environment, the proposed system is tested and evaluated on a database of hand-based near-infrared which including ten gesture poses. Extensive experiments prove that the proposed system provides excellent results of accuracy, precision, sensitivity (recall), and F1-score. Furthermore, a comparison with recently existing systems is reported.},
  keywords = {classes:10,model:cnn,tech:infrared,tech:rgb,tech:rgbd},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/NDPUWIZE/Mahmoud et al. - 2021 - Convolutional neural networks framework for human .pdf}
}

@article{makaussovLowCostIMUBasedRealTime2020,
  title = {A {{Low-Cost}}, {{IMU-Based Real-Time On Device Gesture Recognition Glove}}},
  author = {Makaussov, Oleg and Krassavin, Mikhail and Zhabinets, Maxim and Fazli, Siamac},
  year = {2020},
  month = oct,
  journal = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {3346--3351},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/SMC42975.2020.9283231},
  url = {https://ieeexplore.ieee.org/document/9283231/},
  urldate = {2023-03-07},
  abstract = {This paper evaluates the possibility of performing fine gesture recognition including finger movements on a low-tech device. In particular, we present a solution with a recognition model that is small enough to fit in the memory of a low- tech device and describe related difficulties associated with this approach. Several different Machine Learning techniques are employed and their individual advantages and drawbacks are explored for the task at hand. Our results indicate an average of 95\% accuracy during real-time testing for an eight class decoding task with a custom Recurrent Neural Network approach, that runs on the low-tech device, namely an Arduino Nano 33 BLE. The novelty and strength of this research lies in the fact that we are able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures. The recognition process is conducted on the low-tech device and as a result this solution has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.},
  isbn = {9781728185262},
  keywords = {arduino-nano-ble,classes:{$<$}10,model:rnn,read-priority-1,tech:accelerometer},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{mantyjarviEnablingFastEffortless2004,
  title = {Enabling Fast and Effortless Customisation in Accelerometer Based Gesture Interaction},
  booktitle = {Proceedings of the 3rd International Conference on {{Mobile}} and Ubiquitous Multimedia},
  author = {M{\"a}ntyj{\"a}rvi, Jani and Kela, Juha and Korpip{\"a}{\"a}, Panu and Kallio, Sanna},
  year = {2004},
  month = oct,
  pages = {25--31},
  publisher = {{ACM}},
  address = {{College Park Maryland USA}},
  doi = {10.1145/1052380.1052385},
  url = {https://dl.acm.org/doi/10.1145/1052380.1052385},
  urldate = {2023-03-07},
  abstract = {Accelerometer based gesture control is proposed as a complementary interaction modality for handheld devices. Predetermined gesture commands or freely trainable by the user can be used for controlling functions also in other devices. To support versatility of gesture commands in various types of personal device applications gestures should be customisable, easy and quick to train. In this paper we experiment with a procedure for training/recognizing customised accelerometer based gestures with minimum amount of user effort in training. Discrete Hidden Markov Models (HMM) are applied. Recognition results are presented for an external device, a DVD player gesture commands. A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training. For a set of eight gestures, each trained with two original gestures and with two Gaussian noise-distorted duplicates, the average recognition accuracy was 97\%, and with two original gestures and with four noise-distorted duplicates, the average recognition accuracy was 98\%, cross-validated from a total data set of 240 gestures. Use of procedure facilitates quick and effortless customisation in accelerometer based interaction.},
  isbn = {978-1-58113-981-5},
  langid = {english},
  keywords = {based-on-gloves,classes:8,contains-more-references,from:cite.bib,hardware:soapbox,have-read,model:hmm,tech:accelerometer},
  annotation = {78 citations (Crossref) [2023-07-11] 165 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/9GG99UUB/Mntyjrvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf}
}

@article{mantyjarviGestureInteractionSmall2005,
  title = {Gesture {{Interaction}} for {{Small Handheld Devices}} to {{Support Multimedia Applications}}},
  author = {M{\"a}ntyj{\"a}rvi, Jani and Kallio, S. and Korpip{\"a}{\"a}, Panu and Kela, J. and Plomp, J.},
  year = {2005},
  month = jun,
  journal = {J. Mobile Multimedia},
  url = {https://www.semanticscholar.org/paper/Gesture-Interaction-for-Small-Handheld-Devices-to-M%C3%A4ntyj%C3%A4rvi-Kallio/4945fca3ffa1fe292e0f61ffc4fd9cc51e9c86f8},
  urldate = {2023-03-07},
  abstract = {Accelerometer-based gesture control is proposed as a complementary interaction modality for small handheld devices to enable a variety of multimedia applications. The motivation for experimenting with gesture interaction is justified by the personal and public domain prototype applications developed. The challenges related to developing user-dependent and independent gesture control are presented. In this article, we experiment with methods for user-dependent gesture recognition with a low number of training repetitions, and for feasible user-independent gesture recognition from a moderately large set of gestures. The user-dependent gesture recognition performance of the continuous Hidden Markov Model (HMM) is better when compared to discrete HMM with three gesture repetitions in a training set. With continuous HMM, a recognition accuracy level of 95\% is obtained with or without tilt normalization, while for discrete HMM a best recognition accuracy of 90\% is obtained. The user-independent gesture recognition performance with continuous HMM of 89\% is considerably better compared to tests with discrete HMM, when both are obtained with cross-validation from 2,520 gestures. An important result is that the effect of using tilt normalization notably increases the user-independent gesture recognition performance by 10- 15\% depending on the method used. The chosen methods show great potential for gesture-based interaction in multimedia applications.},
  keywords = {based-on-gloves,classes:18,contains-more-references,model:hmm,participants:7,tech:accelerometer},
  annotation = {19 Citations},
  file = {/Users/brk/Zotero/storage/9E9Q3KL8/Mntyjrvi et al. - 2005 - Gesture Interaction for Small Handheld Devices to .pdf}
}

@article{mantyjarviIdentifyingUsersPortable2005,
  title = {Identifying {{Users}} of {{Portable Devices}} from {{Gait Pattern}} with {{Accelerometers}}},
  author = {Mantyjarvi, J. and Lindholm, M. and Vildjiounaite, E. and Makela, S. and Ailisto, H.},
  year = {2005},
  journal = {Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.},
  volume = {2},
  pages = {973--976},
  publisher = {{IEEE}},
  address = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.1109/ICASSP.2005.1415569},
  url = {http://ieeexplore.ieee.org/document/1415569/},
  urldate = {2023-07-12},
  abstract = {Identifying users of portable devices from gait signals acquired with three-dimensional accelerometers was studied. Three approaches, correlation, frequency domain and data distribution statistics, were used. Test subjects (N=36) walked with fast, normal and slow walking speeds in enrolment and test sessions on separate days wearing the accelerometer device on their belt, at back. It was shown to be possible to identify users with this novel gait recognition method. Best equal error rate (EER=7\%) was achieved with the signal correlation method, while the frequency domain method and two variations of the data distribution statistics method produced EER of 10\%, 18\% and 19\%, respectively.},
  isbn = {9780780388741},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,participants:36,tech:accelerometer},
  annotation = {434 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/D2FUHZWJ/Mantyjarvi et al. - 2005 - Identifying Users of Portable Devices from Gait Pa.pdf}
}

@article{marasovicMotionBasedGestureRecognition2015,
  title = {Motion-{{Based Gesture Recognition Algorithms}} for {{Robot Manipulation}}},
  author = {Marasovi{\'c}, Tea and Papi{\'c}, Vladan and Marasovi{\'c}, Jadranka},
  year = {2015},
  month = may,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {12},
  number = {5},
  pages = {51},
  issn = {1729-8814, 1729-8814},
  doi = {10.5772/60077},
  url = {http://journals.sagepub.com/doi/10.5772/60077},
  urldate = {2023-03-07},
  abstract = {The prevailing trend of integrating inertial sensors in consumer electronics devices has inspired research on new forms of human-computer interaction utilizing hand gestures, which may be set-up on mobile devices themselves. At present, motion gesture recognition is intensely studied, with various recognition techniques being employed and tested. This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings. The study takes two of the most popular and arguably the best recognition methods currently in use - dynamic time warping and hidden Markov model - and sets them against a relatively novel approach founded on distance metric learning. The three selected algorithms are evaluated in terms of their overall performance, accuracy, training time, execution time and storage efficacy. The optimal algorithm is further implemented in a prototype user application, aimed to serve as an interface for controlling the motion of a toy robot via gestures made with a smartphone.},
  langid = {english},
  keywords = {model:dtw,model:hmm},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/H6IC9DRX/Marasovi et al. - 2015 - Motion-Based Gesture Recognition Algorithms for Ro.pdf}
}

@article{marceloGeFightersExperimentGesturebased2006,
  title = {{{GeFighters}}: An {{Experiment}} for {{Gesture-based Interaction Analysis}} in a {{Fighting Game}}},
  author = {Marcelo, Jo{\~a}o and Farias, Thiago and Pessoa, Saulo and Moura, Guilherme and Teichrieb, Veronica},
  year = {2006},
  month = jan,
  abstract = {This paper presents GeFighters, a 3D fighting game that supports gesture based interaction. This application has been used to test and analyze gesture interaction in the context of games that need short and reliable response times. Some inherent aspects of the application have been analyzed, like the impact that interaction causes on frame renderization and response time related to the control of game characters. In order to implement the desired interaction, an input devices management platform, named CIDA, has been used.},
  keywords = {app:gaming,based-on-vision,fiducials,from:cite.bib,tech:rgb},
  file = {/Users/brk/Zotero/storage/4SF4MAVE/Marcelo et al. - 2006 - GeFighters an Experiment for Gesture-based Intera.pdf}
}

@inproceedings{marcusSensingHumanHand1988,
  title = {Sensing Human Hand Motions for Controlling Dexterous Robots},
  author = {Marcus, B. and Churchill, Philip J.},
  year = {1988},
  month = nov,
  url = {https://www.semanticscholar.org/paper/Sensing-human-hand-motions-for-controlling-robots-Marcus-Churchill/ca562cee0ce3781c0bc73e0392bc362e5a5f585c},
  urldate = {2023-07-13},
  abstract = {The Dexterous Hand Master (DHM) system is designed to control dexterous robot hands such as the UTAH/MIT and Stanford/JPL hands. It is the first commercially available device which makes it possible to accurately and confortably track the complex motion of the human finger joints. The DHM is adaptable to a wide variety of human hand sizes and shapes, throughout their full range of motion.},
  keywords = {based-on-gloves,tech:hall-effect},
  file = {/Users/brk/Zotero/storage/TKRBDT3Q/Marcus and Churchill - 1988 - Sensing human hand motions for controlling dextero.pdf}
}

@article{mardiyantoDevelopmentHandGesture2017,
  title = {Development of Hand Gesture Recognition Sensor Based on Accelerometer and Gyroscope for Controlling Arm of Underwater Remotely Operated Robot},
  author = {Mardiyanto, Ronny and Utomo, Mochamad Fajar Rinaldi and Purwanto, Djoko and Suryoatmojo, Heri},
  year = {2017},
  month = aug,
  journal = {2017 International Seminar on Intelligent Technology and Its Applications (ISITIA)},
  pages = {329--333},
  publisher = {{IEEE}},
  address = {{Surabaya}},
  doi = {10.1109/ISITIA.2017.8124104},
  url = {http://ieeexplore.ieee.org/document/8124104/},
  urldate = {2023-03-07},
  abstract = {Hand Gesture Recognition sensor based on accelerometer and gyroscope is a sensor for capturing the positions of operator hand while controlling underwater remotely operated vehicle equipped with an arm. The proposed system has an advantage in its convenience by means of no training or exercise needed for operator before using it. The key issue here is how beginner operator could use easily the underwater remotely operated robot arm without any specific training. The conventional one uses a joystick for controlling the underwater system and it is inconvenience for beginner user as well as less precision. The proposed system consists of two main part: (1) ground station and (2) underwater remotely operated robot arm. This paper proposes the development of hand gesture recognition sensor used by operator at the ground station for controlling robot arm at the underwater robot. The proposed sensor uses accelerometers and gyroscopes installed in elbow, forearm, and wrist. These devices measure 3D position of each joints for constructing 3D position of hand. We design sensor's casing for its convenience of use by using CAD software. Each sensor is connected by Arduino Nano microcontroller having compact circuit and embedded it into sensor's casing. The sensors are connected to a microcontroller acting as master connected to microcontroller slave (sensor part). These sensors value are converted to 3D position by using forward kinematic. The forward kinematic values are sent to the underwater robot by using a wire utilizing Pulse Position Signal. Then, it converted again to servo's movement by using inverse kinematic. The result is operator able to control the underwater remotely robot arm by utilizing hand gesture directly. The last, operator could control the robot gripper based on flex sensor installed in operator's fingers. The accuracy of the sensor has been tested under laboratory condition, it has 98\% of accuracy.},
  isbn = {9781538627082},
  keywords = {app:robot-control,arduino-nano,tech:accelerometer,tech:flex},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@misc{martinabadiTensorFlowLargeScaleMachine2015,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {{Mart\'in Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Man\'e} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi\'egas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year = {2015},
  url = {https://www.tensorflow.org/},
  keywords = {from:cite.bib,type:software-lib}
}

@article{maSignFiSignLanguage2018,
  title = {{{SignFi}}: {{Sign Language Recognition Using WiFi}}},
  shorttitle = {{{SignFi}}},
  author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan and Zhao, Hongyang and Jung, Woosub},
  year = {2018},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {2},
  number = {1},
  pages = {1--21},
  issn = {2474-9567},
  doi = {10.1145/3191755},
  url = {https://dl.acm.org/doi/10.1145/3191755},
  urldate = {2023-07-12},
  abstract = {We propose SignFi to recognize sign language gestures using WiFi. SignFi uses Channel State Information (CSI) measured by WiFi packets as the input and a Convolutional Neural Network (CNN) as the classification algorithm. Existing WiFi-based sign gesture recognition technologies are tested on no more than 25 gestures that only involve hand and/or finger gestures. SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy. SignFi collects CSI measurements to capture wireless signal characteristics of sign gestures. Raw CSI measurements are pre-processed to remove noises and recover CSI changes over sub-carriers and sampling time. Pre-processed CSI measurements are fed to a 9-layer CNN for sign gesture classification. We collect CSI traces and evaluate SignFi in the lab and home environments. There are 8,280 gesture instances, 5,520 from the lab and 2,760 from the home, for 276 sign gestures in total. For 5-fold cross validation using CSI traces of one user, the average recognition accuracy of SignFi is 98.01\%, 98.91\%, and 94.81\% for the lab, home, and lab+home environment, respectively. We also run tests using CSI traces from 5 different users in the lab environment. The average recognition accuracy of SignFi is 86.66\% for 7,500 instances of 150 sign gestures performed by 5 different users.},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,classes:276,model:cnn,participants:5,read-priority-1,tech:wifi},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{matsakisRustLanguage2014,
  title = {The {{Rust}} Language},
  booktitle = {{{ACM SIGAda Ada Letters}}},
  author = {Matsakis, Nicholas D and Klock II, Felix S},
  year = {2014},
  volume = {34},
  pages = {103--104},
  publisher = {{ACM}},
  keywords = {from:cite.bib,type:software-lib}
}

@article{maWiFiSensingChannel2020,
  title = {{{WiFi Sensing}} with {{Channel State Information}}: {{A Survey}}},
  shorttitle = {{{WiFi Sensing}} with {{Channel State Information}}},
  author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan},
  year = {2020},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {3},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3310194},
  url = {https://dl.acm.org/doi/10.1145/3310194},
  urldate = {2023-07-12},
  abstract = {With the high demand for wireless data traffic, WiFi networks have experienced very rapid growth, because they provide high throughput and are easy to deploy. Recently, Channel State Information (CSI) measured by WiFi networks is widely used for different sensing purposes. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this survey gives a comprehensive review of the signal processing techniques, algorithms, applications, and performance results of WiFi sensing with CSI. Different WiFi sensing algorithms and signal processing techniques have their own advantages and limitations and are suitable for different WiFi sensing applications. The survey groups CSI-based WiFi sensing applications into three categories, detection, recognition, and estimation, depending on whether the outputs are binary/multi-class classifications or numerical values. With the development and deployment of new WiFi technologies, there will be more WiFi sensing opportunities wherein the targets may go beyond from humans to environments, animals, and objects. The survey highlights three challenges for WiFi sensing: robustness and generalization, privacy and security, and coexistence of WiFi sensing and networking. Finally, the survey presents three future WiFi sensing trends, i.e., integrating cross-layer network information, multi-device cooperation, and fusion of different sensors, for enhancing existing WiFi sensing capabilities and enabling new WiFi sensing opportunities.},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi,type:survey},
  annotation = {277 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{mccullochLogicalCalculusIdeas1990,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1990},
  month = jan,
  journal = {Bulletin of Mathematical Biology},
  volume = {52},
  number = {1-2},
  pages = {99--115},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/BF02459570},
  url = {http://link.springer.com/10.1007/BF02459570},
  urldate = {2023-06-07},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  langid = {english},
  keywords = {background},
  annotation = {129 Citations}
}

@article{MedicalGalleryBlausen2014,
  title = {Medical Gallery of {{Blausen Medical}} 2014},
  year = {2014},
  journal = {WikiJournal of Medicine},
  volume = {1},
  number = {2},
  issn = {20024436},
  doi = {10.15347/wjm/2014.010},
  url = {https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Blausen_Medical_2014},
  urldate = {2023-07-12},
  keywords = {medical},
  file = {/Users/brk/Zotero/storage/CI57MESF/2014 - Medical gallery of Blausen Medical 2014.pdf}
}

@article{mehdiSignLanguageRecognition2002,
  title = {Sign Language Recognition Using Sensor Gloves},
  author = {Mehdi, S.A. and Khan, Y.N.},
  year = {2002},
  journal = {Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.},
  pages = {2204-2206 vol.5},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICONIP.2002.1201884},
  url = {http://ieeexplore.ieee.org/document/1201884/},
  urldate = {2023-07-02},
  abstract = {This paper examines the possibility of recognizing sign language gestures using sensor gloves. Previously sensor gloves are used in games or in applications with custom gestures. This paper explores their use in Sign Language recognition. This is done by implementing a project called "Talking Hands", and studying the results. The project uses a sensor glove to capture the signs of American Sign Language performed by a user and translates them into sentences of English language. Artificial neural networks are used to recognize the sensor values coming from the sensor glove. These values are then categorized in 24 alphabets of English language and two punctuation symbols introduced by the author. So, mute people can write complete sentences using this application.},
  isbn = {9789810475246},
  keywords = {app:american-sl,app:sign-language,based-on-gloves,claims-to-be-first,classes:26,hardware:5dt-dataglove,have-read,model:ffnn,movement:static},
  annotation = {151 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ID64UHJA/Mehdi and Khan - 2002 - Sign language recognition using sensor gloves.pdf}
}

@inproceedings{mehraSurveyMulticlassClassification2013,
  title = {Survey on {{Multiclass Classification Methods}}},
  author = {Mehra, Neha and Gupta, Surendra},
  year = {2013},
  url = {https://www.semanticscholar.org/paper/Survey-on-Multiclass-Classification-Methods-Mehra-Gupta/f0fcf860031b356a3c68b330735634c00e5d7602},
  urldate = {2023-07-11},
  abstract = {Supervised learning is based on the target value or the desired outputs. Various successful techniques have been proposed to solve the problem in the binary classification case. The multiclass classification case is more delicate one. In this short survey we investigate the various techniques for solving the multiclass classification problem. Various authors and research modified the multiclass classification approach such as one against one, one against all and Directed Acyclic Graph (DAG) which creates many binary classifiers and combines their results to determine the class label of a test pixel. They also describe the various extensible methods that are extended from binary class to solve the multiclass problem and also explain the method in which the classes are arranged into a tree.},
  keywords = {classification,multiclass},
  file = {/Users/brk/Zotero/storage/FPR3M2AA/Mehra and Gupta - 2013 - Survey on Multiclass Classification Methods.pdf}
}

@article{mejia-perezAutomaticRecognitionMexican2022,
  title = {Automatic {{Recognition}} of {{Mexican Sign Language Using}} a {{Depth Camera}} and {{Recurrent Neural Networks}}},
  author = {{Mej{\'i}a-Per{\'e}z}, Kenneth and {C{\'o}rdova-Esparza}, Diana-Margarita and Terven, Juan and {Herrera-Navarro}, Ana-Marcela and {Garc{\'i}a-Ram{\'i}rez}, Teresa and {Ram{\'i}rez-Pedraza}, Alfonso},
  year = {2022},
  month = may,
  journal = {Applied Sciences},
  volume = {12},
  number = {11},
  pages = {5523},
  issn = {2076-3417},
  doi = {10.3390/app12115523},
  url = {https://www.mdpi.com/2076-3417/12/11/5523},
  urldate = {2023-07-02},
  abstract = {Automatic sign language recognition is a challenging task in machine learning and computer vision. Most works have focused on recognizing sign language using hand gestures only. However, body motion and facial gestures play an essential role in sign language interaction. Taking this into account, we introduce an automatic sign language recognition system based on multiple gestures, including hands, body, and face. We used a depth camera (OAK-D) to obtain the 3D coordinates of the motions and recurrent neural networks for classification. We compare multiple model architectures based on recurrent networks such as Long Short-Term Memories (LSTM) and Gated Recurrent Units (GRU) and develop a noise-robust approach. For this work, we collected a dataset of 3000 samples from 30 different signs of the Mexican Sign Language (MSL) containing features coordinates from the face, body, and hands in 3D spatial coordinates. After extensive evaluation and ablation studies, our best model obtained an accuracy of 97\% on clean test data and 90\% on highly noisy data.},
  langid = {english},
  keywords = {app:mexican-sl,app:sign-language,classes:30-50,model:gru,model:lstm},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/BVMX8PCK/Meja-Perz et al. - 2022 - Automatic Recognition of Mexican Sign Language Usi.pdf}
}

@misc{mellisGestureRecognitionUsing,
  title = {Gesture {{Recognition Using Accelerometer}} and {{ESP}}},
  author = {{Mellis}},
  url = {https://create.arduino.cc/projecthub/mellis/gesture-recognition-using-accelerometer-and-esp-71faa1},
  keywords = {hardware:arduino,hardware:esp,tech:accelerometer}
}

@inproceedings{michahialHandGestureRecognition2015,
  title = {Hand Gesture Recognition Using Support Vector Machine},
  author = {Michahial, S. and Azeez, Beebi Hajira and Rani, R.},
  year = {2015},
  url = {https://www.semanticscholar.org/paper/Hand-gesture-recognition-using-support-vector-Michahial-Azeez/4deb239f4366c461ab092f17828aed9a03b52e8d},
  urldate = {2023-07-11},
  abstract = {The images contain measurement information of key interest for a variety of research and application areas. On the other hand, computers have become an inseparable part of our society, influencing many aspects of our daily lives in terms of communication and interaction. The main motive is to develop a system that can simplify the way humans interact with Computers. The system is designed using Canny's edge detection for edge detection and Histogram of gradients for feature extraction and the Support Vector Machine (SVM) Classifier which is widely used for classification and regression testing. SVM training algorithm builds a model that predicts whether a new example falls into one category or other. And the classifier learns from the data points in examples when they are classified belonging to their respective categories.},
  keywords = {model:svm,read-priority-9,tech:rgb},
  file = {/Users/brk/Zotero/storage/JLWRYAFF/Michahial et al. - 2015 - Hand gesture recognition using support vector mach.pdf}
}

@article{ming-hsuanyangRecognizingHandGesture1999,
  title = {Recognizing Hand Gesture Using Motion Trajectories},
  author = {{Ming-Hsuan Yang} and Ahuja, N.},
  year = {1999},
  journal = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
  pages = {466--472},
  publisher = {{IEEE Comput. Soc}},
  address = {{Fort Collins, CO, USA}},
  doi = {10.1109/CVPR.1999.786979},
  url = {http://ieeexplore.ieee.org/document/786979/},
  urldate = {2023-07-11},
  abstract = {We present an algorithm for extracting and classifying two-dimensional motion in an image sequence based on motion trajectories. First, a multiscale segmentation is performed to generate homogeneous regions in each frame. Regions between consecutive frames are then matched to obtain 2-view correspondences. Affine transformations are computed from each pair of corresponding regions to define pixel matches. Pixels matches over consecutive images pairs are concatenated to obtain pixel-level motion trajectories across the image sequence. Motion patterns are learned from the extracted trajectories using a time-delay neural network. We apply the proposed method to recognize 40 hand gestures of American Sign Language. Experimental results show that motion patterns in hand gestures can be extracted and recognized with high recognition rate using motion trajectories.},
  isbn = {9780769501499},
  keywords = {app:american-sl,app:sign-language,based-on-vision,classes:40,model:nn,tech:rgb},
  annotation = {155 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/3ZUDSQQR/Ming-Hsuan Yang and Ahuja - 1999 - Recognizing hand gesture using motion trajectories.pdf}
}

@article{mitraGestureRecognitionSurvey2007,
  title = {Gesture {{Recognition}}: {{A Survey}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Mitra, Sushmita and Acharya, Tinku},
  year = {2007},
  month = may,
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
  volume = {37},
  number = {3},
  pages = {311--324},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2007.893280},
  url = {http://ieeexplore.ieee.org/document/4154947/},
  urldate = {2023-03-07},
  abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
  keywords = {based-on-gloves,based-on-vision,contains-more-references,model:hmm,type:survey},
  annotation = {1146 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/32EDNKX8/Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf}
}

@inproceedings{moeslundComputerVisionBasedHuman1999,
  title = {Computer {{Vision-Based Human Motion Capture}} - {{A Survey}}},
  author = {Moeslund, T. and Bajers, Fredrik},
  year = {1999},
  url = {https://www.semanticscholar.org/paper/Computer-Vision-Based-Human-Motion-Capture-A-Survey-Moeslund-Bajers/4a59c198bdb7ce6b081f6d9477bdf1e2c4324fba},
  urldate = {2023-07-12},
  abstract = {This technical report is the documentation of a survey within computer vision-based human motion capture. The idea with this report is to present previous work in a structural manner using taxonomies at di erent levels. The report is structured in the following way. The rst chapter gives an introduction to new types of interfaces where one class is known as the "Looking at People" domain. Within this class computer vision-based human motion capture is de ned. In chapter two motion capture is described in a larger context describing the di erent sensors used for motion capture. Chapter three presents previous taxonomies which are commented and a new taxonomy is suggested. Chapter four describes a number of assumptions which are used in computer vision-based human motion capture. The next four chapters each give a detailed description of the four classes present in the taxonomy suggested in chapter three. Finally a conclusion is given in the last chapter. Throughout the descriptions of the taxonomy and its four classes a number of examples are given from the literature. Preface This technical report is the documentation of a survey within computer vision-based human motion capture. The writing of the report was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.study titled: "Multiple Cues for Model-Based Human Motion Capture". A large number of papers have been read to write this report. Approximately 2/3 of them (107) are directly concerned with computer vision-based human motion capture. Detailed summaries of these papers can be found in another concurrently written technical report: Summaries of 107 Computer Vision-Based HumanMotion Capture Papers [108]. Whenever I use 'he' throughout the report it should be read as he/she. Finally I would like to thanks Moritz Stoerring for his help editing the report. Aalborg, Denmark, March 1999 Thomas B. Moeslund (tbm@vision.auc.dk)},
  keywords = {based-on-vision,survey-finsh:1998,tech:rgb,type:survey}
}

@inproceedings{moeslundSummaries107Computer1999,
  title = {Summaries of 107 {{Computer Vision-Based Human Motion Capture Papers}}},
  author = {Moeslund, T. and Bajers, Fredrik},
  year = {1999},
  url = {https://www.semanticscholar.org/paper/Summaries-of-107-Computer-Vision-Based-Human-Motion-Moeslund-Bajers/4f0e8c2b9fd0d6852b773b405e81aa768e449813},
  urldate = {2023-07-12},
  abstract = {This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. The survey gives a taxonomy for the papers described in this report. The summaries are presented in alphabetic order, with respect to the surname of the rst Author. I have tried to give objective summaries of the di erent papers and only give subjective critique in an item, comments, reserved for this purpose. I have also tried to use the same amount of energy and space on the di erent summaries. But large variations between the length of the di erent summaries can be observed. This is mainly due to the length of the papers, but also due to my interest in the individual papers. Preface This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. This report has been written as a separate report due to its size alone. The writing of the two reports was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.-study titled: "Multiple Cues for Model-Based Human Motion Capture". Whenever I use 'he' throughout the report is should be read as he/she. Aalborg, Denmark, March 1999. Thomas B. Moeslund (tbm@vision.auc.dk)},
  keywords = {based-on-vision,tech:rgb,type:survey}
}

@article{moeslundSurveyAdvancesVisionbased2006,
  title = {A Survey of Advances in Vision-Based Human Motion Capture and Analysis},
  author = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker},
  year = {2006},
  month = nov,
  journal = {Computer Vision and Image Understanding},
  volume = {104},
  number = {2-3},
  pages = {90--126},
  issn = {10773142},
  doi = {10.1016/j.cviu.2006.08.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314206001263},
  urldate = {2023-07-11},
  abstract = {This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [247]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement.},
  langid = {english},
  keywords = {based-on-vision,tech:rgb,type:survey},
  annotation = {2830 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/7NIXC2ZU/Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf}
}

@article{moeslundSurveyComputerVisionBased2001,
  title = {A {{Survey}} of {{Computer Vision-Based Human Motion Capture}}},
  author = {Moeslund, Thomas B. and Granum, Erik},
  year = {2001},
  month = mar,
  journal = {Computer Vision and Image Understanding},
  volume = {81},
  number = {3},
  pages = {231--268},
  issn = {10773142},
  doi = {10.1006/cviu.2000.0897},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S107731420090897X},
  urldate = {2023-07-12},
  abstract = {A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition. Each process is discussed and divided into subprocesses and/or categories of methods to provide a reference to describe and compare the more than 130 publications covered by the survey. References are included throughout the paper to exemplify important issues and their relations to the various methods. A number of general assumptions used in this research field are identified and the character of these assumptions indicates that the research field is still in an early stage of development. To evaluate the state of the art, the major application areas are identified and performances are analyzed in light of the methods presented in the survey. Finally, suggestions for future research directions are offered.},
  langid = {english},
  keywords = {based-on-vision,tech:rgb,type:survey},
  annotation = {2025 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{mohammedDeepLearningBasedEndtoEnd2019,
  title = {A {{Deep Learning-Based End-to-End Composite System}} for {{Hand Detection}} and {{Gesture Recognition}}},
  author = {Mohammed, Adam Ahmed Qaid and Lv, Jiancheng and Islam, Md. Sajjatul},
  year = {2019},
  month = nov,
  journal = {Sensors},
  volume = {19},
  number = {23},
  pages = {5282},
  issn = {1424-8220},
  doi = {10.3390/s19235282},
  url = {https://www.mdpi.com/1424-8220/19/23/5282},
  urldate = {2023-06-22},
  abstract = {Recent research on hand detection and gesture recognition has attracted increasing interest due to its broad range of potential applications, such as human-computer interaction, sign language recognition, hand action analysis, driver hand behavior monitoring, and virtual reality. In recent years, several approaches have been proposed with the aim of developing a robust algorithm which functions in complex and cluttered environments. Although several researchers have addressed this challenging problem, a robust system is still elusive. Therefore, we propose a deep learning-based architecture to jointly detect and classify hand gestures. In the proposed architecture, the whole image is passed through a one-stage dense object detector to extract hand regions, which, in turn, pass through a lightweight convolutional neural network (CNN) for hand gesture recognition. To evaluate our approach, we conducted extensive experiments on four publicly available datasets for hand detection, including the Oxford, 5-signers, EgoHands, and Indian classical dance (ICD) datasets, along with two hand gesture datasets with different gesture vocabularies for hand gesture recognition, namely, the LaRED and TinyHands datasets. Here, experimental results demonstrate that the proposed architecture is efficient and robust. In addition, it outperforms other approaches in both the hand detection and gesture classification tasks.},
  langid = {english},
  keywords = {model:cnn,tech:rgb},
  annotation = {34 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/V5WL63KJ/Mohammed et al. - 2019 - A Deep Learning-Based End-to-End Composite System .pdf}
}

@article{mohandesAutomationArabicSign2004,
  title = {Automation of the Arabic Sign Language Recognition},
  author = {Mohandes, M. and {A-Buraiky}, S. and Halawani, T. and {Al-Baiyat}, S.},
  year = {2004},
  journal = {Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.},
  pages = {479--480},
  publisher = {{IEEE}},
  address = {{Damascus, Syria}},
  doi = {10.1109/ICTTA.2004.1307840},
  url = {http://ieeexplore.ieee.org/document/1307840/},
  urldate = {2023-03-07},
  abstract = {This paper introduces a system to recognize the Arabic sign language using an instrumented glove and a machine learning method. Interfaces in sign language systems can be categorized as direct-device or vision-based. The direct-device approach uses measurement devices that are in direct contact with the hand such as instrumented gloves, flexion sensors, styli and position-tracking devices. On the other hand, the vision-based approach captures the movement of the singer's hand using a camera that is sometimes aided by making the signer wear a glove that has painted areas indicating the positions of the fingers or knuckles. The proposed system basically consists of a PowerGlove that is connected through the serial port to a workstation running the support vector machine algorithm. Obtained results are promising even though a simple and cheap glove with limited sensors was utilized.},
  isbn = {9780780384828},
  keywords = {app:arabic-sl,app:sign-language,based-on-gloves,hardware:powerglove,have-read,model:svm,pdf:paywalled},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/HX626BIS/Mohandes et al. - 2004 - Automation of the arabic sign language recognition.pdf}
}

@article{mohandesRecognitionTwoHandedArabic2013,
  title = {Recognition of {{Two-Handed Arabic Signs Using}} the {{CyberGlove}}},
  author = {Mohandes, Mohamed A.},
  year = {2013},
  month = mar,
  journal = {Arabian Journal for Science and Engineering},
  volume = {38},
  number = {3},
  pages = {669--677},
  issn = {1319-8025, 2191-4281},
  doi = {10.1007/s13369-012-0378-z},
  url = {http://link.springer.com/10.1007/s13369-012-0378-z},
  urldate = {2023-07-13},
  abstract = {Sign language maps letters, words, and expressions of a certain language to a set of hand gestures enabling an individual to communicate by using hands and gestures rather than by speaking. Systems capable of recognizing sign-language symbols can be used for communication with the hearing-impaired. This paper represents the first attempt to recognize two-handed signs from the Unified Arabic Sign Language Dictionary using the CyberGlove and support vector machines (SVMs). 20 samples from each of 100 two-handed signs were collected from two adult signers. Because the signs are of different lengths, time division is used to standardize sign length. The duration of every sign is divided into a specific number of segments, and the mean and standard deviation of each segment are used to represent the signal in the segment. After pre-processing, principal component analysis is used for feature extraction. For recognition, a SVM is trained on 15 samples from each sign. The performance is obtained by testing the trained SVM on the remaining five samples from each sign. A recognition rate of 99.6\% on the testing data is obtained.},
  langid = {english},
  keywords = {app:arabic-sl,app:sign-language,classes:100,hardware:cyberglove,model:svm,participants:2},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@article{moinWearableBiosensingSystem2020,
  title = {A Wearable Biosensing System with In-Sensor Adaptive Machine Learning for Hand Gesture Recognition},
  author = {Moin, Ali and Zhou, Andy and Rahimi, Abbas and Menon, Alisha and Benatti, Simone and Alexandrov, George and Tamakloe, Senam and Ting, Jonathan and Yamamoto, Natasha and Khan, Yasser and Burghardt, Fred and Benini, Luca and Arias, Ana C. and Rabaey, Jan M.},
  year = {2020},
  month = dec,
  journal = {Nature Electronics},
  volume = {4},
  number = {1},
  pages = {54--63},
  issn = {2520-1131},
  doi = {10.1038/s41928-020-00510-8},
  url = {https://www.nature.com/articles/s41928-020-00510-8},
  urldate = {2023-03-07},
  abstract = {Wearable devices that monitor muscle activity based on surface electromyography could be of use in the development of hand gesture recognition applications. Such devices typically use machine-learning models, either locally or externally, for gesture classification. However, most devices with local processing cannot offer training and updating of the machine-learning model during use, resulting in suboptimal performance under practical conditions. Here we report a wearable surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities. Our system implements a neuro-inspired hyperdimensional computing algorithm locally for real-time gesture classification, as well as model training and updating under variable conditions such as different arm positions and sensor replacement. The system can classify 13 hand gestures with 97.12\% accuracy for two participants when training with a single trial per gesture. A high accuracy (92.87\%) is preserved on expanding to 21 gestures, and accuracy is recovered by 9.5\% by implementing model updates in response to varying conditions, without additional computation on an external device. A surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities can classify human gestures in real time and with high accuracy.},
  langid = {english},
  keywords = {classes:10-29,tech:emg},
  annotation = {222 citations (Crossref) [2023-07-02] 0 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/NMEFHCRT/Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf}
}

@article{moniHMMBasedHand2009,
  title = {{{HMM}} Based Hand Gesture Recognition: {{A}} Review on Techniques and Approaches},
  shorttitle = {{{HMM}} Based Hand Gesture Recognition},
  author = {Moni, M. A. and Ali, A. B. M. Shawkat},
  year = {2009},
  journal = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
  pages = {433--437},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/ICCSIT.2009.5234536},
  url = {http://ieeexplore.ieee.org/document/5234536/},
  urldate = {2023-07-02},
  abstract = {Gesture is one of the most natural and expressive ways of communications between human and computer in a virtual reality system. We naturally use various gestures to express our own intentions in everyday life. Hand gesture is one of the important methods of non-verbal communication for human beings for its freer in movements and much more expressive than any other body parts. Hand gesture recognition has a number of potential applications in human-computer interaction, machine vision, virtual reality, machine control in industry, and so on. As a gesture is a continuous motion on a sequential time series, the HMMs (Hidden Markov Models) must be a prominent recognition tool. The most important thing in hand gesture recognition is what the input features are that best represent the characteristics of the moving hand gesture.This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.},
  isbn = {9781424445196},
  keywords = {based-on-vision,model:hmm,read-priority-1,tech:rgb,type:survey},
  annotation = {76 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/8F5BHH48/Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf}
}

@article{mujahidRealTimeHandGesture2021,
  title = {Real-{{Time Hand Gesture Recognition Based}} on {{Deep Learning YOLOv3 Model}}},
  author = {Mujahid, Abdullah and Awan, Mazhar Javed and Yasin, Awais and Mohammed, Mazin Abed and Dama{\v s}evi{\v c}ius, Robertas and Maskeli{\=u}nas, Rytis and Abdulkareem, Karrar Hameed},
  year = {2021},
  month = may,
  journal = {Applied Sciences},
  volume = {11},
  number = {9},
  pages = {4164},
  issn = {2076-3417},
  doi = {10.3390/app11094164},
  url = {https://www.mdpi.com/2076-3417/11/9/4164},
  urldate = {2023-03-07},
  abstract = {Using gestures can help people with certain disabilities in communicating with other people. This paper proposes a lightweight model based on YOLO (You Only Look Once) v3 and DarkNet-53 convolutional neural networks for gesture recognition without additional preprocessing, image filtering, and enhancement of images. The proposed model achieved high accuracy even in a complex environment, and it successfully detected gestures even in low-resolution picture mode. The proposed model was evaluated on a labeled dataset of hand gestures in both Pascal VOC and YOLO format. We achieved better results by extracting features from the hand and recognized hand gestures of our proposed YOLOv3 based model with accuracy, precision, recall, and an F-1 score of 97.68, 94.88, 98.66, and 96.70\%, respectively. Further, we compared our model with Single Shot Detector (SSD) and Visual Geometry Group (VGG16), which achieved an accuracy between 82 and 85\%. The trained model can be used for real-time detection, both for static hand images and dynamic gestures recorded on a video.},
  langid = {english},
  keywords = {model:cnn,movement:dynamic,movement:static,tech:rgb},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/VWFYIVN9/Mujahid et al. - 2021 - Real-Time Hand Gesture Recognition Based on Deep L.pdf}
}

@article{mummadiRealTimeEmbeddedDetection2018,
  title = {Real-{{Time}} and {{Embedded Detection}} of {{Hand Gestures}} with an {{IMU-Based Glove}}},
  author = {Mummadi, Chaithanya and Leo, Frederic and Verma, Keshav and Kasireddy, Shivaji and Scholl, Philipp and Kempfle, Jochen and Laerhoven, Kristof},
  year = {2018},
  month = jun,
  journal = {Informatics},
  volume = {5},
  number = {2},
  pages = {28},
  issn = {2227-9709},
  doi = {10.3390/informatics5020028},
  url = {http://www.mdpi.com/2227-9709/5/2/28},
  urldate = {2023-03-07},
  abstract = {This article focuses on the use of data gloves for human-computer interaction concepts, where external sensors cannot always fully observe the user's hand. A good concept hereby allows to intuitively switch the interaction context on demand by using different hand gestures. The recognition of various, possibly complex hand gestures, however, introduces unintentional overhead to the system. Consequently, we present a data glove prototype comprising a glove-embedded gesture classifier utilizing data from Inertial Measurement Units (IMUs) in the fingertips. In an extensive set of experiments with 57 participants, our system was tested with 22 hand gestures, all taken from the French Sign Language (LSF) alphabet. Results show that our system is capable of detecting the LSF alphabet with a mean accuracy score of 92\% and an F1 score of 91\%, using complementary filter with a gyroscope-to-accelerometer ratio of 93\%. Our approach has also been compared to the local fusion algorithm on an IMU motion sensor, showing faster settling times and less delays after gesture changes. Real-time performance of the recognition is shown to occur within 63 milliseconds, allowing fluent use of the gestures via Bluetooth-connected systems.},
  langid = {english},
  keywords = {app:french-sl,app:sign-language,classes:10-29,tech:accelerometer},
  annotation = {46 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/VFSAQLD7/Mummadi et al. - 2018 - Real-Time and Embedded Detection of Hand Gestures .pdf}
}

@article{murakamiGestureRecognitionUsing1991,
  title = {Gesture Recognition Using Recurrent Neural Networks},
  author = {Murakami, Kouichi and Taguchi, Hitomi},
  year = {1991},
  journal = {Proceedings of the SIGCHI conference on Human factors in computing systems Reaching through technology - CHI '91},
  pages = {237--242},
  publisher = {{ACM Press}},
  address = {{New Orleans, Louisiana, United States}},
  doi = {10.1145/108844.108900},
  url = {http://portal.acm.org/citation.cfm?doid=108844.108900},
  urldate = {2023-07-02},
  abstract = {A gesture recognition method for Japanese sign language is presented. We have developed a posture recognition system using neural networks which could recognize a finger alphabet of 42 symbols. We then developed a gesture recognition system where each gesture specifies a word. Gesture recognition is more difficult than posture recognition because it has to handle dynamic processes. To deal with dynamic processes we use a recurrent neural network. Here, we describe a gesture recognition method which can recognize continuous gesture. We then discuss the results of our research.},
  isbn = {9780897913836},
  langid = {english},
  keywords = {app:japanese-sl,app:sign-language,based-on-gloves,classes:42,model:rnn,referenced},
  annotation = {405 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/GHGUU73Y/Murakami and Taguchi - 1991 - Gesture recognition using recurrent neural network.pdf}
}

@inproceedings{muraseGestureKeyboardMachine2012,
  title = {Gesture Keyboard with a Machine Learning Requiring Only One Camera},
  booktitle = {Proceedings of the 3rd {{Augmented Human International Conference}}},
  author = {Murase, Taichi and Moteki, Atsunori and Suzuki, Genta and Nakai, Takahiro and Hara, Nobuyuki and Matsuda, Takahiro},
  year = {2012},
  month = mar,
  pages = {1--2},
  publisher = {{ACM}},
  address = {{Meg\`eve France}},
  doi = {10.1145/2160125.2160154},
  url = {https://dl.acm.org/doi/10.1145/2160125.2160154},
  urldate = {2023-07-12},
  abstract = {In this paper, the authors propose a novel gesture-based virtual keyboard (Gesture Keyboard) that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique. Gesture Keyboard tracks the user's fingers and recognizes finger motions to judge keys input in the horizontal direction. Real-Adaboost (Adaptive Boosting), a machine learning technique, uses HOG (Histograms of Oriented Gradients) features in an image of the user's hands to estimate keys in the depth direction. Each virtual key follows a corresponding finger, so it is possible to input characters at the user's preferred hand position even if the user displaces his hands while inputting data. Additionally, because Gesture Keyboard requires only one camera, keyboard-less devices can implement this system easily. We show the effectiveness of utilizing a machine learning technique for estimating depth.},
  isbn = {978-1-4503-1077-2},
  langid = {english},
  keywords = {model:adaboost,tech:rgb,technique:histogram-oriented-gradients},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{mustafaHandGestureRecognition2007,
  title = {Hand {{Gesture Recognition Using Artificial Neural Networks}}},
  author = {Mustafa, M. A.},
  year = {2007},
  url = {https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-Artificial-Neural-Mustafa/64cbb00f06981b59ebea1faf471695280d10a2bb},
  urldate = {2023-07-11},
  abstract = {Hand gesture has been part of human communication, where, young children usually communicate by using gesture before they can talk. Adults may have to also gesture if they need to or they are indeed mute or deaf. Thus the idea of teaching a machine to also learn gestures is very appealing due to its unique mode of communications. A reliable hand gesture recognition system will make the remote control become obsolete. However, many of the new techniques proposed are complicated to be implemented in real time, especially as a human machine interface.  This thesis focuses on recognizing hand gesture in static posture. Since static hand postures not only can express some concepts, but also can act as special transition states in temporal gestures recognition, thus estimating static hand postures is in fact a big topics in gesture recognition. A database consists of 200 gesture images have been built, where five volunteers had help in the making of the database. The images were captured in a controlled environment and the postures are free from occlusion where the background is uncluttered and the hand is assumed to have been localized.    A system was then built to recognize the hand gesture. The captured image will be first preprocessed in order to binarize the palm region, where Sobel edge detection technique has been employed, with later followed by morphological operation. A new feature extraction technique has been developed, based on horizontal and vertical states transition count, and the ratio of hand area with respect to the whole area of image. These set of features have been proven to have high intra class dissimilarity attributes.    In order to have a system that can be easily trained, artificial neural networks has been chosen in the classification stage. A multilayer perceptron with back-propagation algorithm has been developed, thus the system is actually in-built to be used as a human machine interface. The gesture recognition system has been built and tested in Matlab, where simulations have shown promising results. The performance of recognition rate in this research is 95\% which shows a major improvement in comparison to the available methods.},
  keywords = {based-on-vision,model:nn,movement:static,participants:5,type:dataset}
}

@article{myersComparativeStudySeveral1981,
  title = {A Comparative Study of Several Dynamic Time-Warping Algorithms for Connected-Word Recognition},
  author = {Myers, C. S. and Rabiner, L. R.},
  year = {1981},
  journal = {The Bell System Technical Journal},
  volume = {60},
  number = {7},
  pages = {1389--1409},
  doi = {10.1002/j.1538-7305.1981.tb00272.x},
  keywords = {background,from:cite.bib,model:dtw},
  annotation = {274 citations (Crossref) [2023-07-11] 481 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{naidooSouthAfricanSign2010,
  title = {South {{African}} Sign Language Recognition Using Feature Vectors and {{Hidden Markov Models}}},
  author = {Naidoo, Nathan Lyle},
  year = {2010},
  url = {https://www.semanticscholar.org/paper/South-African-sign-language-recognition-using-and-Naidoo/be177e5e05b402da3bd53db0ba8445f87596f7d3},
  urldate = {2023-07-02},
  abstract = {This thesis presents a system for performing whole gesture recognition for South African Sign Language. The system uses feature vectors combined with Hidden Markov models. In order to constuct a feature vector, dynamic segmentation must occur to extract the signer's hand movements. Techniques and methods for normalising variations that occur when recording a signer performing a gesture, are investigated. The system has a classification rate of 69\%.},
  keywords = {model:hmm},
  file = {/Users/brk/Zotero/storage/JX268W5D/Naidoo - 2010 - South African sign language recognition using feat.pdf}
}

@article{nealPatternRecognitionMachine2007,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Neal, Radford M},
  year = {2007},
  month = aug,
  journal = {Technometrics},
  volume = {49},
  number = {3},
  pages = {366--366},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/tech.2007.s518},
  url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2007.s518},
  urldate = {2023-05-30},
  abstract = {the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.},
  langid = {english},
  keywords = {background},
  annotation = {9985 citations (Semantic Scholar/DOI) [2023-05-30]}
}

@inproceedings{nelIntegratedSignLanguage2013,
  title = {An Integrated Sign Language Recognition System},
  booktitle = {Proceedings of the {{South African Institute}} for {{Computer Scientists}} and {{Information Technologists Conference}}},
  author = {Nel, Warren and Ghaziasgar, Mehrdad and Connan, James},
  year = {2013},
  month = oct,
  pages = {179--185},
  publisher = {{ACM}},
  address = {{East London South Africa}},
  doi = {10.1145/2513456.2513491},
  url = {https://dl.acm.org/doi/10.1145/2513456.2513491},
  urldate = {2023-07-02},
  abstract = {The South African Sign Language research group at the University of the Western Cape has created several systems to recognize Sign Language gestures using single parameters. Research has shown that five parameters are required to recognize any sign language gesture: hand shape, location, orientation and motion, as well as facial expressions. Using a single parameter can cause conflicts in recognition of signs that are similarly signed. This paper pioneers research at the group towards combining multiple parameters to better distinguish between similar signs. This eventually aims to enable the recognition of a large SASL vocabulary. The proposed methodology combines hand location and hand shape recognition into one combined recognition system. The recognition approach is applied to 12 SASL signs that consist of six pairs of signs with the same hand shape performed at two different locations. It is shown that the approach is able to achieve a high average recognition accuracy of 79\% across all signs and distinguish between the signs effectively. It is also shown to be robust to variations in test subjects.},
  isbn = {978-1-4503-2112-9},
  langid = {english},
  keywords = {app:sign-language,app:south-african-sl},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/R5QLW7FJ/Nel et al. - 2013 - An integrated sign language recognition system.pdf}
}

@article{netoHighLevelProgramming2010,
  title = {High-level Programming and Control for Industrial Robotics: Using a Hand-held Accelerometer-based Input Device for Gesture and Posture Recognition},
  shorttitle = {High-level Programming and Control for Industrial Robotics},
  author = {Neto, Pedro and Norberto Pires, J. and Paulo Moreira, A.},
  year = {2010},
  month = mar,
  journal = {Industrial Robot: An International Journal},
  volume = {37},
  number = {2},
  pages = {137--147},
  issn = {0143-991X},
  doi = {10.1108/01439911011018911},
  url = {https://www.emerald.com/insight/content/doi/10.1108/01439911011018911/full/html},
  urldate = {2023-07-03},
  abstract = {Purpose               Most industrial robots are still programmed using the typical teaching process, through the use of the robot teach pendant. This is a tedious and time-consuming task that requires some technical expertise, and hence new approaches to robot programming are required. The purpose of this paper is to present a robotic system that allows users to instruct and program a robot with a high-level of abstraction from the robot language.                                         Design/methodology/approach               The paper presents in detail a robotic system that allows users, especially non-expert programmers, to instruct and program a robot just showing it what it should do, in an intuitive way. This is done using the two most natural human interfaces (gestures and speech), a force control system and several code generation techniques. Special attention will be given to the recognition of gestures, where the data extracted from a motion sensor (three-axis accelerometer) embedded in the Wii remote controller was used to capture human hand behaviours. Gestures (dynamic hand positions) as well as manual postures (static hand positions) are recognized using a statistical approach and artificial neural networks.                                         Findings               It is shown that the robotic system presented is suitable to enable users without programming expertise to rapidly create robot programs. The experimental tests showed that the developed system can be customized for different users and robotic platforms.                                         Research limitations/implications               The proposed system is tested on two different robotic platforms. Since the options adopted are mainly based on standards, it can be implemented with other robot controllers without significant changes. Future work will focus on improving the recognition rate of gestures and continuous gesture recognition.                                         Practical implications               The key contribution of this paper is that it offers a practical method to program robots by means of gestures and speech, improving work efficiency and saving time.                                         Originality/value               This paper presents an alternative to the typical robot teaching process, extending the concept of human-robot interaction and co-worker scenario. Since most companies do not have engineering resources to make changes or add new functionalities to their robotic manufacturing systems, this system constitutes a major advantage for small- to medium-sized enterprises.},
  langid = {english},
  keywords = {app:robotics,hardware:wii,movement:dynamic,movement:static},
  annotation = {95 citations (Semantic Scholar/DOI) [2023-07-03]},
  file = {/Users/brk/Zotero/storage/GM28KZG6/Neto et al. - 2010 - Highlevel programming and control for industrial .pdf}
}

@misc{nickgillianGestureRecognitionToolkit,
  title = {Gesture {{Recognition Toolkit}} ({{GRT}})},
  author = {{Nick Gillian}},
  url = {https://github.com/nickgillian/grt},
  abstract = {The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.},
  keywords = {type:software-lib}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  url = {http://neuralnetworksanddeeplearning.com},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {background}
}

@misc{norvigHowWriteSpelling2007,
  title = {How to {{Write}} a {{Spelling Corrector}}},
  author = {Norvig, Peter},
  year = {2007},
  url = {https://norvig.com/spell-correct.html},
  urldate = {2023-05-25},
  keywords = {autocorrect,background},
  file = {/Users/brk/Zotero/storage/ERETGY5C/spell-correct.html}
}

@inproceedings{oikonomidisEfficientModelbased3D2011,
  title = {Efficient Model-Based {{3D}} Tracking of Hand Articulations Using {{Kinect}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2011},
  author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis},
  year = {2011},
  pages = {101.1-101.11},
  publisher = {{British Machine Vision Association}},
  address = {{Dundee}},
  doi = {10.5244/C.25.101},
  url = {http://www.bmva.org/bmvc/2011/proceedings/paper101/index.html},
  urldate = {2023-07-11},
  abstract = {We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor. We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup. Being model based, it provides continuous solutions to the problem of tracking hand articulations. Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz).},
  isbn = {978-1-901725-43-8},
  langid = {english},
  keywords = {hardware:kinect,model:pso},
  annotation = {1008 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/2JTTNNXG/Oikonomidis et al. - 2011 - Efficient model-based 3D tracking of hand articula.pdf}
}

@article{ongAutomaticSignLanguage2005,
  title = {Automatic {{Sign Language Analysis}}: {{A Survey}} and the {{Future}} beyond {{Lexical Meaning}}},
  shorttitle = {Automatic {{Sign Language Analysis}}},
  author = {Ong, S.C.W. and Ranganath, S.},
  year = {2005},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {27},
  number = {6},
  pages = {873--891},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2005.112},
  url = {http://ieeexplore.ieee.org/document/1432718/},
  urldate = {2023-07-02},
  abstract = {Research in automatic analysis of sign language has largely focused on recognizing the lexical (or citation) form of sign gestures as they appear in continuous signing, and developing algorithms that scale well to large vocabularies. However, successful recognition of lexical signs is not sufficient for a full understanding of sign language communication. Nonmanual signals and grammatical processes which result in systematic variations in sign appearance are integral aspects of this communication but have received comparatively little attention in the literature. In this survey, we examine data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures. These are discussed with respect to issues such as modeling transitions between signs in continuous signing, modeling inflectional processes, signer independence, and adaptation. We further examine works that attempt to analyze nonmanual signals and discuss issues related to integrating these with (hand) sign gestures. We also discuss the overall progress toward a true test of sign recognition systems--dealing with natural signing by native signers. We suggest some future directions for this research and also point to contributions it can make to other fields of research. Web-based supplemental materials (appendicies) which contain several illustrative examples and videos of signing can be found at www.computer.org/publications/dlib.},
  langid = {english},
  keywords = {app:sign-language,based-on-gloves,based-on-vision,contains-more-references,type:survey},
  annotation = {595 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/7IUZNIGX/Ong and Ranganath - 2005 - Automatic Sign Language Analysis A Survey and the.pdf}
}

@article{oudahHandGestureRecognition2020,
  title = {Hand {{Gesture Recognition Based}} on {{Computer Vision}}: {{A Review}} of {{Techniques}}},
  shorttitle = {Hand {{Gesture Recognition Based}} on {{Computer Vision}}},
  author = {Oudah, Munir and {Al-Naji}, Ali and Chahl, Javaan},
  year = {2020},
  month = aug,
  journal = {Journal of Imaging},
  volume = {6},
  number = {8},
  pages = {73},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2313-433X},
  doi = {10.3390/jimaging6080073},
  url = {https://www.mdpi.com/2313-433X/6/8/73},
  urldate = {2023-06-23},
  abstract = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human\textendash computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {tech:rgb,type:survey},
  annotation = {178 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/WREJRICA/Oudah et al. - 2020 - Hand Gesture Recognition Based on Computer Vision.pdf}
}

@article{pageCONTINUOUSINSPECTIONSCHEMES1954,
  title = {{{CONTINUOUS INSPECTION SCHEMES}}},
  author = {PAGE, E. S.},
  year = {1954},
  month = jun,
  journal = {Biometrika},
  volume = {41},
  number = {1-2},
  pages = {100--115},
  issn = {0006-3444},
  doi = {10.1093/biomet/41.1-2.100},
  url = {https://doi.org/10.1093/biomet/41.1-2.100},
  urldate = {2023-05-30},
  keywords = {background,model:cusum},
  annotation = {4816 citations (Semantic Scholar/DOI) [2023-05-30]},
  file = {/Users/brk/Zotero/storage/2CCAB6JV/456627.html;/Users/brk/Zotero/storage/886PZALL/456627.html}
}

@article{pageCumulativeSumCharts1961,
  title = {Cumulative {{Sum Charts}}},
  author = {Page, E. S.},
  year = {1961},
  month = feb,
  journal = {Technometrics},
  volume = {3},
  number = {1},
  pages = {1--9},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1961.10489922},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1961.10489922},
  urldate = {2023-05-08},
  abstract = {This paper, presented orally to the Gordon Research Conference on Statistics in Chemistry in July 1960, traces the development of process inspection schemes from the original methods of Shewhart to the new charts using cumulative sums, and surveys the present practice in the operation of schemes based on cumulative sums. In spitc of the completely different appearance of the visual records kept for Shewhart and cumulative sum charts, a continuous sequence of development from the one type of scheme to the other can be established. The criteria by which a particular process inspection scheme is chosen are also developed and the practical choice of schemes is described.},
  keywords = {background,model:cusum},
  annotation = {250 citations (Semantic Scholar/DOI) [2023-05-08]}
}

@article{paoTransformationHumanHand1989,
  title = {Transformation of Human Hand Positions for Robotic Hand Control},
  author = {Pao, L. and Speeter, T.H.},
  year = {1989},
  journal = {Proceedings, 1989 International Conference on Robotics and Automation},
  pages = {1758--1763},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Scottsdale, AZ, USA}},
  doi = {10.1109/ROBOT.1989.100229},
  url = {http://ieeexplore.ieee.org/document/100229/},
  urldate = {2023-07-13},
  abstract = {A method for using the human hand as a multidegree-of-freedom teaching device is described. The algorithm is based on a functional analysis of the human hand and results in an algebraic transformation of human hand positions to corresponding positions in a target domain. The target domain should be of lower dimensionality (fewer degrees of freedom) than the human hand but is not constrained in any other way. The target described here is a sixteen-degree-of-freedom robotic hand, with four fingers of four joints each. The target need not, however be a handlike device but, for each use, should have a kinematic structure with poses similar in functionality to natural poses of the human hand.{$<<$}ETX{$>>$}},
  isbn = {9780818619380},
  keywords = {based-on-gloves,hardware:dataglove},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@article{parkAdvancedMachineLearning2019,
  title = {Advanced {{Machine Learning}} for {{Gesture Learning}} and {{Recognition Based}} on {{Intelligent Big Data}} of {{Heterogeneous Sensors}}},
  author = {Park, Jisun and Jin, Yong and Cho, Seoungjae and Sung, Yunsick and Cho, Kyungeun},
  year = {2019},
  month = jul,
  journal = {Symmetry},
  volume = {11},
  number = {7},
  pages = {929},
  issn = {2073-8994},
  doi = {10.3390/sym11070929},
  url = {https://www.mdpi.com/2073-8994/11/7/929},
  urldate = {2023-06-26},
  abstract = {With intelligent big data, a variety of gesture-based recognition systems have been developed to enable intuitive interaction by utilizing machine learning algorithms. Realizing a high gesture recognition accuracy is crucial, and current systems learn extensive gestures in advance to augment their recognition accuracies. However, the process of accurately recognizing gestures relies on identifying and editing numerous gestures collected from the actual end users of the system. This final end-user learning component remains troublesome for most existing gesture recognition systems. This paper proposes a method that facilitates end-user gesture learning and recognition by improving the editing process applied on intelligent big data, which is collected through end-user gestures. The proposed method realizes the recognition of more complex and precise gestures by merging gestures collected from multiple sensors and processing them as a single gesture. To evaluate the proposed method, it was used in a shadow puppet performance that could interact with on-screen animations. An average gesture recognition rate of 90\% was achieved in the experimental evaluation, demonstrating the efficacy and intuitiveness of the proposed method for editing visualized learning gestures.},
  langid = {english},
  keywords = {type:survey},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ALFHQ8X4/Park et al. - 2019 - Advanced Machine Learning for Gesture Learning and.pdf}
}

@inproceedings{parsaniSingleAccelerometerBased2009,
  title = {A {{Single Accelerometer}} Based {{Wireless Embedded System}} for {{Predefined Dynamic Gesture Recognition}}},
  booktitle = {Proceedings of the {{First International Conference}} on {{Intelligent Human Computer Interaction}}},
  author = {Parsani, Rahul and Singh, Karandeep},
  editor = {Tiwary, U. S. and Siddiqui, Tanveer J. and Radhakrishna, M. and Tiwari, M. D.},
  year = {2009},
  pages = {195--201},
  publisher = {{Springer India}},
  address = {{New Delhi}},
  doi = {10.1007/978-81-8489-203-1\_18},
  url = {http://link.springer.com/10.1007/978-81-8489-203-1_18},
  urldate = {2023-03-07},
  abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction. A complete embedded system which facilitates the data acquisition, analysis, recognition, and the transmission wirelessly, of human dynamic gestures to a computer, is described. An intuitive algorithm for processing the accelerometer data was implemented and tested. This method permits all the analysis to be done by the embedded system processor. The system is capable of recognizing gestures involving a combination of straight line motions in three dimensions. These gestures are then used to control a host computer which executes tasks based on the gesture received. A sample application showing how the gestures can be mapped to the English alphabet is also shown.},
  isbn = {978-81-8489-404-2 978-81-8489-203-1},
  langid = {english},
  keywords = {based-on-gloves,tech:accelerometer},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{patilHandwritingRecognitionFree2016,
  title = {Handwriting {{Recognition}} in {{Free Space Using WIMU-Based Hand Motion Analysis}}},
  author = {Patil, Shashidhar and Kim, Dubeom and Park, Seongsill and Chai, Youngho},
  year = {2016},
  journal = {Journal of Sensors},
  volume = {2016},
  pages = {1--10},
  issn = {1687-725X, 1687-7268},
  doi = {10.1155/2016/3692876},
  url = {http://www.hindawi.com/journals/js/2016/3692876/},
  urldate = {2023-07-11},
  abstract = {We present a wireless-inertial-measurement-unit- (WIMU-) based hand motion analysis technique for handwriting recognition in three-dimensional (3D) space. The proposed handwriting recognition system is not bounded by any limitations or constraints; users have the freedom and flexibility to write characters in free space. It uses hand motion analysis to segment hand motion data from a WIMU device that incorporates magnetic, angular rate, and gravity sensors (MARG) and a sensor fusion algorithm to automatically distinguish segments that represent handwriting from nonhandwriting data in continuous hand motion data. Dynamic time warping (DTW) recognition algorithm is used to recognize handwriting in real-time. We demonstrate that a user can freely write in air using an intuitive WIMU as an input and hand motion analysis device to recognize the handwriting in 3D space. The experimental results for recognizing handwriting in free space show that the proposed method is effective and efficient for other natural interaction techniques, such as in computer games and real-time hand gesture recognition applications.},
  langid = {english},
  keywords = {app:writing,model:dtw,tech:imu},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/T68RSGXI/Patil et al. - 2016 - Handwriting Recognition in Free Space Using WIMU-B.pdf}
}

@article{patilMarathiSignLanguage2022,
  title = {Marathi {{Sign Language Hand Gesture Recognition Using Accelerometer}} and {{3D Printed Gloves}}},
  author = {Patil, Sachin and Joshi, Sarang and Kulkarni, Hrushikesh B. and Hagawane, Pradnesh and Shinde, Pradnya},
  year = {2022},
  month = dec,
  journal = {2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)},
  pages = {72--77},
  publisher = {{IEEE}},
  address = {{Al-Khobar, Saudi Arabia}},
  doi = {10.1109/CICN56167.2022.10008290},
  url = {https://ieeexplore.ieee.org/document/10008290/},
  urldate = {2023-03-07},
  abstract = {Due to the communication abilities impairment, deaf \& dumb peoples are more or less isolated from mainstream societal activities. NGOs, social workers \& Governments in India are putting efforts with multiple initiatives to increase the involvement of these peoples in the mainstream activities happening in the rest of world. This paper proposes to establish an alternative method to streamline the communications among normal \& speech impaired persons by using smart hand glove system equipped with 3D accelerometer modules at the fingertips. The proposed system is delivering the measurement results in very proficient way. The experiments were carried out over 15 persons using the sign gestures, providing good co-relation between the Posed gestures versus system mapped gestures.},
  isbn = {9781665487719},
  keywords = {app:marathi-sl,app:sign-language,tech:accelerometer},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{pavlovicVisualInterpretationHand1997,
  title = {Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review},
  shorttitle = {Visual Interpretation of Hand Gestures for Human-Computer Interaction},
  author = {Pavlovic, Vladimir I. and Sharma, R. and Huang, T.S.},
  year = {1997},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {19},
  number = {7},
  pages = {677--695},
  issn = {01628828},
  doi = {10.1109/34.598226},
  url = {http://ieeexplore.ieee.org/document/598226/},
  urldate = {2023-06-23},
  abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3Dmodel of the human hand or an image appearancemodel of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient ``purposive'' approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction},
  keywords = {based-on-vision,type:survey},
  annotation = {1979 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/7SQ99MKT/Pavlovic et al. - 1997 - Visual interpretation of hand gestures for human-c.pdf}
}

@misc{PDFAdaptiveSubgradient,
  title = {[{{PDF}}] {{Adaptive Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan/413c1142de9d91804d6d11c67ff3fed59c9fc279},
  urldate = {2023-07-11},
  abstract = {This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight. We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  langid = {english},
  keywords = {adagrad,background},
  file = {/Users/brk/Zotero/storage/E6YNVHRW/[PDF] Adaptive Subgradient Methods for Online Lear.pdf;/Users/brk/Zotero/storage/GGADQGGW/413c1142de9d91804d6d11c67ff3fed59c9fc279.html}
}

@article{pengyuhongGestureModelingRecognition2000,
  title = {Gesture Modeling and Recognition Using Finite State Machines},
  author = {{Pengyu Hong} and Turk, M. and Huang, T.S.},
  year = {2000},
  journal = {Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
  pages = {410--415},
  publisher = {{IEEE Comput. Soc}},
  address = {{Grenoble, France}},
  doi = {10.1109/AFGR.2000.840667},
  url = {http://ieeexplore.ieee.org/document/840667/},
  urldate = {2023-07-11},
  abstract = {We propose a state-based approach to gesture learning and recognition. Using spatial clustering and temporal alignment, each gesture is defined to be an ordered sequence of states in spatial-temporal space. The 2D image positions of the centers of the head and both hands of the user are used as features; these are located by a color-based tracking method. From training data of a given gesture, we first learn the spatial information and then group the data into segments that are automatically aligned temporally. The temporal information is further integrated to build a finite state machine (FSM) recognizer. Each gesture has a FSM corresponding to it. The computational efficiency of the FSM recognizers allows us to achieve real-time on-line performance. We apply this technique to build an experimental system that plays a game of "Simon Says" with the user.},
  isbn = {9780769505800},
  keywords = {based-on-vision,model:fsm,tech:rgb},
  annotation = {276 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/G3CD967K/Pengyu Hong et al. - 2000 - Gesture modeling and recognition using finite stat.pdf}
}

@article{phamHandDetectionSegmentation2020,
  title = {Hand Detection and Segmentation Using Multimodal Information from {{Kinect}}},
  author = {Pham, Van-Tien and Le, Thi-Lan and Tran, Thanh-Hai and Nguyen, Thanh Phuong},
  year = {2020},
  month = oct,
  journal = {2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Ha Noi, Vietnam}},
  doi = {10.1109/MAPR49794.2020.9237785},
  url = {https://ieeexplore.ieee.org/document/9237785/},
  urldate = {2023-07-11},
  abstract = {Nowadays, hand gestures are becoming one of the most natural and intuitive ways of communication between human and computer. To this end, a complex process including hand gesture acquisition, hand detection, gesture representation and recognition must be carried out. This paper presents a method that detects hand and segments hand regions from images captured by a Kinect sensor. As Kinect sensor provides not only RGB images as conventional camera, but also depth and skeleton, in our work, we incorporate multi-modal data from Kinect to deal with hand detection and segmentation. Specifically, we use skeleton to approximately determine hand palm. Then a skin based detector will be applied to discard non-skin pixels from the region of interest. Using depth data helps to limit the human body regions and remove false positive regions from the previous steps. Finally, morphological operations will be applied to fill holes in the hand region. The main advantage of this method is very easy to implement and it performs in real-time on an ordinary computer. We evaluate the proposed method on a dataset of hand gestures captured from different viewpoints. Experiment shows that it provides reasonable accuracy at very high frame rate. It also produces comparable performance in comparison with deep learning based methods.},
  isbn = {9781728165554},
  keywords = {hardware:kinect,read-priority-9,tech:rgbd},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{prekopcsakAccelerometerBasedRealTime2008,
  title = {Accelerometer {{Based Real-Time Gesture Recognition}}},
  author = {Prekopcs{\'a}k, Zolt{\'a}n},
  year = {2008},
  url = {https://www.semanticscholar.org/paper/Accelerometer-Based-Real-Time-Gesture-Recognition-Prekopcs%C3%A1k/4e0b17d35696db6aa1cd0ea3565b47ff317e3ad3},
  urldate = {2023-03-07},
  abstract = {Gesture is a natural expression form for humans, but its recognition is a similarly hard problem as speech recognition. In this paper, I present a real-time hand gesture recognition system, which identifies relevant parts in the continous sensor data stream, and classifies them to the most probable gesture. Instead of the usual button-based segmentation, I have created an automatic segmentation method, which makes the interface more natural. The results showed that the two different classifiers reach 97.4\% and 96\% accuracy on a personalized gesture set, and these results can be improved for certain gesture sets with the combination of the two algorithms. Furthermore, the system has great performance and low response time, so the user experience is much better than with previous gesture recognizers.},
  keywords = {based-on-gloves,classes:10,classes:10-29,model:hmm,model:svm},
  file = {/Users/brk/Zotero/storage/Q44WBJN9/Prekopcsk - 2008 - Accelerometer Based Real-Time Gesture Recognition.pdf}
}

@inproceedings{premaratneAustralianSignLanguage2013,
  title = {Australian {{Sign Language Recognition Using Moment Invariants}}},
  booktitle = {Intelligent {{Computing Theories}} and {{Technology}}},
  author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Huang, De-Shuang and Jo, Kang-Hyun and Zhou, Yong-Quan and Han, Kyungsook},
  year = {2013},
  volume = {7996},
  pages = {509--514},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39482-9\_59},
  url = {http://link.springer.com/10.1007/978-3-642-39482-9_59},
  urldate = {2023-03-07},
  abstract = {Human Computer Interaction is geared towards seamless human machine integration without the need for LCDs, Keyboards or Gloves. Systems have already been developed to react to limited hand gestures especially in gaming and in consumer electronics control. Yet, it is a monumental task in bridging the well-developed sign languages in different parts of the world with a machine to interpret the meaning. One reason is the sheer extent of the vocabulary used in sign language and the sequence of gestures needed to communicate different words and phrases. Auslan the Australian Sign Language is comprised of numbers, finger spelling for words used in common practice and a medical dictionary. There are 7415 words listed in Auslan website. This research article tries to implement recognition of numerals using a computer using the static hand gesture recognition system developed for consumer electronics control at the University of Wollongong in Australia. The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures. The system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
  isbn = {978-3-642-39481-2 978-3-642-39482-9},
  keywords = {app:sign-language},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{puWholehomeGestureRecognition2013,
  title = {Whole-Home Gesture Recognition Using Wireless Signals},
  author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
  year = {2013},
  journal = {Proceedings of the 19th annual international conference on Mobile computing \& networking - MobiCom '13},
  pages = {27},
  publisher = {{ACM Press}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2500423.2500436},
  url = {http://dl.acm.org/citation.cfm?doid=2500423.2500436},
  urldate = {2023-06-23},
  abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two- bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94\%.},
  isbn = {9781450319997},
  langid = {english},
  keywords = {based-on-wifi,classes:{$<$}10,tech:wifi},
  annotation = {1010 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/DRDJ9SJ8/Pu et al. - 2013 - Whole-home gesture recognition using wireless sign.pdf}
}

@inproceedings{pylvanainenAccelerometerBasedGesture2005,
  title = {Accelerometer {{Based Gesture Recognition Using Continuous HMMs}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {Pylv{\"a}n{\"a}inen, Timo},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Marques, Jorge S. and {P{\'e}rez de la Blanca}, Nicol{\'a}s and Pina, Pedro},
  year = {2005},
  volume = {3522},
  pages = {639--646},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11492429\_77},
  url = {http://link.springer.com/10.1007/11492429_77},
  urldate = {2023-03-07},
  abstract = {This paper presents a gesture recognition system based on continuous hidden Markov models. Gestures here are hand movements which are recorded by a 3D accelerometer embedded in a handheld device. In addition to standard hidden Markov model classifier, the recognition system has a preprocessing step which removes the effect of device orientation from the data. The performance of the recognizer is evaluated in both user dependent and user independent cases. The effects of sample resolution and sampling rate are studied in the user dependent case.},
  isbn = {978-3-540-26153-7 978-3-540-32237-5},
  keywords = {based-on-gloves,model:hmm,pdf:paywalled,tech:accelerometer},
  annotation = {150 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{qaroushSmartComfortableWearable2021,
  title = {Smart, Comfortable Wearable System for Recognizing {{Arabic Sign Language}} in Real-Time Using {{IMUs}} and Features-Based Fusion},
  author = {Qaroush, Aziz and Yassin, Sara and {Al-Nubani}, Ali and Alqam, Ameer},
  year = {2021},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {184},
  pages = {115448},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.115448},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417421008629},
  urldate = {2023-03-07},
  abstract = {Semantic Scholar extracted view of "Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion" by Aziz Qaroush et al.},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{qianWidar2PassiveHuman2018,
  title = {Widar2.0: {{Passive Human Tracking}} with a {{Single Wi-Fi Link}}},
  shorttitle = {Widar2.0},
  author = {Qian, Kun and Wu, Chenshu and Zhang, Yi and Zhang, Guidong and Yang, Zheng and Liu, Yunhao},
  year = {2018},
  month = jun,
  journal = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
  pages = {350--361},
  publisher = {{ACM}},
  address = {{Munich Germany}},
  doi = {10.1145/3210240.3210314},
  url = {https://dl.acm.org/doi/10.1145/3210240.3210314},
  urldate = {2023-07-12},
  abstract = {This paper presents Widar2.0, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices. Previous works based on either specialized or commercial hardware all require multiple links, preventing their wide adoption in scenarios like homes where typically only one single AP is installed. The key insight underlying Widar2.0 to circumvent the use of multiple links is to leverage multi-dimensional signal parameters from one single link. To this end, we build a unified model accounting for Angle-of-Arrival, Time-of-Flight, and Doppler shifts together and devise an efficient algorithm for their joint estimation. We then design a pipeline to translate the erroneous raw parameters into precise locations, which first finds parameters corresponding to the reflections of interests, then refines range estimates, and ultimately outputs target locations. Our implementation and evaluation on commodity WiFi devices demonstrate that Widar2.0 achieves better or comparable performance to state-of-the-art localization systems, which either use specialized hardwares or require 2 to 40 Wi-Fi links.},
  isbn = {9781450357203},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,claims-to-be-first,tech:wifi},
  annotation = {246 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{qianWidarDecimeterLevelPassive2017,
  title = {Widar: {{Decimeter-Level Passive Tracking}} via {{Velocity Monitoring}} with {{Commodity Wi-Fi}}},
  shorttitle = {Widar},
  author = {Qian, Kun and Wu, Chenshu and Yang, Zheng and Liu, Yunhao and Jamieson, Kyle},
  year = {2017},
  month = jul,
  journal = {Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Chennai India}},
  doi = {10.1145/3084041.3084067},
  url = {https://dl.acm.org/doi/10.1145/3084041.3084067},
  urldate = {2023-07-12},
  abstract = {Various pioneering approaches have been proposed for Wi-Fi-based sensing, which usually employ learning-based techniques to seek appropriate statistical features, yet do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we propose Widar, a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and the user's location and velocity. On this basis, we propose novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity. We implement Widar on commercial Wi-Fi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 25 cm given initial positions and 38 cm without them and a median relative velocity error of 13\%.},
  isbn = {9781450349123},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {215 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{qiMultiSensorGuidedHand2021,
  title = {Multi-{{Sensor Guided Hand Gesture Recognition}} for a {{Teleoperated Robot Using}} a {{Recurrent Neural Network}}},
  author = {Qi, Wen and Ovur, Salih Ertug and Li, Zhijun and Marzullo, Aldo and Song, Rong},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  pages = {6039--6045},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3089999},
  url = {https://ieeexplore.ieee.org/document/9457168/},
  urldate = {2023-03-07},
  abstract = {Touch-free guided hand gesture recognition for human-robot interactions plays an increasingly significant role in teleoperated surgical robot systems. Indeed, despite depth cameras provide more practical information for recognition accuracy enhancement, the instability and computational burden of depth data represent a tricky problem. In this letter, we propose a novel multi-sensor guided hand gesture recognition system for surgical robot teleoperation. A multi-sensor data fusion model is designed for performing interference in the presence of occlusions. A multilayer Recurrent Neural Network (RNN) consisting of a Long Short-Term Memory (LSTM) module and a dropout layer (LSTM-RNN) is proposed for multiple hand gestures classification. Detected hand gestures are used to perform a set of human-robot collaboration tasks on a surgical robot platform. Classification performance and prediction time is compared among the LSTM-RNN model and several traditional Machine Learning (ML) algorithms, such as k-Nearest Neighbor (k-NN) and Support Vector Machines (SVM). Results show that the proposed LSTM-RNN classifier is able to achieve a higher recognition rate and faster inference speed. In addition, the present adaptive data fusion system shows a strong anti-interference capability for hand gesture recognition in real-time.},
  keywords = {app:robot-control,app:surgery,model:knn,model:lstm,model:rnn,model:svm},
  annotation = {62 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/8MHHF5ZG/Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf}
}

@article{quamExperimentalDeterminationHuman1989,
  title = {An {{Experimental Determination}} of {{Human Hand Accuracy}} with a {{DataGlove}}},
  author = {Quam, David L. and Williams, George B. and Agnew, Jeffery R. and Browne, Patricia C.},
  year = {1989},
  month = oct,
  journal = {Proceedings of the Human Factors Society Annual Meeting},
  volume = {33},
  number = {5},
  pages = {315--319},
  issn = {0163-5182},
  doi = {10.1177/154193128903300517},
  url = {http://journals.sagepub.com/doi/10.1177/154193128903300517},
  urldate = {2023-07-14},
  abstract = {A three-part experiment was conducted to determine the accuracy, repeatability and linearity of a human hand manipulating the DataGlove. Accuracy and repeatability of finger flexure were investigated with repeated measurements of three calibration positions. Linearity of finger flexure was investigated with steady finger and thumb curling motions. Accuracy and repeatability of hand location and orientation were investigated with repeated measurements of six hand positions. Finger flexure mean accuracy was 6\textordmasculine{} for the four fingers and 11\textordmasculine{} for the thumb, repeatability was 3\textordmasculine{} for the four fingers and 9\textordmasculine{} for the thumb, and linearity varied from 2 to 5\textordmasculine. Although the mean location accuracy was 1 inch and the mean orientation accuracy was 17\textordmasculine, the position and orientation receiver was observed to twist on the glove back. Across all subjects, the location repeatability was 0.5 inch, while the orientation repeatability was 9\textordmasculine. However, the within-subject location repeatability was 0.13 inches, while the orientation repeatability was 2\textordmasculine.},
  langid = {english},
  keywords = {hardware:dataglove,read-priority-5,untagged},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-14]}
}

@inproceedings{quinlanC4ProgramsMachine1992,
  title = {C4.5: {{Programs}} for {{Machine Learning}}},
  author = {Quinlan, J. Ross},
  year = {1992},
  keywords = {background,from:cite.bib,model:c4.5}
}

@article{ramamoorthyRecognitionDynamicHand2003,
  title = {Recognition of Dynamic Hand Gestures},
  author = {Ramamoorthy, Aditya and Vaswani, Namrata and Chaudhury, Santanu and Banerjee, Subhashis},
  year = {2003},
  month = sep,
  journal = {Pattern Recognition},
  volume = {36},
  number = {9},
  pages = {2069--2081},
  issn = {00313203},
  doi = {10.1016/S0031-3203(03)00042-6},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320303000426},
  urldate = {2023-07-11},
  abstract = {This paper is concerned with the problem of recognition of dynamic hand gestures. We have considered gestures which are sequences of distinct hand poses. In these gestures hand poses can undergo motion and discrete changes. However, continuous deformations of the hand shapes are not permitted. We have developed a recognition engine which can reliably recognize these gestures despite individual variations. The engine also has the ability to detect start and end of gesture sequences in an automated fashion. The recognition strategy uses a combination of static shape recognition (performed using contour discriminant analysis), Kalman filter based hand tracking and a HMM based temporal characterization scheme. The system is fairly robust to background clutter and uses skin color for static shape recognition and tracking. A real time implementation on standard hardware is developed. Experimental results establish the effectiveness of the approach.},
  langid = {english},
  keywords = {app:robotics,based-on-vision,model:hmm,movement:dynamic,tech:rgb},
  annotation = {169 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/C424ZSCP/Ramamoorthy et al. - 2003 - Recognition of dynamic hand gestures.pdf}
}

@article{rashidWearableTechnologiesHand2019,
  title = {Wearable Technologies for Hand Joints Monitoring for Rehabilitation: {{A}} Survey},
  shorttitle = {Wearable Technologies for Hand Joints Monitoring for Rehabilitation},
  author = {Rashid, Adnan and Hasan, Osman},
  year = {2019},
  month = jun,
  journal = {Microelectronics Journal},
  volume = {88},
  pages = {173--183},
  issn = {00262692},
  doi = {10.1016/j.mejo.2018.01.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0026269217305207},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "Wearable technologies for hand joints monitoring for rehabilitation: A survey" by Adnan Rashid et al.},
  langid = {english},
  keywords = {type:survey},
  annotation = {58 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{rautarayVisionBasedHand2015,
  title = {Vision Based Hand Gesture Recognition for Human Computer Interaction: A Survey},
  shorttitle = {Vision Based Hand Gesture Recognition for Human Computer Interaction},
  author = {Rautaray, Siddharth S. and Agrawal, Anupam},
  year = {2015},
  month = jan,
  journal = {Artificial Intelligence Review},
  volume = {43},
  number = {1},
  pages = {1--54},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-012-9356-9},
  url = {http://link.springer.com/10.1007/s10462-012-9356-9},
  urldate = {2023-06-26},
  abstract = {As computers become more pervasive in society, facilitating natural human\textendash computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human\textendash computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.},
  langid = {english},
  keywords = {have-read,type:survey},
  annotation = {1303 Citations},
  file = {/Users/brk/Zotero/storage/78UXQTNQ/Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf}
}

@article{raysarkarHandGestureRecognition2013,
  title = {Hand {{Gesture Recognition Systems}}: {{A Survey}}},
  shorttitle = {Hand {{Gesture Recognition Systems}}},
  author = {RaySarkar, Arpita and Sanyal, G. and Majumder, S.},
  year = {2013},
  month = jun,
  journal = {International Journal of Computer Applications},
  volume = {71},
  number = {15},
  pages = {25--37},
  issn = {09758887},
  doi = {10.5120/12435-9123},
  url = {http://research.ijcaonline.org/volume71/number15/pxc3889123.pdf},
  urldate = {2023-07-12},
  abstract = {Gesture was the first mode of communication for the primitive cave men. Later on human civilization has developed the verbal communication very well. But still nonverbal communication has not lost its weightage. Such non \textendash{} verbal communication are being used not only for the physically challenged people, but also for different applications in diversified areas, such as aviation, surveying, music direction etc. It is the best method to interact with the computer without using other peripheral devices, such as keyboard, mouse. Researchers around the world are actively engaged in development of robust and efficient gesture recognition system, more specially, hand gesture recognition system for various applications. The major steps associated with the hand gesture recognition system are; data acquisition, gesture modeling, feature extraction and hand gesture recognition. There are several sub-steps and methodologies associated with the above steps. Different researchers have followed different algorithm or sometimes have devised their own algorithm. The current research work reviews the work carried out in last twenty years and a brief comparison has been performed to analyze the difficulties encountered by these systems, as well as the limitation. Finally the desired characteristics of a robust and efficient hand gesture recognition system have been described. General Terms Hand gesture recognition, comparison},
  keywords = {tech:accelerometer,tech:rgb,tech:rgbd,type:survey},
  annotation = {69 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/T734BQCQ/RaySarkar et al. - 2013 - Hand Gesture Recognition Systems A Survey.pdf}
}

@article{rekimotoGestureWristGesturePadUnobtrusive2001,
  title = {{{GestureWrist}} and {{GesturePad}}: Unobtrusive Wearable Interaction Devices},
  shorttitle = {{{GestureWrist}} and {{GesturePad}}},
  author = {Rekimoto, Jun},
  year = {2001},
  journal = {Proceedings Fifth International Symposium on Wearable Computers},
  pages = {21--27},
  publisher = {{IEEE Comput. Soc}},
  address = {{Zurich, Switzerland}},
  doi = {10.1109/ISWC.2001.962092},
  url = {http://ieeexplore.ieee.org/document/962092/},
  urldate = {2023-06-23},
  abstract = {In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.},
  isbn = {9780769513188},
  keywords = {based-on-gloves,have-read,tech:accelerometer,tech:capacitive},
  annotation = {450 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/CAUJY4HG/Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf}
}

@article{rempelEffectWristPosture2008,
  title = {Effect of Wrist Posture on Carpal Tunnel Pressure While Typing},
  author = {Rempel, David M. and Keir, Peter J. and Bach, Joel M.},
  year = {2008},
  journal = {Journal of Orthopaedic Research},
  volume = {26},
  number = {9},
  pages = {1269--1273},
  doi = {10.1002/jor.20599},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jor.20599},
  keywords = {background,carpal tunnel syndrome,from:cite.bib,keyboard,medical,neuropathy,occupation,overuse},
  annotation = {62 citations (Crossref) [2023-07-11]}
}

@article{renRobustHandGesture2011,
  title = {Robust Hand Gesture Recognition with Kinect Sensor},
  author = {Ren, Zhou and Meng, Jingjing and Yuan, Junsong and Zhang, Zhengyou},
  year = {2011},
  month = nov,
  journal = {Proceedings of the 19th ACM international conference on Multimedia},
  pages = {759--760},
  publisher = {{ACM}},
  address = {{Scottsdale Arizona USA}},
  doi = {10.1145/2072298.2072443},
  url = {https://dl.acm.org/doi/10.1145/2072298.2072443},
  urldate = {2023-03-07},
  abstract = {Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game.},
  isbn = {9781450306164},
  langid = {english},
  keywords = {classes:10-29,classes:14,hardware:kinect,model:cnn,tech:rgbd},
  annotation = {309 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/2NSG3HQQ/Ren et al. - 2011 - Robust hand gesture recognition with kinect sensor.pdf}
}

@inproceedings{rigollHighPerformanceRealtime1998,
  title = {High Performance Real-Time Gesture Recognition Using {{Hidden Markov Models}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {Rigoll, Gerhard and Kosmala, Andreas and Eickeler, Stefan},
  editor = {Wachsmuth, Ipke and Fr{\"o}hlich, Martin},
  year = {1998},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {69--80},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052990},
  abstract = {An advanced real-time system for gesture recognition is presented, which is able to recognize complex dynamic gestures, such as ''hand waving'', ''spin'', ''pointing'', and ''head moving''. The recognition is based on global motion features, extracted from each difference image of the image sequence. The system uses Hidden Markov Models (HMMs) as statistical classifier. These HMMs are trained on a database of 24 isolated gestures, performed by 14 different people. With the use of global motion features, a recognition rate of 92.9\% is achieved for a person and background independent recognition.},
  isbn = {978-3-540-69782-4},
  langid = {english},
  keywords = {based-on-vision,classes:24,from:cite.bib,model:hmm,participants:14,tech:rgb},
  annotation = {27 citations (Crossref) [2023-07-11] 122 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/BBCJSZZM/Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf}
}

@article{rigollNewImprovedFeature1997,
  title = {New Improved Feature Extraction Methods for Real-Time High Performance Image Sequence Recognition},
  author = {Rigoll, G. and Kosmala, A.},
  year = {1997},
  journal = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume = {4},
  pages = {2901--2904},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Munich, Germany}},
  doi = {10.1109/ICASSP.1997.595396},
  url = {http://ieeexplore.ieee.org/document/595396/},
  urldate = {2023-07-11},
  abstract = {This paper describes new feature extraction methods which can be used very effectively in combination with statistical methods for image sequence recognition. Although these feature extraction methods can be used for a wide variety of image sequence processing applications, the target application presented in this paper is gesture recognition. The novel feature extraction methods have been integrated into an HMM-based gesture recognition system and led to substantial improvements for this system. It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence. This resulted in the fact that it was possible to use the novel features in combination with a new architecture for statistical image sequence recognition. The result of this investigation is a high performance gesture recognition system with significantly improved recognition rates and real-time capabilities.},
  isbn = {9780818679193},
  keywords = {model:hmm},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{rollEffectivenessOccupationalTherapy2016,
  title = {Effectiveness of {{Occupational Therapy Interventions}} for {{Adults With Musculoskeletal Conditions}} of the {{Forearm}}, {{Wrist}}, and {{Hand}}: {{A Systematic Review}}},
  author = {Roll, Shawn C. and Hardison, Mark E.},
  year = {2016},
  month = dec,
  journal = {The American Journal of Occupational Therapy},
  volume = {71},
  number = {1},
  pages = {7101180010p1-7101180010p12},
  issn = {0272-9490},
  doi = {10.5014/ajot.2017.023234},
  url = {https://doi.org/10.5014/ajot.2017.023234},
  keywords = {background,from:cite.bib,medical},
  annotation = {38 citations (Crossref) [2023-07-11] 45 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2023-06-08},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  langid = {english},
  keywords = {background,backpropogation},
  annotation = {9997 citations (Semantic Scholar/DOI) [2023-06-08]}
}

@article{rung-hueiliangRealtimeContinuousGesture1998,
  title = {A Real-Time Continuous Gesture Recognition System for Sign Language},
  author = {{Rung-Huei Liang} and {Ming Ouhyoung}},
  year = {1998},
  journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},
  pages = {558--567},
  publisher = {{IEEE Comput. Soc}},
  address = {{Nara, Japan}},
  doi = {10.1109/AFGR.1998.671007},
  url = {http://ieeexplore.ieee.org/document/671007/},
  urldate = {2023-07-02},
  abstract = {In this paper, a large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a DataGloveTM. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to 4 parameters in a gesture : posture, position, orientation, and motion. We have implemented a prototype system with a lexicon of 250 vocabularies in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signerdependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%. A large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a data glove. Sign language, which is usually known as a set of natural language with formal semantic definitions and syntactic rules, is a large set of hand gestures that are daily used to communicate with the hearing impaired. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to four parameters in a gesture: posture, position, orientation, and motion. The authors have implemented a prototype system with a lexicon of 250 vocabularies and collected 196 training sentences in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signer-dependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%,.},
  isbn = {9780818683442},
  keywords = {app:sign-language,app:taiwanese-sl,based-on-gloves,classes:65,hardware:dataglove,model:hmm},
  annotation = {498 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/8I9EZ3VC/Rung-Huei Liang and Ming Ouhyoung - 1998 - A real-time continuous gesture recognition system .pdf}
}

@article{sagayamHandPostureGesture2017,
  title = {Hand Posture and Gesture Recognition Techniques for Virtual Reality Applications: A Survey},
  shorttitle = {Hand Posture and Gesture Recognition Techniques for Virtual Reality Applications},
  author = {Sagayam, K. Martin and Hemanth, D. Jude},
  year = {2017},
  month = jun,
  journal = {Virtual Reality},
  volume = {21},
  number = {2},
  pages = {91--107},
  issn = {1359-4338, 1434-9957},
  doi = {10.1007/s10055-016-0301-0},
  url = {http://link.springer.com/10.1007/s10055-016-0301-0},
  urldate = {2023-07-02},
  abstract = {Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.},
  langid = {english},
  keywords = {app:vr,model:hmm,type:survey},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{sahooRealTimeHandGesture2022,
  title = {Real-{{Time Hand Gesture Recognition Using Fine-Tuned Convolutional Neural Network}}},
  author = {Sahoo, Jaya Prakash and Prakash, Allam Jaya and P{\l}awiak, Pawe{\l} and Samantray, Saunak},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {3},
  pages = {706},
  issn = {1424-8220},
  doi = {10.3390/s22030706},
  url = {https://www.mdpi.com/1424-8220/22/3/706},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is one of the most effective modes of interaction between humans and computers due to being highly flexible and user-friendly. A real-time hand gesture recognition system should aim to develop a user-independent interface with high recognition performance. Nowadays, convolutional neural networks (CNNs) show high recognition rates in image classification problems. Due to the unavailability of large labeled image samples in static hand gesture images, it is a challenging task to train deep CNN networks such as AlexNet, VGG-16 and ResNet from scratch. Therefore, inspired by CNN performance, an end-to-end fine-tuning method of a pre-trained CNN model with score-level fusion technique is proposed here to recognize hand gestures in a dataset with a low number of gesture images. The effectiveness of the proposed technique is evaluated using leave-one-subject-out cross-validation (LOO CV) and regular CV tests on two benchmark datasets. A real-time American sign language (ASL) recognition system is developed and tested using the proposed technique.},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,model:cnn},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/CUKP99QW/Sahoo et al. - 2022 - Real-Time Hand Gesture Recognition Using Fine-Tune.pdf}
}

@inproceedings{salibaCompactGloveInput2004,
  title = {A {{Compact Glove Input Device}} to {{Measure Human Hand}}, {{Wrist}} and {{Forearm Joint Positions}} for {{Teleoperation Applications}}},
  author = {Saliba, M. and Farrugia, F. and Giordmaina, A.},
  year = {2004},
  url = {https://www.semanticscholar.org/paper/A-Compact-Glove-Input-Device-to-Measure-Human-Hand%2C-Saliba-Farrugia/f99389f732525d8a5603da09b2d3de257b763e43},
  urldate = {2023-03-07},
  abstract = {In this work, we have developed a new glove input device that is able to measure the angular joint positions of two fingers and of the thumb on the human hand, as well as the pitch position of the wrist and the roll position of the radio-ulnar joint of the human forearm. The glove has various new features, including the measurement of forearm roll position, that are not found in other glove input devices described in the literature. The glove contains a number of flexible, plastic bands whose displacement, during joint rotation, is measured using linear potentiometers. The new glove is light, compact, easy to wear and use, robust, and inexpensive, and is intended for use in teleoperation applications in conjunction with a remotely located robot hand/wrist. Another property is that it can be easily adjusted to fit a wide range of human hand sizes. Preliminary testing of the glove has shown that it can achieve an accuracy in position measurement that compares well to that of a number of commercially-produced gloves that are presently in use. Keywords\textemdash glove input device; whole hand input device; teleoperation; human joint position sensing.},
  keywords = {based-on-gloves,hardware:unique,have-read,model:none,participants:1,tech:potentiometer},
  file = {/Users/brk/Zotero/storage/446F9VJ8/Saliba et al. - 2004 - A Compact Glove Input Device to Measure Human Hand.pdf}
}

@inproceedings{schlomerGestureRecognitionWii2008,
  title = {Gesture Recognition with a {{Wii}} Controller},
  booktitle = {Proceedings of the 2nd International Conference on {{Tangible}} and Embedded Interaction},
  author = {Schl{\"o}mer, Thomas and Poppinga, Benjamin and Henze, Niels and Boll, Susanne},
  year = {2008},
  month = feb,
  pages = {11--14},
  publisher = {{ACM}},
  address = {{Bonn Germany}},
  doi = {10.1145/1347390.1347395},
  url = {https://dl.acm.org/doi/10.1145/1347390.1347395},
  urldate = {2023-03-07},
  abstract = {In many applications today user interaction is moving away from mouse and pens and is becoming pervasive and much more physical and tangible. New emerging interaction technologies allow developing and experimenting with new interaction methods on the long way to providing intuitive human computer interaction. In this paper, we aim at recognizing gestures to interact with an application and present the design and evaluation of our sensor-based gesture recognition. As input device we employ the Wii-controller (Wiimote) which recently gained much attention world wide. We use the Wiimote's acceleration sensor independent of the gaming console for gesture recognition. The system allows the training of arbitrary gestures by users which can then be recalled for interacting with systems like photo browsing on a home TV. The developed library exploits Wii-sensor data and employs a hidden Markov model for training and recognizing user-chosen gestures. Our evaluation shows that we can already recognize gestures with a small number of training samples. In addition to the gesture recognition we also present our experiences with the Wii-controller and the implementation of the gesture recognition. The system forms the basis for our ongoing work on multimodal intuitive media browsing and are available to other researchers in the field.},
  isbn = {978-1-60558-004-3},
  langid = {english},
  keywords = {based-on-gloves,classes:{$<$}10,classes:5,hardware:wii,model:hmm,tech:accelerometer},
  annotation = {560 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/QUVGMHED/Schlmer et al. - 2008 - Gesture recognition with a Wii controller.pdf}
}

@misc{scotts.fisherVIRTUALENVIRONMENTSPERSONAL1991,
  title = {{{VIRTUAL ENVIRONMENTS}}, {{PERSONAL SIMULATION}} \& {{TELEPRESENCE}}},
  author = {{Scott S. Fisher}},
  year = {1991},
  url = {https://www.academia.edu/download/76150375/VirtualSimPresence-pdfrev.pdf},
  abstract = {For most people, "duplicating reality" is an assumed, if not obvious goal for any contemporary imaging technology. The proof of the 'ideal' picture is not being able to discern object from representation - to be convinced that one is looking at the real thing. At best, this judgement is usually based on a first order evaluation of 'ease of identification'; i.e. realistic pictures should resemble what they represent. But resemblance is only part of the effect. In summing up prevailing theories on realism in images, Perkins comments: " Pictures inform by packaging information in light in essentially the same form that real objects and scenes package it, and the perceiver unwraps that package in essentially the same way." [2]},
  keywords = {referenced}
}

@article{segenFastAccurate3D1998,
  title = {Fast and Accurate {{3D}} Gesture Recognition Interface},
  author = {Segen, J. and Kumar, S.},
  year = {1998},
  journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
  volume = {1},
  pages = {86--91},
  publisher = {{IEEE Comput. Soc}},
  address = {{Brisbane, Qld., Australia}},
  doi = {10.1109/ICPR.1998.711086},
  url = {http://ieeexplore.ieee.org/document/711086/},
  urldate = {2023-07-12},
  abstract = {A video-based gesture recognition system can serve as a natural and accurate 3D user input device. We describe a two-camera system, that recognizes three gesture classes: two static and one dynamic. For one of these gestures (pointing), the system estimates five parameters of 3D pose: position and pointing direction. The recognition is robust, independent of the user and fast (60 Hz), and the estimated pose is very stable. We describe some of the interface applications that demonstrate the benefits of the system: control of a video game, piloting a virtual reality fly-through, and interaction with a 3D scene editor.},
  isbn = {9780818685125},
  keywords = {based-on-vision,classes:3,movement:dynamic,movement:static,tech:rgb},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{segenHumancomputerInteractionUsing1998,
  title = {Human-Computer Interaction Using Gesture Recognition and {{3D}} Hand Tracking},
  author = {Segen, J. and Kumar, S.},
  year = {1998},
  journal = {Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)},
  volume = {3},
  pages = {188--192},
  publisher = {{IEEE Comput. Soc}},
  address = {{Chicago, IL, USA}},
  doi = {10.1109/ICIP.1998.727164},
  url = {http://ieeexplore.ieee.org/document/727164/},
  urldate = {2023-07-12},
  abstract = {This paper describes a real time system for human-computer interaction through gesture recognition and three dimensional hand-tracking. Using two cameras that are focused at the user's hand the system recognizes three gestures and tracks the hand in three dimensions. The system can simultaneously track two fingers (thumb and pointing finger) and output their poses. The pose for each finger consists of three positional coordinates and two angles (azimuth and elevation). By moving the thumb and the pointing finger in 3D, the user can control 10 degrees of freedom in a smooth and natural fashion. We have used this system as a multi-dimensional input-interface to computer games, terrain navigation software and graphical editors. In addition to providing 10 degrees of control, our system is much more natural and intuitive to use compared to traditional input devices. The system is user independent, and operates at the rate of 60 Hz.},
  isbn = {9780818688218},
  keywords = {based-on-vision,tech:rgb},
  annotation = {60 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{seninDynamicTimeWarping2008,
  title = {Dynamic {{Time Warping Algorithm Review}}},
  author = {Senin, Pavel},
  year = {2008},
  keywords = {background,from:cite.bib,model:dtw,type:survey}
}

@misc{SensorGlove,
  title = {Sensor {{Glove}}},
  url = {https://github.com/SensorGlove/SensorGlove},
  keywords = {hardware:accelerometers}
}

@article{sethujanakiRealTimeRecognition2013,
  title = {Real Time Recognition of {{3D}} Gestures in Mobile Devices},
  author = {Sethu Janaki, V M and Babu, Satish and Sreekanth, S S},
  year = {2013},
  month = dec,
  journal = {2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},
  pages = {149--152},
  publisher = {{IEEE}},
  address = {{Trivandrum, India}},
  doi = {10.1109/RAICS.2013.6745463},
  url = {http://ieeexplore.ieee.org/document/6745463/},
  urldate = {2023-03-07},
  abstract = {Gesture-based user interaction is increasingly relevant today as the use of personal computing devices becomes widespread. Smartphones have several inbuilt sensors like accelerometer, orientation sensor and gyroscope, which are able to provide data on motion of the device in 3D space. This paper proposes a mechanism for real-time recognition of 3D gestures using sensors in mobile devices. 3D gestures are space-drawn gestures computed from 3-axial accelerometer readings. The algorithms discussed in this paper include single value decomposition, dynamic time warping and Mahalanobis distance.},
  isbn = {9781479921782 9781479921775},
  keywords = {model:dtw,model:knn,model:svd,tech:accelerometer},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{shangRobustSignLanguage2017,
  title = {A {{Robust Sign Language Recognition System}} with {{Multiple Wi-Fi Devices}}},
  author = {Shang, Jiacheng and Wu, Jie},
  year = {2017},
  month = aug,
  journal = {Proceedings of the Workshop on Mobility in the Evolving Internet Architecture},
  pages = {19--24},
  publisher = {{ACM}},
  address = {{Los Angeles CA USA}},
  doi = {10.1145/3097620.3097624},
  url = {https://dl.acm.org/doi/10.1145/3097620.3097624},
  urldate = {2023-07-12},
  abstract = {Sign language is important since it provides a way for us to the deaf culture and more opportunities to communicate with those who are deaf or hard of hearing. Since sign language chiefly uses body languages to convey meaning, Human Activity Recognition (HAR) techniques can be used to recognize them for some sign language translation applications. In this paper, we show for the first time that Wi-Fi signals can be used to recognize sign language. The key intuition is that different hand and arm motions introduce different multi-path distortions in Wi-Fi signals and generate different unique patterns in the time-series of Channel State Information (CSI). More specifically, we propose a Wi-Fi signal-based sign language recognition system called WiSign. Different from existing Wi-Fi signal-based human activity recognition systems, WiSign uses 3 Wi-Fi devices to improve the recognition performance. We implemented the WiSign using a TP-Link TL-WR1043ND Wi-Fi router and two Lenovo X100e laptops. The evaluation results show that our system can achieve a mean prediction accuracy of 93.8\% and mean false positive of 1.55\%.},
  isbn = {9781450350594},
  langid = {english},
  keywords = {app:sign-language,based-on-wifi,claims-to-be-first,classes:5,tech:wifi},
  annotation = {39 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{sharmaASL3DCNNAmericanSign2021,
  title = {{{ASL-3DCNN}}: {{American}} Sign Language Recognition Technique Using 3-{{D}} Convolutional Neural Networks},
  shorttitle = {{{ASL-3DCNN}}},
  author = {Sharma, Shikhar and Kumar, Krishan},
  year = {2021},
  month = jul,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {17},
  pages = {26319--26331},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-021-10768-5},
  url = {https://link.springer.com/10.1007/s11042-021-10768-5},
  urldate = {2023-06-22},
  abstract = {The communication between a person from the impaired community with a person who does not understand sign language could be a tedious task. Sign language is the art of conveying messages using hand gestures. Recognition of dynamic hand gestures in American Sign Language (ASL) became a very important challenge that is still unresolved. In order to resolve the challenges of dynamic ASL recognition, a more advanced successor of the Convolutional Neural Networks (CNNs) called 3-D CNNs is employed, which can recognize the patterns in volumetric data like videos. The CNN is trained for classification of 100 words on Boston ASL (Lexicon Video Dataset) LVD dataset with more than 3300 English words signed by 6 different signers. 70\% of the dataset is used for Training while the remaining 30\% dataset is used for testing the model. The proposed work outperforms the existing state-of-art models in terms of precision (3.7\%), recall (4.3\%), and f-measure (3.9\%). The computing time (0.19 seconds per frame) of the proposed work shows that the proposal may be used in real-time applications.},
  langid = {english},
  keywords = {app:sign-language},
  annotation = {44 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@inproceedings{sharmaGestureRecognitionSurvey2013,
  title = {Gesture {{Recognition}} : {{A Survey}} of {{Gesture Recognition Techniques}} Using {{Neural Networks}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Sharma, M. and Chawla, Er Rama},
  year = {2013},
  url = {https://www.semanticscholar.org/paper/Gesture-Recognition-%3A-A-Survey-of-Gesture-using-Sharma-Chawla/70c11356829e83ba2976af1e6a6e69785d0d56ca},
  urldate = {2023-07-02},
  abstract = {Understanding human motions can be posed as a pattern recognition problem. In order to convey visual messages to a receiver, a human expresses motion patterns. Loosely called gestures, these patterns are variable but distinct and have an associated meaning. The Pattern recognition by any computer or machine can be implemented via various methods such as Hidden Harkov Models, Linear Programming and Neural Networks. Each method has its own advantages and disadvantages, which will be studied separately later on. This paper reviews why using ANNs in particular is better suited for analyzing human},
  keywords = {model:nn,type:survey},
  file = {/Users/brk/Zotero/storage/2GC2XFLL/Sharma and Chawla - 2013 - Gesture Recognition  A Survey of Gesture Recognit.pdf}
}

@article{sharmaMultimodalHumancomputerInterface1998,
  title = {Toward Multimodal Human-Computer Interface},
  author = {Sharma, R. and Pavlovic, V.I. and Huang, T.S.},
  year = {1998},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {5},
  pages = {853--869},
  issn = {00189219},
  doi = {10.1109/5.664275},
  url = {http://ieeexplore.ieee.org/document/664275/},
  urldate = {2023-06-23},
  abstract = {Recent advances in various signal processing technologies, coupled with an explosion in the available computing power, have given rise to a number of novel human-computer interaction (HCI) modalities: speech, vision-based gesture recognition, eye tracking, electroencephalograph, etc. Successful embodiment of these modalities into an interface has the potential of easing the HCI bottleneck that has become noticeable with the advances in computing and communication. It has also become increasingly evident that the difficulties encountered in the analysis and interpretation of individual sensing modalities may be overcome by integrating them into a multimodal human-computer interface. We examine several promising directions toward achieving multimodal HCI. We consider some of the emerging novel input modalities for HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level. We discuss the different computational approaches that may be applied at the different levels of modality integration. We also briefly review several demonstrated multimodal HCI systems and applications. Despite all the recent developments, it is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI. This research can benefit from many disparate fields of study that increase our understanding of the different human communication modalities and their potential role in HCI.},
  keywords = {based-on-vision,type:survey},
  annotation = {341 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UDDF98WU/Sharma et al. - 1998 - Toward multimodal human-computer interface.pdf}
}

@article{shengDeepSpatialTemporal2020,
  title = {Deep {{Spatial}}\textendash{{Temporal Model Based Cross-Scene Action Recognition Using Commodity WiFi}}},
  author = {Sheng, Biyun and Xiao, Fu and Sha, Letian and Sun, Lijuan},
  year = {2020},
  month = apr,
  journal = {IEEE Internet of Things Journal},
  volume = {7},
  number = {4},
  pages = {3592--3601},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2020.2973272},
  url = {https://ieeexplore.ieee.org/document/8993738/},
  urldate = {2023-07-12},
  abstract = {With the popularization of Internet-of-Things (IoT) systems, passive action recognition on channel state information (CSI) has attracted much attention. Most conventional work under the machine-learning framework utilizes handcrafted features (e.g., statistic features) that are unable to sufficiently describe the sequence data and heavily rely on designers' experiences. Therefore, how to automatically learn abundant spatial\textendash temporal information from CSI data is a topic worthy of study. In this article, we propose a deep learning framework that integrates spatial features learned from the convolutional neural network (CNN) into the temporal model multilayer bidirectional long short-term memory (Bi-LSTM). Specifically, CSI streams are segmented into a series of patches, from which spatial features are extracted by our designed CNN structure. Considering long-term dependencies between adjacent sequences, the fully connected layer of CNN for each patch is taken as the Bi-LSTM sequential input to further capture temporal features. Our model is appealing in that it can simultaneously learn temporal dynamics and convolutional perceptual representations. To the best of our knowledge, this is the first work to explore deep spatial\textendash temporal features for CSI-based action recognition. Furthermore, in order to solve the problem that the trained model fully fails with environmental changes, we use the off-the-shelf model as the pretrained model and fine-tune it in the new scenario. The transfer method is able to realize cross-scene action recognition with low computational consumption and satisfactory accuracy. We carry out experiments on indoor data and the experimental results validate the effectiveness of our algorithm.},
  keywords = {based-on-wifi,model:cnn,model:lstm,tech:wifi},
  annotation = {36 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{shenWiPassCSIbasedKeystroke2020,
  title = {{{WiPass}}: {{CSI-based Keystroke Recognition}} for {{Numerical Keypad}} of {{Smartphones}}},
  shorttitle = {{{WiPass}}},
  author = {Shen, Xingfa and Yan, Guo and Yang, Jian and Xu, Sheng},
  year = {2020},
  month = oct,
  journal = {2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
  pages = {276--283},
  publisher = {{IEEE}},
  address = {{Zhanjiang, China}},
  doi = {10.1109/YAC51587.2020.9337673},
  url = {https://ieeexplore.ieee.org/document/9337673/},
  urldate = {2023-07-12},
  abstract = {Nowadays, smartphones are everywhere. They play an indispensable role in our lives and makes people convenient to communicate, pay, socialize, etc. However, they also bring a lot of security and privacy risks. Keystroke operations of numeric keypad are often required when users input password to perform mobile payment or input other privacy-sensitive information. Different keystrokes may cause different finger movements that will bring different interference to WiFi signal, which may be reflected by channel state information (CSI). In this paper, we propose WiPass, a password-keystroke recognition system for numerical keypad input on smartphones, which especially occurs frequently in mobile payment APPs. Based on only a public WiFi hotspot deployed in the victim payment scenario, WiPass would extracts and analyzes the CSI data generated by the password-keystroke operation of the smartphone user, and infers the user's payment password by comparing the CSI waveforms of different keystrokes. We implemented the WiPass system by using COTS WiFi AP devices and smartphones. The average keystroke segmentation accuracy was 80.45\%, and the average keystroke recognition accuracy was 74.24\%.},
  isbn = {9781728176840},
  keywords = {app:activity-inference,app:password-inference,based-on-wifi,tech:wifi},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{shyamalaSURVEYVISIONBASED2014,
  title = {A {{SURVEY OF VISION BASED HAND GESTURE RECOGNITION}}},
  author = {Shyamala, M.},
  year = {2014},
  url = {https://www.semanticscholar.org/paper/A-SURVEY-OF-VISION-BASED-HAND-GESTURE-RECOGNITION-Shyamala/01fb6988163209f16021dec3320956de74a8f45d},
  urldate = {2023-07-11},
  abstract = {Gesture recognition is to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. Hand Gestures have greater importance in designing an intelligent and efficient human\textendash computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition, gesture taxonomies, representations. Vision based Gesture recognition has the potential to be a natural and powerful tool supporting efficient and intuitive interaction between the human and the computer. Visual interpretation of hand gestures can help in achieving the ease and naturalness desired for Human Computer Interaction (HCI). It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition.},
  keywords = {movement:dynamic,movement:static,tech:rgb,type:survey}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {J. Mach. Learn. Res.},
  volume = {15},
  pages = {1929--1958},
  keywords = {background,dropout,from:cite.bib}
}

@article{starnerRealtimeAmericanSign1995,
  title = {Real-Time {{American Sign Language}} Recognition from Video Using Hidden {{Markov}} Models},
  author = {Starner, T. and Pentland, A.},
  year = {1995},
  journal = {Proceedings of International Symposium on Computer Vision - ISCV},
  pages = {265--270},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Coral Gables, FL, USA}},
  doi = {10.1109/ISCV.1995.477012},
  url = {http://ieeexplore.ieee.org/document/477012/},
  urldate = {2023-06-22},
  abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
  isbn = {9780818671906},
  keywords = {app:american-sl,app:sign-language,author:starner,based-on-vision,model:hmm,tech:rgb,type:seminal},
  annotation = {994 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/KBBCZYDF/Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf}
}

@article{starnerRealtimeAmericanSign1998,
  title = {Real-Time {{American}} Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, T. and Weaver, J. and Pentland, A.},
  year = {Dec./1998},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1371--1375},
  issn = {01628828},
  doi = {10.1109/34.735811},
  url = {http://ieeexplore.ieee.org/document/735811/},
  urldate = {2023-06-22},
  abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  keywords = {app:sign-language,author:starner,based-on-vision,model:hmm,type:seminal},
  annotation = {1394 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/JITL4ER8/Starner et al. - 1998 - Real-time American sign language recognition using.pdf}
}

@article{starnerVisualRecognitionAmerican1995,
  title = {Visual Recognition of American Sign Language Using Hidden Markov Models},
  author = {Starner, Thad and Pentland, Alex},
  year = {1995},
  month = may,
  abstract = {Abstract\textemdash We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American Sign Language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  keywords = {app:american-sl,app:sign-language,based-on-vision,classes:40,from:cite.bib,model:hmm,tech:rgb},
  file = {/Users/brk/Zotero/storage/5LD7H7UI/Starner - 1995 - Visual Recognition of American Sign Language Using.pdf}
}

@article{sturmanDesignMethodWholehand1993,
  title = {A Design Method for ``Whole-Hand'' Human-Computer Interaction},
  author = {Sturman, David J. and Zeltzer, David},
  year = {1993},
  month = jul,
  journal = {ACM Transactions on Information Systems},
  volume = {11},
  number = {3},
  pages = {219--238},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/159161.159159},
  url = {https://dl.acm.org/doi/10.1145/159161.159159},
  urldate = {2023-07-02},
  abstract = {A disciplined investigation of ``whole-hand interfaces (often glove based, currently) and their appropriate use for the control of complex task domains is embodied by the design method for whole-hand input. This is a series of procedures\textemdash including a common basis for the description, design, and evaluation of whole-hand input, together with an accompanying taxonomy\textemdash that enumerates key issues and points for consideration in the development of whole-hand input. The method helps designers focus on task requirements, isolate problem areas, and choose appropriate whole-hand input strategies for their specified tasks. Several experiments were conducted to validate and demonstrate the use of the design method. The results of the experiments are summarized and discussed.},
  langid = {english},
  keywords = {based-on-gloves,evaluation-of-whole-hand-input,type:seminal},
  annotation = {30 citations (Crossref) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/65J52Q6P/Sturman and Zeltzer - 1993 - A design method for whole-hand human-computer in.pdf}
}

@article{sturmanSurveyGlovebasedInput1994,
  title = {A Survey of Glove-Based Input},
  author = {Sturman, D.J. and Zeltzer, D.},
  year = {1994},
  month = jan,
  journal = {IEEE Computer Graphics and Applications},
  volume = {14},
  number = {1},
  pages = {30--39},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/38.250916},
  url = {https://ieeexplore.ieee.org/document/250916/},
  urldate = {2023-06-23},
  abstract = {Clumsy intermediary devices constrain our interaction with computers and their applications. Glove-based input devices let us apply our manual dexterity to the task. We provide a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input. The bulk of development in glove-based input has taken place very recently, and not all of it is easily accessible in the literature. We present a cross-section of the field to date. Hand-tracking devices may use the following technologies: position tracking, optical tracking, marker systems, silhouette analysis, magnetic tracking or acoustic tracking. Actual glove technologies on the market include: Sayre glove, MIT LED glove, Digital Data Entry Glove, DataGlove, Dexterous HandMaster, Power Glove, CyberGlove and Space Glove. Various applications of glove technologies include projects into the pursuit of natural interfaces, systems for understanding signed languages, teleoperation and robotic control, computer-based puppetry, and musical performance.{$<<$}ETX{$>>$}},
  keywords = {based-on-gloves,read-priority-1,type:seminal,type:survey},
  annotation = {853 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UDDK6KNT/Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf}
}

@misc{sturmanWholehandInput1992,
  title = {Whole-Hand {{Input}}},
  author = {Sturman, David Joel},
  year = {1992},
  url = {https://scholar.googleusercontent.com/scholar?q=cache:Yk_AfzsiGZsJ:scholar.google.com/+sturman+whole+hand+input&hl=en&as_sdt=0,5},
  urldate = {2023-06-28},
  abstract = {This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc},
  keywords = {based-on-gloves,read-priority-5,type:seminal},
  file = {/Users/brk/Zotero/storage/I4VQXME4/Whole-hand Input.pdf;/Users/brk/Zotero/storage/RG78DTFU/scholar.html}
}

@inproceedings{suarezHandGestureRecognition2012,
  title = {Hand Gesture Recognition with Depth Images: {{A}} Review},
  shorttitle = {Hand Gesture Recognition with Depth Images},
  booktitle = {2012 {{IEEE RO-MAN}}: {{The}} 21st {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}}},
  author = {Suarez, Jesus and Murphy, Robin R.},
  year = {2012},
  month = sep,
  pages = {411--417},
  issn = {1944-9437},
  doi = {10.1109/ROMAN.2012.6343787},
  abstract = {This paper presents a literature review on the use of depth for hand tracking and gesture recognition. The survey examines 37 papers describing depth-based gesture recognition systems in terms of (1) the hand localization and gesture classification methods developed and used, (2) the applications where gesture recognition has been tested, and (3) the effects of the low-cost Kinect and OpenNI software libraries on gesture recognition research. The survey is organized around a novel model of the hand gesture recognition process. In the reviewed literature, 13 methods were found for hand localization and 11 were found for gesture classification. 24 of the papers included real-world applications to test a gesture recognition system, but only 8 application categories were found (and three applications accounted for 18 of the papers). The papers that use the Kinect and the OpenNI libraries for hand tracking tend to focus more on applications than on localization and classification methods, and show that the OpenNI hand tracking method is good enough for the applications tested thus far. However, the limitations of the Kinect and other depth sensors for gesture recognition have yet to be tested in challenging applications and environments.},
  keywords = {tech:rgb,tech:rgbd,type:survey},
  annotation = {364 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/EYLG8BUK/6343787.html}
}

@article{sunWiDrawEnablingHandsfree2015,
  title = {{{WiDraw}}: {{Enabling Hands-free Drawing}} in the {{Air}} on {{Commodity WiFi Devices}}},
  shorttitle = {{{WiDraw}}},
  author = {Sun, Li and Sen, Souvik and Koutsonikolas, Dimitrios and Kim, Kyu-Han},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {77--89},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790129},
  url = {https://dl.acm.org/doi/10.1145/2789168.2790129},
  urldate = {2023-07-12},
  abstract = {This paper demonstrates that it is possible to leverage WiFi signals from commodity mobile devices to enable hands-free drawing in the air. While prior solutions require the user to hold a wireless transmitter, or require custom wireless hardware, or can only determine a pre-defined set of hand gestures, this paper introduces WiDraw, the first hand motion tracking system using commodity WiFi cards, and without any user wearables. WiDraw harnesses the Angle-of-Arrival values of incoming wireless signals at the mobile device to track the user's hand trajectory. We utilize the intuition that whenever the user's hand occludes a signal coming from a certain direction, the signal strength of the angle representing the same direction will experience a drop. Our software prototype using commodity wireless cards can track the user's hand with a median error lower than 5 cm. We use WiDraw to implement an in-air handwriting application that allows the user to draw letters, words, and sentences, and achieves a mean word recognition accuracy of 91\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,app:writing,based-on-wifi,tech:wifi},
  annotation = {261 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{supancicDepthBasedHandPose2015,
  title = {Depth-{{Based Hand Pose Estimation}}: {{Data}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Depth-{{Based Hand Pose Estimation}}},
  author = {Supancic, James S. and Rogez, Gregory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
  year = {2015},
  month = dec,
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  pages = {1868--1876},
  publisher = {{IEEE}},
  address = {{Santiago}},
  doi = {10.1109/ICCV.2015.217},
  url = {http://ieeexplore.ieee.org/document/7410574/},
  urldate = {2023-06-26},
  abstract = {Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.},
  isbn = {9781467383912},
  keywords = {type:survey},
  annotation = {155 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/HAGU84CM/Supancic et al. - 2015 - Depth-Based Hand Pose Estimation Data, Methods, a.pdf}
}

@article{takahashiHandGestureCoding1991,
  title = {Hand Gesture Coding Based on Experiments Using a Hand Gesture Interface Device},
  author = {Takahashi, Tomoichi and Kishino, Fumio},
  year = {1991},
  month = mar,
  journal = {ACM SIGCHI Bulletin},
  volume = {23},
  number = {2},
  pages = {67--74},
  issn = {0736-6906},
  doi = {10.1145/122488.122499},
  url = {https://dl.acm.org/doi/10.1145/122488.122499},
  urldate = {2023-07-13},
  abstract = {It would be ideal for computer-human interaction if a computer could understand human gestures. Hand gestures are one means of interaction between computers and humans[1][2]. A hand gesture interface device, the VPL Data Glove               TM               , provides real-time information on a user's hand movement[3].},
  langid = {english},
  keywords = {app:japanese-sl,app:sign-language,based-on-gloves,classes:46,hardware:dataglove,model:pca},
  annotation = {150 citations (Semantic Scholar/DOI) [2023-07-13]},
  file = {/Users/brk/Zotero/storage/WL8YGNIS/Takahashi and Kishino - 1991 - Hand gesture coding based on experiments using a h.pdf}
}

@inproceedings{takemuraEvaluation3DObject1989,
  title = {An Evaluation of 3-{{D}} Object Pointing Using a Field Sequential Stereoscopic Display},
  author = {Takemura, H. and Tomono, A. and Kobayashi, Yukio},
  year = {1989},
  month = dec,
  url = {https://www.semanticscholar.org/paper/An-evaluation-of-3-D-object-pointing-using-a-field-Takemura-Tomono/c92768bd2508b400fd7ba6be012fc3eb46caea6e},
  urldate = {2023-07-13},
  abstract = {Experiments to measure user performance in 3-D object pointing using a field sequential stereoscopic display are described . First , huma.n performance in adjusting a random dot stereogram depth is measured to determine the possibility of pointing at a 3-D object. This experiment shows that the image displayed on the field sequential stereoscopic display can give enough depth information to its user who inputs 3-D coordinate with a mouse. Next, user performance in pointing at 3-D objects using a mouse as an input device is measured . The target for 3-D pointing and an arrow shaped 3-D cursor are in the form of wire frames and displayed stereoscopically. Finally, user performance using a 3-D magnetic tracking input device is tested. These experiments show that it is possible to point at a 3-D object on a field sequential stereoscopic display with relatively high accuracy. However, pointing at objects in depth takes much more time than pointing at objects in a plane. When a mouse is used , the required pointing time heavily depends on the direction of pointing. This tendency is due to the difficulties of depth detection using the field sequential stereoscopic display, and of manipulating a 3-D cursor with a 2-D input device. For the task tested, the 3-D magnetic tracking device was better in terms of the task completion time and error rate.},
  keywords = {based-on-gloves,hardware:dataglove},
  file = {/Users/brk/Zotero/storage/94XM6AL6/Takemura et al. - 1989 - An evaluation of 3-D object pointing using a field.pdf}
}

@article{tanWiFingerLeveragingCommodity2016,
  title = {{{WiFinger}}: Leveraging Commodity {{WiFi}} for Fine-Grained Finger Gesture Recognition},
  shorttitle = {{{WiFinger}}},
  author = {Tan, Sheng and Yang, Jie},
  year = {2016},
  month = jul,
  journal = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {201--210},
  publisher = {{ACM}},
  address = {{Paderborn Germany}},
  doi = {10.1145/2942358.2942393},
  url = {https://dl.acm.org/doi/10.1145/2942358.2942393},
  urldate = {2023-07-12},
  abstract = {Gesture recognition has become increasingly important in human-computer interaction (HCI) and can support a broad array of emerging applications, such as smart home, virtual reality, and mobile gaming. Traditional approaches usually rely on dedicated sensors that are worn by the user or cameras that require line of sight. In this paper, we present fine-grained finger gesture recognition by using a single commodity WiFi device without requiring user to wear any sensors. Our low-cost system, WiFinger, takes advantages of the fine-grained Channel State Information (CSI) available from commodity WiFi devices and the prevalence of WiFi network infrastructures. It senses and identifies subtle movements of finger gestures by examining the unique patterns exhibited in the detailed CSI. In WiFigner, we devise environmental noise removal mechanism to mitigate the effect of signal dynamic due to the environment changes. Moreover, we propose to capture the intrinsic gesture behavior to deal with individual diversity and gesture inconsistency. Our experimental evaluation in both home and office environments demonstrates that our system can achieve over 93\% recognition accuracy and is robust to both environment changes and individual diversity. Results also show that our system can work with WiFi beacon signals and provides accurate gesture recognition under NLOS scenarios.},
  isbn = {9781450341844},
  langid = {english},
  keywords = {based-on-wifi,model:dtw,tech:wifi},
  annotation = {234 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@misc{texasinstrumentsCD74HC4067DataSheet2003,
  title = {{{CD74HC4067}} Data Sheet, Product Information and Support | {{TI}}.Com},
  author = {{Texas Instruments}},
  year = {2003},
  month = jul,
  journal = {CD74HC4067 data sheet, product information and support | TI.com},
  publisher = {{www.ti.com}},
  url = {https://www.ti.com/product/CD74HC4067},
  keywords = {from:cite.bib,type:datasheet}
}

@article{thariqahmedDeviceFreeHuman2020,
  title = {Device Free Human Gesture Recognition Using {{Wi-Fi CSI}}: {{A}} Survey},
  shorttitle = {Device Free Human Gesture Recognition Using {{Wi-Fi CSI}}},
  author = {Thariq Ahmed, Hasmath Farhana and Ahmad, Hafisoh and C.V., Aravind},
  year = {2020},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {87},
  pages = {103281},
  issn = {09521976},
  doi = {10.1016/j.engappai.2019.103281},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197619302441},
  urldate = {2023-07-12},
  abstract = {Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi,type:survey},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@misc{thepandasdevelopmentteamPandasdevPandasPandas2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {{The Pandas Development Team}},
  year = {2020},
  month = feb,
  doi = {10.5281/zenodo.3509134},
  url = {https://doi.org/10.5281/zenodo.3509134},
  howpublished = {Zenodo},
  keywords = {from:cite.bib,type:software-lib}
}

@misc{thomasa.defantiUSNEAR60341631977,
  title = {{{US NEA R60-34-163 FINAL PROJECT REPORT}}},
  author = {{Thomas A. DeFanti} and {Daniel J. Sandin}},
  year = {1977},
  publisher = {{University of Illinois at Chicago Circle}},
  keywords = {hardware:sayre-glove,tech:optical-tubes},
  file = {/Users/brk/Zotero/storage/V95YJFCN/Thomas A. DeFanti and Daniel J. Sandin - 1977 - US NEA R60-34-163 FINAL PROJECT REPORT.pdf}
}

@misc{todo,
  author = {{TODO}}
}

@inproceedings{tuulariSoapBoxPlatformUbiquitous2002,
  title = {{{SoapBox}}: {{A Platform}} for {{Ubiquitous Computing Research}} and {{Applications}}},
  shorttitle = {{{SoapBox}}},
  booktitle = {Pervasive {{Computing}}},
  author = {Tuulari, Esa and {Ylisaukko-oja}, Arto},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Mattern, Friedemann and Naghshineh, Mahmoud},
  year = {2002},
  volume = {2414},
  pages = {125--138},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45866-2\_11},
  url = {http://link.springer.com/10.1007/3-540-45866-2_11},
  urldate = {2023-07-11},
  abstract = {Designing, implementing and evaluating prototypes is a normal way of doing technical research. In recent years we have seen lots of research prototypes specifically designed for context awareness, future user interfaces and intelligent environment research. The problem with this type of specialised prototypes is that their lifetime is rather short and the valuable work done for them is not easily reusable. Our approach has been different as we have deliberately aimed towards a multipurpose platform that would be suitable for various ubiquitous computing related research themes. In this article we present the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box). Its main features are wired and wireless communications, in-built sensors, small size and low power consumption. We also introduce some results of research projects that have already used the platform successfully. Finally we conclude the paper with application scenarios for further work.},
  isbn = {978-3-540-44060-4 978-3-540-45866-1},
  keywords = {based-on-gloves,from:cite.bib,hardware:unique,pdf:paywalled,tech:accelerometer},
  annotation = {74 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{urrehmanDynamicHandGesture2022,
  title = {Dynamic {{Hand Gesture Recognition Using 3D-CNN}} and {{LSTM Networks}}},
  author = {Ur Rehman, Muneeb and Ahmed, Fawad and Attique Khan, Muhammad and Tariq, Usman and Abdulaziz Alfouzan, Faisal and M. Alzahrani, Nouf and Ahmad, Jawad},
  year = {2022},
  journal = {Computers, Materials \& Continua},
  volume = {70},
  number = {3},
  pages = {4675--4690},
  issn = {1546-2226},
  doi = {10.32604/cmc.2022.019586},
  url = {https://www.techscience.com/cmc/v70n3/44942},
  urldate = {2023-06-22},
  abstract = {: Recognition of dynamic hand gestures in real-time is a difficult task because the system can never know when or from where the gesture starts and ends in a video stream. Many researchers have been working on vision-based gesture recognition due to its various applications. This paper proposes a deep learning architecture based on the combination of a 3D Convolutional Neural Network (3D-CNN) and a Long Short-Term Memory (LSTM) network. The proposed architecture extracts spatial-temporal information from video sequences input while avoiding extensive computation. The 3D-CNN is used for the extraction of spectral and spatial features which are then given to the LSTM network through which classification is carried out. The proposed model is a light-weight architecture with only 3.7 million training parameters. The model has been evaluated on 15 classes from the 20BN-jester dataset available publicly. The model was trained on 2000 video-clips per class which were separated into 80\% training and 20\% validation sets. An accuracy of 99\% and 97\% was achieved on training and testing data, respectively. We further show that the combination of 3D-CNN with LSTM gives superior results as compared to MobileNetv2 + LSTM.},
  langid = {english},
  keywords = {dataset:20bn-jester,model:cnn,model:lstm,movement:dynamic,tech:rgb},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/7NE5RC22/Ur Rehman et al. - 2022 - Dynamic Hand Gesture Recognition Using 3D-CNN and .pdf}
}

@inproceedings{vamplewRecognitionSignLanguage2007,
  title = {Recognition of Sign Language Gestures Using Neural Networks},
  booktitle = {Neuropsychological {{Trends}}},
  author = {Vamplew, Simon},
  year = {2007},
  month = apr,
  number = {1},
  pages = {4},
  issn = {1970321X, 19703201},
  doi = {10.7358/neur-2007-001-vamp},
  url = {http://www.ledonline.it/NeuropsychologicalTrends/},
  urldate = {2023-07-02},
  abstract = {This paper describes the structure and performance of the SLARTI sign language recognition system developed at the University of Tasmania. SLARTI uses a modular architecture consisting of multiple feature-recognition neural networks and a nearest-neighbour classifier to recognise Australian sign language (Auslan) hand gestures.},
  keywords = {app:australian-sl,app:sign-language,model:ffnn,model:knn,movement:dynamic,pdf:cant-find},
  annotation = {93 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@book{vanrossumPythonReferenceManual2009,
  title = {Python 3 {{Reference Manual}}},
  author = {Van Rossum, Guido and Drake, Fred L.},
  year = {2009},
  publisher = {{CreateSpace}},
  address = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7},
  keywords = {from:cite.bib,type:software-lib}
}

@article{vasconezHandGestureRecognition2022,
  title = {Hand {{Gesture Recognition Using EMG-IMU Signals}} and {{Deep Q-Networks}}},
  author = {V{\'a}sconez, Juan Pablo and Barona L{\'o}pez, Lorena Isabel and Valdivieso Caraguay, {\'A}ngel Leonardo and Benalc{\'a}zar, Marco E.},
  year = {2022},
  month = dec,
  journal = {Sensors},
  volume = {22},
  number = {24},
  pages = {9613},
  issn = {1424-8220},
  doi = {10.3390/s22249613},
  url = {https://www.mdpi.com/1424-8220/22/24/9613},
  urldate = {2023-03-07},
  abstract = {Hand gesture recognition systems (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) have been studied for different applications in recent years. Most commonly, cutting-edge HGR methods are based on supervised machine learning methods. However, the potential benefits of reinforcement learning (RL) techniques have shown that these techniques could be a viable option for classifying EMGs. Methods based on RL have several advantages such as promising classification performance and online learning from experience. In this work, we developed an HGR system made up of the following stages: pre-processing, feature extraction, classification, and post-processing. For the classification stage, we built an RL-based agent capable of learning to classify and recognize eleven hand gestures\textemdash five static and six dynamic\textemdash using a deep Q-network (DQN) algorithm based on EMG and IMU information. The proposed system uses a feed-forward artificial neural network (ANN) for the representation of the agent policy. We carried out the same experiments with two different types of sensors to compare their performance, which are the Myo armband sensor and the G-force sensor. We performed experiments using training, validation, and test set distributions, and the results were evaluated for user-specific HGR models. The final accuracy results demonstrated that the best model was able to reach up to 97.50\%{$\pm$}1.13\% and 88.15\%{$\pm$}2.84\% for the classification and recognition, respectively, with regard to static gestures, and 98.95\%{$\pm$}0.62\% and 90.47\%{$\pm$}4.57\% for the classification and recognition, respectively, with regard to dynamic gestures with the Myo armband sensor. The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.},
  langid = {english},
  keywords = {tech:emg},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/AJ2WMLPJ/Vsconez et al. - 2022 - Hand Gesture Recognition Using EMG-IMU Signals and.pdf}
}

@inproceedings{vasishtDecimeterLevelLocalizationSingle2016,
  title = {Decimeter-{{Level Localization}} with a {{Single WiFi Access Point}}},
  booktitle = {Symposium on {{Networked Systems Design}} and {{Implementation}}},
  author = {Vasisht, Deepak and Kumar, Swarun and Katabi, D.},
  year = {2016},
  month = mar,
  url = {https://www.semanticscholar.org/paper/Decimeter-Level-Localization-with-a-Single-WiFi-Vasisht-Kumar/3e204852e9315efe3df10b831246471cc52a8655},
  urldate = {2023-07-12},
  abstract = {We present Chronos, a system that enables a single WiFi access point to localize clients to within tens of centimeters. Such a system can bring indoor positioning to homes and small businesses which typically have a single access point.    The key enabler underlying Chronos is a novel algorithm that can compute sub-nanosecond time-of-flight using commodity WiFi cards. By multiplying the time-of-flight with the speed of light, a MIMO access point computes the distance between each of its antennas and the client, hence localizing it. Our implementation on commodity WiFi cards demonstrates that Chronos's accuracy is comparable to state-of-the-art localization systems, which use four or five access points.},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi}
}

@article{virmaniPositionOrientationAgnostic2017,
  title = {Position and {{Orientation Agnostic Gesture Recognition Using WiFi}}},
  author = {Virmani, Aditya and Shahzad, Muhammad},
  year = {2017},
  month = jun,
  journal = {Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
  pages = {252--264},
  publisher = {{ACM}},
  address = {{Niagara Falls New York USA}},
  doi = {10.1145/3081333.3081340},
  url = {https://dl.acm.org/doi/10.1145/3081333.3081340},
  urldate = {2023-07-12},
  abstract = {WiFi based gesture recognition systems have recently proliferated due to the ubiquitous availability of WiFi in almost every modern building. The key limitation of existing WiFi based gesture recognition systems is that they require the user to be in the same configuration (i.e., at the same position and in same orientation) when performing gestures at runtime as when providing training samples, which significantly restricts their practical usability. In this paper, we propose a WiFi based gesture recognition system, namely WiAG, which recognizes the gestures of the user irrespective of his/her configuration. The key idea behind WiAG is that it first requests the user to provide training samples for all gestures in only one configuration and then automatically generates virtual samples for all gestures in all possible configurations by applying our novel translation function on the training samples. Next, for each configuration, it generates a classification model using virtual samples corresponding to that configuration. To recognize gestures of a user at runtime, as soon as the user performs a gesture, WiAG first automatically estimates the configuration of the user and then evaluates the gesture against the classification model corresponding to that estimated configuration. Our evaluation results show that when user's configuration is not the same at runtime as at the time of providing training samples, WiAG significantly improves the gesture recognition accuracy from just 51.4\% to 91.4\%.},
  isbn = {9781450349284},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,classes:6,tech:wifi},
  annotation = {161 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/XLDUCJ4U/Virmani and Shahzad - 2017 - Position and Orientation Agnostic Gesture Recognit.pdf}
}

@article{viterbiErrorBoundsConvolutional1967,
  title = {Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm},
  author = {Viterbi, A.},
  year = {1967},
  month = apr,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {2},
  pages = {260--269},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.1967.1054010},
  url = {http://ieeexplore.ieee.org/document/1054010/},
  urldate = {2023-07-11},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R\_\{0\} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R\_\{0\} and whose performance bears certain similarities to that of sequential decoding algorithms.},
  keywords = {background,model:hmm,type:seminal,viterbi},
  annotation = {5898 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{vuleticSystematicLiteratureReview2019,
  title = {Systematic Literature Review of Hand Gestures Used in Human Computer Interaction Interfaces},
  author = {Vuletic, Tijana and Duffy, Alex and Hay, Laura and McTeague, Chris and Campbell, Gerard and Grealy, Madeleine},
  year = {2019},
  month = sep,
  journal = {International Journal of Human-Computer Studies},
  volume = {129},
  pages = {74--94},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2019.03.011},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581918305676},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "Systematic literature review of hand gestures used in human computer interaction interfaces" by T. Vuletic et al.},
  langid = {english},
  keywords = {have-read,type:survey},
  annotation = {82 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/7VJ7GZXV/Vuletic et al. - 2019 - Systematic literature review of hand gestures used.pdf}
}

@article{wachsVisionbasedHandgestureApplications2011,
  title = {Vision-Based Hand-Gesture Applications},
  author = {Wachs, Juan Pablo and K{\"o}lsch, Mathias and Stern, Helman and Edan, Yael},
  year = {2011},
  month = feb,
  journal = {Communications of the ACM},
  volume = {54},
  number = {2},
  pages = {60--71},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1897816.1897838},
  url = {https://dl.acm.org/doi/10.1145/1897816.1897838},
  urldate = {2023-07-11},
  abstract = {there is strong evidence that future humancomputer interfaces will enable more natural, intuitive communication between people and all kinds of sensor-based devices, thus more closely resembling human-human communication. Progress in the field of human-computer interaction has introduced innovative technologies that empower users to interact with computer systems in increasingly natural and intuitive ways; systems adopting them show increased efficiency, speed, power, and realism. However, users comfortable with traditional interaction methods like mice and keyboards are often unwilling to embrace new, alternative interfaces. Ideally, new interface technologies should be more accessible without requiring long periods of learning and adaptation. They should also provide more natural human-machine communication.},
  langid = {english},
  keywords = {tech:rgb,type:survey},
  annotation = {653 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/AGVRM526/Wachs et al. - 2011 - Vision-based hand-gesture applications.pdf}
}

@article{wanExploreEfficientLocal2016,
  title = {Explore {{Efficient Local Features}} from {{RGB-D Data}} for {{One-Shot Learning Gesture Recognition}}},
  author = {Wan, Jun and Guo, Guodong and Li, Stan Z.},
  year = {2016},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {8},
  pages = {1626--1639},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2015.2513479},
  url = {https://ieeexplore.ieee.org/document/7368923/},
  urldate = {2023-06-22},
  abstract = {Availability of handy RGB-D sensors has brought about a surge of gesture recognition research and applications. Among various approaches, one shot learning approach is advantageous because it requires minimum amount of data. Here, we provide a thorough review about one-shot learning gesture recognition from RGB-D data and propose a novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK). In the review, we analyze the challenges that we are facing, and point out some future research directions which may enlighten researchers in this field. The proposed MFSK feature is robust and invariant to scale, rotation and partial occlusions. To alleviate the insufficiency of one shot training samples, we augment the training samples by artificially synthesizing versions of various temporal scales, which is beneficial for coping with gestures performed at varying speed. We evaluate the proposed method on the Chalearn gesture dataset (CGD). The results show that our approach outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets. When applied to the RGB-D datasets that are not one-shot (e.g., the Cornell Activity Dataset-60 and MSR Daily Activity 3D dataset), the proposed feature also produces very promising results under leave-one-out cross validation or one-shot learning.},
  keywords = {app:medical,app:robot-control,app:surgery,dataset:chalearn-gesture,dataset:cornell-activity-60,dataset:msr-daily-activity-3d,tech:rgbd},
  annotation = {93 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{wangCSIbasedHumanSensing2021,
  title = {{{CSI-based}} Human Sensing Using Model-Based Approaches: A Survey},
  shorttitle = {{{CSI-based}} Human Sensing Using Model-Based Approaches},
  author = {Wang, Zhengjie and Huang, Zehua and Zhang, Chengming and Dou, Wenwen and Guo, Yinjing and Chen, Da},
  year = {2021},
  month = apr,
  journal = {Journal of Computational Design and Engineering},
  volume = {8},
  number = {2},
  pages = {510--523},
  issn = {2288-5048},
  doi = {10.1093/jcde/qwab003},
  url = {https://academic.oup.com/jcde/article/8/2/510/6137731},
  urldate = {2023-07-12},
  abstract = {Abstract             Currently, human sensing draws much attention in the field of ubiquitous computing, and human sensing based on WiFi CSI (channel state information) becomes a hot research topic due to the easy deployment and availability of WiFi devices. Although various human sensing applications based on the CSI signal model are emerging, the model-based approach has not been studied thoroughly. This paper provides a comprehensive survey of the latest model-based human sensing methods and their applications. First, the CSI signal and framework of model-based human sensing methods are introduced. Then, related models and fundamental signal preprocessing techniques are described. Next, typical human sensing applications are investigated, and the crucial characteristics are summarized. Finally, the advantages, limitations, and future research trends of model-based human sensing methods are concluded in this paper.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi,type:survey},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/JW8EU7FV/Wang et al. - 2021 - CSI-based human sensing using model-based approach.pdf}
}

@article{wangDeviceFreeHumanActivity2017,
  title = {Device-{{Free Human Activity Recognition Using Commercial WiFi Devices}}},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
  year = {2017},
  month = may,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {35},
  number = {5},
  pages = {1118--1131},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2017.2679658},
  url = {http://ieeexplore.ieee.org/document/7875148/},
  urldate = {2023-07-12},
  abstract = {Since human bodies are good reflectors of wireless signals, human activities can be recognized by monitoring changes in WiFi signals. However, existing WiFi-based human activity recognition systems do not build models that can quantify the correlation between WiFi signal dynamics and human activities. In this paper, we propose a Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM). CARM is based on two theoretical models. First, we propose a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds. Second, we propose a CSI-activity model that quantifies the relation between human movement speeds and human activities. Based on these two models, we implemented the CARM on commercial WiFi devices. Our experimental results show that the CARM achieves recognition accuracy of 96\% and is robust to environmental changes.},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {293 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{wangEeyesDevicefreeLocationoriented2014,
  title = {E-Eyes: Device-Free Location-Oriented Activity Identification Using Fine-Grained {{WiFi}} Signatures},
  shorttitle = {E-Eyes},
  author = {Wang, Yan and Liu, Jian and Chen, Yingying and Gruteser, Marco and Yang, Jie and Liu, Hongbo},
  year = {2014},
  month = sep,
  journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
  pages = {617--628},
  publisher = {{ACM}},
  address = {{Maui Hawaii USA}},
  doi = {10.1145/2639108.2639143},
  url = {https://dl.acm.org/doi/10.1145/2639108.2639143},
  urldate = {2023-07-12},
  abstract = {Activity monitoring in home environments has become increasingly important and has the potential to support a broad array of applications including elder care, well-being management, and latchkey child safety. Traditional approaches involve wearable sensors and specialized hardware installations. This paper presents device-free location-oriented activity identification at home through the use of existing WiFi access points and WiFi devices (e.g., desktops, thermostats, refrigerators, smartTVs, laptops). Our low-cost system takes advantage of the ever more complex web of WiFi links between such devices and the increasingly fine-grained channel state information that can be extracted from such links. It examines channel features and can uniquely identify both in-place activities and walking movements across a home by comparing them against signal profiles. Signal profiles construction can be semi-supervised and the profiles can be adaptively updated to accommodate the movement of the mobile devices and day-to-day signal calibration. Our experimental evaluation in two apartments of different size demonstrates that our approach can achieve over 96\% average true positive rate and less than 1\% average false positive rate to distinguish a set of in-place and walking activities with only a single WiFi access point. Our prototype also shows that our system can work with wider signal band (802.11ac) with even higher accuracy.},
  isbn = {9781450327831},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {746 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{wangEvaluationLocalSpatiotemporal2009,
  title = {Evaluation of Local Spatio-Temporal Features for Action Recognition},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2009},
  author = {Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  year = {2009},
  pages = {124.1-124.11},
  publisher = {{British Machine Vision Association}},
  address = {{London}},
  doi = {10.5244/C.23.124},
  url = {http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html},
  urldate = {2023-07-11},
  abstract = {Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.},
  isbn = {978-1-901725-39-1},
  langid = {english},
  keywords = {based-on-vision,classes:25,model:svm,type:seminal},
  annotation = {1494 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/QFB8QYCI/Wang et al. - 2009 - Evaluation of local spatio-temporal features for a.pdf}
}

@article{wangGaitRecognitionUsing2016,
  title = {Gait Recognition Using Wifi Signals},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {363--373},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971670},
  url = {https://dl.acm.org/doi/10.1145/2971648.2971670},
  urldate = {2023-07-12},
  abstract = {In this paper, we propose WifiU, which uses commercial WiFi devices to capture fine-grained gait patterns to recognize humans. The intuition is that due to the differences in gaits of different people, the WiFi signal reflected by a walking human generates unique variations in the Channel State Information (CSI) on the WiFi receiver. To profile human movement using CSI, we use signal processing techniques to generate spectrograms from CSI measurements so that the resulting spectrograms are similar to those generated by specifically designed Doppler radars. To extract features from spectrograms that best characterize the walking pattern, we perform autocorrelation on the torso reflection to remove imperfection in spectrograms. We evaluated WifiU on a dataset with 2,800 gait instances collected from 50 human subjects walking in a room with an area of 50 square meters. Experimental results show that WifiU achieves top-1, top-2, and top-3 recognition accuracies of 79.28\%, 89.52\%, and 93.05\%, respectively.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,based-on-wifi,participants:50,tech:wifi},
  annotation = {420 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{wangHumanRespirationDetection2016,
  title = {Human Respiration Detection with Commodity Wifi Devices: Do User Location and Body Orientation Matter?},
  shorttitle = {Human Respiration Detection with Commodity Wifi Devices},
  author = {Wang, Hao and Zhang, Daqing and Ma, Junyi and Wang, Yasha and Wang, Yuxiang and Wu, Dan and Gu, Tao and Xie, Bing},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971744},
  url = {https://dl.acm.org/doi/10.1145/2971648.2971744},
  urldate = {2023-07-12},
  abstract = {Recent research has demonstrated the feasibility of detecting human respiration rate non-intrusively leveraging commodity WiFi devices. However, is it always possible to sense human respiration no matter where the subject stays and faces? What affects human respiration sensing and what's the theory behind? In this paper, we first introduce the Fresnel model in free space, then verify the Fresnel model for WiFi radio propagation in indoor environment. Leveraging the Fresnel model and WiFi radio propagation properties derived, we investigate the impact of human respiration on the receiving RF signals and develop the theory to relate one's breathing depth, location and orientation to the detectability of respiration. With the developed theory, not only when and why human respiration is detectable using WiFi devices become clear, it also sheds lights on understanding the physical limit and foundation of WiFi-based sensing systems. Intensive evaluations validate the developed theory and case studies demonstrate how to apply the theory to the respiration monitoring system design.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:activity-inference,app:breathing-detection,based-on-wifi,tech:wifi},
  annotation = {339 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{wangPlacementMattersUnderstanding2022,
  title = {Placement {{Matters}}: {{Understanding}} the {{Effects}} of {{Device Placement}} for {{WiFi Sensing}}},
  shorttitle = {Placement {{Matters}}},
  author = {Wang, Xuanzhi and Niu, Kai and Xiong, Jie and Qian, Bochong and Yao, Zhiyun and Lou, Tairong and Zhang, Daqing},
  year = {2022},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {6},
  number = {1},
  pages = {1--25},
  issn = {2474-9567},
  doi = {10.1145/3517237},
  url = {https://dl.acm.org/doi/10.1145/3517237},
  urldate = {2023-07-12},
  abstract = {WiFi-based contactless sensing has found numerous applications in the fields of smart home and health care owning to its low-cost, non-intrusive and privacy-preserving characteristics. While promising in many aspects, the limited sensing range and interference issues still exist, hindering the adoption of WiFi sensing in real world. In this paper, inspired by the SNR (signal-to-noise ratio) metric in communication theory, we propose a new metric named SSNR (sensing-signal-to-noise-ratio) to quantify the sensing capability of WiFi systems. We theoretically model the effect of transmitter-receiver distance on sensing coverage. We show that in LoS scenario, the sensing coverage area increases first from a small oval to a maximal one and then decreases. When the transmitter-receiver distance further increases, the coverage area is separated into two ovals located around the two transceivers respectively. We demonstrate that, instead of applying complex signal processing scheme or advanced hardware, by just properly placing the transmitter and receiver, the two well-known issues in WiFi sensing (i.e., small range and severe interference) can be greatly mitigated. Specifically, by properly placing the transmitter and receiver, the coverage of human walking sensing can be expanded by around 200\%. By increasing the transmitter-receiver distance, a target's fine-grained respiration can still be accurately sensed with one interferer sitting just 0.5 m away.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@inproceedings{wangRealTimeLargeVocabulary2001,
  title = {A {{Real-Time Large Vocabulary Continuous Recognition System}} for {{Chinese Sign Language}}},
  booktitle = {Advances in {{Multimedia Information Processing}} \textemdash{} {{PCM}} 2001},
  author = {Wang, Chunli and Gao, Wen and Xuan, Zhaoguo},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Shum, Heung-Yeung and Liao, Mark and Chang, Shih-Fu},
  year = {2001},
  volume = {2195},
  pages = {150--157},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45453-5\_20},
  url = {http://link.springer.com/10.1007/3-540-45453-5_20},
  urldate = {2023-07-02},
  abstract = {In this paper, a real-time system designed for recognizing continuous Chinese Sign Language (CSL) sentences with a 4800 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. The worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including states tying, still frame detecting and fast search algorithm. Experiments were carried out, and for real-time continuous sign recognition, the correct rate is over 90\%.},
  isbn = {978-3-540-42680-6 978-3-540-45453-3},
  keywords = {app:chinese-sl,app:sign-language,based-on-gloves,classes:4800,contains-more-references,hardware:cyberglove,model:hmm,pdf:paywalled,read-priority-1},
  annotation = {31 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{wangRTFallRealTimeContactless2017,
  title = {{{RT-Fall}}: {{A Real-Time}} and {{Contactless Fall Detection System}} with {{Commodity WiFi Devices}}},
  shorttitle = {{{RT-Fall}}},
  author = {Wang, Hao and Zhang, Daqing and Wang, Yasha and Ma, Junyi and Wang, Yuxiang and Li, Shengjie},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {16},
  number = {2},
  pages = {511--526},
  issn = {1536-1233},
  doi = {10.1109/TMC.2016.2557795},
  url = {http://ieeexplore.ieee.org/document/7458198/},
  urldate = {2023-07-12},
  abstract = {This paper presents the design and implementation of RT-Fall, a real-time, contactless, low-cost yet accurate indoor fall detection system using the commodity WiFi devices. RT-Fall exploits the phase and amplitude of the fine-grained Channel State Information (CSI) accessible in commodity WiFi devices, and for the first time fulfills the goal of segmenting and detecting the falls automatically in real-time, which allows users to perform daily activities naturally and continuously without wearing any devices on the body. This work makes two key technical contributions. First, we find that the CSI phase difference over two antennas is a more sensitive base signal than amplitude for activity recognition, which can enable very reliable segmentation of fall and fall-like activities. Second, we discover the sharp power profile decline pattern of the fall in the time-frequency domain and further exploit the insight for new feature extraction and accurate fall segmentation/detection. Experimental results in four indoor scenarios demonstrate that RT-fall consistently outperforms the state-of-the-art approach WiFall with 14~percent higher sensitivity and 10~percent higher specificity on average.},
  keywords = {app:activity-inference,app:fall-detection,based-on-wifi,tech:wifi},
  annotation = {390 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@misc{wangTimeSeriesClassification2016,
  title = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}: {{A Strong Baseline}}},
  shorttitle = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}},
  author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  year = {2016},
  month = dec,
  number = {arXiv:1611.06455},
  eprint = {1611.06455},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.06455},
  url = {http://arxiv.org/abs/1611.06455},
  urldate = {2023-05-13},
  abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
  archiveprefix = {arxiv},
  keywords = {background,time-series},
  annotation = {1024 citations (Semantic Scholar/arXiv) [2023-05-13]},
  file = {/Users/brk/Zotero/storage/F8CWCQYK/Wang et al. - 2016 - Time Series Classification from Scratch with Deep .pdf;/Users/brk/Zotero/storage/NUUI7HZK/1611.html}
}

@inproceedings{wangTrafficPoliceGesture2008,
  title = {Traffic {{Police Gesture Recognition}} Using {{Accelerometers}}},
  author = {Wang, Ben},
  year = {2008},
  url = {https://www.semanticscholar.org/paper/Traffic-Police-Gesture-Recognition-using-Wang/59715d7a74376e7a21ada77f3a7af6b854a35545},
  urldate = {2023-07-11},
  abstract = {When an automatic traffic light system is not used due to too heavy traffic, the traffic would be controlled by traffic police gesture. This paper is about the design of a system so that the traffic lights can follow the traffic police gestures. To simplify the system, a unique mapping between the traffic police gestures and the orientation and movement of hands is defined. The hand motion characters are extracted by fixing a 3-axis accelerometer on the back of each hand. A 2level hierarchical classifier is used to recognize the gestures. First the gestures are categorized into three groups, according to the movement of each hand. Then a gesture is recognized by comparing it with the predefined templates. This real-time recognition algorithm is implemented by a micro-controller. It is envisaged that this will help drivers.},
  keywords = {app:traffic-police,based-on-gloves,tech:accelerometer},
  file = {/Users/brk/Zotero/storage/69LXNATR/Wang - 2008 - Traffic Police Gesture Recognition using Accelerom.pdf}
}

@article{wangUnderstandingModelingWiFi2015,
  title = {Understanding and {{Modeling}} of {{WiFi Signal Based Human Activity Recognition}}},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {65--76},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790093},
  url = {https://dl.acm.org/doi/10.1145/2789168.2790093},
  urldate = {2023-07-12},
  abstract = {Some pioneer WiFi signal based human activity recognition systems have been proposed. Their key limitation lies in the lack of a model that can quantitatively correlate CSI dynamics and human activities. In this paper, we propose CARM, a CSI based human Activity Recognition and Monitoring system. CARM has two theoretical underpinnings: a CSI-speed model, which quantifies the correlation between CSI value dynamics and human movement speeds, and a CSI-activity model, which quantifies the correlation between the movement speeds of different human body parts and a specific human activity. By these two models, we quantitatively build the correlation between CSI value dynamics and a specific human activity. CARM uses this correlation as the profiling mechanism and recognizes a given activity by matching it to the best-fit profile. We implemented CARM using commercial WiFi devices and evaluated it in several different environments. Our results show that CARM achieves an average accuracy of greater than 96\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,tech:wifi},
  annotation = {790 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/CCUH4UTT/Wang et al. - 2015 - Understanding and Modeling of WiFi Signal Based Hu.pdf}
}

@inproceedings{wangUserindependentAccelerometerbasedGesture2013,
  title = {User-Independent Accelerometer-Based Gesture Recognition for Mobile Devices},
  booktitle = {{{ADCAIJ}}: {{Advances}} in {{Distributed Computing}} and {{Artificial Intelligence Journal}}},
  author = {Wang, Xian and Tarr{\'i}o, Paula and Bernardos, Ana Mar{\'i}a and Metola, Eduardo and Casar, Jos{\'e} Ram{\'o}n},
  year = {2013},
  month = jul,
  volume = {1},
  pages = {11--25},
  issn = {2255-2863},
  doi = {10.14201/ADCAIJ20121311125},
  url = {https://revistas.usal.es/index.php/2255-2863/article/view/ADCAIJ20121311125},
  urldate = {2023-03-07},
  abstract = {Many mobile devices embed nowadays inertial sensors. This enables new forms of human-computer interaction through the use of gestures (movements performed with the mobile device) as a way of communication. This paper presents an accelerometer-based gesture recognition system for mobile devices which is able to recognize a collection of 10 different hand gestures. The system was conceived to be light and to operate in a user-independent manner in real time. The recognition system was implemented in a smart phone and evaluated through a collection of user tests, which showed a recognition accuracy similar to other state-of-the art techniques and a lower computational complexity. The system was also used to build a human-robot interface that enables controlling a wheeled robot with the gestures made with the mobile phone},
  keywords = {app:robot-control,classes:10-29,tech:accelerometer},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/2YFHA534/Wang et al. - 2013 - User-independent accelerometer-based gesture recog.pdf}
}

@article{wangWeCanHear2014,
  title = {We Can Hear You with {{Wi-Fi}}!},
  author = {Wang, Guanhua and Zou, Yongpan and Zhou, Zimu and Wu, Kaishun and Ni, Lionel M.},
  year = {2014},
  month = sep,
  journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
  pages = {593--604},
  publisher = {{ACM}},
  address = {{Maui Hawaii USA}},
  doi = {10.1145/2639108.2639112},
  url = {https://dl.acm.org/doi/10.1145/2639108.2639112},
  urldate = {2023-07-12},
  abstract = {Recent literature advances Wi-Fi signals to ``see'' people's motions and locations. This paper asks the following question: Can Wi-Fi ``hear'' our talks? We present WiHear, which enables Wi-Fi signals to ``hear'' our talks without deploying any devices. To achieve this, WiHear needs to detect and analyze fine-grained radio reflections from mouth movements. WiHear solves this micro-movement detection problem by introducing Mouth Motion Profile that leverages partial multipath effects and wavelet packet transformation. Since Wi-Fi signals do not require line-of-sight, WiHear can ``hear'' people talks within the radio range. Further, WiHear can simultaneously ``hear'' multiple people's talks leveraging MIMO technology. We implement WiHear on both USRP N210 platform and commercial Wi-Fi infrastructure. Results show that within our pre-defined vocabulary, WiHear can achieve detection accuracy of 91 percent on average for single individual speaking no more than six words and up to 74 percent for no more than three people talking simultaneously. Moreover, the detection accuracy can be further improved by deploying multiple receivers from different angles.},
  isbn = {9781450327831},
  langid = {english},
  keywords = {app:activity-inference,app:speech-detection,based-on-wifi,read-priority-1,tech:wifi},
  annotation = {410 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{wangWiFallDeviceFreeFall2017,
  title = {{{WiFall}}: {{Device-Free Fall Detection}} by {{Wireless Networks}}},
  shorttitle = {{{WiFall}}},
  author = {Wang, Yuxi and Wu, Kaishun and Ni, Lionel M.},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {16},
  number = {2},
  pages = {581--594},
  issn = {1536-1233},
  doi = {10.1109/TMC.2016.2557792},
  url = {http://ieeexplore.ieee.org/document/7458186/},
  urldate = {2023-07-12},
  abstract = {The world population is in the midst of a unique and irreversible process of aging. Fall, which is one of the major health threats and obstacles to independent living of elders, will aggravate the global pressure in elders' health care and injury rescue. Thus, automatic fall detection is highly in need. Current proposed fall detection systems either need hardware installation or disrupt people's daily life. These limitations make it hard to widely deploy fall detection systems in residential settings. In this work, we analyze the wireless signal propagation model considering human activities influence. We then propose a novel and truly unobtrusive detection method based on the advanced wireless technologies, which we call as WiFall. WiFall employs the time variability and special diversity of Channel State Information (CSI) as the indicator of human activities. As CSI is readily available in prevalent in-use wireless infrastructures, WiFall withdraws the need for hardware modification, environmental setup and worn or taken devices. We implement WiFall on laptops equipped with commercial 802.11n NICs. Two typical indoor scenarios and several layout schemes are examined. As demonstrated by the experimental results, WiFall yielded 87\% detection precision with false alarm rate of 18\% in average.},
  keywords = {app:activity-inference,app:fall-detection,based-on-wifi,tech:wifi},
  annotation = {721 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{waskomSeabornStatisticalData2021,
  title = {Seaborn: Statistical Data Visualization},
  author = {Waskom, Michael L.},
  year = {2021},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {60},
  pages = {3021},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.03021},
  url = {https://doi.org/10.21105/joss.03021},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {1622 citations (Crossref) [2023-07-11] 1607 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{watsonSurveyGestureRecognition1993,
  title = {A {{Survey}} of {{Gesture Recognition Techniques}}},
  author = {Watson, R.},
  year = {1993},
  url = {https://www.semanticscholar.org/paper/A-Survey-of-Gesture-Recognition-Techniques-Watson/23ec699d9a8c19e3807c53f85c564b9cfa172fff},
  urldate = {2023-07-02},
  abstract = {Processing speeds have increased dramatically bitmapped displays allow graph ics to be rendered and updated at increasing rates and in general computers have advanced to the point where they can assist humans in complex tasks Yet input technologies seem to cause the major bottleneck in performing these tasks under utilising the available resources and restricting the expressiveness of application use We use our hands constantly to interact with things pick them up move them transform their shape or activate them in some way In the same uncon scious way we gesticulate in communicating fundamental ideas stop come closer over there no agreed and so on Gestures are thus a natural and intuitive form of both interaction and communication This report develops the motivations for gestural input and surveys current gesture recognition techniques A recognition technique under development at TCD as part of the GLAD IN ART EP project is also introduced},
  keywords = {based-on-gloves,based-on-vision,type:survey},
  file = {/Users/brk/Zotero/storage/EHEKTWGH/Watson - 1993 - A Survey of Gesture Recognition Techniques.pdf}
}

@inproceedings{weimerSyntheticVisualEnvironment1989,
  title = {A Synthetic Visual Environment with Hand Gesturing and Voice Input},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems {{Wings}} for the Mind - {{CHI}} '89},
  author = {Weimer, D. and Ganapathy, S. K.},
  year = {1989},
  pages = {235--240},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/67449.67495},
  url = {http://portal.acm.org/citation.cfm?doid=67449.67495},
  urldate = {2023-07-13},
  abstract = {This paper describes a practical synthetic visual environment for use in CAD and teleoperation. Instead of using expensive head mounted display systems, we use a standard display and compute smooth shaded images using an AT\&T Pixel Machine. The interface uses a VPL DataGlove [9] to track the hand, bringing the synthetic world into the same space as the hand. Hand gesturing is used to implement a virtual control panel, and some 3D modeling tasks. When simple speech recognition was added it markedly improved the interface. We also outline what extensions might be needed for using this kind of interface for teleoperation.},
  isbn = {978-0-89791-301-0},
  langid = {english},
  keywords = {based-on-gloves,hardware:dataglove},
  annotation = {138 citations (Semantic Scholar/DOI) [2023-07-13]}
}

@misc{WelcomeUCRTime,
  title = {Welcome to the {{UCR Time Series Classification}}/{{Clustering Page}}},
  url = {https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/},
  urldate = {2023-05-13},
  keywords = {dataset:ucr-time-series-classification,type:dataset},
  file = {/Users/brk/Zotero/storage/VML93JCD/time_series_data_2018.html}
}

@article{wenMachineLearningGlove2020,
  title = {Machine {{Learning Glove Using Self}}-{{Powered Conductive Superhydrophobic Triboelectric Textile}} for {{Gesture Recognition}} in {{VR}}/{{AR Applications}}},
  author = {Wen, Feng and Sun, Zhongda and He, Tianyiyi and Shi, Qiongfeng and Zhu, Minglu and Zhang, Zixuan and Li, Lianhui and Zhang, Ting and Lee, Chengkuo},
  year = {2020},
  month = jul,
  journal = {Advanced Science},
  volume = {7},
  number = {14},
  pages = {2000261},
  issn = {2198-3844, 2198-3844},
  doi = {10.1002/advs.202000261},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202000261},
  urldate = {2023-03-07},
  abstract = {The rapid progress of Internet of things (IoT) technology raises an imperative demand on human machine interfaces (HMIs) which provide a critical linkage between human and machines. Using a glove as an intuitive and low-cost HMI can expediently track the motions of human fingers, resulting in a straightforward communication media of human\textendash machine interactions. When combining several triboelectric textile sensors and proper machine learning technique, it has great potential to realize complex gesture recognition with the minimalist-designed glove for the comprehensive control in both real and virtual space. However, humidity or sweat may negatively affect the triboelectric output as well as the textile itself. Hence, in this work, a facile carbon nanotubes/thermoplastic elastomer (CNTs/TPE) coating approach is investigated in detail to achieve superhydrophobicity of the triboelectric textile for performance improvement. With great energy harvesting and human motion sensing capabilities, the glove using the superhydrophobic textile realizes a low-cost and self-powered interface for gesture recognition. By leveraging machine learning technology, various gesture recognition tasks are done in real time by using gestures to achieve highly accurate virtual reality/augmented reality (VR/AR) controls including gun shooting, baseball pitching, and flower arrangement, with minimized effect from sweat during operation.},
  langid = {english},
  keywords = {classes:{$<$}10,classes:9,model:cnn,tech:flex},
  annotation = {189 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/ETVGGB62/Wen et al. - 2020 - Machine Learning Glove Using SelfPowered Conducti.pdf}
}

@article{wheatlandStateArtHand2015,
  title = {State of the {{Art}} in {{Hand}} and {{Finger Modeling}} and {{Animation}}},
  author = {Wheatland, Nkenge and Wang, Yingying and Song, Huaguang and Neff, Michael and Zordan, Victor and J{\"o}rg, Sophie},
  year = {2015},
  month = may,
  journal = {Computer Graphics Forum},
  volume = {34},
  number = {2},
  pages = {735--760},
  issn = {01677055},
  doi = {10.1111/cgf.12595},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.12595},
  urldate = {2023-07-12},
  abstract = {The human hand is a complex biological system able to perform numerous tasks with impressive accuracy and dexterity. Gestures furthermore play an important role in our daily interactions, and humans are particularly skilled at perceiving and interpreting detailed signals in communications. Creating believable hand motions for virtual characters is an important and challenging task. Many new methods have been proposed in the Computer Graphics community within the last years, and significant progress has been made towards creating convincing, detailed hand and finger motions. This state of the art report presents a review of the research in the area of hand and finger modeling and animation. Starting with the biological structure of the hand and its implications for how the hand moves, we discuss current methods in motion capturing hands, data-driven and physics-based algorithms to synthesize their motions, and techniques to make the appearance of the hand model surface more realistic. We then focus on areas in which detailed hand motions are crucial such as manipulation and communication. Our report concludes by describing emerging trends and applications for virtual hand animation.},
  langid = {english},
  keywords = {hand-modelling},
  annotation = {71 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/B7NNE55B/Wheatland et al. - 2015 - State of the Art in Hand and Finger Modeling and A.pdf}
}

@article{whiteheadAbstractHiddenMarkov2014,
  title = {Abstract {{Hidden Markov Models}} Have Been Effectively Used in Time Series Based Pattern Recognition Problems in the Past. {{This}} Work Explores Using {{Hidden Markov Models}} ({{HMM}}) to Do {{3D}} Gesture Recognition from Accelerometer Data. {{Our}} Work Differs from Much of the Previous Work in That We Examine the Use of Discreet {{HMMs}} Rather than Continuous {{HMMs}}. {{An}} Interesting Side Effect of This Is That Method Is Therefore Theoretically Transportable to Other Devices That Have a {{3D}} Sensor Output System. {{In}} Essence This Brings Us a Mechanism to Use the {{HMM}} Model across a Series of Different Sensor Devices for Gesture Recognition. {{We}} Achieve Recognition Results with Accuracy Rates Approaching 90 Percent for Users Who Are Not in the Training Samples. {{The}} Speed of Our System Is Also of Interest as We Are Able to Classify Gestures at a Rate of Several Hundred Times per Second. {{As}} Long as the Sen-Sor System Is Capable of Outputting Information about the 3 Axes of Motion, and the Outputs Can Be Discretized to Volumetrically Equivalent Cubic Sub-Spaces; That Information Can Then Be Used in This Generic Model for Accurate, High Speed Gesture Recognition.},
  author = {Whitehead, A.D.},
  year = {2014},
  month = apr,
  journal = {GSTF Journal on Computing (JoC)},
  volume = {3},
  doi = {10.7603/s40601-013-0042-9},
  keywords = {from:cite.bib,model:hmm},
  annotation = {0 citations (Crossref) [2023-07-11] 1 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{whitePrinciplesNeurodynamicsPerceptrons1963,
  title = {Principles of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  shorttitle = {Principles of {{Neurodynamics}}},
  author = {White, B. W. and Rosenblatt, Frank},
  year = {1963},
  month = dec,
  journal = {The American Journal of Psychology},
  volume = {76},
  number = {4},
  eprint = {1419730},
  eprinttype = {jstor},
  pages = {705},
  issn = {00029556},
  doi = {10.2307/1419730},
  url = {https://www.jstor.org/stable/1419730?origin=crossref},
  urldate = {2023-06-07},
  abstract = {Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.},
  keywords = {background,perceptrons},
  annotation = {2231 citations (Semantic Scholar/DOI) [2023-06-07]}
}

@article{wilsonParametricHiddenMarkov1999,
  title = {Parametric Hidden {{Markov}} Models for Gesture Recognition},
  author = {Wilson, A.D. and Bobick, A.F.},
  year = {Sept./1999},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {9},
  pages = {884--900},
  issn = {01628828},
  doi = {10.1109/34.790429},
  url = {http://ieeexplore.ieee.org/document/790429/},
  urldate = {2023-06-22},
  abstract = {A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction.},
  keywords = {based-on-vision,claims-to-be-first,hardware:stive,model:hmm,tech:rgb},
  annotation = {675 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/P9TEDP8E/Wilson and Bobick - 1999 - Parametric hidden Markov models for gesture recogn.pdf}
}

@article{wiseEvaluationFiberOptic1990,
  title = {Evaluation of a Fiber Optic Glove for Semi-Automated Goniometric Measurements},
  author = {Wise, Sam and Gardner, William and Sabelman, Eric and Valainis, Erik and Wong, Yuriko and Glass, Karen and Drace, John and Rosen, Joseph M.},
  year = {1990},
  journal = {The Journal of Rehabilitation Research and Development},
  volume = {27},
  number = {4},
  pages = {411},
  issn = {0748-7711},
  doi = {10.1682/JRRD.1990.10.0411},
  url = {http://www.rehab.research.va.gov/jour/90/27/4/pdf/wise.pdf},
  urldate = {2023-07-13},
  abstract = {Normal subjects were used to evaluate a fiber optic instrumented glove for semi-automated goniometric measurement. The glove electronically records and transmits hand and finger position to a host computer by measuring the amount of joint flexion. The glove was put through a series of range-of-motion (ROM) tests with five subjects. Metacarpal (MP) and proximal interphalangeal (PIP) joint angles of the five digits were compared during repetitive standardized motions to evaluate the glove's repeatability. The results showed an overall error of 5.6 degrees, as compared to an error of between 5 and 8 degrees with manual measurement. Additional tests were done to determine factors such as fit, grip force, and wrist motion that may contribute to the overall error. The glove should have applicability to some aspects of hand evaluation as a semi-automated goniometric measurement device.},
  langid = {english},
  keywords = {based-on-gloves,hardware:dataglove,referenced,tech:fibre-optic},
  annotation = {141 citations (Semantic Scholar/DOI) [2023-07-13]},
  file = {/Users/brk/Zotero/storage/NISVG4S7/Wise et al. - 1990 - Evaluation of a fiber optic glove for semi-automat.pdf}
}

@article{wongMultiFeaturesCapacitiveHand2021,
  title = {Multi-{{Features Capacitive Hand Gesture Recognition Sensor}}: {{A Machine Learning Approach}}},
  shorttitle = {Multi-{{Features Capacitive Hand Gesture Recognition Sensor}}},
  author = {Wong, W. K. and Juwono, Filbert H. and Khoo, Brendan Teng Thiam},
  year = {2021},
  month = mar,
  journal = {IEEE Sensors Journal},
  volume = {21},
  number = {6},
  pages = {8441--8450},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2021.3049273},
  url = {https://ieeexplore.ieee.org/document/9314169/},
  urldate = {2023-03-07},
  abstract = {Gesture recognition technology enables machines to understand human gestures. The technology is considered as a key enabler for gaming and virtual reality applications. In this paper, we propose an effective, low-cost capacitive sensor device to recognize hand gestures. In particular, we designed a prototype of a wearable capacitive sensor unit to capture the capacitance values from the electrodes placed on finger phalanges. The sensor captures finger capacitance values. Each gesture has specific finger capacitance values. We applied a running median filter to the output of the sensor and extracted 15 features for gesture classification training and testing tasks. Subsequently, various analyses were performed to provide more insights into the sensing data. We applied and compared two machine learning algorithms: Error Correction Output Code Support Vector Machines (ECOC-SVM) and \$\{K\}\$ -Nearest Neighbour (KNN) classifiers. The training and testing recognition rates were observed for both intra-participant and inter-participant data sets. Further, we introduced a feature compression approach derived from correlation analysis to reduce the complexity of the machine learning algorithms. Using cross validation, we achieved a classification rate of approximately 99\% for intra-participant data. We achieved a lower recognition rate of 97\% (average cross validation testing) for compressed feature data set using both machine learning approaches. For the inter-participant data, the recognition rate was 99\% (normalized feature data) using KNN and 97\% using ECOC-SVM. The research findings show that our recognition system is competitive and has an immense potential for further study.},
  keywords = {model:knn,model:svm,tech:capacitive},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{wuDeepDynamicNeural2016,
  title = {Deep {{Dynamic Neural Networks}} for {{Multimodal Gesture Segmentation}} and {{Recognition}}},
  author = {Wu, Di and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
  year = {2016},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {8},
  pages = {1583--1597},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2016.2537340},
  url = {https://ieeexplore.ieee.org/document/7423804/},
  urldate = {2023-06-22},
  abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
  keywords = {model:cnn,model:ffnn,model:hmm,tech:rgbd},
  annotation = {373 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UNZIHXE3/Wu et al. - 2016 - Deep Dynamic Neural Networks for Multimodal Gestur.pdf}
}

@inproceedings{wuGestureRecognition3D2009,
  title = {Gesture {{Recognition}} with a 3-{{D Accelerometer}}},
  booktitle = {Ubiquitous {{Intelligence}} and {{Computing}}},
  author = {Wu, Jiahui and Pan, Gang and Zhang, Daqing and Qi, Guande and Li, Shijian},
  editor = {Zhang, Daqing and Portmann, Marius and Tan, Ah-Hwee and Indulska, Jadwiga},
  year = {2009},
  volume = {5585},
  pages = {25--38},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02830-4\_4},
  url = {http://link.springer.com/10.1007/978-3-642-02830-4_4},
  urldate = {2023-03-07},
  abstract = {Gesture-based interaction, as a natural way for human-computer interaction, has a wide range of applications in ubiquitous computing environment. This paper presents an acceleration-based gesture recognition approach, called FDSVM ( Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer. With FDSVM, firstly, the acceleration data of a gesture is collected and represented by a frame-based descriptor, to extract the discriminative information. Then a SVM-based multi-class gesture classifier is built for recognition in the nonlinear gesture feature space. Extensive experimental results on a data set with 3360 gesture samples of 12 gestures over weeks demonstrate that the proposed FDSVM approach significantly outperforms other four methods: DTW, Naive Bayes, C4.5 and HMM. In the user-dependent case, FDSVM achieves the recognition rate of 99.38\% for the 4 direction gestures and 95.21\% for all the 12 gestures. In the user-independent case, it obtains the recognition rate of 98.93\% for 4 gestures and 89.29\% for 12 gestures. Compared to other accelerometer-based gesture recognition approaches reported in literature FDSVM gives the best resulrs for both user-dependent and user-independent cases.},
  isbn = {978-3-642-02829-8 978-3-642-02830-4},
  keywords = {based-on-gloves,classes:12,from:cite.bib,model:c4.5,model:dtw,model:hmm,model:nb,model:svm,read-priority-1,tech:accelerometer},
  annotation = {133 citations (Crossref) [2023-07-11] 240 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/UFY9XY4F/Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf}
}

@article{wuNonInvasiveDetectionMoving2015,
  title = {Non-{{Invasive Detection}} of {{Moving}} and {{Stationary Human With WiFi}}},
  author = {Wu, Chenshu and Yang, Zheng and Zhou, Zimu and Liu, Xuefeng and Liu, Yunhao and Cao, Jiannong},
  year = {2015},
  month = nov,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {33},
  number = {11},
  pages = {2329--2342},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2015.2430294},
  url = {http://ieeexplore.ieee.org/document/7102722/},
  urldate = {2023-07-12},
  abstract = {Non-invasive human sensing based on radio signals has attracted a great deal of research interest and fostered a broad range of innovative applications of localization, gesture recognition, smart health-care, etc., for which a primary primitive is to detect human presence. Previous works have studied the detection of moving humans via signal variations caused by human movements. For stationary people, however, existing approaches often employ a prerequisite scenario-tailored calibration of channel profile in human-free environments. Based on in-depth understanding of human motion induced signal attenuation reflected by PHY layer channel state information (CSI), we propose DeMan, a unified scheme for non-invasive detection of moving and stationary human on commodity WiFi devices. DeMan takes advantage of both amplitude and phase information of CSI to detect moving targets. In addition, DeMan considers human breathing as an intrinsic indicator of stationary human presence and adopts sophisticated mechanisms to detect particular signal patterns caused by minute chest motions, which could be destroyed by significant whole-body motion or hidden by environmental noises. By doing this, DeMan is capable of simultaneously detecting moving and stationary people with only a small number of prior measurements for model parameter determination, yet without the cumbersome scenario-specific calibration. Extensive experimental evaluation in typical indoor environments validates the great performance of DeMan in various human poses and locations and diverse channel conditions. Particularly, DeMan provides a detection rate of around 95\% for both moving and stationary people, while identifies human-free scenarios by 96\%, all of which outperforms existing methods by about 30\%.},
  keywords = {app:activity-inference,app:breathing-detection,based-on-wifi,tech:wifi},
  annotation = {266 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{wuWearableSystemRecognizing2016,
  title = {A {{Wearable System}} for {{Recognizing American Sign Language}} in {{Real-Time Using IMU}} and {{Surface EMG Sensors}}},
  author = {Wu, Jian and Sun, Lu and Jafari, Roozbeh},
  year = {2016},
  month = sep,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {20},
  number = {5},
  pages = {1281--1290},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2016.2598302},
  url = {http://ieeexplore.ieee.org/document/7552525/},
  urldate = {2023-03-07},
  abstract = {A sign language recognition system translates signs performed by deaf individuals into text/speech in real time. Inertial measurement unit and surface electromyography (sEMG) are both useful modalities to detect hand/arm gestures. They are able to capture signs and the fusion of these two complementary sensor modalities will enhance system performance. In this paper, a wearable system for recognizing American Sign Language (ASL) in real time is proposed, fusing information from an inertial sensor and sEMG sensors. An information gain-based feature selection scheme is used to select the best subset of features from a broad range of well-established features. Four popular classification algorithms are evaluated for 80 commonly used ASL signs on four subjects. The experimental results show 96.16\% and 85.24\% average accuracies for intra-subject and intra-subject cross session evaluation, respectively, with the selected feature subset and a support vector machine classifier. The significance of adding sEMG for ASL recognition is explored and the best channel of sEMG is highlighted.},
  keywords = {app:sign-language,classes:50-99,classes:80,impressive,model:dt,model:knn,model:nb,model:svm,tech:accelerometer,tech:emg},
  annotation = {166 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/2J63995L/Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf}
}

@article{wuWiTrajRobustIndoor2023,
  title = {{{WiTraj}}: {{Robust Indoor Motion Tracking With WiFi Signals}}},
  shorttitle = {{{WiTraj}}},
  author = {Wu, Dan and Zeng, Youwei and Gao, Ruiyang and Li, Shenjie and Li, Yang and Shah, Rahul C. and Lu, Hong and Zhang, Daqing},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {22},
  number = {5},
  pages = {3062--3078},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2021.3133114},
  url = {https://ieeexplore.ieee.org/document/9645160/},
  urldate = {2023-07-12},
  abstract = {WiFi-based device-free motion tracking systems track persons without requiring them to carry any device. Existing work has explored signal parameters such as time-of-flight (ToF), angle-of-arrival (AoA), and Doppler-frequency-shift (DFS) extracted from WiFi channel state information (CSI) to locate and track people in a room. However, they are not robust due to unreliable estimation of signal parameters. ToF and AoA estimations are not accurate for current standards-compliant WiFi devices that typically have only two antennas and limited channel bandwidth. On the other hand, DFS can be extracted relatively easily on current devices but is susceptible to the high noise level and random phase offset in CSI measurement, which results in a speed-sign-ambiguity problem and renders ambiguous walking speeds. This paper proposes WiTraj, a device-free indoor motion tracking system using commodity WiFi devices. WiTraj improves tracking robustness from three aspects: 1) It significantly improves DFS estimation quality by using the ratio of the CSI from two antennas of each receiver, 2) To better track human walking, it leverages multiple receivers placed at different viewing angles to capture human walking and then intelligently combines the best views to achieve a robust trajectory reconstruction, and, 3) It differentiates walking from in-place activities, which are typically interleaved in daily life, so that non-walking activities do not cause tracking errors. Experiments show that WiTraj can significantly improve tracking accuracy in typical environments compared to existing DFS-based systems. Evaluations across 9 participants and 3 different environments show that the median tracking error {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$<$}2.5\textbackslash\%\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mo{$><<$}/mml:mo{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}mml:mo{$>$}.{$<$}/mml:mo{$><$}mml:mn{$>$}5{$<$}/mml:mn{$><$}mml:mo{$>\%<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="wu-ieq1-3133114.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} for typical room-sized trajectories.},
  keywords = {based-on-wifi,tech:wifi},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/CHVHRXH7/Wu et al. - 2023 - WiTraj Robust Indoor Motion Tracking With WiFi Si.pdf}
}

@article{wynantsThreeMythsRisk2019,
  title = {Three Myths about Risk Thresholds for Prediction Models},
  author = {Wynants, Laure and {van Smeden}, Maarten and McLernon, David J. and Timmerman, Dirk and Steyerberg, Ewout W. and Van Calster, Ben and {on behalf of the Topic Group `Evaluating diagnostic tests and prediction models' of the STRATOS initiative}},
  year = {2019},
  month = oct,
  journal = {BMC Medicine},
  volume = {17},
  number = {1},
  pages = {192},
  issn = {1741-7015},
  doi = {10.1186/s12916-019-1425-3},
  url = {https://doi.org/10.1186/s12916-019-1425-3},
  urldate = {2023-03-24},
  abstract = {Clinical prediction models are useful in estimating a patient's risk of having a certain disease or experiencing an event in the future based on their current characteristics. Defining an appropriate risk threshold to recommend intervention is a key challenge in bringing a risk prediction model to clinical application; such risk thresholds are often defined in an ad hoc way. This is problematic because tacitly assumed costs of false positive and false negative classifications may not be clinically sensible. For example, when choosing the risk threshold that maximizes the proportion of patients correctly classified, false positives and false negatives are assumed equally costly. Furthermore, small to moderate sample sizes may lead to unstable optimal thresholds, which requires a particularly cautious interpretation of results.},
  keywords = {background},
  annotation = {68 citations (Semantic Scholar/DOI) [2023-03-24]},
  file = {/Users/brk/Zotero/storage/R6RTYDFE/Wynants et al. - 2019 - Three myths about risk thresholds for prediction m.pdf;/Users/brk/Zotero/storage/V9FAH7AE/s12916-019-1425-3.html}
}

@inproceedings{xieAccelerometerGestureRecognition2014,
  title = {Accelerometer {{Gesture Recognition}}},
  author = {Xie, Michael},
  year = {2014},
  url = {https://www.semanticscholar.org/paper/Accelerometer-Gesture-Recognition-Xie/c8bb38e46b7d97cc1a4ccd4a9d6df9717c1c3ff7},
  urldate = {2023-03-07},
  abstract = {Our goal is to make gesture-based input for smartphones and smartwatches accurate and feasible to use. With a custom Android application to record accelerometer data for 5 gestures, we developed a highly accurate SVM classifier using only 1 training example per class. Our novel Dynamic-Threshold Truncation algorithm during preprocessing improved accuracy on 1 training example per class by 14\% and the addition of axis-wise Discrete Fourier Transform coefficient features improved accuracy on 1 training example per class by 5\%. With 5 gesture classes, 1 training example for each class, and 30 test examples for each class, our classifier achieves 96\% accuracy. With 5 training examples per class, the classifier achieves 98\% accuracy, which is greater than the 10-example accuracy of other efforts using HMM's[1, 2]. This makes it feasible for a real-time implementation of accelerometer-based gesture recognition to identify user-defined gestures with high accuracy while requiring little training effort from the user.},
  keywords = {classes:{$<$}10,classes:5,model:svm,tech:accelerometer},
  file = {/Users/brk/Zotero/storage/EC67HB44/Xie - 2014 - Accelerometer Gesture Recognition.pdf}
}

@article{xuFingerwritingSmartwatchCase2015,
  title = {Finger-Writing with {{Smartwatch}}: {{A Case}} for {{Finger}} and {{Hand Gesture Recognition}} Using {{Smartwatch}}},
  shorttitle = {Finger-Writing with {{Smartwatch}}},
  author = {Xu, Chao and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2015},
  month = feb,
  journal = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
  pages = {9--14},
  publisher = {{ACM}},
  address = {{Santa Fe New Mexico USA}},
  doi = {10.1145/2699343.2699350},
  url = {https://dl.acm.org/doi/10.1145/2699343.2699350},
  urldate = {2023-07-11},
  abstract = {Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the fitness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user's arm, hand and possibly finger movements using its accelerometer and gyroscope sensors. Although user's arm and hand gestures are likely to be identified with ease using the smartwatch sensors, it is not clear how much of user's finger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures. We identify essential features of accelerometer and gyroscope data that reflect the movements of tendons (passing through the wrist) when performing a finger or a hand gesture. With these features, we build a classifier that can uniquely identify 37 (13 finger, 14 hand and 10 arm) gestures with an accuracy of 98\textbackslash\%. We further extend our gesture recognition to identify the characters written by the user with her index finger on a surface, and show that such finger-writing can also be accurately recognized with nearly 95\% accuracy. Our presented results will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
  isbn = {9781450333917},
  langid = {english},
  keywords = {classes:37,hardware:smartwatch,tech:imu},
  annotation = {228 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{xuHandGestureInteraction2006,
  title = {Hand {{Gesture Interaction}} for {{Virtual Training}} of {{SPG}}},
  author = {Xu, Deyou and Yao, Wuyun and Zhang, Yongliang},
  year = {2006},
  month = nov,
  journal = {16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)},
  pages = {672--676},
  publisher = {{IEEE}},
  address = {{Hangzhou}},
  doi = {10.1109/ICAT.2006.68},
  url = {https://ieeexplore.ieee.org/document/4089336/},
  urldate = {2023-07-02},
  abstract = {We develop a virtual reality based driving training system of self-propelled gun (SPG). In order to make the interface of the system more powerful and natural, hand gesture interaction need to be incorporated into the system's interface. This paper discusses the use of hand gestures for interaction with the virtual training environment. We employ static hand gestures which coupled with hand translations and rotations as the method of interacting with the virtual training environment. An 18-sensor data glove is chosen for monitoring the movements of the fingers and the wrist. The feed-forward neural network is developed for recognizing gestures for use in virtual training application of artillery self-propelled gun (SPG). We present our approach for the algorithm design and implementation, and the use of the gestures in our application. The presented hand gesture interaction method can be effectively used in our virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way},
  isbn = {9780769527543},
  keywords = {model:ffnn,movement:static,pdf:paywalled},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{xuzhangFrameworkHandGesture2011,
  title = {A {{Framework}} for {{Hand Gesture Recognition Based}} on {{Accelerometer}} and {{EMG Sensors}}},
  author = {{Xu Zhang} and {Xiang Chen} and {Yun Li} and Lantz, V. and {Kongqiao Wang} and {Jihai Yang}},
  year = {2011},
  month = nov,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  volume = {41},
  number = {6},
  pages = {1064--1076},
  issn = {1083-4427, 1558-2426},
  doi = {10.1109/TSMCA.2011.2116004},
  url = {http://ieeexplore.ieee.org/document/5735233/},
  urldate = {2023-03-07},
  abstract = {This paper presents a framework for hand gesture recognition based on the information fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors. In our framework, the start and end points of meaningful gesture segments are detected automatically by the intensity of the EMG signals. A decision tree and multistream hidden Markov models are utilized as decision-level fusion to get the final results. For sign language recognition (SLR), experimental results on the classification of 72 Chinese Sign Language (CSL) words demonstrate the complementary functionality of the ACC and EMG sensors and the effectiveness of our framework. Additionally, the recognition of 40 CSL sentences is implemented to evaluate our framework for continuous SLR. For gesture-based control, a real-time interactive system is built as a virtual Rubik's cube game using 18 kinds of hand gestures as control commands. While ten subjects play the game, the performance is also examined in user-specific and user-independent classification. Our proposed framework facilitates intelligent and natural control in gesture-based interaction.},
  keywords = {app:sign-language,classes:50-99,classes:72,model:decision-tree,model:hmm,tech:accelerometer,tech:emg},
  annotation = {518 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{yamatoRecognizingHumanAction1992,
  title = {Recognizing Human Action in Time-Sequential Images Using Hidden {{Markov}} Model},
  author = {Yamato, J. and Ohya, J. and Ishii, K.},
  year = {1992},
  journal = {Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages = {379--385},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Champaign, IL, USA}},
  doi = {10.1109/CVPR.1992.223161},
  url = {http://ieeexplore.ieee.org/document/223161/},
  urldate = {2023-07-11},
  abstract = {A human action recognition method based on a hidden Markov model (HMM) is proposed. It is a feature-based bottom-up approach that is characterized by its learning capability and time-scale invariability. To apply HMMs, one set of time-sequential images is transformed into an image feature vector sequence, and the sequence is converted into a symbol sequence by vector quantization. In learning human action categories, the parameters of the HMMs, one per category, are optimized so as to best describe the training sequences from the category. To recognize an observed sequence, the HMM which best matches the sequence is chosen. Experimental results for real time-sequential images of sports scenes show recognition rates higher than 90\%. The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.{$<<$}ETX{$>>$}},
  isbn = {9780818628559},
  keywords = {based-on-vision,from:cite.bib,model:hmm,model:vector-quantization,tech:rgb,type:seminal},
  annotation = {610 citations (Crossref) [2023-07-11] 1566 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/M2J27ASX/Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf}
}

@article{yangDynamicHandGesture2012,
  title = {Dynamic Hand Gesture Recognition Using Hidden {{Markov}} Models},
  author = {Yang, Zhong and Li, Yi and Chen, Weidong and Zheng, Yang},
  year = {2012},
  month = jul,
  journal = {2012 7th International Conference on Computer Science \& Education (ICCSE)},
  pages = {360--365},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/ICCSE.2012.6295092},
  url = {http://ieeexplore.ieee.org/document/6295092/},
  urldate = {2023-07-11},
  abstract = {Hand gesture has become a powerful means for human-computer interaction. Traditional gesture recognition just consider hand trajectory. For some specific applications, such as virtual reality, more natural gestures are needed, which are complex and contain movement in 3-D space. In this paper, we introduce an HMM-based method to recognize complex single hand gestures. Gesture images are gained by a common web camera. Skin color is used to segment hand area from the image to form a hand image sequence. Then we put forward a state-based spotting algorithm to split continuous gestures. After that, feature extraction is executed on each gesture. Features used in the system contain hand position, velocity, size, and shape. We raise a data aligning algorithm to align feature vector sequences for training. Then an HMM is trained alone for each gesture. The recognition results demonstrate that our methods are effective and accurate.},
  isbn = {9781467302425 9781467302418 9781467302401},
  keywords = {model:hmm,tech:rgb},
  annotation = {68 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{yanminzhuVisionBasedHand2013,
  title = {Vision {{Based Hand Gesture Recognition}}},
  author = {{Yanmin Zhu} and {Zhibo Yang} and {Bo Yuan}},
  year = {2013},
  month = apr,
  journal = {2013 International Conference on Service Sciences (ICSS)},
  pages = {260--265},
  publisher = {{IEEE}},
  address = {{Shenzhen}},
  doi = {10.1109/ICSS.2013.40},
  url = {http://ieeexplore.ieee.org/document/6519802/},
  urldate = {2023-07-11},
  abstract = {With the development of ubiquitous computing, current user interaction approaches with keyboard, mouse and pen are not sufficient. Due to the limitation of these devices the useable command set is also limited. Direct use of hands as an input device is an attractive method for providing natural Human Computer Interaction which has evolved from text-based interfaces through 2D graphical-based interfaces, multimedia-supported interfaces, to fully fledged multi-participant Virtual Environment (VE) systems. Imagine the human-computer interaction of the future: A 3D- application where you can move and rotate objects simply by moving and rotating your hand - all without touching any input device. In this paper a review of vision based hand gesture recognition is presented. The existing approaches are categorized into 3D model based approaches and appearance based approaches, highlighting their advantages and shortcomings and identifying the open issues.},
  isbn = {9781467362580 9780769549729},
  keywords = {tech:rgb,type:survey},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{yeasinVisualUnderstandingDynamic2000,
  title = {Visual Understanding of Dynamic Hand Gestures},
  author = {Yeasin, M. and Chaudhuri, S.},
  year = {2000},
  month = nov,
  journal = {Pattern Recognition},
  volume = {33},
  number = {11},
  pages = {1805--1817},
  issn = {00313203},
  doi = {10.1016/S0031-3203(99)00175-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320399001752},
  urldate = {2023-07-11},
  abstract = {Analysis of a dynamic hand gesture requires processing a spatio-temporal image sequence. The actual length of the sequence varies with each instantiation of the gesture. The key idea behind solving the problem is to translate the richness of the human gestural communication power to a machine for a better man\vphantom\{\}machine interaction. We propose a novel vision-based system for automatic interpretation of a limited set of dynamic hand gestures. This involves extracting the temporal signature of the hand motion from the performed gesture. The concept of motion energy is used to estimate the dominant motion from an image sequence. To achieve the desired result, we introduce the concept of modeling the dynamic hand gesture using a "nite state machine. The temporal signature is subsequently analyzed by the "nite state machine to interpret automatically the performed gesture. ( 2000 Pattern Recognition Society. Published by Elsevier},
  langid = {english},
  keywords = {based-on-vision,classes:5,model:fsm,tech:rgb},
  annotation = {107 citations (Semantic Scholar/DOI) [2023-07-11]},
  file = {/Users/brk/Zotero/storage/3YFE7NYK/Yeasin and Chaudhuri - 2000 - Visual understanding of dynamic hand gestures.pdf}
}

@article{yoonHandGestureRecognition2001,
  title = {Hand Gesture Recognition Using Combined Features of Location, Angle and Velocity},
  author = {Yoon, Ho-Sub and Soh, Jung and Bae, Younglae J. and Seung Yang, Hyun},
  year = {2001},
  month = jan,
  journal = {Pattern Recognition},
  volume = {34},
  number = {7},
  pages = {1491--1501},
  issn = {00313203},
  doi = {10.1016/S0031-3203(00)00096-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320300000960},
  urldate = {2023-07-11},
  abstract = {The use of hand gesture provides an attractive alternative to cumbersome interface devices for human\textendash computer interaction (HCI). Many hand gesture recognition methods using visual analysis have been proposed: syntactical analysis, neural networks, the hidden Markov model (HMM). In our research, an HMM is proposed for various types of hand gesture recognition. In the preprocessing stage, this approach consists of three different procedures for hand localization, hand tracking and gesture spotting. The hand location procedure detects hand candidate regions on the basis of skin-color and motion. The hand tracking algorithm finds the centroids of the moving hand regions, connects them, and produces a hand trajectory. The gesture spotting algorithm divides the trajectory into real and meaningless segments. To construct a feature database, this approach uses a combined and weighted location, angle and velocity feature codes, and employs a k-means clustering algorithm for the HMM codebook. In our experiments, 2400 trained gestures and 2400 untrained gestures are used for training and testing, respectively. Those experimental results demonstrate that the proposed approach yields a satisfactory and higher recognition rate for user images of different hand size, shape and skew angle.},
  langid = {english},
  keywords = {based-on-vision,model:hmm,tech:rgb},
  annotation = {272 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{yousefiSurveyBehaviorRecognition2017,
  title = {A {{Survey}} on {{Behavior Recognition Using WiFi Channel State Information}}},
  author = {Yousefi, Siamak and Narui, Hirokazu and Dayal, Sankalp and Ermon, Stefano and Valaee, Shahrokh},
  year = {2017},
  month = oct,
  journal = {IEEE Communications Magazine},
  volume = {55},
  number = {10},
  pages = {98--104},
  issn = {0163-6804},
  doi = {10.1109/MCOM.2017.1700082},
  url = {http://ieeexplore.ieee.org/document/8067693/},
  urldate = {2023-07-12},
  abstract = {In this article, we present a survey of recent advances in passive human behavior recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems. The movement of the human body parts cause changes in the wireless signal reflections, which result in variations in the CSI. By analyzing the data streams of CSIs for different activities and comparing them against stored models, human behavior can be recognized. This is done by extracting features from CSI data streams and using machine learning techniques to build models and classifiers. The techniques from the literature that are presented herein have great performance; however, instead of the machine learning techniques employed in these works, we propose to use deep learning techniques such as long-short term memory (LSTM) recurrent neural networking (RNN) and show the improved performance. We also discuss different challenges such as environment change, frame rate selection, and the multi-user scenario; and finally suggest possible directions for future work.},
  keywords = {based-on-wifi,model:lstm,model:rnn,tech:wifi,type:survey},
  annotation = {245 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{yuanHandGestureRecognition2020,
  title = {Hand {{Gesture Recognition}} Using {{Deep Feature Fusion Network}} Based on {{Wearable Sensors}}},
  author = {Yuan, Guan and Liu, Xiao and Yan, Qiuyan and Qiao, Shaojie and Wang, Zhixiao and Yuan, Li},
  year = {2020},
  journal = {IEEE Sensors Journal},
  pages = {1--1},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2020.3014276},
  url = {https://ieeexplore.ieee.org/document/9158332/},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is an important way for human machine interaction, and it is widely used in many areas, such as health care, smart home, virtual reality as well as other areas. While many valuable efforts have been made, it still lacks efficient ways to capture fine grain hand gesture as well as track data of long distance dependency in complex gesture. In this paper, we firstly design a novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles. Secondly, an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures. In order to track detailed motion features, a convolutional neural network based feature fusion strategy is given to fuse data from multi-sensors by extracting both shallow and deep features. Moreover, a residual module is introduced to avoid over fitting and gradient vanishing during deepening the neural network. Thirdly, a long short term memory (LSTM) model with fused feature vector as input is introduced to classify complex hand motions into corresponding categories. Results of comprehensive experiments demonstrate that our work performs better than related algorithms, especially in America sign language (with the precise of 99.93\%) and Chinese sign language (with the precise of 96.1\%).},
  keywords = {app:american-sl,app:chinese-sl,app:sign-language,model:cnn,model:lstm,model:nn,tech:flex},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{zabulisVisionBasedHandGesture2009,
  title = {Vision-{{Based Hand Gesture Recognition}} for {{Human-Computer Interaction}}},
  booktitle = {The {{Universal Access Handbook}}},
  author = {Zabulis, Xenophon and Baltzakis, Haris and Argyros, Antonis},
  editor = {Stephanidis, Constantine},
  year = {2009},
  month = jun,
  volume = {20091047},
  pages = {1--30},
  publisher = {{CRC Press}},
  doi = {10.1201/9781420064995-c34},
  url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420064995-c34},
  urldate = {2023-06-26},
  abstract = {In recent years, research efforts seeking to provide more natural, human-centered means of interacting with computers have gained growing interest. A particularly important direction is that of perceptive user interfaces, where the computer is endowed with perceptive capabilities that allow it to acquire both implicit and explicit information about the user and the environment. Vision has the potential of carrying a wealth of information in a non-intrusive manner and at a low cost, therefore it constitutes a very attractive sensing modality for developing perceptive user interfaces. Proposed approaches for vision-driven interactive user interfaces resort to technologies such as head tracking, face and facial expression recognition, eye tracking and gesture recognition. In this paper, we focus our attention to vision-based recognition of hand gestures. The first part of the paper provides an overview of the current state of the art regarding the recognition of hand gestures as these are observed and recorded by typical video cameras. In order to make the review of the related literature tractable, this paper does not discuss:},
  isbn = {978-0-8058-6280-5 978-1-4200-6499-5},
  langid = {english},
  keywords = {based-on-vision,type:survey},
  annotation = {171 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/24YMVRP3/Zabulis et al. - 2009 - Vision-Based Hand Gesture Recognition for Human-Co.pdf}
}

@article{zamankhanHandGestureRecognition2012,
  title = {Hand {{Gesture Recognition}}: {{A Literature Review}}},
  shorttitle = {Hand {{Gesture Recognition}}},
  author = {Zaman Khan, Rafiqul},
  year = {2012},
  month = jul,
  journal = {International Journal of Artificial Intelligence \& Applications},
  volume = {3},
  number = {4},
  pages = {161--174},
  issn = {09762191},
  doi = {10.5121/ijaia.2012.3412},
  url = {http://www.airccse.org/journal/ijaia/papers/3412ijaia12.pdf},
  urldate = {2023-07-03},
  abstract = {Hand gesture recognition system received great attention in the recent few years because of its manifoldness applications and the ability to interact with machine efficiently through human computer interaction. In this paper a survey of recent hand gesture recognition systems is presented. Key issues of hand gesture recognition system are presented with challenges of gesture system. Review methods of recent postures and gestures recognition system presented as well. Summary of research results of hand gesture methods, databases, and comparison between main gesture recognition phases are also given. Advantages and drawbacks of the discussed systems are explained finally.},
  keywords = {model:fuzzy,model:hmm,model:nn,tech:rgb,type:survey},
  annotation = {165 citations (Semantic Scholar/DOI) [2023-07-03]},
  file = {/Users/brk/Zotero/storage/LTRX4QL9/Zaman Khan - 2012 - Hand Gesture Recognition A Literature Review.pdf}
}

@inproceedings{zeltzerIntegratedGraphicalSimulation1989,
  title = {An Integrated Graphical Simulation Platform},
  author = {Zeltzer, D. and Pieper, S. and Sturman, D.},
  year = {1989},
  url = {https://www.semanticscholar.org/paper/An-integrated-graphical-simulation-platform-Zeltzer-Pieper/fc652d52ab473d2315f9431d43e1df84eb829637},
  urldate = {2023-07-13},
  abstract = {This paper describes an integrated graphical simulation platform (IGSP) which provides a framework for constructing interactive simulations, specifically those oriented towards task level animation that is, animation for which the user specifies the tasks to be performed and the system determines the correct sequence of events and selection of tools to use in accomplishing that task . Our prototype system, which we call bolio, allows diverse applications to interact within a run-time environment, displaying their results on a common 3D graphics platform. Here we describe aspects of bolio 's design that allow applications to interact through a common network of constraints, including an integrated suite of tools that simulate kinematic , dynamic, and event-driven processes in virtual worlds. In addition , the IGSP architecture allows us to easily integrate gestural input from a new device the DataGlove that allows non-expert users to manipulate virtual objects directly in these microworlds.},
  keywords = {based-on-gloves,untagged},
  file = {/Users/brk/Zotero/storage/54FTNNNV/Zeltzer et al. - 1989 - An integrated graphical simulation platform.pdf}
}

@article{zengAnalyzingShopperBehavior2015,
  title = {Analyzing {{Shopper}}'s {{Behavior}} through {{WiFi Signals}}},
  author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2015},
  month = may,
  journal = {Proceedings of the 2nd workshop on Workshop on Physical Analytics},
  pages = {13--18},
  publisher = {{ACM}},
  address = {{Florence Italy}},
  doi = {10.1145/2753497.2753508},
  url = {https://dl.acm.org/doi/10.1145/2753497.2753508},
  urldate = {2023-07-12},
  abstract = {Substantial progress in WiFi-based indoor localization has proven that pervasiveness of WiFi can be exploited beyond its traditional use of internet access to enable a variety of sensing applications. Understanding shopper's behavior through physical analytics can provide crucial insights to the business owner in terms of effectiveness of promotions, arrangement of products and efficiency of services. However, analyzing shopper's behavior and browsing patterns is challenging. Since video surveillance can not used due to high cost and privacy concerns, it is necessary to design novel techniques that can provide accurate and efficient view of shopper's behavior. In this work, we propose WiFi-based sensing of shopper's behavior in a retail store. Specifically, we show that various states of a shopper such as standing near the entrance to view a promotion or walking quickly to proceed towards the intended item can be accurately classified by profiling Channel State Information (CSI) of WiFi. We recognize a few representative states of shopper's behavior at the entrance and inside the store, and show how CSI-based profile can be used to detect that a shopper is in one of the states with very high accuracy ({$\approx$} 90\%). We discuss the potential and limitations of CSI-based sensing of shopper's behavior and physical analytics in general.},
  isbn = {9781450334983},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,classes:5,tech:wifi},
  annotation = {104 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{zengFarSensePushingRange2019,
  title = {{{FarSense}}: {{Pushing}} the {{Range Limit}} of {{WiFi-based Respiration Sensing}} with {{CSI Ratio}} of {{Two Antennas}}},
  shorttitle = {{{FarSense}}},
  author = {Zeng, Youwei and Wu, Dan and Xiong, Jie and Yi, Enze and Gao, Ruiyang and Zhang, Daqing},
  year = {2019},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {3},
  number = {3},
  pages = {1--26},
  issn = {2474-9567},
  doi = {10.1145/3351279},
  url = {https://dl.acm.org/doi/10.1145/3351279},
  urldate = {2023-07-12},
  abstract = {The past few years have witnessed the great potential of exploiting channel state information retrieved from commodity WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. On the other hand, most home environments only have one WiFi access point and it may not be located in the same room as the target. This sensing range constraint greatly limits the application of the proposed approaches in real life.             This paper presents FarSense--the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair. FarSense works well even when one of the transceivers is located in another room, moving a big step towards real-life deployment. We propose two novel schemes to achieve this goal: (1) Instead of applying the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range; (2) The division operation further enables us to utilize the phase information which is not usable with one single antenna for sensing. The orthogonal amplitude and phase are elaborately combined to address the "blind spots" issue and further increase the sensing range. Extensive experiments show that FarSense is able to accurately monitor human respiration even when the target is 8 meters away from the transceiver pair, increasing the sensing range by more than 100\%.1 We believe this is the first system to enable through-wall respiration sensing with commodity WiFi devices and the proposed method could also benefit other sensing applications.},
  langid = {english},
  keywords = {based-on-wifi,tech:wifi},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/9DWYS5JZ/Zeng et al. - 2019 - FarSense Pushing the Range Limit of WiFi-based Re.pdf}
}

@article{zengWiWhoWiFiBasedPerson2016,
  title = {{{WiWho}}: {{WiFi-Based Person Identification}} in {{Smart Spaces}}},
  shorttitle = {{{WiWho}}},
  author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2016},
  month = apr,
  journal = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Vienna}},
  doi = {10.1109/IPSN.2016.7460727},
  url = {https://ieeexplore.ieee.org/document/7460727/},
  urldate = {2023-07-12},
  abstract = {There has been a growing interest in equipping the objects and environment surrounding the user with sensing capabilities. Smart indoor spaces such as smart homes and offices can implement the sensing and processing functionality, relieving users from the need of wearing or carrying smart devices. Enabling such smart spaces requires device-free effortless sensing of user's identity and activities. Device-free sensing using WiFi has shown great potential in such scenarios, however, fundamental questions such as person identification have remained unsolved. In this paper, we present WiWho, a framework that can identify a person from a small group of people in a device-free manner using WiFi. We show that Channel State Information (CSI) used in recent WiFi can identify a person's steps and walking gait. The walking gait being distinguishing characteristics for different people, WiWho uses CSI-based gait for person identification. We demonstrate how step and walk analysis can be used to identify a person's walking gait from CSI, and how this information can be used to identify a person. WiWho does not require a person to carry any device and is effortless since it only requires the person to walk for a few steps (e.g. entering a home or an office). We evaluate WiWho using experiments at multiple locations with a total of 20 volunteers, and show that it can identify a person with average accuracy of 92\% to 80\% from a group of 2 to 6 people. We also show that in most cases walking as few as 2-3 meters is sufficient to recognize a person's gait and identify the person. We discuss the potential and challenges of WiFi- based person identification with respect to smart space applications.},
  isbn = {9781509008025},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,based-on-wifi,participants:20,tech:wifi},
  annotation = {303 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{zhangEgoGestureNewDataset2018,
  title = {{{EgoGesture}}: {{A New Dataset}} and {{Benchmark}} for {{Egocentric Hand Gesture Recognition}}},
  shorttitle = {{{EgoGesture}}},
  author = {Zhang, Yifan and Cao, Congqi and Cheng, Jian and Lu, Hanqing},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Multimedia},
  volume = {20},
  number = {5},
  pages = {1038--1050},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2018.2808769},
  url = {https://ieeexplore.ieee.org/document/8299578/},
  urldate = {2023-07-02},
  abstract = {Gesture is a natural interface in human\textendash computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24~000 gesture samples and 3~000~000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
  keywords = {classes:50-99,type:dataset},
  annotation = {143 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{zhangGestureRecognitionBased2020,
  title = {Gesture Recognition Based on Deep Deformable {{3D}} Convolutional Neural Networks},
  author = {Zhang, Yifan and Shi, Lei and Wu, Yi and Cheng, Ke and Cheng, Jian and Lu, Hanqing},
  year = {2020},
  month = nov,
  journal = {Pattern Recognition},
  volume = {107},
  pages = {107416},
  issn = {00313203},
  doi = {10.1016/j.patcog.2020.107416},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320320302193},
  urldate = {2023-07-11},
  abstract = {Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jester's official leader-board until the submission time. The code and the trained models are released for better communication and future works.},
  langid = {english},
  keywords = {dataset:chalearn-isogd,dataset:egogesture,dataset:jester,model:cnn,movement:dynamic},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@article{zhangHandGestureRecognition2009,
  title = {Hand Gesture Recognition and Virtual Game Control Based on {{3D}} Accelerometer and {{EMG}} Sensors},
  author = {Zhang, Xu and Chen, Xiang and Wang, Wen-hui and Yang, Ji-hai and Lantz, Vuokko and Wang, Kong-qiao},
  year = {2009},
  month = feb,
  journal = {Proceedings of the 14th international conference on Intelligent user interfaces},
  pages = {401--406},
  publisher = {{ACM}},
  address = {{Sanibel Island Florida USA}},
  doi = {10.1145/1502650.1502708},
  url = {https://dl.acm.org/doi/10.1145/1502650.1502708},
  urldate = {2023-03-07},
  abstract = {This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7\% in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.},
  isbn = {9781605581682},
  langid = {english},
  keywords = {based-on-gloves,classes:{$<$}10,classes:8,model:hmm,model:nearest-conflicting-neighbors,tech:accelerometer,tech:emg},
  annotation = {229 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/E3IQUGNT/Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf}
}

@article{zhangMudraUserfriendlyFinegrained2016,
  title = {Mudra: {{User-friendly Fine-grained Gesture Recognition}} Using {{WiFi Signals}}},
  shorttitle = {Mudra},
  author = {Zhang, Ouyang and Srinivasan, Kannan},
  year = {2016},
  month = dec,
  journal = {Proceedings of the 12th International on Conference on emerging Networking EXperiments and Technologies},
  pages = {83--96},
  publisher = {{ACM}},
  address = {{Irvine California USA}},
  doi = {10.1145/2999572.2999582},
  url = {https://dl.acm.org/doi/10.1145/2999572.2999582},
  urldate = {2023-07-12},
  abstract = {There has been a great interest in recognizing gestures using wireless communication signals. We are motivated in detecting extremely fine, subtle finger gestures with WiFi signals. We envision this technology to find applications in finger-gesture control, disabled-friendly devices, physical therapy etc. The requirements of mm-level sensitivity and user-friendly feature using existing WiFi signals pose great challenges. Here, we present Mudra, a fine-grained finger gesture recognition system which leverages WiFi signals to enable a near-human-to-machine interaction with finger motion. Mudra uses a two-antenna receiver to detect and recognize finger gesture. It uses the signals received from one antenna to cancel the signal from the other. This "cancellation" is extremely sensitive to and enables us detect small variation in channel due to finger movements. Since Mudra decodes gestures with existing WiFi transmissions, Mudra enables gesture recognition without sacrificing WiFi transmission opportunities. Besides, Mudra is user-friendly with no need of user training. To demonstrate Mudra, we implement prototype on the NI-based SDR platform and use COTS WiFi adapter. We evaluate Mudra in a typical office environment. The results show that our system can achieve 96\% accuracy.},
  isbn = {9781450342926},
  langid = {english},
  keywords = {app:activity-inference,based-on-wifi,classes:9,tech:wifi},
  annotation = {56 citations (Semantic Scholar/DOI) [2023-07-12]}
}

@article{zhangRealTimeSurfaceEMG2019,
  title = {Real-{{Time Surface EMG Pattern Recognition}} for {{Hand Gestures Based}} on an {{Artificial Neural Network}}},
  author = {{Zhang} and {Yang} and {Qian} and {Zhang}},
  year = {2019},
  month = jul,
  journal = {Sensors},
  volume = {19},
  number = {14},
  pages = {3170},
  issn = {1424-8220},
  doi = {10.3390/s19143170},
  url = {https://www.mdpi.com/1424-8220/19/14/3170},
  urldate = {2023-06-22},
  abstract = {In recent years, surface electromyography (sEMG) signals have been increasingly used in pattern recognition and rehabilitation. In this paper, a real-time hand gesture recognition model using sEMG is proposed. We use an armband to acquire sEMG signals and apply a sliding window approach to segment the data in extracting features. A feedforward artificial neural network (ANN) is founded and trained by the training dataset. A test method is used in which the gesture will be recognized when recognized label times reach the threshold of activation times by the ANN classifier. In the experiment, we collected real sEMG data from twelve subjects and used a set of five gestures from each subject to evaluate our model, with an average recognition rate of 98.7\% and an average response time of 227.76 ms, which is only one-third of the gesture time. Therefore, the pattern recognition system might be able to recognize a gesture before the gesture is completed.},
  langid = {english},
  keywords = {classes:{$<$}10,model:ffnn,tech:emg},
  annotation = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/NV2YLHJB/Zhang et al. - 2019 - Real-Time Surface EMG Pattern Recognition for Hand.pdf}
}

@article{zhangStackedLSTMBasedDynamic2021,
  title = {Stacked {{LSTM-Based Dynamic Hand Gesture Recognition}} with {{Six-Axis Motion Sensors}}},
  author = {Zhang, Yidi and Ran, Mengyuan and Liao, Jun and Su, Guoxin and Liu, Ming and Liu, Li},
  year = {2021},
  month = oct,
  journal = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {2568--2575},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/SMC52423.2021.9658659},
  url = {https://ieeexplore.ieee.org/document/9658659/},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition can be exploited to benefit ubiquitous applications using sensors. Currently, the inherent complexity of human physical activities makes it difficult to accurately recognize gestures with wearable sensors, especially in real time. To this end, a real-time hand gesture recognition system is presented in this paper. In particular, sliding window technology and y-axis threshold are used to detect intended gestures from a continuous data stream and then the segmented data are classified by applying a stacked Long Short-Term Memory (LSTM) model. After noise is removed, six-axis sensor data from wrist-worn devices are fed into the model without requiring feature engineering. We use twelve common hand gestures to evaluate the performance of our model. The experimental results demonstrate the feasibility of our proposed system with an accuracy of 99.8\% on average. Our approach allows for an accurate and nonindividual hand gesture recognition. It holds potential to be integrated into a smart watch or other wearable devices for intuitive human computer interaction.},
  isbn = {9781665442077},
  keywords = {classes:12,model:lstm,tech:accelerometer},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@inproceedings{zhangVisionbasedSignLanguage2004,
  title = {A Vision-Based Sign Language Recognition System Using Tied-Mixture Density {{HMM}}},
  booktitle = {Proceedings of the 6th International Conference on {{Multimodal}} Interfaces},
  author = {Zhang, Liang-Guo and Chen, Yiqiang and Fang, Gaolin and Chen, Xilin and Gao, Wen},
  year = {2004},
  month = oct,
  pages = {198--204},
  publisher = {{ACM}},
  address = {{State College PA USA}},
  doi = {10.1145/1027933.1027967},
  url = {https://dl.acm.org/doi/10.1145/1027933.1027967},
  urldate = {2023-07-02},
  abstract = {In this paper, a vision-based medium vocabulary Chinese sign language recognition (SLR) system is presented. The proposed recognition system consists of two modules. In the first module, techniques of robust hands detection, background subtraction and pupils detection are efficiently combined to precisely extract the feature information with the aid of simple colored gloves in the unconstrained environment. Meanwhile, an effective and efficient hierarchical feature description scheme with different scale features to characterize sign language is proposed, where principal component analysis (PCA) is employed to characterize the finger features more elaborately. In the second part, a Tied-Mixture Density Hidden Markov Models (TMDHMM) framework for SLR is proposed, which can speed up the recognition without the significant loss of recognition accuracy compared with the continuous hidden Markov models (CHMM). Experimental results based on 439 frequently used Chinese sign language (CSL) words show that the proposed methods can work well for the medium vocabulary SLR in the environment without special constraints and the recognition accuracy is up to 92.5\%.},
  isbn = {978-1-58113-995-2},
  langid = {english},
  keywords = {app:chinese-sl,app:sign-language,based-on-vision,classes:439,model:hmm,model:pca,tech:rgb},
  annotation = {69 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/JGGD4TLX/Zhang et al. - 2004 - A vision-based sign language recognition system us.pdf}
}

@article{zhangWidar3ZeroEffortCrossDomain2021,
  title = {Widar3.0: {{Zero-Effort Cross-Domain Gesture Recognition}} with {{Wi-Fi}}},
  shorttitle = {Widar3.0},
  author = {Zhang, Yi and Zheng, Yue and Qian, Kun and Zhang, Guidong and Liu, Yunhao and Wu, Chenshu and Yang, Zheng},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3105387},
  url = {https://ieeexplore.ieee.org/document/9516988/},
  urldate = {2023-03-07},
  abstract = {With the development of signal processing technology, the ubiquitous Wi-Fi devices open an unprecedented opportunity to solve the challenging human gesture recognition problem by learning motion representations from wireless signals. Wi-Fi-based gesture recognition systems, although yield good performance on specific data domains, are still practically difficult to be used without explicit adaptation efforts to new domains. Various pioneering approaches have been proposed to resolve this contradiction but extra training efforts are still necessary for either data collection or model re-training when new data domains appear. To advance cross-domain recognition and achieve fully zero-effort recognition, we propose Widar3.0, a Wi-Fi-based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all general model that requires only one-time training but can adapt to different data domains. Experiments on various domain factors (i.e. environments, locations, and orientations of persons) demonstrate the accuracy of 92.7\% for in-domain recognition and 82.6\%-92.4\% for cross-domain recognition without model re-training, outperforming the state-of-the-art solutions.},
  keywords = {based-on-wifi,classes:{$<$}10,classes:10,classes:10-29,classes:6,model:cnn,model:rnn-gru,tech:wifi},
  annotation = {256 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/9FJSTD3F/Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf}
}

@article{zhangWiFiIDHumanIdentification2016,
  title = {{{WiFi-ID}}: {{Human Identification Using WiFi Signal}}},
  shorttitle = {{{WiFi-ID}}},
  author = {Zhang, Jin and Wei, Bo and Hu, Wen and Kanhere, Salil S.},
  year = {2016},
  month = may,
  journal = {2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)},
  pages = {75--82},
  publisher = {{IEEE}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/DCOSS.2016.30},
  url = {http://ieeexplore.ieee.org/document/7536315/},
  urldate = {2023-07-12},
  abstract = {Prior research has shown the potential of device-free WiFi sensing for human activity recognition. In this paper, we show for the first time WiFi signals can also be used to uniquely identify people. There is strong evidence that suggests that all humans have a unique gait. An individual's gait will thus create unique perturbations in the WiFi spectrum. We propose a system called WiFi-ID that analyses the channel state information to extract unique features that are representative of the walking style of that individual and thus allow us to uniquely identify that person. We implement WiFi-ID on commercial off-the-shelf devices. We conduct extensive experiments to demonstrate that our system can uniquely identify people with average accuracy of 93\% to 77\% from a group of 2 to 6 people, respectively. We envisage that this technology can find many applications in small office or smart home settings.},
  isbn = {9781509014606},
  keywords = {app:activity-inference,app:human-identification,based-on-wifi,participants:2,participants:6,tech:wifi},
  annotation = {181 citations (Semantic Scholar/DOI) [2023-07-12]},
  file = {/Users/brk/Zotero/storage/PZ2KLPXF/Zhang et al. - 2016 - WiFi-ID Human Identification Using WiFi Signal.pdf}
}

@article{zhaoMultifeatureGestureRecognition2016,
  title = {Multi-Feature Gesture Recognition Based on {{Kinect}}},
  author = {Zhao, Yue and Liu, Yunda and Dong, Min and Bi, Sheng},
  year = {2016},
  month = jun,
  journal = {2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
  pages = {392--396},
  publisher = {{IEEE}},
  address = {{Chengdu, China}},
  doi = {10.1109/CYBER.2016.7574856},
  url = {http://ieeexplore.ieee.org/document/7574856/},
  urldate = {2023-06-22},
  abstract = {Human Computer Interaction (HCI) has been a popular research area during the last few years. Compared with the tradition HCI methods such as using a keyboard or mouse, people prefer to have their tasks done in a more natural way. As an essential form of non-verbal communication in daily life, gesture is a good choice to turn the ideas into reality. Although various recognition methods are proposed to solve the problem, these methods are time-tensed, space-tensed or miscellaneous. This paper introduced a new method to recognize the hand gesture correctly and efficiently. The recognition is done through two phases: the skeleton phase concerning capturing and processing skeleton feature of the hand gesture, and the hand phase focusing on extracting hand contour feature of the hand gesture. Experimental results confirm an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
  isbn = {9781509027330},
  keywords = {hardware:kinect,tech:rgbd},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]}
}

@article{zhaoRealtimeHeadGesture2017,
  title = {Real-Time Head Gesture Recognition on Head-Mounted Displays Using Cascaded Hidden {{Markov}} Models},
  author = {Zhao, Jingbo and Allison, Robert S.},
  year = {2017},
  month = oct,
  journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {2361--2366},
  publisher = {{IEEE}},
  address = {{Banff, AB}},
  doi = {10.1109/SMC.2017.8122975},
  url = {http://ieeexplore.ieee.org/document/8122975/},
  urldate = {2023-03-07},
  abstract = {Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.},
  isbn = {9781538616451},
  keywords = {classes:{$<$}10,classes:9,hardware:oculus-rift,model:hmm,tech:rgb},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/JUTLYHW4/Zhao and Allison - 2017 - Real-time head gesture recognition on head-mounted.pdf}
}

@inproceedings{zimmermanHandGestureInterface1987,
  title = {A Hand Gesture Interface Device},
  booktitle = {Proceedings of the {{SIGCHI}}/{{GI}} Conference on {{Human}} Factors in Computing Systems and Graphics Interface  - {{CHI}} '87},
  author = {Zimmerman, Thomas G. and Lanier, Jaron and Blanchard, Chuck and Bryson, Steve and Harvill, Young},
  year = {1987},
  pages = {189--192},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/29933.275628},
  url = {http://portal.acm.org/citation.cfm?doid=29933.275628},
  urldate = {2023-03-07},
  abstract = {This paper reports on the development of a hand to machine interface device that provides real-time gesture, position and orientation information. The key element is a glove and the device as a whole incorporates a collection of technologies. Analog flex sensors on the glove measure finger bending. Hand position and orientation are measured either by ultrasonics, providing five degrees of freedom, or magnetic flux sensors, which provide six degrees of freedom. Piezoceramic benders provide the wearer of the glove with tactile feedback. These sensors are mounted on the light-weight glove and connected to the driving hardware via a small cable. Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
  isbn = {978-0-89791-213-6},
  langid = {english},
  keywords = {model:none,tech:flex},
  annotation = {501 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/CTAKLA8C/Zimmerman et al. - 1987 - A hand gesture interface device.pdf}
}

@article{zinnenNewApproachEnable2008,
  title = {A New Approach to Enable Gesture Recognition in Continuous Data Streams},
  author = {Zinnen, Andreas and Schiele, Bernt},
  year = {2008},
  journal = {2008 12th IEEE International Symposium on Wearable Computers},
  pages = {33--40},
  publisher = {{IEEE}},
  address = {{Pittaburgh, PA, USA}},
  doi = {10.1109/ISWC.2008.4911581},
  url = {http://ieeexplore.ieee.org/document/4911581/},
  urldate = {2023-03-07},
  abstract = {Gesture recognition has great potential for mobile and wearable computing. Most papers in this area focus on classifying different gestures, but do not evaluate the distinctiveness of gestures in continuous recordings of gestures in daily life. This paper presents a new approach for the important and challenging problem of gesture recognition in continuous data streams. We use turning points of arm movements to identify segments of interest in the continuous data stream. The recognition algorithm considers both the direction of movements between turning points and the shape of the turning points for classification. Using the new method, seven gestures of different complexity are evaluated against a realistic background class of daily gestures in five different scenarios.},
  isbn = {9781424426379},
  keywords = {based-on-vision,classes:7,pdf:paywalled,tech:rgb},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-02]}
}
