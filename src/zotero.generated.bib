@article{abdelnasserWigestUbiquitousWifibased2014,
  title = {Wigest: {{A Ubiquitous Wifi-based Gesture Recognition System}}},
  shorttitle = {Wigest},
  author = {Abdelnasser, Heba and Harras, Khaled and Youssef, Moustafa},
  year = {2014},
  journal = {Qatar Foundation Annual Research Conference Proceedings Volume 2014 Issue 1},
  publisher = {{Hamad bin Khalifa University Press (HBKU Press)}},
  address = {{Qatar National Convention Center (QNCC), Doha, Qatar,}},
  doi = {10.5339/qfarc.2014.ITPP1127},
  urldate = {2023-06-23},
  abstract = {We present WiGest: a system that leverages changes in WiFi signal strength to sense in-air hand gestures around the user's mobile device. Compared to related work, WiGest is unique in using standard WiFi equipment, with no modifications, and no training for gesture recognition. The system identifies different signal change primitives, from which we construct mutually independent gesture families. These families can be mapped to distinguishable application actions. We address various challenges including cleaning the noisy signals, gesture type and attributes detection, reducing false positives due to interfering humans, and adapting to changing signal polarity. We implement a proof-of-concept prototype using off-the-shelf laptops and extensively evaluate the system in both an office environment and a typical apartment with standard WiFi access points. Our results show that WiGest detects the basic primitives with an accuracy of 87.5\% using a single AP only, including through-the-wall non-line-of-sight scenarios. This accuracy increases to 96\% using three overheard APs. In addition, when evaluating the system using a multi-media player application, we achieve a classification accuracy of 96\%. This accuracy is robust to the presence of other interfering humans, highlighting WiGest's ability to enable future ubiquitous hands-free gesture-based interaction with mobile devices.},
  langid = {english},
  keywords = {based-on:wifi,classes:7,fidelity:arm,segmentation:implicit,tech:wifi,type:paper},
  annotation = {415 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.qscience.com/content/papers/10.5339/qfarc.2014.ITPP1127}},
  file = {/Users/brk/Zotero/storage/J79GHKIF/Abdelnasser et al. - 2014 - Wigest A Ubiquitous Wifi-based Gesture Recognitio.pdf}
}
% == BibTeX quality report for abdelnasserWigestUbiquitousWifibased2014:
% ? unused Conference name ("Qatar Foundation Annual Research Conference Proceedings")
% ? unused Library catalog ("Semantic Scholar")

@misc{abramsgentileentertainmentPowerGlove1989,
  title = {{{PowerGlove}}},
  author = {{Abrams Gentile Entertainment}},
  year = {1989},
  howpublished = {\url{https://web.archive.org/web/20150525222703/http://www.ageinc.com/tech/index.html}},
  keywords = {hardware:powerglove,have-read,tech:flex}
}

@misc{adanarriagaCharaChorderTypeSpeed2022,
  title = {{{CharaChorder}} - {{Type}} at the Speed of Thought},
  author = {{Adan Arriaga}},
  year = {2022},
  howpublished = {\url{https://www.charachorder.com/}},
  keywords = {background,type:product}
}

@article{adibSeeWallsWiFi2013,
  title = {See through Walls with {{WiFi}}!},
  author = {Adib, Fadel and Katabi, Dina},
  year = {2013},
  month = aug,
  journal = {Proceedings of the ACM SIGCOMM 2013 conference on SIGCOMM},
  pages = {75--86},
  publisher = {{ACM}},
  address = {{Hong Kong China}},
  doi = {10.1145/2486001.2486039},
  urldate = {2023-07-12},
  abstract = {Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings.},
  isbn = {9781450320566},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,fidelity:arm,reference:doi.org/10.1002/0471200611,reference:doi.org/10.1007/978-3-540-75703-0\_7,reference:doi.org/10.1016/j.jfranklin.2008.04.001,reference:doi.org/10.1016/S0963-8695(00)00039-6,reference:doi.org/10.1080/07350015.2014.926818,reference:doi.org/10.1093/AGEING/26.1.15,reference:doi.org/10.1109/APS.2005.1552508,reference:doi.org/10.1109/ARRAY.2010.5613313,reference:doi.org/10.1109/ARRAY.2010.5613314,reference:doi.org/10.1109/ISWC.2004.12,reference:doi.org/10.1109/LAWP.2009.2021586,reference:doi.org/10.1109/LGRS.2007.900735,reference:doi.org/10.1109/LGRS.2008.924002,reference:doi.org/10.1109/RADAR.2010.5494489,reference:doi.org/10.1109/TAP.2010.2050424,reference:doi.org/10.1109/TASSP.1985.1164649,reference:doi.org/10.1109/TGRS.2009.2012698,reference:doi.org/10.1109/TGRS.2009.2012849,reference:doi.org/10.1109/TGRS.2009.2037219,reference:doi.org/10.1109/TGRS.2011.2164411,reference:doi.org/10.1109/TPWRS.2017.2748164,reference:doi.org/10.1145/1859995.1859997,reference:doi.org/10.1145/2018436.2018454,reference:doi.org/10.1145/2030613.2030647,reference:doi.org/10.1145/2377677.2377722,tech:wifi,type:paper},
  annotation = {681 citations (Semantic Scholar/DOI) [2023-07-12] --- DOIs\_of\_references: ["10.1109/TGRS.2009.2037219","10.1109/MWSYM.2007.380529","10.1109/TASSP.1985.1164649","10.1145/1614320.1614327","10.1109/LGRS.2007.900735","10.1016/S0963-8695(00)00039-6","10.1109/RADAR.2010.5494489","10.1109/ARRAY.2010.5613313","10.1109/ARRAY.2010.5613314","10.1109/APS.2005.1552508","10.1145/2018436.2018454","10.1002/0471200611","10.1145/2030613.2030647","10.1007/978-3-540-75703-0\_7","10.1109/TGRS.2009.2012849","10.1145/2377677.2377722","10.1145/1859995.1859997","10.1109/TAP.2010.2050424"] ---},
  note = {\url{https://dl.acm.org/doi/10.1145/2486001.2486039}},
  file = {/Users/brk/Zotero/storage/QWSG2ZW8/Adib and Katabi - 2013 - See through walls with WiFi!.pdf}
}
% == BibTeX quality report for adibSeeWallsWiFi2013:
% ? unused Conference name ("SIGCOMM'13: ACM SIGCOMM 2013 Conference")
% ? unused Library catalog ("Semantic Scholar")

@article{agarapDeepLearningUsing2018,
  title = {Deep {{Learning}} Using {{Rectified Linear Units}} ({{ReLU}})},
  author = {Agarap, Abien Fred},
  year = {2018},
  journal = {ArXiv},
  volume = {abs/1803.08375},
  keywords = {background,from:cite.bib}
}

@article{ahaInstancebasedLearningAlgorithms1991,
  title = {Instance-Based Learning Algorithms},
  author = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
  year = {1991},
  month = jan,
  journal = {Machine Learning},
  volume = {6},
  number = {1},
  pages = {37--66},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00153759},
  urldate = {2023-07-14},
  abstract = {Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-world databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.},
  langid = {english},
  keywords = {background,model:instance-based-learning},
  annotation = {2576 citations (Semantic Scholar/DOI) [2023-07-14]},
  note = {\url{http://link.springer.com/10.1007/BF00153759}},
  file = {/Users/brk/Zotero/storage/QNMUDZH2/Aha et al. - 1991 - Instance-based learning algorithms.pdf}
}
% == BibTeX quality report for ahaInstancebasedLearningAlgorithms1991:
% ? unused Journal abbreviation ("Mach Learn")
% ? unused Library catalog ("Semantic Scholar")

@article{ahmedDFWiSLRDeviceFreeWiFibased2020,
  title = {{{DF-WiSLR}}: {{Device-Free Wi-Fi-based Sign Language Recognition}}},
  shorttitle = {{{DF-WiSLR}}},
  author = {Ahmed, Hasmath Farhana Thariq and Ahmad, Hafisoh and Narasingamurthi, Kulasekharan and Harkat, Houda and Phang, Swee King},
  year = {2020},
  month = nov,
  journal = {Pervasive and Mobile Computing},
  volume = {69},
  pages = {101289},
  issn = {15741192},
  doi = {10.1016/j.pmcj.2020.101289},
  urldate = {2023-07-12},
  abstract = {Recent advancements in wireless technologies enable pervasive and device free gesture recognition that enable assisted living utilizing off the shelf commercial Wi-Fi devices. This paper proposes a Device-Free Wi-Fi-based Sign Language Recognition (DF-WiSLR) for recognizing 30 static and 19 dynamic sign gestures. The raw Channel State Information (CSI) acquired from the Wi-Fi device for 49 sign gestures, with a volunteer performing the sign gestures in home and office environments. The proposed system adopts machine learning classifiers such as SVM, KNN, RF, NB, and a deep learning classifier CNN, for measuring the gesture recognition accuracy. To address the practical limitation of building a voluminous dataset, DF-WiSLR augments the originally acquired CSI values with Additive White Gaussian Noise (AWGN). Higher-order cumulant features of orders 2, 3, and 4 are extracted from the original and augmented data, as the machine learning classifiers demand manual feature extraction. To reduce the computational complexity of machine learning classifiers, an informative and reduced optimal feature subset is selected using MIFS. Whilst the pre-processed original and augmented CSI values directly fed as input to an 8-layer deep CNN, it performs auto feature extraction and selection. DF-WiSLR reported better recognition accuracies with SVM for static and dynamic gestures in both home and office environments. SVM achieved 93.4\% 98.8\% and 98.9\% accuracies in home and office environments respectively, for static gestures. For dynamic gestures, 92.3\% recognition accuracy achieved in home environment. On augmented data, the corresponding gesture recognition accuracy values reported are 97.1\%, 99.9\%, 99.9\%, and 98.5\%.},
  langid = {english},
  keywords = {app:sign-language,based-on:wifi,classes:49,model:cnn,model:knn,model:naive-bayes,model:nn,model:random-forests,model:svm,movement:dynamic,movement:static,tech:wifi,type:paper},
  annotation = {10 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1574119220301267}}
}
% == BibTeX quality report for ahmedDFWiSLRDeviceFreeWiFibased2020:
% ? unused Library catalog ("Semantic Scholar")

@article{ahmedRealtimeSignLanguage2021,
  title = {Real-Time Sign Language Framework Based on Wearable Device: Analysis of {{MSL}}, {{DataGlove}}, and Gesture Recognition},
  shorttitle = {Real-Time Sign Language Framework Based on Wearable Device},
  author = {Ahmed, M. A. and Zaidan, B. B. and Zaidan, A. A. and Alamoodi, A. H. and Albahri, O. S. and {Al-Qaysi}, Z. T. and Albahri, A. S. and Salih, Mahmood M.},
  year = {2021},
  month = aug,
  journal = {Soft Computing},
  volume = {25},
  number = {16},
  pages = {11101--11122},
  issn = {1432-7643, 1433-7479},
  doi = {10.1007/s00500-021-05855-6},
  urldate = {2023-03-07},
  abstract = {Researchers have been inspired to use technology to enable people with hearing and speech impairment to communicate and engage with others around them. Sensory approach to recognition facilitates real-time and accurate recognition of signs. Thus, this study proposes a Malaysian Sign Language (MSL) recognition framework. The framework consists of three sub-modules for the recognition of static isolated signs based on data collected from a DataGlove. The first module focuses on the characteristics of signs, yielding sign recognition system requirements. The second module describes the different steps required to develop a wearable sign-capture device. The third module discusses the real-time SL recognition approach, which uses a template-matching algorithm to recognize acquired data. The final design of the DataGlove with 65 data channel fulfils the requirement identified from an analysis of MSL. The DataGlove is able to record data for all of the signs (both dynamic and static) of MSL due to the wide range of captured hand features. As a result, the recognition engine can accurately recognize complex signs.},
  langid = {english},
  keywords = {/unread,app:sign-language,based-on:gloves,pdf:paywalled,type:paper},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://link.springer.com/10.1007/s00500-021-05855-6}}
}
% == BibTeX quality report for ahmedRealtimeSignLanguage2021:
% ? unused Journal abbreviation ("Soft Comput")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{ahujaHandGestureRecognition2015,
  title = {Hand {{Gesture Recognition Using PCA}}},
  author = {Ahuja, M. and Singh, Amardeep},
  year = {2015},
  urldate = {2023-07-12},
  abstract = {Interacting with physical world using expressive body movements is much easier and effective than just speaking. Gesture recognition turns up to be important field in the recent years. Communication through gestures has been used since early ages not only by physically challenged persons but nowadays for many other applications. As most predominantly hand is use to perform gestures, Hand Gesture Recognition have been widely accepted for numerous applications such as human computer interactions, robotics, sign language recognition, etc. This paper focuses on bare hand gesture recognition system by proposing a scheme using a database-driven hand gesture recognition based upon skin color model approach and thresholding approach along with an effective template matching with can be effectively used for human robotics applications and similar other applications.. Initially, hand region is segmented by applying skin color model in YCbCr color space. In the next stage otsu thresholding is applied to separate foreground and background. Finally, template based matching technique is developed using Principal Component Analysis (PCA) for recognition. The system is tested with the controlled and uncontrolled database and shows 100\% accuracy with controlled database and 91.43\% with low brightness images.},
  keywords = {app:hand-gesture-recognition,based-on:vision,exclude:poor-quality,model:pca,tech:rgb,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Hand-Gesture-Recognition-Using-PCA-Ahuja-Singh/43b7e88adbcdfcf5b25d337821c94a0f0ee525b3}},
  file = {/Users/brk/Zotero/storage/6NS5M2E7/Ahuja and Singh - 2015 - Hand Gesture Recognition Using PCA.pdf}
}
% == BibTeX quality report for ahujaHandGestureRecognition2015:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{aklAccelerometerbasedGestureRecognition2010,
  title = {Accelerometer-Based Gesture Recognition via Dynamic-Time Warping, Affinity Propagation, \&\#x00026; Compressive Sensing},
  author = {Akl, Ahmad and Valaee, Shahrokh},
  year = {2010},
  journal = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages = {2270--2273},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/ICASSP.2010.5495895},
  urldate = {2023-03-07},
  abstract = {We propose a gesture recognition system based primarily on a single 3-axis accelerometer. The system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition. A dictionary of 18 gestures is defined and a database of over 3,700 repetitions is created from 7 users. Our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition, to the best of our knowledge. The proposed system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
  isbn = {9781424442959},
  keywords = {based-on:gloves,classes:18,contains-more-references,dataset:custom,fidelity:arm,have-read,observations:3780,participants:7,reference:doi.org/10.1007/11492429\_77,reference:doi.org/10.1007/s10115-004-0154-9,reference:doi.org/10.1016/j.pmcj.2009.07.007,reference:doi.org/10.1109/CVPRW.2008.4563176,reference:doi.org/10.1109/MSP.2007.914731,reference:doi.org/10.1145/1502650.1502708,repetitions:30,tech:accelerometer,type:paper},
  annotation = {142 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1016/j.pmcj.2009.07.007","10.1109/MSP.2007.914731","10.1007/11492429\_77","10.1145/1502650.1502708","10.1109/CVPRW.2008.4563176","10.1109/MSP.2007.914730","10.1007/s10115-004-0154-9","10.1109/CVPRW.2008.4563176","10.1109/MSP.2007.914730","10.1007/s10115-004-0154-9","10.1145/1502650.1502708","10.1007/11492429\_77"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/5495895/}},
  file = {/Users/brk/Zotero/storage/85QXVKVJ/Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf}
}
% == BibTeX quality report for aklAccelerometerbasedGestureRecognition2010:
% ? unused Library catalog ("Semantic Scholar")

@article{aklNovelAccelerometerBasedGesture2011,
  title = {A {{Novel Accelerometer-Based Gesture Recognition System}}},
  author = {Akl, Ahmad and Feng, Chen and Valaee, Shahrokh},
  year = {2011},
  month = dec,
  journal = {IEEE Transactions on Signal Processing},
  volume = {59},
  number = {12},
  pages = {6197--6205},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2011.2165707},
  urldate = {2023-03-07},
  abstract = {In this paper, we address the problem of gesture recognition using the theory of random projection (RP) and by formulating the whole recognition problem as an {$\mathscr{l}$}1-minimization problem. The gesture recognition system operates primarily on data from a single 3-axis accelerometer and comprises two main stages: a training stage and a testing stage. For training, the system employs dynamic time warping as well as affinity propagation to create exemplars for each gesture while for testing, the system projects all candidate traces and also the unknown trace onto the same lower dimensional subspace for recognition. A dictionary of 18 gestures is defined and a database of over 3700 traces is created from seven subjects on which the system is tested and evaluated. To the best of our knowledge, our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition. The system achieves almost perfect user-dependent recognition, and mixed-user and user-independent recognition accuracies that are highly competitive with systems based on statistical methods and with the other accelerometer-based gesture recognition systems available in the literature.},
  keywords = {based-on:gloves,claims-to-be-best,classes:18,fidelity:arm,from:cite.bib,have-read,model:dynamic-time-warping,participants:7,tech:accelerometer,type:msc},
  annotation = {100 citations (Crossref) [2023-07-11] 170 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1016/j.pmcj.2009.07.007","10.1016/j.pmcj.2009.07.007","10.1016/j.pmcj.2009.07.007","10.1145/1502650.1502708","10.1126/SCIENCE.1136800","10.1016/j.pmcj.2009.07.007"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/5993550/}},
  file = {/Users/brk/Zotero/storage/SAQUJ2PY/Akl et al. - 2011 - A Novel Accelerometer-Based Gesture Recognition Sy.pdf}
}
% == BibTeX quality report for aklNovelAccelerometerBasedGesture2011:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Signal Process.")
% ? unused Library catalog ("Semantic Scholar")

@article{al-qanessWiGeRWiFiBasedGesture2016,
  title = {{{WiGeR}}: {{WiFi-Based Gesture Recognition System}}},
  shorttitle = {{{WiGeR}}},
  author = {{Al-qaness}, Mohammed and Li, Fangmin},
  year = {2016},
  month = jun,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {5},
  number = {6},
  pages = {92},
  issn = {2220-9964},
  doi = {10.3390/ijgi5060092},
  urldate = {2023-06-22},
  abstract = {Recently, researchers around the world have been striving to develop and modernize human\textendash computer interaction systems by exploiting advances in modern communication systems. The priority in this field involves exploiting radio signals so human\textendash computer interaction will require neither special devices nor vision-based technology. In this context, hand gesture recognition is one of the most important issues in human\textendash computer interfaces. In this paper, we present a novel device-free WiFi-based gesture recognition system (WiGeR) by leveraging the fluctuations in the channel state information (CSI) of WiFi signals caused by hand motions. We extract CSI from any common WiFi router and then filter out the noise to obtain the CSI fluctuation trends generated by hand motions. We design a novel and agile segmentation and windowing algorithm based on wavelet analysis and short-time energy to reveal the specific pattern associated with each hand gesture and detect duration of the hand motion. Furthermore, we design a fast dynamic time warping algorithm to classify our system's proposed hand gestures. We implement and test our system through experiments involving various scenarios. The results show that WiGeR can classify gestures with high accuracy, even in scenarios where the signal passes through multiple walls.},
  langid = {english},
  keywords = {based-on:wifi,classes:7,fidelity:finger,model:dynamic-time-warping,movement:dynamic,movement:static,tech:wifi,type:paper},
  annotation = {84 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://www.mdpi.com/2220-9964/5/6/92}},
  file = {/Users/brk/Zotero/storage/DV7B98FA/Al-qaness and Li - 2016 - WiGeR WiFi-Based Gesture Recognition System.pdf}
}
% == BibTeX quality report for al-qanessWiGeRWiFiBasedGesture2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJGI")
% ? unused Library catalog ("Semantic Scholar")

@article{al-shamaylehSystematicLiteratureReview2018,
  title = {A Systematic Literature Review on Vision Based Gesture Recognition Techniques},
  author = {{Al-Shamayleh}, Ahmad Sami and Ahmad, Rodina and Abushariah, Mohammad A. M. and Alam, Khubaib Amjad and Jomhari, Nazean},
  year = {2018},
  month = nov,
  journal = {Multimedia Tools and Applications},
  volume = {77},
  number = {21},
  pages = {28121--28184},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-018-5971-z},
  urldate = {2023-06-26},
  abstract = {Human Computer Interaction (HCI) technologies are rapidly evolving the way we interact with computing devices and adapting to the constantly increasing demands of modern paradigms. One of the most useful tools in this regard is the integration of Human-to-Human Interaction gestures to facilitate communication and expressing ideas. Gesture recognition requires the integration of postures, gestures, face expressions and movements for communicating or conveying certain messages. The aim of this study is to aggregate and synthesize experiences and accumulated knowledge about Vision-Based Recognition (VBR) techniques. The major objective of conducting this Systematic Literature Review (SLR) is to highlight the state-of-the-art in the context of vision-based gesture recognition with specific focus on hand gesture recognition (HGR) techniques and enabling technologies. After a careful systematic selection process, 100 studies relevant to the four research questions were selected. This process was followed by data collection, a detailed analysis, and a synthesis of the selected studies. The results reveal that among the VBR techniques, HGR is a predominant and highly focused area of research. Research focus is also found to be converging towards sign language recognition. Potential applications of HGR techniques include desktop applications, smart environments, entertainment, sign language interpretation, virtual reality and gamification. Although various experimental research efforts have been devoted to gestures recognition, there are still numerous open issues and research challenges in this field. Lastly, considering the results from this SLR, potential future research directions are suggested, including a much needed focus on grammatical interpretation, hybrid approaches, smartphone devices, normalization, and real-life systems.},
  langid = {english},
  keywords = {app:sign-language,based-on:vision,read-priority-1,type:survey},
  annotation = {71 Citations},
  note = {\url{http://link.springer.com/10.1007/s11042-018-5971-z}},
  file = {/Users/brk/Zotero/storage/BL4SNP7J/Al-Shamayleh et al. - 2018 - A systematic literature review on vision based ges.pdf}
}
% == BibTeX quality report for al-shamaylehSystematicLiteratureReview2018:
% ? unused Journal abbreviation ("Multimed Tools Appl")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{alaniHandGestureRecognition2018,
  title = {Hand Gesture Recognition Using an Adapted Convolutional Neural Network with Data Augmentation},
  booktitle = {2018 4th {{International Conference}} on {{Information Management}} ({{ICIM}})},
  author = {Alani, Ali A. and Cosma, Georgina and Taherkhani, Aboozar and McGinnity, T.M},
  year = {2018},
  month = may,
  pages = {5--12},
  publisher = {{IEEE}},
  address = {{Oxford}},
  doi = {10.1109/INFOMAN.2018.8392660},
  urldate = {2023-07-20},
  abstract = {Hand gestures provide a natural way for humans to interact with computers to perform a variety of different applications. However, factors such as the complexity of hand gesture structures, differences in hand size, hand posture, and environmental illumination can influence the performance of hand gesture recognition algorithms. Recent advances in Deep Learning have significantly advanced the performance of image recognition systems. In particular, the Deep Convolutional Neural Network has demonstrated superior performance in image representation and classification, compared to conventional machine learning approaches. This paper proposes an Adapted Deep Convolutional Neural Network (ADCNN) suitable for hand gesture recognition tasks. Data augmentation is initially applied which shifts images both horizontally and vertically to an extent of 20\% of the original dimensions randomly, in order to numerically increase the size of the dataset and to add the robustness needed for a deep learning approach. These images are input into the proposed ADCNN model which is empowered by the presence of network initialization (ReLU and Softmax) and L2 Regularization to eliminate the problem of data overfitting. With these modifications, the experimental results using the ADCNN model demonstrate that it is an effective method of increasing the performance of CNN for hand gesture recognition. The model was trained and tested using 3750 static hand gesture images, which incorporate variations in features such as scale, rotation, translation, illumination and noise. The proposed ADCNN was compared to a baseline Convolutional Neural Network and the results show that the proposed ADCNN achieved a classification recognition accuracy of 99.73\%, and a 4\% improvement over the baseline Convolutional Neural Network model (95.73\%).},
  isbn = {978-1-5386-6147-5},
  keywords = {model:cnn,model:nn,pdf:paywalled,untagged},
  note = {\url{https://ieeexplore.ieee.org/document/8392660/}}
}
% == BibTeX quality report for alaniHandGestureRecognition2018:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("DOI.org (Crossref)")

@article{alazraiDatasetWiFibasedHumantohuman2020,
  title = {A Dataset for {{Wi-Fi-based}} Human-to-Human Interaction Recognition},
  author = {Alazrai, Rami and Awad, Ali and Alsaify, Baha'A. and Hababeh, Mohammad and Daoud, Mohammad I.},
  year = {2020},
  month = aug,
  journal = {Data in Brief},
  volume = {31},
  pages = {105668},
  issn = {23523409},
  doi = {10.1016/j.dib.2020.105668},
  urldate = {2023-07-12},
  abstract = {This paper presents a dataset for Wi-Fi-based human-tohuman interaction recognition that comprises twelve different interactions performed by 40 different pairs of subjects in an indoor environment. Each pair of subjects performed ten trials of each of the twelve interactions and the total number of trials recorded in our dataset for all the 40 pairs of subjects is 4800 trials (i.e., 40 pairs of subjects \texttimes{} 12 interactions \texttimes{} 10 trials). The publicly available CSI tool [1] is used to record the Wi-Fi signals transmitted from a commercial off-the-shelf access point, namely the Sagemcom 2704 access point, to a desktop computer that is equipped with an Intel 5300 network interface card. The recorded Wi-Fi signals consist of the Received Signal Strength Indicator (RSSI) values and the Channel State Information (CSI) values. Unlike the publicly available Wi-Fi-based human activity datasets, which mainly have focused on activities performed by a single human, our dataset provides a collection of Wi-Fi signals that are recorded for 40 different pairs of subjects while performing twelve two-person interactions. The presented dataset can be exploited to advance Wi-Fi-based human activity recognition in different aspects, such as the use of},
  langid = {english},
  keywords = {based-on:wifi,classes:12,dataset:alazrai-wifi,fidelity:arm,participants:40,tech:wifi,type:dataset},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S235234092030562X}},
  file = {/Users/brk/Zotero/storage/TNBL8TTY/Alazrai et al. - 2020 - A dataset for Wi-Fi-based human-to-human interacti.pdf}
}
% == BibTeX quality report for alazraiDatasetWiFibasedHumantohuman2020:
% ? unused Library catalog ("Semantic Scholar")

@article{alazraiEndtoEndDeepLearning2020,
  title = {An {{End-to-End Deep Learning Framework}} for {{Recognizing Human-to-Human Interactions Using Wi-Fi Signals}}},
  author = {Alazrai, Rami and Hababeh, Mohammad and Alsaify, Baha A. and Ali, Mostafa Z. and Daoud, Mohammad I.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {197695--197710},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3034849},
  urldate = {2023-07-12},
  abstract = {Channel state information (CSI)-based human activity recognition plays an essential role in various application domains, such as security, healthcare, and Internet of Things. Most existing CSI-based activity recognition approaches rely on manually designed features that are classified using traditional classification methods. Furthermore, the use of deep learning methods for CSI-based activity recognition is still at its infancy with most of the existing approaches focus on recognizing single-human activities. The current study explores the feasibility of utilizing deep learning methods to recognize human-to-human interactions (HHIs) using CSI signals. Particularly, we introduce an end-to-end deep learning framework that comprises three phases, which are the input, feature extraction, and recognition phases. The input phase converts the raw CSI signals into CSI images that comprise time, frequency, and spatial information. In the feature extraction phase, a novel convolutional neural network (CNN) is designed to automatically extract deep features from the CSI images. Finally, the extracted features are fed to the recognition phase to identify the class of the HHI associated with each CSI image. The performance of our proposed framework is assessed using a publicly available CSI dataset that was acquired from 40 different pairs of subjects while performing 13 HHIs. Our proposed framework achieved an average recognition accuracy of 86.3\% across all HHIs. Moreover, the experiments indicate that our proposed framework enabled significant improvements over the results achieved using three state-of-the-art pre-trained CNNs as well as the results obtained using four different conventional classifiers that employs traditional handcrafted features.},
  keywords = {based-on:wifi,classes:13,dataset:alazrai-wifi,fidelity:arm,model:cnn,model:nn,participants:40,tech:wifi,type:paper},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/9243938/}},
  file = {/Users/brk/Zotero/storage/JAT5WT5H/Alazrai et al. - 2020 - An End-to-End Deep Learning Framework for Recogniz.pdf}
}
% == BibTeX quality report for alazraiEndtoEndDeepLearning2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{aliKeystrokeRecognitionUsing2015,
  title = {Keystroke {{Recognition Using WiFi Signals}}},
  author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {90--102},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790109},
  urldate = {2023-07-12},
  abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of Channel State Information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal based keystroke recognition system called WiKey. WiKey consists of two Commercial Off-The-Shelf (COTS) WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves more than 97.5\textbackslash\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,classes:37,fidelity:finger,participants:10,read-priority-1,tech:wifi,type:paper},
  annotation = {487 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2789168.2790109}},
  file = {/Users/brk/Zotero/storage/DGBM94AK/Ali et al. - 2015 - Keystroke Recognition Using WiFi Signals.pdf}
}
% == BibTeX quality report for aliKeystrokeRecognitionUsing2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking")
% ? unused Library catalog ("Semantic Scholar")

@article{aliRecognizingKeystrokesUsing2017,
  title = {Recognizing {{Keystrokes Using WiFi Devices}}},
  author = {Ali, Kamran and Liu, Alex X. and Wang, Wei and Shahzad, Muhammad},
  year = {2017},
  month = may,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {35},
  number = {5},
  pages = {1175--1190},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2017.2680998},
  urldate = {2023-07-12},
  abstract = {Keystroke privacy is critical for ensuring the security of computer systems and the privacy of human users as what is being typed could be passwords or privacy sensitive information. In this paper, we show for the first time that WiFi signals can also be exploited to recognize keystrokes. The intuition is that while typing a certain key, the hands and fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time-series of channel state information (CSI) values, which we call CSI-waveform for that key. In this paper, we propose a WiFi signal-based keystroke recognition system called WiKey. WiKey consists of two commercial off-the-shelf WiFi devices, a sender (such as a router) and a receiver (such as a laptop). The sender continuously emits signals and the receiver continuously receives signals. When a human subject types on a keyboard, WiKey recognizes the typed keys based on how the CSI values at the WiFi signal receiver end. We implemented the WiKey system using a TP-Link TL-WR1043ND WiFi router and a Lenovo X200 laptop. WiKey achieves over 97.5\% detection rate for detecting the keystroke and 96.4\% recognition accuracy for classifying single keys. In real-world experiments, WiKey can recognize keystrokes in a continuously typed sentence with an accuracy of 93.5\%. WiKey can also recognize complete words inside a sentence with over 85\% accuracy.},
  keywords = {app:activity-inference,app:keystroke-inference,based-on:wifi,fidelity:finger,tech:wifi,type:paper},
  annotation = {86 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7875144/}}
}
% == BibTeX quality report for aliRecognizingKeystrokesUsing2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE J. Select. Areas Commun.")
% ? unused Library catalog ("Semantic Scholar")

@misc{AlphaStarMasteringRealtime2019,
  title = {{{AlphaStar}}: {{Mastering}} the Real-Time Strategy Game {{StarCraft II}}},
  shorttitle = {{{AlphaStar}}},
  year = {2019},
  month = jan,
  journal = {Google DeepMind},
  urldate = {2023-11-12},
  abstract = {Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a ``grand challenge'' for AI research.},
  howpublished = {\url{https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/}},
  langid = {english},
  keywords = {/unread},
  file = {/Users/brk/Zotero/storage/6WNE4AEC/alphastar-mastering-the-real-time-strategy-game-starcraft-ii.html}
}

@article{alviPakistanSignLanguage2007,
  title = {Pakistan {{Sign Language Recognition Using Statistical Template Matching}}},
  author = {Alvi, A. and Azhar, M. and Usman, M. and Mumtaz, Suleman and Rafiq, Sameer and Rehman, RaziUr and Ahmed, Israr},
  year = {2007},
  month = mar,
  journal = {World Academy of Science, Engineering and Technology, International Journal of Computer, Electrical, Automation, Control and Information Engineering},
  urldate = {2023-07-12},
  abstract = {Sign language recognition has been a topic of research since the first data glove was developed. Many researchers have attempted to recognize sign language through various techniques. However none of them have ventured into the area of Pakistan Sign Language (PSL). The Boltay Haath project aims at recognizing PSL gestures using Statistical Template Matching. The primary input device is the DataGlove5 developed by 5DT. Alternative approaches use camera-based recognition which, being sensitive to environmental changes are not always a good choice. This paper explains the use of Statistical Template Matching for gesture recognition in Boltay Haath. The system recognizes one handed alphabet signs from PSL. Keywords\textemdash Gesture Recognition, Pakistan Sign Language, Data Glove, Human Computer Interaction, Template Matching, Boltay Haath},
  keywords = {app:american-sl,app:pakistan-sl,app:sign-language,based-on:gloves,classes:26,classes:33,fidelity:finger,from:cite.bib,hardware:5dt-dataglove,have-read,model:statistical-template-matching,movement:static,participants:6,tech:accelerometer,tech:flex,type:paper},
  annotation = {--- DOIs\_of\_references: ["10.3390/technologies11040083","10.1109/T-C.1972.223477","10.1007/3-540-47873-6\_7","10.1145/2402676","10.1109/ICME.2000.869596","10.3390/s22030706","10.1109/IJCNN.2000.860762","10.1002/lob.10440"] ---},
  note = {\url{https://www.semanticscholar.org/paper/Pakistan-Sign-Language-Recognition-Using-Template-Alvi-Azhar/2ca46ccbde2346b336adc12948b234c75ed37686}},
  file = {/Users/brk/Zotero/storage/K5JIJI47/Alvi et al. - 2007 - Pakistan Sign Language Recognition Using Statistic.pdf}
}
% == BibTeX quality report for alviPakistanSignLanguage2007:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{alySurveyMulticlassClassification2005,
  title = {Survey on Multiclass Classification Methods},
  author = {Aly, Mohamed},
  year = {2005},
  journal = {Neural Netw},
  volume = {19},
  number = {1-9},
  pages = {2},
  publisher = {{Citeseer}},
  keywords = {background}
}

@article{alzubaidiNovelAssistiveGlove2023,
  title = {A {{Novel Assistive Glove}} to {{Convert Arabic Sign Language}} into {{Speech}}},
  author = {Alzubaidi, Mohammad A. and Otoom, Mwaffaq and Abu Rwaq, Areen M.},
  year = {2023},
  month = mar,
  journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
  volume = {22},
  number = {2},
  pages = {1--16},
  issn = {2375-4699, 2375-4702},
  doi = {10.1145/3545113},
  urldate = {2023-03-07},
  abstract = {People with speech disorders often communicate through special gestures and sign language gestures. However, other people around them might not understand the meaning of those gestures. The research described in this article is aimed at providing an assistive device to help those people communicate with others by translating their gestures into a spoken voice that others can understand. The proposed device includes an electronic glove that is worn on the hand. It employs an MPU6050 accelerometer/gyro with 6 degrees of freedom to continuously monitor hand orientation and movement, plus a potentiometer for each finger, to monitor changes in finger posture. The signals from the MPU6050 and the potentiometers are routed to an Arduino board, where they are processed to determine the meaning of each gesture, which is then voiced using the audio streams stored in an SD memory card. The audio output drives a speaker, allowing the listener to understand the meaning of each gesture. We built a database with the help of 10 deaf people who cannot speak. We asked them to wear the glove while performing a set of 40 Arabic sign language words and recorded the resulting data stream from the glove. That data was then used to train seven different learning algorithms. The results showed that the Decision Tree learning algorithm achieved the highest accuracy of 98\%. A usability study was then conducted to determine the usefulness of the assistive device in real-time.},
  langid = {english},
  keywords = {app:arabic-sl,app:sign-language,based-on:gloves,classes:40,dataset:custom,fidelity:finger,has-picture,model:decision-tree,model:knn,model:logistic,model:naive-bayes,model:random-forests,movement:dynamic,participants:10,reference:doi.org/10.1017/CBO9780511863189,reference:doi.org/10.1109/ISWC.2003.1241392,reference:doi.org/10.1109/TNSRE.2019.2905658,reference:doi.org/10.1111/J.1442-9993.2001.01070.PP.X,reference:doi.org/10.1126/science.abf7652,reference:doi.org/10.1353/SLS.1994.0002,reference:doi.org/10.14257/IJUNESST.2015.8.1.12,reference:doi.org/10.17632/46HTWNP833.1,reference:doi.org/10.5120/21131-4058,repetitions:1,segmentation:explicit,tech:accelerometer,tech:potentiometer,type:paper,unimpressive},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1017/CBO9780511863189","10.5120/21131-4058","10.1109/TNSRE.2019.2905658","10.1126/science.abf7652","10.1109/ISWC.2003.1241392","10.1111/J.1442-9993.2001.01070.PP.X","10.14257/IJUNESST.2015.8.1.12","10.17632/46HTWNP833.1","10.1353/SLS.1994.0002"] ---},
  note = {\url{https://dl.acm.org/doi/10.1145/3545113}},
  file = {/Users/brk/Zotero/storage/NBR6BM4U/Alzubaidi et al. - 2023 - A Novel Assistive Glove to Convert Arabic Sign Lan.pdf}
}
% == BibTeX quality report for alzubaidiNovelAssistiveGlove2023:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("ACM Trans. Asian Low-Resour. Lang. Inf. Process.")
% ? unused Library catalog ("Semantic Scholar")

@article{ammaAirwritingWearableHandwriting2014,
  title = {Airwriting: A Wearable Handwriting Recognition System},
  shorttitle = {Airwriting},
  author = {Amma, Christoph and Georgi, Marcus and Schultz, Tanja},
  year = {2014},
  month = jan,
  journal = {Personal and Ubiquitous Computing},
  volume = {18},
  number = {1},
  pages = {191--203},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-013-0637-3},
  urldate = {2023-07-11},
  abstract = {We present a wearable input system which enables interaction through 3D handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. The handwriting gestures are captured wirelessly by motion sensors applying accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a support vector machine to identify those data segments which contain handwriting. The recognition stage uses hidden Markov models (HMMs) to generate a text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary. A statistical language model is used to enhance recognition performance and to restrict the search space. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than traditional vocabularies for known systems. In a first experiment, we evaluate the spotting algorithm on a realistic data set including everyday activities. In a second experiment, we report the results from a nine-user experiment on handwritten sentence recognition. Finally, we evaluate the end-to-end system on a small but realistic data set.},
  langid = {english},
  keywords = {app:writing,based-on:gloves,classes:26,fidelity:arm,hardware:custom,have-read,language-model,model:hmm,model:svm,observations:68,participants:9,reference:doi.org/10.1007/978-3-642-02710-9,reference:doi.org/10.1016/j.patcog.2007.11.016,reference:doi.org/10.1109/34.824821,reference:doi.org/10.1109/5.18626,reference:doi.org/10.1109/ASRU.2001.1034625,reference:doi.org/10.1109/MPRV.2008.40,reference:doi.org/10.1109/TIT.2002.1003838,reference:doi.org/10.1109/tsp.2004.832562,reference:doi.org/10.1145/3583133.3595827,reference:doi.org/10.18653/v1/2022.naacl-demo.1,reference:doi.org/10.21437/ICSLP.2002-151,reference:doi.org/10.21437/ICSLP.2002-303,reference:doi.org/10.5120/IJCA2016909103,reference:doi.org/10.52842/conf.acadia.2012.429,repetitions:25,segmentation:implicit,tech:accelerometer,tech:imu,type:paper},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1109/34.824821","10.52842/conf.acadia.2012.429","10.1109/5.18626","10.1109/ASRU.2001.1034625","10.21437/ICSLP.2002-151","10.5120/IJCA2016909103","10.21437/ICSLP.2002-303","10.1109/5.18626","10.1109/ASRU.2001.1034625","10.52842/conf.acadia.2012.429","10.21437/ICSLP.2002-303","10.1145/3583133.3595827","10.1109/MPRV.2008.40","10.18653/v1/2022.naacl-demo.1","10.1016/j.patcog.2007.11.016","10.1109/tsp.2004.832562","10.1109/TIT.2002.1003838","10.5120/IJCA2016909103","10.1007/978-3-642-02710-9"] ---},
  note = {\url{http://link.springer.com/10.1007/s00779-013-0637-3}},
  file = {/Users/brk/Zotero/storage/BADE4R9Q/Amma et al. - 2014 - Airwriting a wearable handwriting recognition sys.pdf}
}
% == BibTeX quality report for ammaAirwritingWearableHandwriting2014:
% ? unused Journal abbreviation ("Pers Ubiquit Comput")
% ? unused Library catalog ("Semantic Scholar")

@misc{analogdevicesADXL335DatasheetProduct2010,
  title = {{{ADXL335 Datasheet}} and {{Product Info}} | {{Analog Devices}}},
  author = {{AnalogDevices}},
  year = {2010},
  journal = {ADXL335 Datasheet and Product Info | Analog Devices},
  publisher = {{www.analog.com}},
  howpublished = {\url{https://www.analog.com/en/products/adxl335.html\#product-overview}},
  keywords = {from:cite.bib,type:datasheet}
}
% == BibTeX quality report for analogdevicesADXL335DatasheetProduct2010:
% ? Title looks like it was stored in title-case in Zotero

@article{anwarHandGestureRecognition2019,
  title = {Hand {{Gesture Recognition}}: {{A Survey}}},
  shorttitle = {Hand {{Gesture Recognition}}},
  author = {Anwar, Shamama and Sinha, Subham Kumar and Vivek, Snehanshu and Ashank, Vishal},
  editor = {Nath, Vijay and Mandal, Jyotsna Kumar},
  year = {2019},
  volume = {511},
  pages = {365--371},
  doi = {10.1007/978-981-13-0776-8_33},
  urldate = {2023-03-07},
  abstract = {A human\textendash computer interaction is generally limited to taking input from the user using handheld devices like keyboard, mouse, or scanners. With the advancement in computers, the user interaction approaches have also advanced. Direct use of hands as an input device is an attractive method for providing natural Human\textendash Computer Interaction. It is also helpful for people who use sign language. The chapter aims to study the existing methods for Hand Gesture Recognition and provide a comparative analysis of the same. The entire process of hand gesture recognition is divided into three phases: hand detection, hand tracking, and recognition. The chapter includes a review of the different methods used for the hand gesture recognition. The recognition phase is classified based on the way the input is received as glove based or vision based. For recognition, various methods like Feature extraction, Hidden Markov Model (HMM), Principal Component Analysis (PCA) are compared along with the reported accuracy.},
  langid = {english},
  keywords = {based-on:gloves,based-on:vision,contains-more-references,type:survey},
  annotation = {15 citations (Crossref) [2023-07-21] df --- DOIs\_of\_references: ["10.1109/TMM.2018.2856094","10.1109/JSEN.2018.2859815","10.1109/TPAMI.2016.2537340","10.1109/BIOROB.2016.7523814","10.1109/TNNLS.2017.2716952","10.1109/THMS.2016.2611824","10.1109/TCSVT.2018.2855416","10.1109/INFOMAN.2018.8392660","10.1109/DSC.2018.00087","10.1109/TIM.2015.2498560","10.1109/LSP.2016.2590470","10.1109/ICIP.2015.7351216","10.1109/TMM.2014.2374357","10.1109/TPAMI.2015.2461544","10.1109/ROMAN.2012.6343787","10.1007/978-3-642-31968-6\_89","10.1007/s11265-015-1090-5"] ---},
  note = {\url{http://link.springer.com/10.1007/978-981-13-0776-8_33}},
  file = {/Users/brk/Zotero/storage/IY3XGDPZ/Anwar et al. - 2019 - Hand Gesture Recognition A Survey.pdf}
}
% == BibTeX quality report for anwarHandGestureRecognition2019:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Electrical Engineering")

@misc{arduinoArduinoHome2005,
  title = {Arduino - {{Home}}},
  author = {{Arduino}},
  year = {2005},
  journal = {Arduino - Home},
  publisher = {{www.arduino.cc}},
  howpublished = {\url{https://www.arduino.cc/}},
  keywords = {from:cite.bib,type:datasheet}
}
% == BibTeX quality report for arduinoArduinoHome2005:
% ? Title looks like it was stored in title-case in Zotero

@misc{arduinoArduinoNano332016,
  title = {Arduino {{Nano}} 33 {{BLE}} \textemdash{} {{Arduino Official Store}}},
  author = {{Arduino}},
  year = {2016},
  journal = {Arduino Official Store},
  publisher = {{store.arduino.cc}},
  howpublished = {\url{https://store.arduino.cc/products/arduino-nano-33-ble}},
  keywords = {from:cite.bib,type:datasheet}
}
% == BibTeX quality report for arduinoArduinoNano332016:
% ? Title looks like it was stored in title-case in Zotero

@misc{arreguiSensorGlove2017,
  title = {Sensor {{Glove}}},
  author = {Arregui, Patxi Xabier Quintana},
  year = {2017},
  howpublished = {\url{https://sensorglove.wixsite.com/sensorglove/producto}},
  keywords = {/unread,background,based-on:gloves,from:cite.bib,type:product}
}
% == BibTeX quality report for arreguiSensorGlove2017:
% ? Title looks like it was stored in title-case in Zotero

@article{asadi-aghbolaghiSurveyDeepLearning2017,
  title = {A {{Survey}} on {{Deep Learning Based Approaches}} for {{Action}} and {{Gesture Recognition}} in {{Image Sequences}}},
  author = {{Asadi-Aghbolaghi}, Maryam and Clapes, Albert and Bellantonio, Marco and Escalante, Hugo Jair and {Ponce-Lopez}, Victor and Baro, Xavier and Guyon, Isabelle and Kasaei, Shohreh and Escalera, Sergio},
  year = {2017},
  month = may,
  journal = {2017 12th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2017)},
  pages = {476--483},
  publisher = {{IEEE}},
  address = {{Washington, DC, DC, USA}},
  doi = {10.1109/FG.2017.150},
  urldate = {2023-07-12},
  abstract = {The interest in action and gesture recognition has grown considerably in the last years. In this paper, we present a survey on current deep learning methodologies for action and gesture recognition in image sequences. We introduce a taxonomy that summarizes important aspects of deep learning for approaching both tasks. We review the details of the proposed architectures, fusion strategies, main datasets, and competitions. We summarize and discuss the main works proposed so far with particular interest on how they treat the temporal dimension of data, discussing their main features and identify opportunities and challenges for future research.},
  isbn = {9781509040230},
  keywords = {based-on:vision,model:cnn,model:lstm,model:nn,tech:rgb,type:survey},
  annotation = {160 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7961779/}},
  file = {/Users/brk/Zotero/storage/4BDF37WP/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf;/Users/brk/Zotero/storage/Y9X75IZ5/Asadi-Aghbolaghi et al. - 2017 - A Survey on Deep Learning Based Approaches for Act.pdf}
}
% == BibTeX quality report for asadi-aghbolaghiSurveyDeepLearning2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{asgharioskoeiMyoelectricControlSystems2007,
  title = {Myoelectric Control Systems\textemdash{{A}} Survey},
  author = {Asghari Oskoei, Mohammadreza and Hu, Huosheng},
  year = {2007},
  month = oct,
  journal = {Biomedical Signal Processing and Control},
  volume = {2},
  number = {4},
  pages = {275--294},
  issn = {17468094},
  doi = {10.1016/j.bspc.2007.07.009},
  urldate = {2023-07-30},
  abstract = {Semantic Scholar extracted view of "Myoelectric control systems - A survey" by M. A. Oskoei et al.},
  langid = {english},
  keywords = {based-on:gloves,reference:doi.org/10.1016/S1050-6411(00)00025-0,reference:doi.org/10.1016/S1050-6411(01)00033-5,reference:doi.org/10.1016/S1350-4533(99)00066-1,reference:doi.org/10.1109/10.661154,reference:doi.org/10.1109/10.764944,reference:doi.org/10.1109/86.895950,reference:doi.org/10.5772/14146,tech:emg,type:survey},
  annotation = {1117 citations (Semantic Scholar/DOI) [2023-07-30] --- DOIs\_of\_references: ["10.1109/10.764944","10.1109/86.895950","10.1016/S1050-6411(00)00025-0","10.5772/14146"] ---},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1746809407000547}},
  file = {/Users/brk/Zotero/storage/AGWXRKIX/Asghari Oskoei and Hu - 2007 - Myoelectric control systems—A survey.pdf}
}
% == BibTeX quality report for asgharioskoeiMyoelectricControlSystems2007:
% ? unused Library catalog ("Semantic Scholar")

@misc{atltvheadAtltvheadGestureRecognition,
  title = {Atltvhead {{Gesture Recognition Bracer}}},
  author = {{ATLTVHEAD}},
  abstract = {Atltvhead Gesture Recognition Bracer - A TensorflowLite gesture detector for the atltvhead project and exploration into Data Science This repository is my spin on Jennifer Wang's and Google Tensorflow's magic wand project, but for arm gestures},
  keywords = {tech:accelerometer}
}
% == BibTeX quality report for atltvheadAtltvheadGestureRecognition:
% ? Title looks like it was stored in title-case in Zotero

@article{atzoriNinaproDatabaseResource2015,
  title = {The {{Ninapro}} Database: {{A}} Resource for {{sEMG}} Naturally Controlled Robotic Hand Prosthetics},
  shorttitle = {The {{Ninapro}} Database},
  author = {Atzori, Manfredo and Muller, Henning},
  year = {2015},
  month = aug,
  journal = {2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages = {7151--7154},
  publisher = {{IEEE}},
  address = {{Milan}},
  doi = {10.1109/EMBC.2015.7320041},
  urldate = {2023-07-02},
  abstract = {The dexterous natural control of robotic prosthetic hands with non-invasive techniques is still a challenge: surface electromyography gives some control capabilities but these are limited, often not natural and require long training times; the application of pattern recognition techniques recently started to be applied in practice. While results in the scientific literature are promising they have to be improved to reach the real needs. The Ninapro database aims to improve the field of naturally controlled robotic hand prosthetics by permitting to worldwide research groups to develop and test movement recognition and force control algorithms on a benchmark database. Currently, the Ninapro database includes data from 67 intact subjects and 11 amputated subject performing approximately 50 different movements. The data are aimed at permitting the study of the relationships between surface electromyography, kinematics and dynamics. The Ninapro acquisition protocol was created in order to be easy to be reproduced. Currently, the number of datasets included in the database is increasing thanks to the collaboration of several research groups.},
  isbn = {9781424492718},
  keywords = {app:medical,classes:50,dataset:ninapro,fidelity:finger,hardware:cyberglove-ii,have-read,model:none,movement:dynamic,participants:78,segmentation:explicit,tech:accelerometer,tech:emg,tech:flex,type:dataset},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7320041/}},
  file = {/Users/brk/Zotero/storage/58UAI4GS/Atzori and Muller - 2015 - The Ninapro database A resource for sEMG naturall.pdf}
}
% == BibTeX quality report for atzoriNinaproDatabaseResource2015:
% ? unused Library catalog ("Semantic Scholar")

@article{avolaExploitingRecurrentNeural2019,
  title = {Exploiting {{Recurrent Neural Networks}} and {{Leap Motion Controller}} for the {{Recognition}} of {{Sign Language}} and {{Semaphoric Hand Gestures}}},
  author = {Avola, Danilo and Bernardi, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Multimedia},
  volume = {21},
  number = {1},
  pages = {234--245},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2018.2856094},
  urldate = {2023-07-02},
  abstract = {Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human\textendash human communication and human\textendash computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96\% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:14,classes:28,dataset:shrec,fidelity:finger,have-read,model:nn,model:rnn,movement:dynamic,movement:static,participants:20,tech:rgb,type:paper},
  annotation = {108 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/8410764/}},
  file = {/Users/brk/Zotero/storage/SQ8WF2YE/Avola et al. - 2019 - Exploiting Recurrent Neural Networks and Leap Moti.pdf}
}
% == BibTeX quality report for avolaExploitingRecurrentNeural2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Multimedia")
% ? unused Library catalog ("Semantic Scholar")

@article{azadDynamic3DHand2019,
  title = {Dynamic {{3D Hand Gesture Recognition}} by {{Learning Weighted Depth Motion Maps}}},
  author = {Azad, Reza and {Asadi-Aghbolaghi}, Maryam and Kasaei, Shohreh and Escalera, Sergio},
  year = {2019},
  month = jun,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {29},
  number = {6},
  pages = {1729--1740},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2018.2855416},
  urldate = {2023-07-20},
  note = {\url{https://ieeexplore.ieee.org/document/8410578/}}
}
% == BibTeX quality report for azadDynamic3DHand2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Circuits Syst. Video Technol.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{bagnallGreatTimeSeries2017,
  title = {The Great Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  shorttitle = {The Great Time Series Classification Bake Off},
  author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
  year = {2017},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {31},
  number = {3},
  pages = {606--660},
  issn = {1573-756X},
  doi = {10.1007/s10618-016-0483-9},
  urldate = {2023-05-13},
  abstract = {In the last 5~years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.},
  langid = {english},
  keywords = {background,reference:doi.org/10.1007/978-0-387-35973-1\_1406,reference:doi.org/10.21914/ANZIAMJ.V54I0.6177,reference:doi.org/10.3389/fnins.2014.00293,type:survey},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-05-13] --- DOIs\_of\_references: ["10.1007/978-0-387-35973-1\_1406","10.21914/ANZIAMJ.V54I0.6177","10.3389/fnins.2014.00293"] ---},
  note = {\url{https://doi.org/10.1007/s10618-016-0483-9}},
  file = {/Users/brk/Zotero/storage/SRFG34ZY/Bagnall et al. - 2017 - The great time series classification bake off a r.pdf}
}
% == BibTeX quality report for bagnallGreatTimeSeries2017:
% ? unused Journal abbreviation ("Data Min Knowl Disc")
% ? unused Library catalog ("Springer Link")

@article{bansalGestureRecognitionSurvey2016,
  title = {Gesture {{Recognition}}: {{A Survey}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Bansal, Bharti},
  year = {2016},
  month = apr,
  journal = {International Journal of Computer Applications},
  volume = {139},
  number = {2},
  pages = {8--10},
  issn = {09758887},
  doi = {10.5120/ijca2016909103},
  urldate = {2023-03-07},
  abstract = {With increasing use of computers in our daily lives, lately there has been a rapid increase in the efforts to develop a better human computer interaction interface. The need of easy to use and advance types of human-computer interaction with natural interfaces is more than ever. In the present framework, the UI (User Interface) of a computer allows user to interact with electronic devices with graphical icons and visual indicators, which is still inconvenient and not suitable for working in virtual environments. An interface which allow user to communicate through gestures is the next step in the direction of advance human computer interface. In the present paper author explore different aspects of gesture recognition techniques.},
  keywords = {based-on:vision,type:survey},
  annotation = {650 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://www.ijcaonline.org/research/volume139/number2/bansal-2016-ijca-909103.pdf}},
  file = {/Users/brk/Zotero/storage/DXTI6K39/Bansal - 2016 - Gesture Recognition A Survey.pdf}
}
% == BibTeX quality report for bansalGestureRecognitionSurvey2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJCA")
% ? unused Library catalog ("Semantic Scholar")

@article{bansalGestureRecognitionSurvey2016a,
  title = {Gesture {{Recognition}}: {{A Survey}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Bansal, Bharti},
  year = {2016},
  month = apr,
  journal = {International Journal of Computer Applications},
  volume = {139},
  number = {2},
  pages = {8--10},
  issn = {09758887},
  doi = {10.5120/ijca2016909103},
  urldate = {2023-08-02},
  abstract = {With increasing use of computers in our daily lives, lately there has been a rapid increase in the efforts to develop a better human computer interaction interface. The need of easy to use and advance types of human-computer interaction with natural interfaces is more than ever. In the present framework, the UI (User Interface) of a computer allows user to interact with electronic devices with graphical icons and visual indicators, which is still inconvenient and not suitable for working in virtual environments. An interface which allow user to communicate through gestures is the next step in the direction of advance human computer interface. In the present paper author explore different aspects of gesture recognition techniques.},
  note = {\url{http://www.ijcaonline.org/research/volume139/number2/bansal-2016-ijca-909103.pdf}}
}
% == BibTeX quality report for bansalGestureRecognitionSurvey2016a:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJCA")
% ? unused Library catalog ("Semantic Scholar")

@article{baudelCharadeRemoteControl1993,
  title = {Charade: Remote Control of Objects Using Free-Hand Gestures},
  shorttitle = {Charade},
  author = {Baudel, Thomas and {Beaudouin-Lafon}, Michel},
  year = {1993},
  month = jul,
  journal = {Communications of the ACM},
  volume = {36},
  number = {7},
  pages = {28--35},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/159544.159562},
  urldate = {2023-06-23},
  abstract = {This paper presents an application that uses hand gesture input to control a computer while giving a presentation. In order to develop a prototype of this application, we have defined an interaction model, a notation for gestures, and a set of guidelines to design gestural command sets. This works aims to define interaction styles that work in computerized reality environments. In our application, gestures are used for interacting with the computer as well as for communicating with other people or operating other devices.},
  langid = {english},
  keywords = {based-on:gloves,fidelity:finger,hardware:polhemus,hardware:vpl-dataglove,have-read,reference:doi.org/10.1109/2.84835,reference:doi.org/10.1136/BMJ.293.6546.517,reference:doi.org/10.1145/1125021.1125079,reference:doi.org/10.1145/142621.142646,reference:doi.org/10.1145/800250.807503,reference:doi.org/10.1162/pres.1992.1.1.1,reference:doi.org/10.1201/B15703-15,tech:flex,type:paper},
  annotation = {582 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1162/pres.1992.1.1.1","10.1136/BMJ.293.6546.517","10.1145/1125021.1125079"] ---},
  note = {\url{https://dl.acm.org/doi/10.1145/159544.159562}},
  file = {/Users/brk/Zotero/storage/WRSWXDSK/Baudel and Beaudouin-Lafon - 1993 - Charade remote control of objects using free-hand.pdf}
}
% == BibTeX quality report for baudelCharadeRemoteControl1993:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("Semantic Scholar")

@article{baumMaximizationTechniqueOccurring1970,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  month = feb,
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {1},
  pages = {164--171},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177697196},
  urldate = {2023-07-11},
  abstract = {Semantic Scholar extracted view of "A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains" by L. Baum et al.},
  langid = {english},
  keywords = {background,baum-welsch,model:hmm},
  annotation = {4728 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://projecteuclid.org/euclid.aoms/1177697196}},
  file = {/Users/brk/Zotero/storage/888D4KE6/Baum et al. - 1970 - A Maximization Technique Occurring in the Statisti.pdf}
}
% == BibTeX quality report for baumMaximizationTechniqueOccurring1970:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Ann. Math. Statist.")
% ? unused Library catalog ("Semantic Scholar")

@article{baumStatisticalInferenceProbabilistic1966,
  title = {Statistical {{Inference}} for {{Probabilistic Functions}} of {{Finite State Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted},
  year = {1966},
  journal = {The Annals of Mathematical Statistics},
  volume = {37},
  number = {6},
  pages = {1554--1563},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/aoms/1177699147},
  keywords = {background,model:finite-state-markov-chains,model:hmm},
  annotation = {2933 citations (Semantic Scholar/DOI) [2023-06-19]},
  note = {\url{https://doi.org/10.1214/aoms/1177699147}},
  file = {/Users/brk/Zotero/storage/VMY4KUTL/Baum and Petrie - 1966 - Statistical Inference for Probabilistic Functions .pdf}
}
% == BibTeX quality report for baumStatisticalInferenceProbabilistic1966:
% ? Title looks like it was stored in title-case in Zotero

@article{bergstraRandomSearchHyperParameter2012a,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Y.},
  year = {2012},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {13},
  pages = {281--305},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.}
}
% == BibTeX quality report for bergstraRandomSearchHyperParameter2012a:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("ResearchGate")

@article{berrarCaveatsPitfallsROC2012,
  title = {Caveats and Pitfalls of {{ROC}} Analysis in Clinical Microarray Research (and How to Avoid Them)},
  author = {Berrar, Daniel and Flach, Peter},
  year = {2012},
  month = jan,
  journal = {Briefings in Bioinformatics},
  volume = {13},
  number = {1},
  pages = {83--97},
  issn = {1467-5463},
  doi = {10.1093/bib/bbr008},
  urldate = {2023-03-24},
  abstract = {The receiver operating characteristic (ROC) has emerged as the gold standard for assessing and comparing the performance of classifiers in a wide range of disciplines including the life sciences. ROC curves are frequently summarized in a single scalar, the area under the curve (AUC). This article discusses the caveats and pitfalls of ROC analysis in clinical microarray research, particularly in relation to (i) the interpretation of AUC (especially a value close to 0.5); (ii) model comparisons based on AUC; (iii) the differences between ranking and classification; (iv) effects due to multiple hypotheses testing; (v) the importance of confidence intervals for AUC; and (vi) the choice of the appropriate performance metric. With a discussion of illustrative examples and concrete real-world studies, this article highlights critical misconceptions that can profoundly impact the conclusions about the observed performance.},
  keywords = {background},
  annotation = {91 citations (Semantic Scholar/DOI) [2023-03-24]},
  note = {\url{https://doi.org/10.1093/bib/bbr008}},
  file = {/Users/brk/Zotero/storage/RLN68QM9/Berrar and Flach - 2012 - Caveats and pitfalls of ROC analysis in clinical m.pdf}
}
% == BibTeX quality report for berrarCaveatsPitfallsROC2012:
% ? unused Library catalog ("Silverchair")

@inproceedings{bevilacquaContinuousRealtimeGesture2010,
  title = {Continuous {{Realtime Gesture Following}} and {{Recognition}}},
  booktitle = {Gesture in {{Embodied Communication}} and {{Human-Computer Interaction}}},
  author = {Bevilacqua, Fr{\'e}d{\'e}ric and Zamborlin, Bruno and Sypniewski, Anthony and Schnell, Norbert and Gu{\'e}dy, Fabrice and Rasamimanana, Nicolas},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kopp, Stefan and Wachsmuth, Ipke},
  year = {2010},
  volume = {5934},
  pages = {73--84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12553-9_7},
  urldate = {2023-03-07},
  abstract = {We present a HMM based system for real-time gesture analysis. The system outputs continuously parameters relative to the gesture time progression and its likelihood. These parameters are computed by comparing the performed gesture with stored reference gestures. The method relies on a detailed modeling of multidimensional temporal curves. Compared to standard HMM systems, the learning procedure is simplified using prior knowledge allowing the system to use a single example for each class. Several applications have been developed using this system in the context of music education, music and dance performances and interactive installation. Typically, the estimation of the time progression allows for the synchronization of physical gestures to sound files by time stretching/compressing audio buffers or videos.},
  isbn = {978-3-642-12552-2 978-3-642-12553-9},
  keywords = {app:performing-arts,based-on:gloves,contains-more-references,fidelity:arm,have-read,model:hmm,repetitions:1,tech:accelerometer,technique:one-shot,type:paper,unimpressive},
  annotation = {207 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-3-642-12553-9_7}},
  file = {/Users/brk/Zotero/storage/9YT2MAGT/Bevilacqua et al. - 2010 - Continuous Realtime Gesture Following and Recognit.pdf}
}
% == BibTeX quality report for bevilacquaContinuousRealtimeGesture2010:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{bhagatIndianSignLanguage2019,
  title = {Indian {{Sign Language Gesture Recognition}} Using {{Image Processing}} and {{Deep Learning}}},
  author = {Bhagat, Neel Kamal and Vishnusai, Y. and Rathna, G. N.},
  year = {2019},
  month = dec,
  journal = {2019 Digital Image Computing: Techniques and Applications (DICTA)},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Perth, Australia}},
  doi = {10.1109/DICTA47822.2019.8945850},
  urldate = {2023-06-22},
  abstract = {Speech impaired people use hand based gestures to communicate. Unfortunately, the vast majority of the people are not aware of the semantics of these gestures. In a attempt to bridge the same, we propose a real time hand gesture recognition system based on the data captured by the Microsoft Kinect RGB-D camera. Given that there is no one to one mapping between the pixels of the depth and the RGB camera, we used computer vision techniques like 3D contruction and affine transformation. After achieving one to one mapping, segmentation of the hand gestures was done from the background noise. Convolutional Neural Networks (CNNs) were utilised for training 36 static gestures relating to Indian Sign Language (ISL) alphabets and numbers. The model achieved an accuracy of 98.81\% on training using 45,000 RGB images and 45,000 depth images. Further Convolutional LSTMs were used for training 10 ISL dynamic word gestures and an accuracy of 99.08\% was obtained by training 1080 videos. The model showed accurate real time performance on prediction of ISL static gestures, leaving a scope for further research on sentence formation through gestures. The model also showed competitive adaptability to American Sign Language (ASL) gestures when the ISL models weights were transfer learned to ASL and it resulted in giving 97.71\% accuracy.},
  isbn = {9781728138572},
  keywords = {app:sign-language,based-on:vision,fidelity:finger,pdf:paywalled,type:paper},
  annotation = {26 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/8945850/}}
}
% == BibTeX quality report for bhagatIndianSignLanguage2019:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{binhRealTimeHandTracking2005,
  title = {Real-{{Time Hand Tracking}} and {{Gesture Recognition System}}},
  author = {Binh, N. and Shuichi, Enokida and Ejima, T.},
  year = {2005},
  urldate = {2023-07-11},
  abstract = {In this paper, we introduce a hand gesture recognition system to recognize real time gesture in unconstrained environments. The system consists of three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). We have used a Kalman filter and hand blobs analysis for hand tracking to obtain motion descriptors and hand region. It is fairy robust to background cluster and uses skin color for hand gesture tracking and recognition. Furthermore, there have been proposed to improve the overall performance of the approach: (1) Intelligent selection of training images and (2) Adaptive threshold gesture to remove non-gesture pattern that helps to qualify an input pattern as a gesture. A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed. In the experiments, we have tested our system to vocabulary of 36 gestures including the America sign language (ASL) letter spelling alphabet and digits, and results effectiveness of the approach.},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:36,fidelity:finger,have-read,model:hmm,model:kalman-filter,tech:rgb,type:paper},
  annotation = {--- DOIs\_of\_references: ["10.1096/fj.201800173R","10.1038/nbt.2839","10.1136/annrheumdis-2022-eular.1677","10.1038/nbt.2839","10.1096/fj.201800173R"] ---},
  note = {\url{https://www.semanticscholar.org/paper/Real-Time-Hand-Tracking-and-Gesture-Recognition-Binh-Shuichi/04cad56dd4fda01a3fa03a47dca2fa11d02695bf}},
  file = {/Users/brk/Zotero/storage/8BDC7ZZS/Binh et al. - 2005 - Real-Time Hand Tracking and Gesture Recognition Sy.pdf}
}
% == BibTeX quality report for binhRealTimeHandTracking2005:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{bishopMaximumLikelihoodAlignment1986,
  title = {Maximum Likelihood Alignment of {{DNA}} Sequences},
  author = {Bishop, M.J. and Thompson, E.A.},
  year = {1986},
  month = jul,
  journal = {Journal of Molecular Biology},
  volume = {190},
  number = {2},
  pages = {159--165},
  issn = {00222836},
  doi = {10.1016/0022-2836(86)90289-5},
  urldate = {2023-11-13},
  abstract = {Semantic Scholar extracted view of "Maximum likelihood alignment of DNA sequences." by M. Bishop et al.},
  langid = {english},
  keywords = {/unread},
  annotation = {133 citations (Semantic Scholar/DOI) [2023-11-13]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/0022283686902895}}
}
% == BibTeX quality report for bishopMaximumLikelihoodAlignment1986:
% ? unused Library catalog ("Semantic Scholar")

@book{bishopPatternRecognitionMachine2007,
  title = {Pattern {{Recognition}} and {{Machine Learning}} ({{Information Science}} and {{Statistics}})},
  author = {Bishop, Christopher},
  year = {2007},
  month = oct,
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
  isbn = {978-0-387-31073-2}
}
% == BibTeX quality report for bishopPatternRecognitionMachine2007:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("ResearchGate")

@article{bloitShorttimeViterbiOnline2008,
  title = {Short-Time {{Viterbi}} for Online {{HMM}} Decoding: {{Evaluation}} on a Real-Time Phone Recognition Task},
  shorttitle = {Short-Time {{Viterbi}} for Online {{HMM}} Decoding},
  author = {Bloit, Julien and Rodet, Xavier},
  year = {2008},
  month = mar,
  journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages = {2121--2124},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2008.4518061},
  urldate = {2023-03-07},
  abstract = {In this paper we present the implementation of an online HMM decoding process. The algorithm derives an online version of the Viterbi algorithm on successive variable length windows, iteratively storing portions of the optimal state path. We explicit the relation between the hidden layer's topology and the applicability and performance prediction of the algorithm. We evaluate the validity and performance of the algorithm on a phone recognition task on a database of continuous speech from a native French speaker. We specifically study the latency-accuracy performance of the algorithm.},
  isbn = {9781424414833 9781424414840},
  keywords = {background},
  annotation = {52 citations (Semantic Scholar/DOI) [2023-03-07]},
  note = {\url{http://ieeexplore.ieee.org/document/4518061/}},
  file = {/Users/brk/Zotero/storage/BTJHRA6M/Bloit and Rodet - 2008 - Short-time Viterbi for online HMM decoding Evalua.pdf}
}
% == BibTeX quality report for bloitShorttimeViterbiOnline2008:
% ? unused Conference name ("ICASSP 2008 - 2008 IEEE International Conference on Acoustics, Speech and Signal Processing")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{boltPutthatthereVoiceGesture1980,
  title = {``{{Put-that-there}}'': {{Voice}} and Gesture at the Graphics Interface},
  shorttitle = {``{{Put-that-there}}''},
  booktitle = {Proceedings of the 7th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Bolt, Richard A.},
  year = {1980},
  month = jul,
  series = {{{SIGGRAPH}} '80},
  pages = {262--270},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800250.807503},
  urldate = {2023-07-03},
  abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality. The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
  isbn = {978-0-89791-021-7},
  keywords = {based-on:vision,fidelity:arm,Gesture,Graphics,Graphics interface,have-read,Man-machine interfaces,Space sensing,Spatial data management,Speech input,type:paper,Voice input},
  annotation = {1893 citations (Semantic Scholar/DOI) [2023-07-03]},
  note = {\url{https://dl.acm.org/doi/10.1145/800250.807503}},
  file = {/Users/brk/Zotero/storage/8U4B3N9X/Bolt - 1980 - “Put-that-there” Voice and gesture at the graphic.pdf}
}
% == BibTeX quality report for boltPutthatthereVoiceGesture1980:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("ACM Digital Library")

@misc{bostromEthicalIssuesAdvanced,
  title = {Ethical {{Issues In Advanced Artificial Intelligence}}},
  author = {Bostrom, Nick},
  urldate = {2023-11-12},
  abstract = {The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded.},
  howpublished = {\url{https://nickbostrom.com/ethics/ai}},
  keywords = {/unread},
  file = {/Users/brk/Zotero/storage/485MX535/ai.html}
}
% == BibTeX quality report for bostromEthicalIssuesAdvanced:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{bowdenLinguisticFeatureVector2004,
  title = {A {{Linguistic Feature Vector}} for the {{Visual Interpretation}} of {{Sign Language}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2004},
  author = {Bowden, Richard and Windridge, David and Kadir, Timor and Zisserman, Andrew and Brady, Michael},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Pajdla, Tom{\'a}s and Matas, Ji{\v r}{\'i}},
  year = {2004},
  volume = {3021},
  pages = {390--401},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24670-1_30},
  urldate = {2023-07-11},
  abstract = {This paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data. Key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion. This high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans. Moreover, such a description broadly generalises temporal activities naturally overcoming variability of people and environments. A second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of Markov chains combined with Independent Component Analysis. We demonstrate classification rates as high as 97.67\% for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required.},
  isbn = {978-3-540-21984-2 978-3-540-24670-1},
  langid = {english},
  keywords = {app:british-sl,app:sign-language,based-on:vision,claims-to-be-best,classes:49,contains-more-references,fidelity:finger,have-read,model:markov-chain,tech:rgb,type:paper},
  annotation = {182 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://link.springer.com/10.1007/978-3-540-24670-1_30}},
  file = {/Users/brk/Zotero/storage/RZMUR5ET/Bowden et al. - 2004 - A Linguistic Feature Vector for the Visual Interpr.pdf}
}
% == BibTeX quality report for bowdenLinguisticFeatureVector2004:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@inproceedings{bowdenVisionBasedInterpretation2003,
  title = {Vision Based {{Interpretation}} of {{Natural Sign Languages}}},
  author = {Bowden, R. and Zisserman, Andrew and Kadir, T. and Brady, M.},
  year = {2003},
  month = apr,
  urldate = {2023-07-11},
  abstract = {This manuscript outlines our current demonstration system for translating visual Sign to written text. The system is based around a broad description of scene activity that naturally generalizes, reducing training requirements and allowing the knowledge base to be explicitly stated. This allows the same system to be used for different sign languages requiring only a change of the knowledge base.},
  keywords = {app:british-sl,app:sign-language,based-on:vision,classes:55,fidelity:finger,have-read,tech:rgb,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Vision-based-Interpretation-of-Natural-Sign-Bowden-Zisserman/86e9d0c4d14456932043fa0fb22f39e62f2de688}},
  file = {/Users/brk/Zotero/storage/BRKUR3W3/Bowden et al. - 2003 - Vision based Interpretation of Natural Sign Langua.pdf}
}
% == BibTeX quality report for bowdenVisionBasedInterpretation2003:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  urldate = {2023-07-11},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  keywords = {background,model:random-forests,type:seminal},
  note = {\url{http://link.springer.com/10.1023/A:1010933404324}},
  file = {/Users/brk/Zotero/storage/HSTITUV6/Breiman - 2001 - [No title found].pdf}
}
% == BibTeX quality report for breimanRandomForests2001:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{brooksDataGloveManmachineInterface1989,
  title = {The {{DataGlove}} as a Man-Machine Interface for Robotics},
  author = {Brooks, Martin},
  year = {1989},
  journal = {The Second IARP Workshop on Medical and Healthcare Robotics},
  pages = {213--225},
  keywords = {based-on:gloves,have-read,type:paper}
}

@inproceedings{buchmannFingARtipsGestureBased2004,
  title = {{{FingARtips}}: Gesture Based Direct Manipulation in {{Augmented Reality}}},
  shorttitle = {{{FingARtips}}},
  booktitle = {Proceedings of the 2nd International Conference on {{Computer}} Graphics and Interactive Techniques in {{Australasia}} and {{South East Asia}}},
  author = {Buchmann, Volkert and Violich, Stephen and Billinghurst, Mark and Cockburn, Andy},
  year = {2004},
  month = jun,
  pages = {212--221},
  publisher = {{ACM}},
  address = {{Singapore}},
  doi = {10.1145/988834.988871},
  urldate = {2023-07-02},
  abstract = {This paper presents a technique for natural, fingertip-based interaction with virtual objects in Augmented Reality (AR) environments. We use image processing software and finger- and hand-based fiducial markers to track gestures from the user, stencil buffering to enable the user to see their fingers at all times, and fingertip-based haptic feedback devices to enable the user to feel virtual objects. Unlike previous AR interfaces, this approach allows users to interact with virtual content using natural hand gestures. The paper describes how these techniques were applied in an urban planning interface, and also presents preliminary informal usability results.},
  isbn = {978-1-58113-883-2},
  langid = {english},
  keywords = {app:augmented-reality,based-on:vision,fidelity:finger,fiducials,have-read,tech:rgb,type:paper},
  annotation = {290 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/988834.988871}},
  file = {/Users/brk/Zotero/storage/ZSNVZFEU/Buchmann et al. - 2004 - FingARtips gesture based direct manipulation in A.pdf}
}
% == BibTeX quality report for buchmannFingARtipsGestureBased2004:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("GRAPHITE04: International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia")
% ? unused Library catalog ("Semantic Scholar")

@article{cabibihanSuitabilityOpenlyAccessible2021,
  title = {Suitability of the {{Openly Accessible 3D Printed Prosthetic Hands}} for {{War-Wounded Children}}},
  author = {Cabibihan, John-John and Alkhatib, Farah and Mudassir, Mohammed and Lambert, Laurent A. and {Al-Kwifi}, Osama S. and Diab, Khaled and Mahdi, Elsadig},
  year = {2021},
  month = jan,
  journal = {Frontiers in Robotics and AI},
  volume = {7},
  pages = {594196},
  issn = {2296-9144},
  doi = {10.3389/frobt.2020.594196},
  urldate = {2023-07-12},
  abstract = {The field of rehabilitation and assistive devices is being disrupted by innovations in desktop 3D printers and open-source designs. For upper limb prosthetics, those technologies have demonstrated a strong potential to aid those with missing hands. However, there are basic interfacing issues that need to be addressed for long term usage. The functionality, durability, and the price need to be considered especially for those in difficult living conditions. We evaluated the most popular designs of body-powered, 3D printed prosthetic hands. We selected a representative sample and evaluated its suitability for its grasping postures, durability, and cost. The prosthetic hand can perform three grasping postures out of the 33 grasps that a human hand can do. This corresponds to grasping objects similar to a coin, a golf ball, and a credit card. Results showed that the material used in the hand and the cables can withstand a 22 N normal grasping force, which is acceptable based on standards for accessibility design. The cost model showed that a 3D printed hand could be produced for as low as \$19. For the benefit of children with congenital missing limbs and for the war-wounded, the results can serve as a baseline study to advance the development of prosthetic hands that are functional yet low-cost.},
  keywords = {background,medical},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://www.frontiersin.org/articles/10.3389/frobt.2020.594196/full}},
  file = {/Users/brk/Zotero/storage/KXV4GLFK/Cabibihan et al. - 2021 - Suitability of the Openly Accessible 3D Printed Pr.pdf}
}
% == BibTeX quality report for cabibihanSuitabilityOpenlyAccessible2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Front. Robot. AI")
% ? unused Library catalog ("Semantic Scholar")

@article{caiRGBDDatasetsUsing2017,
  title = {{{RGB-D}} Datasets Using Microsoft Kinect or Similar Sensors: A Survey},
  shorttitle = {{{RGB-D}} Datasets Using Microsoft Kinect or Similar Sensors},
  author = {Cai, Ziyun and Han, Jungong and Liu, Li and Shao, Ling},
  year = {2017},
  month = feb,
  journal = {Multimedia Tools and Applications},
  volume = {76},
  number = {3},
  pages = {4313--4355},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-016-3374-6},
  urldate = {2023-06-22},
  abstract = {RGB-D data has turned out to be a very useful representation of an indoor scene for solving fundamental computer vision problems. It takes the advantages of the color image that provides appearance information of an object and also the depth image that is immune to the variations in color, illumination, rotation angle and scale. With the invention of the low-cost Microsoft Kinect sensor, which was initially used for gaming and later became a popular device for computer vision, high quality RGB-D data can be acquired easily. In recent years, more and more RGB-D image/video datasets dedicated to various applications have become available, which are of great importance to benchmark the state-of-the-art. In this paper, we systematically survey popular RGB-D datasets for different applications including object recognition, scene classification, hand gesture recognition, 3D-simultaneous localization and mapping, and pose estimation. We provide the insights into the characteristics of each important dataset, and compare the popularity and the difficulty of those datasets. Overall, the main goal of this survey is to give a comprehensive description about the available RGB-D datasets and thus to guide researchers in the selection of suitable datasets for evaluating their algorithms.},
  langid = {english},
  keywords = {based-on:vision,hardware:kinect,read-priority-5,tech:rgbd,type:survey},
  annotation = {115 Citations},
  note = {\url{http://link.springer.com/10.1007/s11042-016-3374-6}},
  file = {/Users/brk/Zotero/storage/Q3HIZYJF/Cai et al. - 2017 - RGB-D datasets using microsoft kinect or similar s.pdf}
}
% == BibTeX quality report for caiRGBDDatasetsUsing2017:
% ? unused Journal abbreviation ("Multimed Tools Appl")
% ? unused Library catalog ("Semantic Scholar")

@article{campbellDeepBlue2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane, A.Joseph and Hsu, Feng-hsiung},
  year = {2002},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1-2},
  pages = {57--83},
  issn = {00043702},
  doi = {10.1016/S0004-3702(01)00129-1},
  urldate = {2023-11-12},
  abstract = {Semantic Scholar extracted view of "Deep Blue" by Murray Campbell et al.},
  langid = {english},
  keywords = {/unread},
  annotation = {1098 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0004370201001291}}
}
% == BibTeX quality report for campbellDeepBlue2002:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{chaiTwoStreamsRecurrent2016,
  title = {Two Streams {{Recurrent Neural Networks}} for {{Large-Scale Continuous Gesture Recognition}}},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Chai, Xiujuan and Liu, Zhipeng and Yin, Fang and Liu, Zhuang and Chen, Xilin},
  year = {2016},
  month = dec,
  pages = {31--36},
  doi = {10.1109/ICPR.2016.7899603},
  abstract = {In this paper, we tackle the continuous gesture recognition problem with a two streams Recurrent Neural Networks (2S-RNN) for the RGB-D data input. In our framework, the spotting-recognition strategy is used, that means the continuous gestures are first segmented into separated gestures, and then each isolated gesture is recognized by using the 2S-RNN. Concretely, the gesture segmentation is based on the accurate hand positions provided by the hand detector trained from Faster R-CNN. While in the recognition module, 2S-RNN is designed to efficiently fuse multi-modal features, i.e. the RGB and depth channels. The experimental results on both the validation and test sets of the Continuous Gesture Dataset (ConGD) have shown promising performance of the proposed framework. We ranked 1st in the ChaLearn LAP Large-scale Continuous Gesture Recognition Challenge with the mean Jaccard Index of 0.286915.},
  keywords = {based-on:vision,classes:249,dataset:chalearn,dataset:chalearn-lap,dataset:continuous-gesture-dataset,fidelity:finger,have-read,model:nn,model:rnn,participants:21,tech:rgbd,technique:histogram-oriented-gradients,type:paper},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/KTTVPYPW/Chai et al. - 2016 - Two streams Recurrent Neural Networks for Large-Sc.pdf}
}
% == BibTeX quality report for chaiTwoStreamsRecurrent2016:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("IEEE Xplore")

@article{charayaphanImageProcessingSystem1992,
  title = {Image Processing System for Interpreting Motion in {{American Sign Language}}},
  author = {Charayaphan, C. and Marble, A.E.},
  year = {1992},
  month = sep,
  journal = {Journal of Biomedical Engineering},
  volume = {14},
  number = {5},
  pages = {419--425},
  issn = {01415425},
  doi = {10.1016/0141-5425(92)90088-3},
  urldate = {2023-07-15},
  langid = {english},
  keywords = {/unread},
  annotation = {99 citations (Semantic Scholar/DOI) [2023-07-15]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/0141542592900883}}
}
% == BibTeX quality report for charayaphanImageProcessingSystem1992:
% ? unused Library catalog ("DOI.org (Crossref)")

@article{chatzisComprehensiveStudyDeep2020,
  title = {A {{Comprehensive Study}} on {{Deep Learning-Based 3D Hand Pose Estimation Methods}}},
  author = {Chatzis, Theocharis and Stergioulas, Andreas and Konstantinidis, Dimitrios and Dimitropoulos, Kosmas and Daras, Petros},
  year = {2020},
  month = sep,
  journal = {Applied Sciences},
  volume = {10},
  number = {19},
  pages = {6850},
  issn = {2076-3417},
  doi = {10.3390/app10196850},
  urldate = {2023-06-26},
  abstract = {The field of 3D hand pose estimation has been gaining a lot of attention recently, due to its significance in several applications that require human-computer interaction (HCI). The utilization of technological advances, such as cost-efficient depth cameras coupled with the explosive progress of Deep Neural Networks (DNNs), has led to a significant boost in the development of robust markerless 3D hand pose estimation methods. Nonetheless, finger occlusions and rapid motions still pose significant challenges to the accuracy of such methods. In this survey, we provide a comprehensive study of the most representative deep learning-based methods in literature and propose a new taxonomy heavily based on the input data modality, being RGB, depth, or multimodal information. Finally, we demonstrate results on the most popular RGB and depth-based datasets and discuss potential research directions in this rapidly growing field.},
  langid = {english},
  keywords = {based-on:vision,model:deep-learning,model:ffnn,model:nn,read-priority-9,tech:rgbd,type:survey},
  annotation = {25 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/2076-3417/10/19/6850}},
  file = {/Users/brk/Zotero/storage/5NQQ3Z7T/Chatzis et al. - 2020 - A Comprehensive Study on Deep Learning-Based 3D Ha.pdf}
}
% == BibTeX quality report for chatzisComprehensiveStudyDeep2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{chaudharySurveyHandGesture2011,
  title = {A {{Survey}} on {{Hand Gesture Recognition}} in {{Context}} of {{Soft Computing}}},
  booktitle = {Advanced {{Computing}}},
  author = {Chaudhary, Ankit and Raheja, J. L. and Das, Karen and Raheja, Sonia},
  editor = {Meghanathan, Natarajan and Kaushik, Brajesh Kumar and Nagamalai, Dhinaharan},
  year = {2011},
  volume = {133},
  pages = {46--55},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17881-8_5},
  urldate = {2023-07-11},
  abstract = {Hand gestures recognition is the natural way of Human Machine interaction and today many researchers in the academia and industry are interested in this direction. It enables human being to interact with machine very easily and conveniently without wearing any extra device. It can be applied from sign language recognition to robot control and from virtual reality to intelligent home systems. In this paper we are discussing work done in the area of hand gesture recognition where focus is on the soft computing based methods like artificial neural network, fuzzy logic, genetic algorithms, etc. We also described hand detection methods in the preprocessed image for detecting the hand image. Most researchers used fingertips for hand detection in appearance based modeling. Finally we are comparing results given by different researchers after their implementation.},
  isbn = {978-3-642-17880-1 978-3-642-17881-8},
  keywords = {based-on:vision,have-read,model:fuzzy,reference:doi.org/10.1016/0020-0190(72)90045-2,reference:doi.org/10.1016/j.patcog.2003.11.007,reference:doi.org/10.1017/S0263574701213782,reference:doi.org/10.1109/34.598226,reference:doi.org/10.1109/ARTCom.2009.177,reference:doi.org/10.1109/GMAI.2006.48,reference:doi.org/10.1109/MSP.2005.1425898,reference:doi.org/10.1109/TCE.2009.5373803,reference:doi.org/10.1145/1646353.1646360,reference:doi.org/10.4108/ICST.IMMERSCOM2007.2081,reference:doi.org/10.5120/IJCA2016909103,reference:doi.org/10.5860/choice.42-5687,type:survey},
  annotation = {67 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1016/j.patcog.2003.11.007","10.1109/TEC.1961.5219197","10.1016/0020-0190(72)90045-2","10.1109/TPAMI.2005.112","10.1145/1389586.1389595","10.1109/34.598226","10.1017/S0263574701213782","10.1109/TCE.2009.5373803","10.5860/choice.42-5687","10.5120/IJCA2016909103","10.1109/ARTCom.2009.177","10.1109/GMAI.2006.48","10.1145/1646353.1646360"] ---},
  note = {\url{http://link.springer.com/10.1007/978-3-642-17881-8_5}},
  file = {/Users/brk/Zotero/storage/5XEY3IHQ/Chaudhary et al. - 2011 - A Survey on Hand Gesture Recognition in Context of.pdf}
}
% == BibTeX quality report for chaudharySurveyHandGesture2011:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Communications in Computer and Information Science")

@article{chenCrossDomainWiFiSensing2023,
  title = {Cross-{{Domain WiFi Sensing}} with {{Channel State Information}}: {{A Survey}}},
  shorttitle = {Cross-{{Domain WiFi Sensing}} with {{Channel State Information}}},
  author = {Chen, Chen and Zhou, Gang and Lin, Youfang},
  year = {2023},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {11},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3570325},
  urldate = {2023-07-12},
  abstract = {The past years have witnessed the rapid conceptualization and development of wireless sensing based on               Channel State Information (CSI)               with commodity WiFi devices. Recent studies have demonstrated the vast potential of WiFi sensing in detection, recognition, and estimation applications. However, the widespread deployment of WiFi sensing systems still faces a significant challenge: how to ensure the sensing performance when exposing a pre-trained sensing system to new domains, such as new environments, different configurations, and unseen users, without data collection and system retraining. This survey provides a comprehensive review of recent research efforts on cross-domain WiFi Sensing. We first introduce the mathematical model of CSI and explore the impact of different domains on CSI. Then we present a general workflow of cross-domain WiFi sensing systems, which consists of signal processing and cross-domain sensing. Five cross-domain sensing algorithms, including domain-invariant feature extraction, virtual sample generation, transfer learning, few-shot learning and big data solution, are summarized to show how they achieve high sensing accuracy when encountering new domains. The advantages and limitations of each algorithm are also summarized and the performance comparison is made based on different applications. Finally, we discuss the remaining challenges to further promote the practical usability of cross-domain WiFi sensing systems.},
  langid = {english},
  keywords = {based-on:wifi,pdf:paywalled,tech:wifi,type:survey},
  annotation = {115 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3570325}}
}
% == BibTeX quality report for chenCrossDomainWiFiSensing2023:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("ACM Comput. Surv.")
% ? unused Library catalog ("Semantic Scholar")

@article{chengSurvey3DHand2016,
  title = {Survey on {{3D Hand Gesture Recognition}}},
  author = {Cheng, Hong and Yang, Lu and Liu, Zicheng},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {26},
  number = {9},
  pages = {1659--1673},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2015.2469551},
  urldate = {2023-06-22},
  abstract = {Three-dimensional hand gesture recognition has attracted increasing research interests in computer vision, pattern recognition, and human-computer interaction. The emerging depth sensors greatly inspired various hand gesture recognition approaches and applications, which were severely limited in the 2D domain with conventional cameras. This paper presents a survey of some recent works on hand gesture recognition using 3D depth sensors. We first review the commercial depth sensors and public data sets that are widely used in this field. Then, we review the state-of-the-art research for 3D hand gesture recognition in four aspects: 1) 3D hand modeling; 2) static hand gesture recognition; 3) hand trajectory gesture recognition; and 4) continuous hand gesture recognition. While the emphasis is on 3D hand gesture recognition approaches, the related applications and typical systems are also briefly summarized for practitioners.},
  keywords = {based-on:vision,type:survey},
  annotation = {265 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7208833/}}
}
% == BibTeX quality report for chengSurvey3DHand2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Circuits Syst. Video Technol.")
% ? unused Library catalog ("Semantic Scholar")

@article{chenHandGestureRecognition2003,
  title = {Hand Gesture Recognition Using a Real-Time Tracking Method and Hidden {{Markov}} Models},
  author = {Chen, Feng-Sheng and Fu, Chih-Ming and Huang, Chung-Lin},
  year = {2003},
  month = aug,
  journal = {Image and Vision Computing},
  volume = {21},
  number = {8},
  pages = {745--758},
  issn = {0262-8856},
  doi = {10.1016/S0262-8856(03)00070-2},
  urldate = {2023-06-23},
  abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90\%.},
  langid = {english},
  keywords = {app:sign-language,app:taiwanese-sl,based-on:vision,classes:20,fidelity:finger,have-read,model:hmm,participants:20,tech:rgb,type:paper},
  annotation = {528 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.sciencedirect.com/science/article/pii/S0262885603000702}},
  file = {/Users/brk/Zotero/storage/7FR9BDWF/Chen et al. - 2003 - Hand gesture recognition using a real-time trackin.pdf;/Users/brk/Zotero/storage/9ZPXDS8C/S0262885603000702.html}
}
% == BibTeX quality report for chenHandGestureRecognition2003:
% ? unused Library catalog ("ScienceDirect")

@article{chenSurveyHandGesture2013,
  title = {A {{Survey}} on {{Hand Gesture Recognition}}},
  author = {Chen, Lingchen and Wang, Feng and Deng, Hui and Ji, Kaifan},
  year = {2013},
  month = dec,
  journal = {2013 International Conference on Computer Sciences and Applications},
  pages = {313--316},
  publisher = {{IEEE}},
  address = {{Wuhan, China}},
  doi = {10.1109/CSA.2013.79},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition has become one of the key techniques of human-computer interaction (HCI). Many researchers are devoted in this field. In this paper, firstly the history of hand gesture recognition is discussed and the technical difficulties are also enumerated. Then, we analyze the definition of hand gesture and introduce the basic principle of it. The approaches for hand gesture recognition, such as vision-based, glove-based and depth-based, are contrasted briefly in this paper. But the former two methods are too simple and not natural enough. Currently, the new finger identification and hand gesture recognition technique with Kinect depth data is the most popular research direction. Finally, we discuss the application prospective of hand gesture recognition based on Kinect.},
  isbn = {9780769551258},
  keywords = {based-on:gloves,based-on:vision,hardware:kinect,pdf:paywalled,type:survey},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/6835606/}}
}
% == BibTeX quality report for chenSurveyHandGesture2013:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("2013 International Conference on Computer Sciences and Applications (CSA)")
% ? unused Library catalog ("Semantic Scholar")

@article{chenSurveyHandPose2020,
  title = {A {{Survey}} on {{Hand Pose Estimation}} with {{Wearable Sensors}} and {{Computer-Vision-Based Methods}}},
  author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
  year = {2020},
  month = feb,
  journal = {Sensors},
  volume = {20},
  number = {4},
  pages = {1074},
  issn = {1424-8220},
  doi = {10.3390/s20041074},
  urldate = {2023-06-23},
  abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
  langid = {english},
  keywords = {based-on:gloves,based-on:vision,have-read,type:survey},
  annotation = {54 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/1424-8220/20/4/1074}},
  file = {/Users/brk/Zotero/storage/Y67RWII7/Chen et al. - 2020 - A Survey on Hand Pose Estimation with Wearable Sen.pdf}
}
% == BibTeX quality report for chenSurveyHandPose2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{chenWiFiCSIBased2019,
  title = {{{WiFi CSI Based Passive Human Activity Recognition Using Attention Based BLSTM}}},
  author = {Chen, Zhenghua and Zhang, Le and Jiang, Chaoyang and Cao, Zhiguang and Cui, Wei},
  year = {2019},
  month = nov,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {18},
  number = {11},
  pages = {2714--2724},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2018.2878233},
  urldate = {2023-07-12},
  abstract = {Human activity recognition can benefit various applications including healthcare services and context awareness. Since human actions will influence WiFi signals, which can be captured by the channel state information (CSI) of WiFi, WiFi CSI based human activity recognition has gained more and more attention. Due to the complex relationship between human activities and WiFi CSI measurements, the accuracies of current recognition systems are far from satisfactory. In this paper, we propose a new deep learning based approach, i.e., attention based bi-directional long short-term memory (ABLSTM), for passive human activity recognition using WiFi CSI signals. The BLSTM is employed to learn representative features in two directions from raw sequential CSI measurements. Since the learned features may have different contributions for final activity recognition, we leverage on an attention mechanism to assign different weights for all the learned features. Real experiments have been carried out to evaluate the performance of the proposed ABLSTM for human activity recognition. The experimental results show that our proposed ABLSTM is able to achieve the best recognition performance for all activities when compared with some benchmark approaches.},
  keywords = {based-on:wifi,model:lstm,model:nn,tech:wifi,type:paper},
  annotation = {192 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/8514811/}}
}
% == BibTeX quality report for chenWiFiCSIBased2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. on Mobile Comput.")
% ? unused Library catalog ("Semantic Scholar")

@article{cheokReviewHandGesture2019,
  title = {A Review of Hand Gesture and Sign Language Recognition Techniques},
  author = {Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
  year = {2019},
  month = jan,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {10},
  number = {1},
  pages = {131--153},
  issn = {1868-8071, 1868-808X},
  doi = {10.1007/s13042-017-0705-5},
  urldate = {2023-06-26},
  abstract = {Hand gesture recognition serves as a key for overcoming many difficulties and providing convenience for human life. The ability of machines to understand human activities and their meaning can be utilized in a vast array of applications. One specific field of interest is sign language recognition. This paper provides a thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research. The techniques reviewed are suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification, where the various algorithms at each stage are elaborated and their merits compared. Further, we also discuss the challenges and limitations faced by gesture recognition research in general, as well as those exclusive to sign language recognition. Overall, it is hoped that the study may provide readers with a comprehensive introduction into the field of automated gesture and sign language recognition, and further facilitate future research efforts in this area.},
  langid = {english},
  keywords = {based-on:vision,tech:rgb,type:survey},
  annotation = {301 Citations},
  note = {\url{http://link.springer.com/10.1007/s13042-017-0705-5}},
  file = {/Users/brk/Zotero/storage/62H2TEM2/Cheok et al. - 2019 - A review of hand gesture and sign language recogni.pdf}
}
% == BibTeX quality report for cheokReviewHandGesture2019:
% ? unused Journal abbreviation ("Int. J. Mach. Learn. & Cyber.")
% ? unused Library catalog ("Semantic Scholar")

@article{chettyThroughtheWallSensingPersonnel2012,
  title = {Through-the-{{Wall Sensing}} of {{Personnel Using Passive Bistatic WiFi Radar}} at {{Standoff Distances}}},
  author = {Chetty, Kevin and Smith, Graeme E. and Woodbridge, Karl},
  year = {2012},
  month = apr,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {50},
  number = {4},
  pages = {1218--1226},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2011.2164411},
  urldate = {2023-08-02},
  abstract = {In this paper, we investigate the feasibility of uncooperatively and covertly detecting people moving behind walls using passive bistatic WiFi radar at standoff distances. A series of experiments was conducted which involved personnel targets moving inside a building within the coverage area of a WiFi access point. These targets were monitored from outside the building using a 2.4-GHz passive multistatic receiver, and the data were processed offline to yield range and Doppler information. The results presented show the first through-the-wall (TTW) detections of moving personnel using passive WiFi radar. The measured Doppler shifts agree with those predicted by bistatic theory. Further analysis of the data revealed that the system is limited by the signal-to-interference ratio (SIR), and not the signal-to-noise ratio. We have also shown that a new interference suppression technique based on the CLEAN algorithm can improve the SIR by approximately 19 dB. These encouraging initial findings demonstrate the potential for using passive WiFi radar as a low-cost TTW detection sensor with widespread applicability.},
  keywords = {app:activity-inference,based-on:wifi,reference:doi.org/10.1007/s11235-008-9087-z,reference:doi.org/10.1017/S0373463300014740,reference:doi.org/10.1049/IET-RSN:20060100,reference:doi.org/10.1049/IET-SPR.2009.0058,reference:doi.org/10.1049/IP-F-1:19860097,reference:doi.org/10.1049/IP-RSN:20045017,reference:doi.org/10.1049/IP-RSN:20045026,reference:doi.org/10.1049/IP-RSN:20059064,reference:doi.org/10.1109/GLOCOMW.2009.5360690,reference:doi.org/10.1109/PASSIVE.2008.4786997,reference:doi.org/10.1109/RADAR.2008.4720719,reference:doi.org/10.1109/RADAR.2008.4720988,reference:doi.org/10.1109/RADAR.2009.4976964,reference:doi.org/10.1109/RADAR.2010.5494627,reference:doi.org/10.1109/TAES.2009.5089551,reference:doi.org/10.1109/TAES.2010.5595580,reference:doi.org/10.1109/TAES.2011.5705673,reference:doi.org/10.1109/TAP.2009.2036131,reference:doi.org/10.1109/TGRS.2008.2010052,reference:doi.org/10.1109/TGRS.2008.2010251,reference:doi.org/10.1109/TGRS.2009.2012866,reference:doi.org/10.1109/TGRS.2009.2017053,reference:doi.org/10.1109/TGRS.2009.2019728,reference:doi.org/10.1109/TGRS.2009.2030321,reference:doi.org/10.1109/TMTT.2004.834186,reference:doi.org/10.1117/12.478688,reference:doi.org/10.1201/9781315218144,untagged},
  annotation = {--- DOIs\_of\_references: ["10.1109/TGRS.2009.2012866","10.1109/TGRS.2008.2010251","10.1109/TGRS.2009.2017053","10.1109/TAES.2009.5089551","10.1109/TGRS.2010.2042298","10.1109/TGRS.2009.2037219","10.1049/IET-RSN:20060100","10.1109/RADAR.2010.5494533","10.1049/IP-RSN:20045026","10.1109/TAP.2009.2036131","10.1109/RADAR.2009.4976964","10.1049/IP-RSN:20045017","10.1109/TGRS.2009.2019728","10.1109/RADAR.2010.5494627","10.1109/GLOCOMW.2009.5360690","10.1049/IP-F-1:19860097","10.1109/RADAR.2008.4720988","10.1201/9781315218144","10.1109/TMTT.2004.834186","10.1109/RADAR.2008.4720719","10.1007/s11235-008-9087-z","10.1049/IP-RSN:20059064","10.1109/TGRS.2008.2010052","10.1117/12.478688","10.1049/IET-SPR.2009.0058"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/6020778/}},
  file = {/Users/brk/Zotero/storage/YZNEI26J/Chetty et al. - 2012 - Through-the-Wall Sensing of Personnel Using Passiv.pdf}
}
% == BibTeX quality report for chettyThroughtheWallSensingPersonnel2012:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Geosci. Remote Sensing")
% ? unused Library catalog ("Semantic Scholar")

@article{chuSensorBasedHandGesture2021,
  title = {A {{Sensor-Based Hand Gesture Recognition System}} for {{Japanese Sign Language}}},
  author = {Chu, Xianzhi and Liu, Jiang and Shimamoto, Shigeru},
  year = {2021},
  month = mar,
  journal = {2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)},
  pages = {311--312},
  publisher = {{IEEE}},
  address = {{Nara, Japan}},
  doi = {10.1109/LifeTech52111.2021.9391981},
  urldate = {2023-03-07},
  abstract = {In this paper, we propose a sensor-based data acquisition glove for Japanese Sign Language (JSL) hand gesture recognition. Five flex sensors, an Inertial Measurement Unit (IMU), and three Force Sensing Resistors (FSRs) are used to detect the bending degree of fingers and hand movement information. The detected data are transmitted to the computer by an Arduino Micro. The average accuracy of the hand gesture recognition for a single subject, using the Support Vector Machine (SVM) based and the Dynamic Time Wrapping (DTW) based algorithm are 96.9\% and 94.5\%, respectively. Our proposed system also achieves an average recognition accuracy of about 82.5\% for the cross-recognition among three subjects. The experimental results indicate that our proposed system has great potential for JSL hand gesture recognition.},
  isbn = {9781665418751},
  keywords = {/unread,app:sign-language,based-on:gloves,pdf:paywalled,type:paper},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9391981/}}
}
% == BibTeX quality report for chuSensorBasedHandGesture2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{collialfaroUserIndependentHandGesture2022,
  title = {User-{{Independent Hand Gesture Recognition Classification Models Using Sensor Fusion}}},
  author = {Colli Alfaro, Jose Guillermo and Trejos, Ana Luisa},
  year = {2022},
  month = feb,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1321},
  issn = {1424-8220},
  doi = {10.3390/s22041321},
  urldate = {2023-07-11},
  abstract = {Recently, it has been proven that targeting motor impairments as early as possible while using wearable mechatronic devices for assisted therapy can improve rehabilitation outcomes. However, despite the advanced progress on control methods for wearable mechatronic devices, the need for a more natural interface that allows for better control remains. To address this issue, electromyography (EMG)-based gesture recognition systems have been studied as a potential solution for human\textendash machine interface applications. Recent studies have focused on developing user-independent gesture recognition interfaces to reduce calibration times for new users. Unfortunately, given the stochastic nature of EMG signals, the performance of these interfaces is negatively impacted. To address this issue, this work presents a user-independent gesture classification method based on a sensor fusion technique that combines EMG data and inertial measurement unit (IMU) data. The Myo Armband was used to measure muscle activity and motion data from healthy subjects. Participants were asked to perform seven types of gestures in four different arm positions while using the Myo on their dominant limb. Data obtained from 22 participants were used to classify the gestures using three different classification methods. Overall, average classification accuracies in the range of 67.5\textendash 84.6\% were obtained, with the Adaptive Least-Squares Support Vector Machine model obtaining accuracies as high as 92.9\%. These results suggest that by using the proposed sensor fusion approach, it is possible to achieve a more natural interface that allows better control of wearable mechatronic devices during robot assisted therapies.},
  langid = {english},
  keywords = {app:medical,based-on:gloves,classes:7,contains-more-references,dataset:custom,fidelity:hand,hardware:myo-armband,has-picture,have-read,model:nn,model:svm,movement:static,participants:22,reference:doi.org/10.1007/s00779-018-1152-3,reference:doi.org/10.1016/j.robot.2014.08.012,reference:doi.org/10.1088/1361-6579/abef56,reference:doi.org/10.1109/10.204774,reference:doi.org/10.1109/ACIRS.2017.7986086,reference:doi.org/10.1109/ICAC3N53548.2021.9725647,reference:doi.org/10.1109/ICORR.2019.8779533,reference:doi.org/10.1109/ICRA.2013.6630718,reference:doi.org/10.1109/IJCNN.2018.8489150,reference:doi.org/10.1109/ROBIO49542.2019.8961582,reference:doi.org/10.1109/TBME.2013.2250502,reference:doi.org/10.1109/TNSRE.2014.2304470,reference:doi.org/10.18637/JSS.V046.I07,reference:doi.org/10.2478/V10170-011-0033-Z,reference:doi.org/10.3389/fnins.2017.00379,reference:doi.org/10.3390/s17061370,reference:doi.org/10.3390/s18092767,reference:doi.org/10.5555/2627435.2670313,repetitions:10,segmentation:explicit,tech:emg,tech:imu,type:paper},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1016/j.jocs.2018.04.019","10.1007/s10846-017-0725-0","10.1186/s12984-017-0284-4","10.1109/TBME.2003.813539","10.1016/j.eswa.2012.01.102","10.1109/IROS.2016.7759384","10.3389/fnins.2017.00379","10.3390/s18092767","10.1007/978-3-319-27707-3\_19","10.5555/2627435.2670313","10.1109/ROBIO49542.2019.8961582","10.1109/TNSRE.2014.2304470","10.18637/JSS.V046.I07","10.1109/ICAC3N53548.2021.9725647","10.1109/IJCNN.2018.8489150","10.3390/s17061370","10.1088/1361-6579/abef56","10.1109/ICRA.2013.6630718","10.1109/TBME.2013.2250502","10.1016/J.MEDENGPHY.2021.03.010","10.1007/978-3-319-48036-7\_2","10.1109/TNSRE.2011.2163529","10.1109/TRO.2012.2226386","10.1016/j.inffus.2016.09.005","10.2478/V10170-011-0033-Z","10.1016/j.robot.2014.08.012"] ---},
  note = {\url{https://www.mdpi.com/1424-8220/22/4/1321}},
  file = {/Users/brk/Zotero/storage/BBADBX3U/Colli Alfaro and Trejos - 2022 - User-Independent Hand Gesture Recognition Classifi.pdf}
}
% == BibTeX quality report for collialfaroUserIndependentHandGesture2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{cooperSignLanguageRecognition2011,
  title = {Sign {{Language Recognition}}},
  booktitle = {Visual {{Analysis}} of {{Humans}}},
  author = {Cooper, Helen and Holt, Brian and Bowden, Richard},
  editor = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker and Sigal, Leonid},
  year = {2011},
  pages = {539--562},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-0-85729-997-0_27},
  urldate = {2023-03-07},
  abstract = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a precis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
  isbn = {978-0-85729-996-3 978-0-85729-997-0},
  langid = {english},
  keywords = {app:sign-language,based-on:gloves,based-on:vision,contains-more-references,have-read,reference:doi.org/10.1007/11612704\_55,reference:doi.org/10.1007/978-3-540-24598-8\_17,reference:doi.org/10.1007/978-3-642-35749-7\_20,reference:doi.org/10.1007/s00138-005-0003-1,reference:doi.org/10.1007/s00138-016-0776-4,reference:doi.org/10.1007/s10209-007-0096-6,reference:doi.org/10.1007/s10209-007-0104-x,reference:doi.org/10.1016/j.patcog.2008.09.010,reference:doi.org/10.1016/j.patrec.2007.10.011,reference:doi.org/10.1080/01596306.2010.504362,reference:doi.org/10.1093/DEAFED/ENI001,reference:doi.org/10.1109/34.506414,reference:doi.org/10.1109/34.735811,reference:doi.org/10.1109/ICBBE.2011.5781474,reference:doi.org/10.1109/ICME.2004.1394355,reference:doi.org/10.1109/msmc.2021.3124031,reference:doi.org/10.1109/RATFG.2001.938914,reference:doi.org/10.1109/TPAMI.2002.1023803,reference:doi.org/10.1109/TPAMI.2010.205,reference:doi.org/10.1126/science.1260318,reference:doi.org/10.1186/s13362-021-00114-7,reference:doi.org/10.13053/RCS-111-1-5,reference:doi.org/10.1353/SLS.1989.0027,reference:doi.org/10.2112/JCR-SI115-061.1,reference:doi.org/10.2307/4141588,reference:doi.org/10.2991/MMEBC-16.2016.112,reference:doi.org/10.47460/ATHENEA.V2I4.18,reference:doi.org/10.54941/ahfe1001871,type:survey},
  annotation = {293 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1016/j.patcog.2008.09.010","10.1007/978-3-540-24598-8\_17","10.2307/4141588","10.1186/s13362-021-00114-7","10.1109/TPAMI.2010.205","10.1007/s00138-016-0776-4","10.1007/11612704\_55","10.1109/ICME.2004.1394355","10.2991/MMEBC-16.2016.112","10.1007/978-3-642-35749-7\_20","10.1109/34.735811","10.54941/ahfe1001871","10.1016/j.patrec.2008.12.010","10.1007/s10209-007-0096-6","10.1016/j.patrec.2007.10.011","10.13053/RCS-111-1-5","10.1126/science.1260318","10.1007/s10209-007-0104-x","10.1109/TPAMI.2002.1023803","10.1109/ICBBE.2011.5781474","10.1109/34.506414"] ---},
  note = {\url{https://link.springer.com/10.1007/978-0-85729-997-0_27}},
  file = {/Users/brk/Zotero/storage/X944QEXB/Cooper et al. - 2011 - Sign Language Recognition.pdf}
}
% == BibTeX quality report for cooperSignLanguageRecognition2011:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@misc{cybergloveincWirelessCyberGloveII,
  title = {Wireless {{CyberGlove II Motion Capture Data Glove}}},
  author = {{CyberGlove Inc}},
  howpublished = {\url{http://www.cyberglovesystems.com/cyberglove-ii/}},
  keywords = {from:cite.bib,type:product}
}
% == BibTeX quality report for cybergloveincWirelessCyberGloveII:
% ? Title looks like it was stored in title-case in Zotero

@article{damasioAnimatingVirtualHumans2002,
  title = {Animating Virtual Humans Using Hand Postures},
  author = {Damasio, F.W. and Musse, S.R.},
  year = {2002},
  journal = {Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing},
  pages = {437},
  publisher = {{IEEE Comput. Soc}},
  address = {{Fortaleza-CE, Brazil}},
  doi = {10.1109/SIBGRA.2002.1167207},
  urldate = {2023-07-02},
  abstract = {Interaction between human and computer has been used in a large scale in computer graphics and virtual reality. This paper presents a system to provide interaction between user and virtual humans. The system uses a data glove and an artificial neural network system responsible for the recognition of hand postures.},
  isbn = {9780769518466},
  keywords = {based-on:gloves,have-read,model:ffnn,model:nn,movement:static,pdf:paywalled,type:paper},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/1167207/}}
}
% == BibTeX quality report for damasioAnimatingVirtualHumans2002:
% ? Possibly abbreviated journal title Proceedings. XV Brazilian Symposium on Computer Graphics and Image Processing
% ? unused Conference name ("15th Brazilian Symposium on Computer Graphics and Image Processing")
% ? unused Library catalog ("Semantic Scholar")

@article{dangAirGestureRecognition2020,
  title = {Air {{Gesture Recognition Using WLAN Physical Layer Information}}},
  author = {Dang, Xiaochao and Liu, Yang and Hao, Zhanjun and Tang, Xuhao and Shao, Chenguang},
  year = {2020},
  month = aug,
  journal = {Wireless Communications and Mobile Computing},
  volume = {2020},
  pages = {1--14},
  issn = {1530-8669, 1530-8677},
  doi = {10.1155/2020/8546237},
  urldate = {2023-07-12},
  abstract = {In recent years, the researchers have witnessed the important role of air gesture recognition in human-computer interactive (HCI), smart home, and virtual reality (VR). The traditional air gesture recognition method mainly depends on external equipment (such as special sensors and cameras) whose costs are high and also with a limited application scene. In this paper, we attempt to utilize channel state information (CSI) derived from a WLAN physical layer, a Wi-Fibased air gesture recognition system, namely, WiNum, which solves the problems of users' privacy and energy consumption compared with the approaches using wearable sensors and depth cameras. In the process of recognizing the WiNum method, the collected raw data of CSI should be screened, among which can reflect the gesture motion. Meanwhile, the screened data should be preprocessed by noise reduction and linear transformation. After preprocessing, the joint of amplitude information and phase information is extracted, to match and recognize different air gestures by using the S-DTW algorithm which combines dynamic time warping algorithm (DTW) and support vector machine (SVM) properties. Comprehensive experiments demonstrate that under two different indoor scenes, WiNum can achieve higher recognition accuracy for air number gestures; the average recognition accuracy of each motion reached more than 93\%, in order to achieve effective recognition of air gestures.},
  langid = {english},
  keywords = {based-on:wifi,classes:10,fidelity:arm,model:dynamic-time-warping,model:svm,tech:wifi,type:paper},
  note = {\url{https://www.hindawi.com/journals/wcmc/2020/8546237/}},
  file = {/Users/brk/Zotero/storage/9WJ72HZQ/Dang et al. - 2020 - Air Gesture Recognition Using WLAN Physical Layer .pdf}
}
% == BibTeX quality report for dangAirGestureRecognition2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{davisVisualGestureRecognition1994,
  title = {Visual Gesture Recognition},
  booktitle = {{{IEE Proceedings}} - {{Vision}}, {{Image}}, and {{Signal Processing}}},
  author = {Davis, J.},
  year = {1994},
  volume = {141},
  pages = {101},
  issn = {1350245X},
  doi = {10.1049/ip-vis:19941058},
  urldate = {2023-07-11},
  abstract = {Presents a method for recognising human-hand gestures using a model based approach. A finite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of left, right, up, down, grab, rotate, and stop.},
  langid = {english},
  keywords = {based-on:vision,model:finite-state-machine,movement:dynamic,pdf:paywalled,tech:rgb,type:paper},
  annotation = {272 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941058}}
}
% == BibTeX quality report for davisVisualGestureRecognition1994:
% ? unused Issue ("2")
% ? unused Journal abbreviation ("IEE Proc., Vis. Image Process.")
% ? unused Library catalog ("Semantic Scholar")

@article{dipietroSurveyGloveBasedSystems2008,
  title = {A {{Survey}} of {{Glove-Based Systems}} and {{Their Applications}}},
  author = {Dipietro, L. and Sabatini, A.M. and Dario, P.},
  year = {2008},
  month = jul,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {38},
  number = {4},
  pages = {461--482},
  issn = {1094-6977, 1558-2442},
  doi = {10.1109/TSMCC.2008.923862},
  urldate = {2023-06-23},
  abstract = {Hand movement data acquisition is used in many engineering applications ranging from the analysis of gestures to the biomedical sciences. Glove-based systems represent one of the most important efforts aimed at acquiring hand movement data. While they have been around for over three decades, they keep attracting the interest of researchers from increasingly diverse fields. This paper surveys such glove systems and their applications. It also analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research. A foremost goal of this paper is to provide readers who are new to the area with a basis for understanding glove systems technology and how it can be applied, while offering specialists an updated picture of the breadth of applications in several engineering and biomedical sciences areas.},
  keywords = {based-on:gloves,contains-more-references,have-read,reference:doi.org/10.1002/SMJ.2706,reference:doi.org/10.1007/3-540-40063-X\_74,reference:doi.org/10.1007/3-540-46616-9\_25,reference:doi.org/10.1007/s00221-006-0840-9,reference:doi.org/10.1007/s00221-007-1136-4,reference:doi.org/10.1016/j.ijhcs.2005.08.004,reference:doi.org/10.1016/S0924-980X(97)00086-6,reference:doi.org/10.1017/S0263574702004708,reference:doi.org/10.1054/jhsb.1999.0360,reference:doi.org/10.1055/S-2001-12082,reference:doi.org/10.1089/109493103769710523,reference:doi.org/10.1097/00003086-199211000-00035,reference:doi.org/10.1109/2.84835,reference:doi.org/10.1109/3477.623236,reference:doi.org/10.1109/38.250916,reference:doi.org/10.1109/4233.826858,reference:doi.org/10.1109/5.662873,reference:doi.org/10.1109/5326.760563,reference:doi.org/10.1109/6.642965,reference:doi.org/10.1109/70.964669,reference:doi.org/10.1109/7333.948460,reference:doi.org/10.1109/ACCAI53970.2022.9752580,reference:doi.org/10.1109/AFGR.2002.1004188,reference:doi.org/10.1109/AFGR.2004.1301590,reference:doi.org/10.1109/AINA.2006.347,reference:doi.org/10.1109/BSN.2006.20,reference:doi.org/10.1109/CBMS.2007.6,reference:doi.org/10.1109/CCECE.1999.804914,reference:doi.org/10.1109/COMPSAC54236.2022.00033,reference:doi.org/10.1109/ICAT.2006.68,reference:doi.org/10.1109/ICKS.2007.10,reference:doi.org/10.1109/ICME.2006.262558,reference:doi.org/10.1109/ICMI.2002.1166990,reference:doi.org/10.1109/ICRA.2019.8794034,reference:doi.org/10.1109/ICSMC.1999.825357,reference:doi.org/10.1109/ICSMC.2006.384834,reference:doi.org/10.1109/IECON.1995.483940,reference:doi.org/10.1109/IEMBS.1998.747104,reference:doi.org/10.1109/IEMBS.2004.1403632,reference:doi.org/10.1109/IEMBS.2004.1403713,reference:doi.org/10.1109/IEMBS.2004.1404326,reference:doi.org/10.1109/IEMBS.2006.259307,reference:doi.org/10.1109/IEMBS.2006.260574,reference:doi.org/10.1109/IEMBS.2006.260947,reference:doi.org/10.1109/IMTC.1998.679825,reference:doi.org/10.1109/IMTC.2001.928844,reference:doi.org/10.1109/IRDS.2002.1043880,reference:doi.org/10.1109/IROS.1999.812786,reference:doi.org/10.1109/IROS.2003.1249694,reference:doi.org/10.1109/ISSMDBS.2006.360106,reference:doi.org/10.1109/ISWC.1998.729544,reference:doi.org/10.1109/ISWC.2001.962093,reference:doi.org/10.1109/ISWC.2002.1167217,reference:doi.org/10.1109/ISWC.2002.1167238,reference:doi.org/10.1109/ISWC.2003.1241392,reference:doi.org/10.1109/ISWC.2004.2,reference:doi.org/10.1109/IV.2006.55,reference:doi.org/10.1109/IV.2007.23,reference:doi.org/10.1109/IWVR.2006.1707518,reference:doi.org/10.1109/JSEN.2007.894132,reference:doi.org/10.1109/ROBOT.1989.100229,reference:doi.org/10.1109/ROBOT.1998.677377,reference:doi.org/10.1109/ROBOT.1998.681418,reference:doi.org/10.1109/ROBOT.2000.845252,reference:doi.org/10.1109/ROBOT.2001.932879,reference:doi.org/10.1109/ROBOT.2002.1014763,reference:doi.org/10.1109/ROBOT.2003.1241922,reference:doi.org/10.1109/SIBGRA.2002.1167207,reference:doi.org/10.1109/TC.2003.1223635,reference:doi.org/10.1109/TIE.2005.858736,reference:doi.org/10.1109/TITB.2005.854510,reference:doi.org/10.1109/TMM.2003.814795,reference:doi.org/10.1109/TNSRE.2007.891367,reference:doi.org/10.1109/TNSRE.2007.891393,reference:doi.org/10.1109/TPAMI.2005.112,reference:doi.org/10.1109/TRO.2004.833816,reference:doi.org/10.1109/TVCG.2007.37,reference:doi.org/10.1109/VECIMS.2006.250779,reference:doi.org/10.1109/VR.2001.913781,reference:doi.org/10.1109/VR.2016.7504682,reference:doi.org/10.1109/VSMM.1997.622351,reference:doi.org/10.1111/j.1467-8659.1995.cgf143\_0067.x,reference:doi.org/10.1115/1.1481363,reference:doi.org/10.1115/1.2829491,reference:doi.org/10.1145/1101616.1101635,reference:doi.org/10.1145/1242073.1242272,reference:doi.org/10.1145/223355.223695,reference:doi.org/10.1145/3304181.3304194,reference:doi.org/10.1145/354324.354340,reference:doi.org/10.1145/365024.365114,reference:doi.org/10.1145/67449.67495,reference:doi.org/10.1152/JN.1998.79.3.1307,reference:doi.org/10.1155/2016/8347478,reference:doi.org/10.1162/pres.1992.1.1.18,reference:doi.org/10.1201/9780203491409.CH16,reference:doi.org/10.1523/JNEUROSCI.18-23-10105.1998,reference:doi.org/10.1682/JRRD.2003.03.0181,reference:doi.org/10.2514/6.2001-4944,type:survey},
  annotation = {652 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1109/ROBOT.2000.845252","10.1162/pres.1992.1.1.18","10.1109/TVCG.2007.37","10.1109/ISWC.2001.962093","10.1109/IMTC.1998.679825","10.1109/ISPASS55109.2022.00008","10.1109/70.964669","10.1109/ICAT.2006.68","10.1109/ISWC.2001.962130","10.1109/CCECE.1999.804914","10.1115/1.2829490","10.1109/MCG.1997.10010","10.1117/12.2638771","10.1109/AFGR.2004.1301590","10.1109/TIE.2003.814765","10.1109/MEMS46641.2020.9056118","10.1109/38.250916","10.1109/IEMBS.2004.1404364","10.1109/CBMS.2007.6","10.1109/ROBOT.2002.1014763","10.1109/2.84835","10.1115/1.2829491","10.1016/j.ijhcs.2005.08.004","10.1111/j.1467-8659.1995.cgf143\_0067.x","10.1109/ICME.2006.262558","10.1109/IROS.1999.812786","10.1109/ROBOT.1998.681418","10.1109/6.642965","10.1682/JRRD.2003.03.0181","10.1109/AFGR.2002.1004188","10.1007/s00221-006-0840-9","10.1109/VECIMS.2006.250779","10.1115/1.1481363","10.1109/ISWC.2002.1167238","10.1109/IRDS.2002.1043880","10.1109/COMPSAC54236.2022.00033","10.1109/TNSRE.2007.891367","10.1145/365024.365114","10.1152/JN.1998.79.3.1307","10.1109/ICRA.2019.8794034","10.1109/ICSMC.2006.384834","10.1145/122488.122499","10.1201/9780203491409.CH16","10.1109/TPAMI.2005.112","10.1109/JSEN.2007.894132","10.1109/IMTC.2001.928844","10.1109/AFGR.1998.671007","10.1109/ACCAI53970.2022.9752580","10.1016/S0924-980X(97)00086-6","10.1109/IWVR.2006.1707518","10.1109/ICSMC.1999.825357","10.1109/IEMBS.2004.1404326","10.1109/IEMBS.2006.260947","10.1109/VR.2016.7504682","10.1109/ROBOT.2001.932879","10.1109/IEMBS.2006.259307","10.1109/4233.826858","10.1109/IV.2006.55","10.1523/JNEUROSCI.18-23-10105.1998","10.1002/SMJ.2706","10.1109/IV.2006.55","10.1109/ISWC.2002.1167217","10.1007/s00221-007-1136-4","10.1109/BSN.2006.20","10.1109/WCICA.2000.859920","10.1109/IROS.2003.1249694","10.1109/ISWC.1999.806717","10.1054/jhsb.1999.0360","10.1109/VSMM.1997.622351","10.1109/IEMBS.2006.260574","10.1109/IEMBS.2004.1403632","10.1145/354324.354340","10.2514/6.2001-4944","10.1109/ISWC.1998.729544","10.1109/IECON.1995.483940","10.1109/5.662873","10.1109/AINA.2006.347","10.1109/TITB.2005.854510","10.1007/3-540-46616-9\_25","10.1109/ISWC.2004.2","10.1145/3304181.3304194","10.1109/7333.948460","10.1109/IEMBS.1998.747104","10.1109/ROBOT.1998.677377","10.1109/5326.760563","10.1109/ROBOT.1998.677377","10.1109/IEMBS.2004.1403713","10.1109/VR.2001.913781","10.1109/TIE.2005.858736","10.1177/14759217211021942","10.1155/2016/8347478","10.1055/S-2001-12082","10.1145/1101616.1101635","10.1109/TC.2003.1223635","10.1109/ISWC.2003.1241392"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/4539650/}},
  file = {/Users/brk/Zotero/storage/RWUPUG9F/Dipietro et al. - 2008 - A Survey of Glove-Based Systems and Their Applicat.pdf}
}
% == BibTeX quality report for dipietroSurveyGloveBasedSystems2008:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Syst., Man, Cybern. C")
% ? unused Library catalog ("Semantic Scholar")

@misc{duchiOnlineLearningStochastic,
  title = {Online {{Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John C. and Hazan, Elad and Singer, Y.},
  urldate = {2023-07-11},
  abstract = {This work describes and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal functions that can be chosen in hindsight. We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  howpublished = {\url{https://www.semanticscholar.org/paper/Adaptive-Subgradient-Methods-for-Online-Learning-Duchi-Hazan/413c1142de9d91804d6d11c67ff3fed59c9fc279}},
  langid = {english},
  keywords = {adagrad,background},
  file = {/Users/brk/Zotero/storage/E6YNVHRW/[PDF] Adaptive Subgradient Methods for Online Lear.pdf;/Users/brk/Zotero/storage/GGADQGGW/413c1142de9d91804d6d11c67ff3fed59c9fc279.html}
}
% == BibTeX quality report for duchiOnlineLearningStochastic:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{durbinBiologicalSequenceAnalysis1998,
  title = {Biological {{Sequence Analysis}}: {{Probabilistic Models}} of {{Proteins}} and {{Nucleic Acids}}},
  shorttitle = {Biological {{Sequence Analysis}}},
  author = {Durbin, Richard and Eddy, Sean R. and Krogh, Anders and Mitchison, Graeme},
  year = {1998},
  month = apr,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511790492},
  urldate = {2023-11-13},
  abstract = {Probabilistic models are becoming increasingly important in analysing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analysing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it aims to be accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time present the state-of-the-art in this new and highly important field.},
  isbn = {978-0-521-62041-3 978-0-521-62971-3 978-0-511-79049-2},
  keywords = {/unread},
  annotation = {4520 citations (Semantic Scholar/DOI) [2023-11-13]},
  note = {\url{https://www.cambridge.org/core/product/identifier/9780511790492/type/book}}
}
% == BibTeX quality report for durbinBiologicalSequenceAnalysis1998:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@incollection{DynamicTimeWarping2007,
  title = {Dynamic {{Time Warping}}},
  booktitle = {Information {{Retrieval}} for {{Music}} and {{Motion}}},
  year = {2007},
  pages = {69--84},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74048-3_4},
  abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
  isbn = {978-3-540-74048-3},
  keywords = {background,model:dynamic-time-warping},
  note = {\url{https://doi.org/10.1007/978-3-540-74048-3_4}}
}
% == BibTeX quality report for DynamicTimeWarping2007:
% Missing required field 'author'
% ? Title looks like it was stored in title-case in Zotero

@article{eickelerHiddenMarkovModel1998,
  title = {Hidden {{Markov}} Model Based Continuous Online Gesture Recognition},
  author = {Eickeler, S. and Kosmala, A. and Rigoll, G.},
  year = {1998},
  journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
  volume = {2},
  pages = {1206--1208},
  publisher = {{IEEE Comput. Soc}},
  address = {{Brisbane, Qld., Australia}},
  doi = {10.1109/ICPR.1998.711914},
  urldate = {2023-07-11},
  abstract = {Presents the extension of an existing vision-based gesture recognition system using hidden Markov models(HMMs). Several improvements have been carried out in order to increase the capabilities and the functionality of the system. These improvements include position independent recognition, rejection of unknown gestures, and continuous online recognition of spontaneous gestures. We show that especially the latter requirement is highly complicated and demanding, if we allow the user to move in front of the camera without any restrictions and to perform the gestures spontaneously at any arbitrary moment. We present solutions to this problem by modifying the HMM-based decoding process and by introducing online feature extraction and evaluation methods.},
  isbn = {9780818685125},
  keywords = {based-on:vision,classes:24,from:cite.bib,have-read,model:hmm,tech:rgb,type:paper},
  annotation = {39 citations (Crossref) [2023-07-11] 140 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/711914/}},
  file = {/Users/brk/Zotero/storage/H2I7WA59/Eickeler et al. - 1998 - Hidden Markov model based continuous online gestur.pdf}
}
% == BibTeX quality report for eickelerHiddenMarkovModel1998:
% ? Possibly abbreviated journal title Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)
% ? unused Conference name ("Fourteenth International Conference on Pattern Recognition")
% ? unused Library catalog ("Semantic Scholar")

@article{elbadawyArabicSignLanguage2017,
  title = {Arabic Sign Language Recognition with {{3D}} Convolutional Neural Networks},
  author = {ElBadawy, Menna and Elons, A. S. and Shedeed, Howida A. and Tolba, M. F.},
  year = {2017},
  month = dec,
  journal = {2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)},
  pages = {66--71},
  publisher = {{IEEE}},
  address = {{Cairo}},
  doi = {10.1109/INTELCIS.2017.8260028},
  urldate = {2023-06-22},
  abstract = {Sign Language recognition is very important for communication purposes between Hearing Impaired (HI) people and hearing ones. Arabic Sign Language Recognition field became widespread because of its difficult nature and numerous details. Most researchers employed different input sensors, features extractors, and classifiers on static and dynamic data. These different ways were customized and employed in our previous work in the Arabic Sign Language Recognition field. In this paper, features extractor with deep behavior was used to deal with the minor details of Arabic Sign Language. 3D Convolutional Neural Network (CNN) was used to recognize 25 gestures from Arabic sign language dictionary. The recognition system was fed with data from depth maps. The system achieved 98\% accuracy for observed data and 85\% average accuracy for new data. The results could be improved as more data from more different signers are included.},
  isbn = {9781538608210},
  keywords = {app:sign-language,based-on:vision,fidelity:finger,pdf:cant-find,type:paper},
  annotation = {43 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/8260028/}}
}
% == BibTeX quality report for elbadawyArabicSignLanguage2017:
% ? unused Library catalog ("Semantic Scholar")

@article{elmanDistributedRepresentationsSimple2004,
  title = {Distributed Representations, Simple Recurrent Networks, and Grammatical Structure},
  author = {Elman, Jeffrey L.},
  year = {2004},
  journal = {Machine Learning},
  volume = {7},
  pages = {195--225},
  keywords = {background,from:cite.bib,model:elman-rnn,type:seminal}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = mar,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {03640213},
  doi = {10.1207/s15516709cog1402_1},
  urldate = {2023-07-13},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  keywords = {background,model:elman-rnn,model:nn,model:rnn,type:seminal},
  annotation = {9998 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://doi.wiley.com/10.1207/s15516709cog1402_1}}
}
% == BibTeX quality report for elmanFindingStructureTime1990:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{elmezainGestureRecognitionAlphabets2007,
  title = {Gesture {{Recognition}} for {{Alphabets}} from {{Hand Motion Trajectory Using Hidden Markov Models}}},
  author = {Elmezain, Mahmoud and {Al-Hamadi}, Ayoub and Krell, Gerald and {El-Etriby}, Sherif and Michaelis, Bernd},
  year = {2007},
  month = dec,
  journal = {2007 IEEE International Symposium on Signal Processing and Information Technology},
  pages = {1192--1197},
  publisher = {{IEEE}},
  address = {{Giza, Egypt}},
  doi = {10.1109/ISSPIT.2007.4458209},
  urldate = {2023-07-11},
  abstract = {This paper describes a method to recognize the alphabets from a single hand motion using Hidden Markov Models (HMM). In our method, gesture recognition for alphabets is based on three main stages; preprocessing, feature extraction and classification. In preprocessing stage, color and depth information are used to detect both hands and face in connection with morphological operation. After the detection of the hand, the tracking will take place in further step in order to determine the motion trajectory so-called gesture path. The second stage, feature extraction enhances the gesture path which gives us a pure path and also determines the orientation between the center of gravity and each point in a pure path. Thereby, the orientation is quantized to give a discrete vector that used as input to HMM. In the final stage, the gesture of alphabets is recognized by using Left-Right Banded model (LRB) in conjunction with Baum-Welch algorithm (BW) for training the parameters of HMM. Therefore, the best path is obtained by Viterbi algorithm using a gesture database. In our experiment, 520 trained gestures are used for training and also 260 tested gestures for testing. Our method recognizes the alphabets from A to Z and achieves an average recognition rate of 92.3\%.},
  isbn = {9781424418343 9781424418350},
  keywords = {based-on:vision,classes:26,have-read,model:hmm,tech:rgbd,type:paper},
  annotation = {49 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/4458209/}},
  file = {/Users/brk/Zotero/storage/3ARHQX63/Elmezain et al. - 2007 - Gesture Recognition for Alphabets from Hand Motion.pdf}
}
% == BibTeX quality report for elmezainGestureRecognitionAlphabets2007:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{elmezainHandGestureRecognition2009,
  title = {Hand {{Gesture Recognition Based}} on {{Combined Features Extraction}}},
  author = {Elmezain, M. and {Al-Hamadi}, A. and Michaelis, B.},
  year = {2009},
  month = dec,
  journal = {International Journal of Electrical and Computer Engineering},
  urldate = {2023-07-11},
  abstract = {Hand gesture is an active area of research in the vision community, mainly for the purpose of sign language recognition and Human Computer Interaction. In this paper, we propose a system to recognize alphabet characters (A-Z) and numbers (0-9) in real-time from stereo color image sequences using Hidden Markov Models (HMMs). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, color and 3D depth map are used to detect hands where the hand trajectory will take place in further step using Mean-shift algorithm and Kalman filter. In the feature extraction stage, 3D combined features of location, orientation and velocity with respected to Cartesian systems are used. And then, k-means clustering is employed for HMMs codeword. The final stage so-called classification, BaumWelch algorithm is used to do a full train for HMMs parameters. The gesture of alphabets and numbers is recognized using Left-Right Banded model in conjunction with Viterbi algorithm. Experimental results demonstrate that, our system can successfully recognize hand gestures with 98.33\% recognition rate. Keywords\textemdash Gesture Recognition, Computer Vision \& Image Processing, Pattern Recognition.},
  keywords = {based-on:vision,classes:36,have-read,model:hmm,model:k-means,model:kalman-filter,tech:rgb,type:paper},
  annotation = {--- DOIs\_of\_references: ["10.1109/ISPA.2009.5297719","10.1109/ECAI.2017.8166414","10.1109/ISPA.2009.5297719","10.1109/ECAI.2017.8166414","10.1109/ISPA.2009.5297719","10.1109/ECAI.2017.8166414"] ---},
  note = {\url{https://kns.cnki.net/kcms/detail/detail.aspx?FileName=SDSG201804021&DbName=CJFDLAST2019&DbCode=CJFD}},
  file = {/Users/brk/Zotero/storage/GP7SDB8D/Elmezain et al. - 2009 - Hand Gesture Recognition Based on Combined Feature.pdf}
}
% == BibTeX quality report for elmezainHandGestureRecognition2009:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{elmezainRealTimeCapableSystem2008,
  title = {Real-{{Time Capable System}} for {{Hand Gesture Recognition Using Hidden Markov Models}} in {{Stereo Color Image Sequences}}},
  author = {Elmezain, M. and {Al-Hamadi}, A. and Michaelis, B.},
  year = {2008},
  journal = {J. WSCG},
  urldate = {2023-07-11},
  abstract = {This paper proposes a system to recognize the alphabets and numbers in real time from color image sequences by the motion trajectory of a single hand using Hidden Markov Models (HMM). Our system is based on three main stages; automatic segmentation and preprocessing of the hand regions, feature extraction and classification. In automatic segmentation and preprocessing stage, YCbCr color space and depth information are used to detect hands and face in connection with morphological operation where Gaussian Mixture Model (GMM) is used for computing the skin probability. After the hand is detected and the centroid point of the hand region is determined, the tracking will take place in the further steps to determine the hand motion trajectory by using a search area around the hand region. In the feature extraction stage, the orientation is determined between two consecutive points from hand motion trajectory and then it is quantized to give a discrete vector that is used as input to HMM. The final stage so-called classification, Baum-Welch algorithm (BW) is used to do a full train for HMM parameters. The gesture of alphabets and numbers is recognized by using Left-Right Banded model (LRB) in conjunction with Forward algorithm. In our experiment, 720 trained gestures are used for training and also 360 tested gestures for testing. Our system recognizes the alphabets from A to Z and numbers from 0 to 9 and achieves an average recognition rate of 94.72\%.},
  keywords = {based-on:vision,classes:36,have-read,model:gmm,model:hmm,tech:rgb,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Real-Time-Capable-System-for-Hand-Gesture-Using-in-Elmezain-Al-Hamadi/aa17055855518ac47031bc509f5b7aa1b82fdcff}},
  file = {/Users/brk/Zotero/storage/UM4SSI5U/Elmezain et al. - 2008 - Real-Time Capable System for Hand Gesture Recognit.pdf}
}
% == BibTeX quality report for elmezainRealTimeCapableSystem2008:
% ? Possibly abbreviated journal title J. WSCG
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@misc{experimentaltelevisioncenterComputerImageCorporation1969,
  title = {Computer {{Image Corporation Archive}}},
  author = {{Experimental Television Center}},
  year = {1969},
  journal = {Computer Image Corporation Archive},
  abstract = {The Computer Image Corporation Archive contains an extensive film and video collection documenting work produced using machines developed by Computer Image Corporation of Denver, Colorado (1960-1985).},
  howpublished = {\url{https://www.experimentaltvcenter.org/computer-image-corporation-archive/}},
  keywords = {based-on:gloves,hardware:animac,have-read}
}
% == BibTeX quality report for experimentaltelevisioncenterComputerImageCorporation1969:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fangRealTimeHandGesture2007,
  title = {A {{Real-Time Hand Gesture Recognition Method}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Fang, Yikai and Wang, Kongqiao and Cheng, Jian and Lu, Hanqing},
  year = {2007},
  month = jul,
  pages = {995--998},
  issn = {1945-788X},
  doi = {10.1109/ICME.2007.4284820},
  abstract = {Compared with the traditional interaction approaches, such as keyboard, mouse, pen, etc, vision based hand interaction is more natural and efficient. In this paper, we proposed a robust real-time hand gesture recognition method. In our method, firstly, a specific gesture is required to trigger the hand detection followed by tracking; then hand is segmented using motion and color cues; finally, in order to break the limitation of aspect ratio encountered in most of learning based hand gesture methods, the scale-space feature detection is integrated into gesture recognition. Applying the proposed method to navigation of image browsing, experimental results show that our method achieves satisfactory performance.},
  keywords = {based-on:vision,classes:6,have-read,tech:rgb,tech:rgbd,type:paper},
  annotation = {227 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/HUIYKT2Y/Fang et al. - 2007 - A Real-Time Hand Gesture Recognition Method.pdf;/Users/brk/Zotero/storage/4T92MUB7/4284820.html}
}
% == BibTeX quality report for fangRealTimeHandGesture2007:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("IEEE Xplore")

@article{fatmiComparingANNSVM2019,
  title = {Comparing {{ANN}}, {{SVM}}, and {{HMM}} Based {{Machine Learning Methods}} for {{American Sign Language Recognition}} Using {{Wearable Motion Sensors}}},
  author = {Fatmi, Rabeet and Rashad, Sherif and Integlia, Ryan},
  year = {2019},
  month = jan,
  journal = {2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)},
  pages = {0290--0297},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CCWC.2019.8666491},
  urldate = {2023-07-02},
  abstract = {Millions of people with speech and hearing impairments, worldwide, communicate through sign languages every day. In the same way that voice recognition provides a simple communication platform for most users, gesture recognition is a natural means of correspondence for the hearing-impaired. In this paper, we explore the problem of translating/converting sign language to speech, and propose an improved solution using different machine learning techniques. We seek to build a system that can be employed in the daily lives of people with hearing impairments, in order to enhance communication and collaboration between the hearing-impaired community and those untrained in American Sign Language (ASL). The system architecture is based on using wearable motion sensors and machine learning techniques. In this study, we propose a solution using Artificial Neural Networks (ANN) and Support Vector Machines (SVM), and compare their accuracy with the Hidden Markov Model (HMM) results from our previous work to recognize ASL words. Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.},
  isbn = {9781728105543},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,model:ffnn,model:hmm,model:nn,model:svm,pdf:paywalled,tech:imu,type:paper},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/8666491/}}
}
% == BibTeX quality report for fatmiComparingANNSVM2019:
% ? unused Library catalog ("Semantic Scholar")

@article{fei-feiliBayesianHierarchicalModel2005,
  title = {A {{Bayesian Hierarchical Model}} for {{Learning Natural Scene Categories}}},
  author = {{Fei-Fei Li} and Perona, P.},
  year = {2005},
  journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume = {2},
  pages = {524--531},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.16},
  urldate = {2023-07-19},
  abstract = {We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a "theme". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.},
  isbn = {9780769523729},
  keywords = {/unread,model:dense-sampling,untagged},
  annotation = {4029 citations (Semantic Scholar/DOI) [2023-07-19]},
  note = {\url{http://ieeexplore.ieee.org/document/1467486/}}
}
% == BibTeX quality report for fei-feiliBayesianHierarchicalModel2005:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{feinerVisualizingDimensionalVirtual1990,
  title = {Visualizing {\emph{n}} -Dimensional Virtual Worlds with {\emph{n}} -Vision},
  booktitle = {Proceedings of the 1990 Symposium on {{Interactive 3D}} Graphics  - {{SI3D}} '90},
  author = {Feiner, S. K. and Beshers, Clifford},
  year = {1990},
  pages = {37--38},
  publisher = {{ACM Press}},
  address = {{Snowbird, Utah, United States}},
  doi = {10.1145/91385.91412},
  urldate = {2023-07-13},
  abstract = {There are many applications in science, mathematics , statistics, and business, in which it is important to explore an d manipulate clam in more than three dimensions . In thes e applications, data can be defined by points in Euclidean n-space . A point's position is then specified with it coordinates . each o f which determines its position relative to one of It mutuall y perpendicular axes . We describe here research that has as its goa l the development of interaction techniques and metaphors for th e 4D and higher-dimensional worlds that this data represents .},
  isbn = {978-0-89791-351-5},
  langid = {english},
  keywords = {based-on:gloves,hardware:vpl-dataglove,have-read,referenced,tech:flex,type:paper},
  annotation = {133 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=91385.91412}},
  file = {/Users/brk/Zotero/storage/HJJIL3ZW/Feiner and Beshers - 1990 - Visualizing n -dimensional virtual worlds w.pdf}
}
% == BibTeX quality report for feinerVisualizingDimensionalVirtual1990:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 1990 symposium")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{felsBuildingAdaptiveInterfaces1990,
  title = {Building Adaptive Interfaces with Neural Networks: {{The}} Glove-Talk Pilot Study},
  shorttitle = {Building Adaptive Interfaces with Neural Networks},
  booktitle = {{{IFIP TC13 International Conference}} on {{Human-Computer Interaction}}},
  author = {Fels, S. and Hinton, Geoffrey E.},
  year = {1990},
  month = aug,
  urldate = {2023-07-13},
  abstract = {Semantic Scholar extracted view of "Building adaptive interfaces with neural networks: The glove-talk pilot study" by S. Fels et al.},
  keywords = {based-on:gloves,have-read,model:nn,pdf:cant-find,referenced,tech:flex,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Building-adaptive-interfaces-with-neural-networks\%3A-Fels-Hinton/4886fa9820ada07ca905cfd6a9ee973ce070b10b}}
}
% == BibTeX quality report for felsBuildingAdaptiveInterfaces1990:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Semantic Scholar")

@article{felsGloveTalkIIaNeuralnetworkInterface1998,
  title = {Glove-{{TalkII-a}} Neural-Network Interface Which Maps Gestures to Parallel Formant Speech Synthesizer Controls},
  author = {Fels, S.S. and Hinton, G.E.},
  year = {1998},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {1},
  pages = {205--212},
  issn = {10459227},
  doi = {10.1109/72.655042},
  urldate = {2023-07-02},
  abstract = {Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.},
  keywords = {based-on:gloves,hardware:contactglove,hardware:cyberglove,hardware:polhemus,have-read,model:ffnn,model:nn,tech:flex,tech:touch,type:paper},
  annotation = {73 citations (Crossref) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/655042/}},
  file = {/Users/brk/Zotero/storage/4P97DLUY/Fels and Hinton - 1998 - Glove-TalkII-a neural-network interface which maps.pdf}
}
% == BibTeX quality report for felsGloveTalkIIaNeuralnetworkInterface1998:
% ? unused Journal abbreviation ("IEEE Trans. Neural Netw.")
% ? unused Library catalog ("Semantic Scholar")

@article{fengWiMultiThreePhaseSystem2019,
  title = {Wi-{{Multi}}: {{A Three-Phase System}} for {{Multiple Human Activity Recognition With Commercial WiFi Devices}}},
  shorttitle = {Wi-{{Multi}}},
  author = {Feng, Chunhai and Arshad, Sheheryar and Zhou, Siwang and Cao, Dun and Liu, Yonghe},
  year = {2019},
  month = aug,
  journal = {IEEE Internet of Things Journal},
  volume = {6},
  number = {4},
  pages = {7293--7304},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2019.2915989},
  urldate = {2023-07-12},
  abstract = {Channel state information-based activity recognition has gathered immense attention over recent years. Many existing works achieved desirable performance in various applications, including healthcare, security, and Internet of Things, with different machine learning algorithms. However, they usually fail to consider the availability of enough samples to be trained. Besides, many applications only focus on the scenario where only single subject presents. To address these challenges, in this paper, we propose a three-phase system Wi-multi that targets at recognizing multiple human activities in a wireless environment. Different system phases are applied according to the size of available collected samples. Specifically, distance-based classification using dynamic time warping is applied when there are few samples in the profile. Then, support vector machine is employed when representative features can be extracted from training samples. Lastly, recurrent neural networks is exploited when a large number of samples are available. Extensive experiments results show that Wi-multi achieves an accuracy of 96.1\% on average. It is also able to achieve a desirable tradeoff between accuracy and efficiency in different phases.},
  keywords = {based-on:wifi,classes:4,classes:5,model:dynamic-time-warping,tech:wifi,type:paper},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/8710328/}}
}
% == BibTeX quality report for fengWiMultiThreePhaseSystem2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Internet Things J.")
% ? unused Library catalog ("Semantic Scholar")

@misc{fifthdimensiontechnologiesDataGlove52005,
  title = {{{DataGlove5}}},
  author = {{Fifth Dimension Technologies}},
  year = {2005},
  howpublished = {\url{https://5dt.com/5dt-data-glove-ultra/}},
  keywords = {based-on:gloves,from:cite.bib,have-read,type:product}
}

@inproceedings{fisherTelepresenceMasterGlove1987,
  title = {Telepresence {{Master Glove Controller For Dexterous Robotic End-Effectors}}},
  booktitle = {Cambridge {{Symposium}}\_{{Intelligent Robotics Systems}}},
  author = {Fisher, Scott S.},
  editor = {Casasent, David P.},
  year = {1987},
  month = mar,
  pages = {396},
  address = {{Cambridge, MA}},
  doi = {10.1117/12.937753},
  urldate = {2023-07-12},
  abstract = {This paper describes recent research in the Aerospace Human Factors Research Division at NASA's Ames Research Center to develop a glove-like, control and data-recording device (DataGlove) that records and transmits to a host computerin real time, and at appropriate resolution, a numeric data-record of a user's hand/finger shape and dynamics. System configuration and performance specifications are detailed, and current research is discussed investigating its applications in operator control of dexterous robotic end-effectors and for use as a human factors research tool in evaluation of operator hand function requirements and performance in other specialized task environments.},
  keywords = {based-on:gloves,hardware:polhemus,hardware:vpl-dataglove,have-read,tech:flex,tech:optical-tubes,type:seminal},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.937753}},
  file = {/Users/brk/Zotero/storage/WEK8VIH3/Fisher - 1987 - Telepresence Master Glove Controller For Dexterous.pdf}
}
% == BibTeX quality report for fisherTelepresenceMasterGlove1987:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{fisherVirtualEnvironmentDisplay1987,
  title = {Virtual Environment Display System},
  booktitle = {Proceedings of the 1986 Workshop on {{Interactive 3D}} Graphics  - {{SI3D}} '86},
  author = {Fisher, S. S. and McGreevy, M. and Humphries, J. and Robinett, W.},
  year = {1987},
  pages = {77--87},
  publisher = {{ACM Press}},
  address = {{Chapel Hill, North Carolina, United States}},
  doi = {10.1145/319120.319127},
  urldate = {2023-07-13},
  abstract = {A head-mounted, wide-angle, stereoscopic display system controlled by operator position, voice and gesture has been developed for use as a multipurpose interface environment. The system provides a multisensory, interactive display environment in which a user can virtually explore a 360-degree synthesized or remotely sensed environment and can viscerally interact with its components. Primary applications of the system are in telerobotics, management of large-scale integrated information systems, and human factors research. System configuration, application scenarios, and research directions are described.},
  isbn = {978-0-89791-228-0},
  langid = {english},
  keywords = {based-on:gloves,have-read,type:paper},
  annotation = {396 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=319120.319127}}
}
% == BibTeX quality report for fisherVirtualEnvironmentDisplay1987:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 1986 workshop")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{flachPrecisionRecallGainCurvesPR2015,
  title = {Precision-{{Recall-Gain Curves}}: {{PR Analysis Done Right}}},
  shorttitle = {Precision-{{Recall-Gain Curves}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Flach, Peter and Kull, Meelis},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-24},
  keywords = {background},
  note = {\url{https://proceedings.neurips.cc/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html}},
  file = {/Users/brk/Zotero/storage/5B3MNYUA/Flach and Kull - 2015 - Precision-Recall-Gain Curves PR Analysis Done Rig.pdf}
}
% == BibTeX quality report for flachPrecisionRecallGainCurvesPR2015:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Neural Information Processing Systems")

@misc{frankhofmannSensorGloveAnthropomorphicRobot1995,
  title = {{{SensorGlove}}: {{Anthropomorphic Robot Hand}}},
  author = {{Frank Hofmann} and {J\"urgen Henz}},
  year = {1995},
  abstract = {The TU Berlin SensorGlove was developed at the institute for Real-Time Systems and Robotics of the computer science department at the Technical University of Berlin. It was part of the project "Anthropomorphic Robot Hand", in which it was developed in the framework of a student and diploma thesis. It was shown at the Hanover Industrial Fair in 1993, arousing widespread interest. The improved second prototype was presented at the CeBIT 1995. The patented glove is used as a computer input device for human grasping movements and hand gestures. It is equipped with different types of sensors enabling highly precise measurements (cf. Technical Data, Section 5). A closer description follows below.},
  howpublished = {\url{https://pdv.cs.tu-berlin.de/forschung/SensorGlove2_engl.html}},
  keywords = {based-on:gloves,from:cite.bib,hardware,hardware:custom,have-read,tech:capacitive-pressure,tech:flex,type:product}
}
% == BibTeX quality report for frankhofmannSensorGloveAnthropomorphicRobot1995:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{freemanOrientationHistogramsHand1995,
  title = {Orientation {{Histograms}} for {{Hand Gesture Recognition}}},
  author = {Freeman, W. and Roth, Michal},
  year = {1995},
  urldate = {2023-07-03},
  abstract = {We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture class cation and interpolation. This method is simple and fast to compute, and o ers some robustness to scene illumination changes. We have implemented a real-time version, which can distinguish a small vocabulary of about 10 di erent hand gestures. All the computation occurs on a workstation; special hardware is used only to digitize the image. A user can operate a computer graphic crane under hand gesture control, or play a game. We discuss limitations of this method. For moving or \textbackslash dynamic gestures", the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition. Reprinted from: IEEE Intl. Wkshp. on Automatic Face and Gesture Recognition, Zurich, June,},
  keywords = {app:gaming,based-on:vision,classes:10,movement:static,pdf:cant-find,technique:histogram-oriented-gradients,type:seminal},
  note = {\url{https://www.semanticscholar.org/paper/Orientation-Histograms-for-Hand-Gesture-Recognition-Freeman-Roth/2a63c0ae8cb411040a29ad85f2d009a17bf5a9a2}},
  file = {/Users/brk/Zotero/storage/N6NR3A34/Freeman and Roth - 1995 - Orientation Histograms for Hand Gesture Recognitio.pdf}
}
% == BibTeX quality report for freemanOrientationHistogramsHand1995:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{frieslaarRobustSouthAfrican2014,
  title = {Robust {{South African}} Sign Language Gesture Recognition Using Hand Motion and Shape},
  author = {Frieslaar, I.},
  year = {2014},
  urldate = {2023-07-02},
  abstract = {Research has shown that five fundamental parameters are required to recognize any sign language gesture: hand shape, hand motion, hand location, hand orientation and facial expressions. The South African Sign Language (SASL) research group at the University of the Western Cape (UWC) has created several systems to recognize sign language gestures using single parameters. These systems are, however, limited to a vocabulary size of 20 \textendash{} 23 signs, beyond which the recognition accuracy is expected to decrease. The first aim of this research is to investigate the use of two parameters \textendash{} hand motion and hand shape \textendash{} to recognise a larger vocabulary of SASL gestures at a high accuracy. Also, the majority of related work in the field of sign language gesture recognition using these two parameters makes use of Hidden Markov Models (HMMs) to classify gestures. Hidden Markov Support Vector Machines (HM-SVMs) are a relatively new technique that make use of Support Vector Machines (SVMs) to simulate the functions of HMMs. Research indicates that HM-SVMs may perform better than HMMs in some applications. To our knowledge, they have not been applied to the field of sign language gesture recognition. This research compares the use of these two techniques in the context of SASL gesture recognition. The results indicate that, using two parameters results in a 15\% increase in accuracy over the use of a single parameter. Also, it is shown that HM-SVMs are a more accurate technique than HMMs, generally performing better or at least as good as HMMs.},
  keywords = {based-on:vision,classes:23,have-read,model:hm-svm,model:hmm,model:svm,participants:15,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Robust-South-African-sign-language-gesture-using-Frieslaar/af10d1bb6ef67ab9c8b624b5fcafd9dd2e2daf69}},
  file = {/Users/brk/Zotero/storage/U9U3CGTS/Frieslaar - 2014 - Robust South African sign language gesture recogni.pdf}
}
% == BibTeX quality report for frieslaarRobustSouthAfrican2014:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{fukumotoBodyCoupledFingerRing1997,
  title = {``{{Body}} Coupled {{FingerRing}}'': Wireless Wearable Keyboard},
  shorttitle = {``{{Body}} Coupled {{FingerRing}}''},
  author = {Fukumoto, Masaaki and Tonomura, Yoshinobu},
  year = {1997},
  month = mar,
  journal = {Proceedings of the ACM SIGCHI Conference on Human factors in computing systems},
  pages = {147--154},
  publisher = {{ACM}},
  address = {{Atlanta Georgia USA}},
  doi = {10.1145/258549.258636},
  urldate = {2023-07-30},
  abstract = {A really wearable input device ``FingeRing'' is developed for coming wearable PDAs. By attaching ring shaped sensors on each finger, many commands or characters can be input by finger-tip typing action. ``FingeRing'' can be used on any typing surface such as a knee or desk, so quick operation is realized in any situation while standing or walking. To improve wearability, a very small, ultra low power wireless transmitter is developed that uses the human body ss part of an electric circuit. ``Direct Cou-pling'' method enables stable communication even when body contacts any grounded surface. A new symbol coding method that combines order and chord typing is also proposed, and useful typing patterns are chosen by typing speed evaluations. Expert users of musical keyboards can input 52 different symbols at speeds of over 200 symbols per minute by using the combination of FingeRing and the new coding method.},
  isbn = {9780897918022},
  langid = {english},
  keywords = {app:typing,based-on:gloves,classes:52,dataset:custom,fidelity:finger,hardware:custom,have-read,model:band-pass-filter,movement:dynamic,participants:5,tech:accelerometer,type:paper},
  annotation = {209 citations (Semantic Scholar/DOI) [2023-07-30]},
  note = {\url{https://dl.acm.org/doi/10.1145/258549.258636}},
  file = {/Users/brk/Zotero/storage/ECBSDYF3/Fukumoto and Tonomura - 1997 - “Body coupled FingerRing” wireless wearable keybo.pdf}
}
% == BibTeX quality report for fukumotoBodyCoupledFingerRing1997:
% ? unused Conference name ("CHI97: ACM Conference on Human Factors & Computing Systems")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{funkeUsing3DConvolutional2019,
  title = {Using {{3D Convolutional Neural Networks}} to {{Learn Spatiotemporal Features}} for {{Automatic Surgical Gesture Recognition}} in {{Video}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2019},
  author = {Funke, Isabel and Bodenstedt, Sebastian and Oehme, Florian and Von Bechtolsheim, Felix and Weitz, J{\"u}rgen and Speidel, Stefanie},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  volume = {11768},
  pages = {467--475},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32254-0_52},
  urldate = {2023-06-22},
  abstract = {Automatically recognizing surgical gestures is a crucial step towards a thorough understanding of surgical skill. Possible areas of application include automatic skill assessment, intra-operative monitoring of critical surgical steps, and semi-automation of surgical tasks. Solutions that rely only on the laparoscopic video and do not require additional sensor hardware are especially attractive as they can be implemented at low cost in many scenarios. However, surgical gesture recognition based only on video is a challenging problem that requires effective means to extract both visual and temporal information from the video. Previous approaches mainly rely on frame-wise feature extractors, either handcrafted or learned, which fail to capture the dynamics in surgical video. To address this issue, we propose to use a 3D Convolutional Neural Network (CNN) to learn spatiotemporal features from consecutive video frames. We evaluate our approach on recordings of robot-assisted suturing on a bench-top model, which are taken from the publicly available JIGSAWS dataset. Our approach achieves high frame-wise surgical gesture recognition accuracies of more than 84\%, outperforming comparable models that either extract only spatial features or model spatial and low-level temporal information separately. For the first time, these results demonstrate the benefit of spatiotemporal CNNs for video-based surgical gesture recognition.},
  isbn = {978-3-030-32253-3 978-3-030-32254-0},
  langid = {english},
  keywords = {app:medical,app:surgery,based-on:vision,classes:10,have-read,model:cnn,model:nn,participants:8,tech:rgb,type:paper},
  annotation = {61 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-3-030-32254-0_52}},
  file = {/Users/brk/Zotero/storage/2P6JU4JY/Funke et al. - 2019 - Using 3D Convolutional Neural Networks to Learn Sp.pdf}
}
% == BibTeX quality report for funkeUsing3DConvolutional2019:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{galkaInertialMotionSensing2016,
  title = {Inertial {{Motion Sensing Glove}} for {{Sign Language Gesture Acquisition}} and {{Recognition}}},
  author = {Galka, Jakub and Masior, Mariusz and Zaborski, Mateusz and Barczewska, Katarzyna},
  year = {2016},
  month = aug,
  journal = {IEEE Sensors Journal},
  volume = {16},
  number = {16},
  pages = {6310--6316},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2016.2583542},
  urldate = {2023-03-07},
  abstract = {The most popular systems for automatic sign language recognition are based on vision. They are user-friendly, but very sensitive to changes in regard to recording conditions. This paper presents a description of the construction of a more robust system-an accelerometer glove-as well as its application in the recognition of sign language gestures. The basic data regarding inertial motion sensors and the design of the gesture acquisition system as well as project proposals are presented. The evaluation of the solution presents the results of the gesture recognition attempt by using a selected set of sign language gestures with a described method based on Hidden Markov Model (HMM) and parallel HMM approaches. The proposed usage of parallel HMM for sensor-fusion modeling reduced the equal error rate by more than 60\%, while preserving 99.75\% recognition accuracy.},
  keywords = {app:sign-language,based-on:gloves,classes:40,contains-more-references,fidelity:finger,from:cite.bib,hardware:custom,have-read,model:hmm,participants:5,read-priority-1,repetitions:10,segmentation:explicit,tech:accelerometer,type:paper},
  annotation = {81 citations (Crossref) [2023-07-11] 92 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7497574/}},
  file = {/Users/brk/Zotero/storage/AI4LXML7/Galka et al. - 2016 - Inertial Motion Sensing Glove for Sign Language Ge.pdf}
}
% == BibTeX quality report for galkaInertialMotionSensing2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Sensors J.")
% ? unused Library catalog ("Semantic Scholar")

@article{gaoSIGNLANGUAGERECOGNITION2000,
  title = {{{SIGN LANGUAGE RECOGNITION BASED ON HMM}}/{{ANN}}/{{DP}}},
  author = {Gao, Wen and Ma, Jiyong and Wu, Jiangqin and {Wang Chunli}},
  year = {2000},
  month = aug,
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  volume = {14},
  number = {05},
  pages = {587--602},
  issn = {0218-0014, 1793-6381},
  doi = {10.1142/S0218001400000386},
  urldate = {2023-07-02},
  abstract = {In this paper, a system designed for helping the deaf to communicate with others is presented. Some useful new ideas are proposed in design and implementation. An algorithm based on geometrical analysis for the purpose of extracting invariant feature to signer position is presented. An ANN\textendash DP combined approach is employed for segmenting subwords automatically from the data stream of sign signals. To tackle the epenthesis movement problem, a DP-based method has been used to obtain the context-dependent models. Some techniques for system implementation are also given, including fast matching, frame prediction and search algorithms. The implemented system is able to recognize continuous large vocabulary Chinese Sign Language. Experiments show that proposed techniques in this paper are efficient on either recognition speed or recognition performance.},
  langid = {english},
  keywords = {app:sign-language,model:ffnn,model:hmm,model:nn,pdf:paywalled},
  annotation = {99 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.worldscientific.com/doi/abs/10.1142/S0218001400000386}}
}
% == BibTeX quality report for gaoSIGNLANGUAGERECOGNITION2000:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Int. J. Patt. Recogn. Artif. Intell.")
% ? unused Library catalog ("Semantic Scholar")

@misc{garyj.grimesUSPatentDigital1981,
  title = {{{US Patent}} for {{Digital}} Data Entry Glove Interface Device {{Patent}} ({{Patent}} \# 4,414,537 Issued {{November}} 8, 1983) - {{Justia Patents Search}}},
  author = {{Gary J. Grimes}},
  year = {1981},
  urldate = {2023-07-13},
  abstract = {A man-machine interface is disclosed for translating discrete hand positions into electrical signals representing alpha-numeric characters. The interface comprises a glove having sensors positioned with respect to the hand for detecting the flex of finger joints and sensors for detecting the contact between various portions of the hand. Additional sensors detect the movement of the hand with respect to a gravitational vector and a horizontal plane of reference. Further additional sensors detect the twisting and flexing of the wrist. The additional sensors are associated with prescribed mode signals which determine whether subsequently formed or priorly formed character specifying hand positions are to be entered for transmission. The alpha-numeric characters associated with the formed character specifying hand positions are transmitted only when the appropriate mode signal results. The forming and moving of the hand actuates various combinations of sensors so that electrical signals representing the specified characters are generated and transmitted.},
  howpublished = {\url{https://patents.justia.com/patent/4414537}},
  keywords = {app:sign-language,based-on:gloves,have-read,tech:flex,tech:tilt,tech:touch,type:patent},
  file = {/Users/brk/Zotero/storage/J9WZWAX2/US Patent for Digital data entry glove interface d.pdf;/Users/brk/Zotero/storage/FQNIK68R/4414537.html}
}

@misc{geoffreyhintonCourseraNeuralNetworks2012,
  title = {Coursera: {{Neural Networks}} for {{Machine Learning}}},
  author = {{Geoffrey Hinton}},
  year = {2012},
  howpublished = {\url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}},
  keywords = {background,technique:rmsprop}
}
% == BibTeX quality report for geoffreyhintonCourseraNeuralNetworks2012:
% ? Title looks like it was stored in title-case in Zotero

@article{ghotkarDynamicHandGesture2016,
  title = {Dynamic {{Hand Gesture Recognition}} Using {{Hidden Markov Model}} by {{Microsoft Kinect Sensor}}},
  author = {Ghotkar, Archana and Vidap, Pujashree and Deo, Kshitish},
  year = {2016},
  month = sep,
  journal = {International Journal of Computer Applications},
  volume = {150},
  number = {5},
  pages = {5--9},
  issn = {09758887},
  doi = {10.5120/ijca2016911498},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is one of the leading applications of human computer interaction. With diversity of applications of hand gesture recognition, sign language interpretation is the most demanding application. In this paper, dynamic hand gesture recognition for few subset of Indian sign language recognition was considered. The use of depth camera such as Kinect sensor gave skeleton information of signer body. After detailed study of dynamic ISL vocabulary with reference to skeleton joint information, angle has identified as a feature with reference to two moving hand. Here, real time video has been captured and gesture was recognized using Hidden Markov Model (HMM). Ten state HMM model was designed and normalized angle feature of dynamic sign was being observed. Maximum likelihood probability symbol was considered as a recognized gesture. Algorithm has been tested on ISL 20 dynamic signs of total 800 training set of four persons and achieved 89.25\% average accuracy. General Terms Human Computer Interaction, Pattern Recognition, Machine Learning},
  keywords = {app:indian-sl,app:sign-language,based-on:vision,classes:20,fidelity:finger,hardware:kinect,have-read,model:hmm,participants:4,type:paper},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://www.ijcaonline.org/archives/volume150/number5/ghotkar-2016-ijca-911498.pdf}},
  file = {/Users/brk/Zotero/storage/7RH4TGA8/Ghotkar et al. - 2016 - Dynamic Hand Gesture Recognition using Hidden Mark.pdf}
}
% == BibTeX quality report for ghotkarDynamicHandGesture2016:
% ? unused Journal abbreviation ("IJCA")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{gillianGestureRecognitionToolkit2014,
  title = {The Gesture Recognition Toolkit},
  booktitle = {J. {{Mach}}. {{Learn}}. {{Res}}.},
  author = {Gillian, Nicholas Edward and Paradiso, Joseph A.},
  year = {2014},
  keywords = {from:cite.bib,type:software-lib}
}
% == BibTeX quality report for gillianGestureRecognitionToolkit2014:
% ? Unsure about the formatting of the booktitle

@article{guyonChaLearnGestureChallenge2012,
  title = {{{ChaLearn}} Gesture Challenge: {{Design}} and First Results},
  shorttitle = {{{ChaLearn}} Gesture Challenge},
  author = {Guyon, Isabelle and Athitsos, Vassilis and Jangyodsuk, Pat and Hamner, Ben and Escalante, Hugo Jair},
  year = {2012},
  month = jun,
  journal = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Providence, RI, USA}},
  doi = {10.1109/CVPRW.2012.6239178},
  urldate = {2023-08-06},
  abstract = {We organized a challenge on gesture recognition: http://gesture.chalearn.org. We made available a large database of 50,000 hand and arm gestures videorecorded with a Kinect\texttrademark{} camera providing both RGB and depth images. We used the Kaggle platform to automate submissions and entry evaluation. The focus of the challenge is on ``one-shot-learning'', which means training gesture classifiers from a single video clip example of each gesture. The data are split into subtasks, each using a small vocabulary of 8 to 12 gestures, related to a particular application domain: hand signals used by divers, finger codes to represent numerals, signals used by referees, marchalling signals to guide vehicles or aircrafts, etc. We limited the problem to single users for each task and to the recognition of short sequences of gestures punctuated by returning the hands to a resting position. This situation is encountered in computer interface applications, including robotics, education, and gaming. The challenge setting fosters progress in transfer learning by providing for training a large number of sub-tasks related to, but different from the tasks on which the competitors are tested.},
  isbn = {9781467316125 9781467316118 9781467316101},
  keywords = {based-on:vision,classes:100,classes:12,classes:8,dataset:chalearn,fidelity:finger,hardware:kinect,observations:50000,participants:20,tech:rgbd,type:dataset},
  note = {\url{http://ieeexplore.ieee.org/document/6239178/}},
  file = {/Users/brk/Zotero/storage/L4ZKUNKN/Guyon et al. - 2012 - ChaLearn gesture challenge Design and first resul.pdf}
}
% == BibTeX quality report for guyonChaLearnGestureChallenge2012:
% ? unused Conference name ("2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)")
% ? unused Library catalog ("Semantic Scholar")

@article{hahnloserDigitalSelectionAnalogue2000,
  title = {Digital Selection and Analogue Amplification Coexist in a Cortex-Inspired Silicon Circuit},
  author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
  year = {2000},
  month = jun,
  journal = {Nature},
  volume = {405},
  number = {6789},
  pages = {947--951},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/35016072},
  urldate = {2023-07-11},
  abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
  langid = {english},
  keywords = {background,relu},
  annotation = {1114 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://www.nature.com/articles/35016072}}
}
% == BibTeX quality report for hahnloserDigitalSelectionAnalogue2000:
% ? unused Library catalog ("Semantic Scholar")

@article{hakimDynamicHandGesture2019,
  title = {Dynamic {{Hand Gesture Recognition Using 3DCNN}} and {{LSTM}} with {{FSM Context-Aware Model}}},
  author = {Hakim, Noorkholis Luthfil and Shih, Timothy K. and Kasthuri Arachchi, Sandeli Priyanwada and Aditya, Wisnu and Chen, Yi-Cheng and Lin, Chih-Yang},
  year = {2019},
  month = dec,
  journal = {Sensors},
  volume = {19},
  number = {24},
  pages = {5429},
  issn = {1424-8220},
  doi = {10.3390/s19245429},
  urldate = {2023-06-22},
  abstract = {With the recent growth of Smart TV technology, the demand for unique and beneficial applications motivates the study of a unique gesture-based system for a smart TV-like environment. Combining movie recommendation, social media platform, call a friend application, weather updates, chatting app, and tourism platform into a single system regulated by natural-like gesture controller is proposed to allow the ease of use and natural interaction. Gesture recognition problem solving was designed through 24 gestures of 13 static and 11 dynamic gestures that suit to the environment. Dataset of a sequence of RGB and depth images were collected, preprocessed, and trained in the proposed deep learning architecture. Combination of three-dimensional Convolutional Neural Network (3DCNN) followed by Long Short-Term Memory (LSTM) model was used to extract the spatio-temporal features. At the end of the classification, Finite State Machine (FSM) communicates the model to control the class decision results based on application context. The result suggested the combination data of depth and RGB to hold 97.8\% of accuracy rate on eight selected gestures, while the FSM has improved the recognition rate from 89\% to 91\% in a real-time performance.},
  langid = {english},
  keywords = {based-on:vision,classes:24,have-read,model:cnn,model:finite-state-machine,model:lstm,model:nn,movement:dynamic,movement:static,tech:rgbd,type:paper},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/1424-8220/19/24/5429}},
  file = {/Users/brk/Zotero/storage/W6XRMHDD/Hakim et al. - 2019 - Dynamic Hand Gesture Recognition Using 3DCNN and L.pdf}
}
% == BibTeX quality report for hakimDynamicHandGesture2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{halperinToolReleaseGathering2011,
  title = {Tool Release: Gathering 802.11n Traces with Channel State Information},
  shorttitle = {Tool Release},
  author = {Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David},
  year = {2011},
  month = jan,
  journal = {ACM SIGCOMM Computer Communication Review},
  volume = {41},
  number = {1},
  pages = {53--53},
  issn = {0146-4833},
  doi = {10.1145/1925861.1925870},
  urldate = {2023-07-12},
  abstract = {We are pleased to announce the release of a tool that records detailed measurements of the wireless channel along with received 802.11 packet traces. It runs on a commodity 802.11n NIC, and records Channel State Information (CSI) based on the 802.11 standard. Unlike Receive Signal Strength Indicator (RSSI) values, which merely capture the total power received at the listener, the CSI contains information about the channel between sender and receiver at the level of individual data subcarriers, for each pair of transmit and receive antennas.             Our toolkit uses the Intel WiFi Link 5300 wireless NIC with 3 antennas. It works on up-to-date Linux operating systems: in our testbed we use Ubuntu 10.04 LTS with the 2.6.36 kernel. The measurement setup comprises our customized versions of Intel's close-source firmware and open-source iwlwifi wireless driver, userspace tools to enable these measurements, access point functionality for controlling both ends of the link, and Matlab (or Octave) scripts for data analysis. We are releasing the binary of the modified firmware, and the source code to all the other components.},
  langid = {english},
  keywords = {based-on:wifi,tech:wifi,type:seminal},
  annotation = {1239 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/1925861.1925870}},
  file = {/Users/brk/Zotero/storage/XSXCNE49/Halperin et al. - 2011 - Tool release gathering 802.11n traces with channe.pdf}
}
% == BibTeX quality report for halperinToolReleaseGathering2011:
% ? unused Journal abbreviation ("SIGCOMM Comput. Commun. Rev.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{hamdyaliComparativeStudyUser2014,
  title = {A {{Comparative Study}} of {{User Dependent}} and {{Independent Accelerometer-Based Gesture Recognition Algorithms}}},
  booktitle = {Distributed, {{Ambient}}, and {{Pervasive Interactions}}},
  author = {Hamdy Ali, Aya and Atia, Ayman and Sami, Mostafa},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Streitz, Norbert and Markopoulos, Panos},
  year = {2014},
  volume = {8530},
  pages = {119--129},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-07788-8_12},
  urldate = {2023-03-07},
  abstract = {In this paper, we introduce an evaluation of accelerometer-based gesture recognition algorithms in user dependent and independent cases. Gesture recognition has many algorithms and this evaluation includes Hidden Markov Models, Support Vector Machine, K-nearest neighbor, Artificial Neural Net-work and Dynamic Time Warping. Recognition results are based on acceleration data collected from 12 users. We evaluated the algorithms based on the recognition accuracy related to different number of gestures from two datasets. Evaluation results show that the best accuracy for 8 and 18 gestures is achieved with dynamic time warping and K-nearest neighbor algorithms.},
  isbn = {978-3-319-07787-1 978-3-319-07788-8},
  langid = {english},
  keywords = {based-on:gloves,classes:18,classes:8,fidelity:arm,hardware:wiimote,have-read,model:dynamic-time-warping,model:ffnn,model:hmm,model:knn,model:nn,model:svm,participants:12,repetitions:10,repetitions:25,tech:accelerometer,type:paper,unimpressive},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-3-319-07788-8_12}},
  file = {/Users/brk/Zotero/storage/CXIM8DZI/Hamdy Ali et al. - 2014 - A Comparative Study of User Dependent and Independ.pdf}
}
% == BibTeX quality report for hamdyaliComparativeStudyUser2014:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{haoWiSLContactlessFineGrained2020,
  title = {Wi-{{SL}}: {{Contactless Fine-Grained Gesture Recognition Uses Channel State Information}}},
  shorttitle = {Wi-{{SL}}},
  author = {Hao, Zhanjun and Duan, Yu and Dang, Xiaochao and Liu, Yang and Zhang, Daiyang},
  year = {2020},
  month = jul,
  journal = {Sensors},
  volume = {20},
  number = {14},
  pages = {4025},
  issn = {1424-8220},
  doi = {10.3390/s20144025},
  urldate = {2023-07-12},
  abstract = {In recent years, with the development of wireless sensing technology and the widespread popularity of WiFi devices, human perception based on WiFi has become possible, and gesture recognition has become an active topic in the field of human-computer interaction. As a kind of gesture, sign language is widely used in life. The establishment of an effective sign language recognition system can help people with aphasia and hearing impairment to better interact with the computer and facilitate their daily life. For this reason, this paper proposes a contactless fine-grained gesture recognition method using Channel State Information (CSI), namely Wi-SL. This method uses a commercial WiFi device to establish the correlation mapping between the amplitude and phase difference information of the subcarrier level in the wireless signal and the sign language action, without requiring the user to wear any device. We combine an efficient denoising method to filter environmental interference with an effective selection of optimal subcarriers to reduce the computational cost of the system. We also use K-means combined with a Bagging algorithm to optimize the Support Vector Machine (SVM) classification (KSB) model to enhance the classification of sign language action data. We implemented the algorithms and evaluated them for three different scenarios. The experimental results show that the average accuracy of Wi-SL gesture recognition can reach 95.8\%, which realizes device-free, non-invasive, high-precision sign language gesture recognition.},
  langid = {english},
  keywords = {app:sign-language,based-on:wifi,model:k-means,model:svm,tech:wifi,type:paper},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://www.mdpi.com/1424-8220/20/14/4025}},
  file = {/Users/brk/Zotero/storage/IM4P4L7G/Hao et al. - 2020 - Wi-SL Contactless Fine-Grained Gesture Recognitio.pdf}
}
% == BibTeX quality report for haoWiSLContactlessFineGrained2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{harrisArrayProgrammingNumPy2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del R{\'i}o, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/s41586-020-2649-2},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {7070 citations (Crossref) [2023-07-11]},
  note = {\url{https://doi.org/10.1038/s41586-020-2649-2}}
}

@article{harrisonOmniTouchWearableMultitouch2011,
  title = {{{OmniTouch}}: Wearable Multitouch Interaction Everywhere},
  shorttitle = {{{OmniTouch}}},
  author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
  year = {2011},
  month = oct,
  journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
  pages = {441--450},
  publisher = {{ACM}},
  address = {{Santa Barbara California USA}},
  doi = {10.1145/2047196.2047255},
  urldate = {2023-06-23},
  abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
  isbn = {9781450307161},
  langid = {english},
  keywords = {based-on:vision,hardware:custom,have-read,tech:rgb,type:paper},
  annotation = {587 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/2047196.2047255}},
  file = {/Users/brk/Zotero/storage/H768WQNS/Harrison et al. - 2011 - OmniTouch wearable multitouch interaction everywh.pdf}
}
% == BibTeX quality report for harrisonOmniTouchWearableMultitouch2011:
% ? unused Conference name ("UIST '11: The 24th Annual ACM Symposium on User Interface Software and Technology")
% ? unused Library catalog ("Semantic Scholar")

@article{harshithSurveyVariousGesture2010,
  title = {Survey on {{Various Gesture Recognition Techniques}} for {{Interfacing Machines Based}} on {{Ambient Intelligence}}},
  author = {Harshith, C and Shastry, Karthik R. and Ravindran, Manoj and Srikanth, M.V.V.N.S and Lakshmikhanth, Naveen},
  year = {2010},
  month = nov,
  journal = {International Journal of Computer Science \& Engineering Survey},
  volume = {1},
  number = {2},
  pages = {31--42},
  issn = {09763252},
  doi = {10.5121/ijcses.2010.1203},
  urldate = {2023-06-22},
  abstract = {Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions. Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods, appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.},
  keywords = {based-on:gloves,reference:doi.org/10.1007/11427445\_16,reference:doi.org/10.1007/978-3-540-75175-5\_101,reference:doi.org/10.1007/978-3-642-04292-8\_44,reference:doi.org/10.1007/BFb0028333,reference:doi.org/10.1016/S0262-8856(03)00070-2,reference:doi.org/10.1093/ACPROF:OSO/9780199684359.003.0003,reference:doi.org/10.1109/CVPRW.2008.4563169,reference:doi.org/10.1109/icss.2013.40,type:survey},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1007/11427445\_16","10.1007/BFb0028333","10.1016/S0262-8856(03)00070-2","10.1007/978-3-642-04292-8\_44","10.1007/978-3-540-75175-5\_101","10.1109/CVPRW.2008.4563169","10.1093/ACPROF:OSO/9780199684359.003.0003","10.1109/icss.2013.40","10.1007/11427445\_16","10.1016/S0262-8856(03)00070-2","10.1007/978-3-642-04292-8\_44","10.1007/BFb0028333","10.1007/978-3-540-75175-5\_101","10.1109/CVPRW.2008.4563169","10.1109/icss.2013.40","10.1093/ACPROF:OSO/9780199684359.003.0003","10.1109/CVPRW.2008.4563169","10.1109/icss.2013.40","10.1093/ACPROF:OSO/9780199684359.003.0003"] ---},
  note = {\url{http://www.airccse.org/journal/ijcses/papers/1110ijcses03.pdf}},
  file = {/Users/brk/Zotero/storage/UDQVJPM7/Harshith et al. - 2010 - Survey on Various Gesture Recognition Techniques f.pdf}
}
% == BibTeX quality report for harshithSurveyVariousGesture2010:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJCSES")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{hassanpourVisionBasedHand2008,
  title = {{{Vision}}\-{{Based Hand Gesture Recognition}} for {{Human Computer Interaction}}: {{A Review}}},
  shorttitle = {{{Vision}}\-{{Based Hand Gesture Recognition}} for {{Human Computer Interaction}}},
  author = {Hassanpour, R. and Wong, Stephan and Shahbahrami, A. and Wong, J.},
  year = {2008},
  urldate = {2023-07-11},
  abstract = {Evolution of user interfaces shapes the change in the human\-computer interaction. With the rapid emergence of three\-dimensional (3\-D) applications; the need for a new type of interaction device arises as traditional devices such as mouse, keyboard, and joystick become inefficient and cumbersome within these~virtual environments. Intuitive and naturalness characteristics of ``Ha nd Gestures'' in human computer interaction have been the driving force and motivation to develop an interaction device which can replace current unwieldy tools. This study is a survey on the methods of analyzing, modeling and recognizing hand gestures in the context of human\- computer interaction. Taxonomy of the methods based on the applications that they have been developed for and the approaches that they have used to represent gestures is presented. Direction of future developments is also discussed.},
  keywords = {based-on:vision,have-read,tech:rgb,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/Vision\%C2\%ADBased-Hand-Gesture-Recognition-for-Human-A-Hassanpour-Wong/1285c27b3a3b2304274b88676fc20086bf5cc3dd}},
  file = {/Users/brk/Zotero/storage/79Q9UGWM/Hassanpour et al. - 2008 - Vision­Based Hand Gesture Recognition for Human Co.pdf}
}
% == BibTeX quality report for hassanpourVisionBasedHand2008:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{hernandez-rebollarAcceleGloveWholehandInput2002,
  title = {The {{AcceleGlove}}: A Whole-Hand Input Device for Virtual Reality},
  shorttitle = {The {{AcceleGlove}}},
  booktitle = {{{ACM SIGGRAPH}} 2002 Conference Abstracts and Applications},
  author = {{Hernandez-Rebollar}, Jose L. and Kyriakopoulos, Nicholas and Lindeman, Robert W.},
  year = {2002},
  month = jul,
  pages = {259--259},
  publisher = {{ACM}},
  address = {{San Antonio Texas}},
  doi = {10.1145/1242073.1242272},
  urldate = {2023-07-02},
  abstract = {We present The AcceleGlove, a novel whole-hand input device to manipulate three different virtual objects: a virtual hand, icons on a virtual desktop and a virtual keyboard using the 26 postures of the American Sign Language (ASL) alphabet.},
  isbn = {978-1-58113-525-1},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,classes:26,hardware:custom,have-read,movement:static,tech:accelerometer,type:paper},
  annotation = {75 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/1242073.1242272}},
  file = {/Users/brk/Zotero/storage/GPITAK45/Hernandez-Rebollar et al. - 2002 - The AcceleGlove a whole-hand input device for vir.pdf}
}
% == BibTeX quality report for hernandez-rebollarAcceleGloveWholehandInput2002:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("SIGGRAPH02: The 29th International Conference on Computer Graphics and Interactive Techniques")
% ? unused Library catalog ("Semantic Scholar")

@article{heumerGraspRecognitionUncalibrated2007,
  title = {Grasp {{Recognition}} with {{Uncalibrated Data Gloves}} - {{A Comparison}} of {{Classification Methods}}},
  author = {Heumer, Guido and Amor, Heni Ben and Weber, Matthias and Jung, Bernhard},
  year = {2007},
  month = mar,
  journal = {2007 IEEE Virtual Reality Conference},
  pages = {19--26},
  publisher = {{IEEE}},
  address = {{Charlotte, NC, USA}},
  doi = {10.1109/VR.2007.352459},
  urldate = {2023-07-11},
  abstract = {This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out, where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was analyzed with 28 classifiers including different types of neural networks, decision trees, Bayes nets, and lazy learners. Each classifier was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.},
  isbn = {9781424409051},
  keywords = {based-on:gloves,fidelity:finger,hardware:cyberglove-ii,have-read,model:decision-tree,model:nn,tech:accelerometer,tech:flex,type:paper},
  annotation = {64 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://ieeexplore.ieee.org/document/4161001/}},
  file = {/Users/brk/Zotero/storage/3422NZ4Q/Heumer et al. - 2007 - Grasp Recognition with Uncalibrated Data Gloves - .pdf}
}
% == BibTeX quality report for heumerGraspRecognitionUncalibrated2007:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{heWiGWiFiBasedGesture2015,
  title = {{{WiG}}: {{WiFi-Based Gesture Recognition System}}},
  shorttitle = {{{WiG}}},
  author = {He, Wenfeng and Wu, Kaishun and Zou, Yongpan and Ming, Zhong},
  year = {2015},
  month = aug,
  journal = {2015 24th International Conference on Computer Communication and Networks (ICCCN)},
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/ICCCN.2015.7288485},
  urldate = {2023-06-22},
  abstract = {Most recently, gesture recognition has increasingly attracted intense academic and industrial interest due to its various applications in daily life, such as home automation, mobile games. Present approaches for gesture recognition, mainly including vision-based, sensor-based and RF-based, all have certain limitations which hinder their practical use in some scenarios. For example, the vision-based approaches fail to work well in poor light conditions and the sensor-based ones require users to wear devices. To address these, we propose WiG in this paper, a device-free gesture recognition system based solely on Commercial Off-The-Shelf (COTS) WiFi infrastructures and devices. Compared with existing Radio Frequency (RF)-based systems, WiG stands out for its systematic simplicity, extremely low cost and high practicability. We implemented WiG in indoor environment and conducted experiments to evaluate its performance in two typical scenarios. The results demonstrate that WiG can achieve an average recognition accuracy of 92\% in line-of-sight scenario and average accuracy of 88\% in the none-line-of sight scenario.},
  isbn = {9781479999644},
  keywords = {based-on:wifi,tech:wifi,type:paper},
  annotation = {132 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7288485/}}
}
% == BibTeX quality report for heWiGWiFiBasedGesture2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@misc{hirokazokatoARToolKit1999,
  title = {{{ARToolKit}}},
  author = {{Hirokazo Kato}},
  year = {1999},
  howpublished = {\url{https://www.hitl.washington.edu/artoolkit/documentation/}},
  keywords = {/unread}
}

@inproceedings{hochreiterGradientFlowRecurrent2001,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: The {{Difficulty}} of {{Learning Long-Term Dependencies}}},
  shorttitle = {Gradient {{Flow}} in {{Recurrent Nets}}},
  author = {Hochreiter, S. and Bengio, Yoshua},
  year = {2001},
  urldate = {2023-07-11},
  abstract = {D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\textbackslash M[ X N@]\_\^O\textbackslash `JaNcb V RcQ W d EGKeL(\^(QgfhKeLOE?i)\^(QSj ETNPfPQkRl[ V R)m"[ X \^(KeLOEG\^ npo qarpo m"[ X \^(KeLOEG\^tsAu EGNPb V \^ v wyx zlwO\{(|(\}{$<\sptilde$}OC\vphantom\{\}((xp\{ay.\textasciitilde A\}\_\textasciitilde{} Cl3\#|{$<$}Azw\#|l6 (|  JpfhL XV EG\^O QgJ  ETFOR] \^O\textbackslash JNPb V RcQ X E)ETR 6EGKeLOETNcKMLOE F ETN V RcQgJp\^(\^OE ZgZ E i \^(Qkj EGNPfhQSRO E OE2m1Jp\^ RcNY E VZ sO! \textexclamdown{} q.n sCD X KGKa8\textcent EG\^ RPNhE\textcurrency\textsterling{} \textyen\textbrokenbar Q ZgZ Es m\textsection J\^ RPNO E VZ s({\" } X  EG\textcopyright\#EKas\# V \^ V  V s(H a \guillemotleft a{$\lnot$}3\- \textregistered\#|.Y{\= }y\vphantom\{\} xa\textdegree OC\vphantom\{\}l\{x  yxlY\textasciitilde 3\{|  {$\pm$}2Pz   V J Z J U N V fhKTJp\^(Q  ETFOR J\textbackslash{} D vYf3RPEGb{\' }f V \^(\textsection JpbF X RPETN@D KTQEG\^(KTE i \^(QSjpEGNPfhQSR4v{$\mu$}J\textbackslash{} U\textparagraph Z JaNPEG\^(K{$\cdot$}E jYQ V (Q{\c  }D V \^ R V m V N3R V aOs\#1 o \textexclamdown Ga r U QNhE\^OoTE1{$\fracslash$}4\guillemotright,] R VZ vC1{$\fracslash$}2 3{$\fracslash$}4  x {$\pm$} x  \#\textquestiondown{} \}\`A 3t\}lC\vphantom\{\}2P\vphantom\{\}{$<\sptilde$} {$\lnot$}t[ X NPE\^\textsection D KeL(b{\' }Qg(L X \textcopyright yETN ]  DY]\_\'A JNPfhJ\~A\^A Z j EToQ V a rpopo2\"A X  V \^(J(sCD \AA )QSRPoTEGN ZgV \^( \AE{} \#|\{3{\= }|.(C\}.C\textquestiondown Y\vphantom\{\}p Pzw},
  keywords = {background,vanishing-gradient},
  note = {\url{https://www.semanticscholar.org/paper/Gradient-Flow-in-Recurrent-Nets\%3A-the-Difficulty-of-Hochreiter-Bengio/aed054834e2c696807cc8b227ac7a4197196e211}}
}
% == BibTeX quality report for hochreiterGradientFlowRecurrent2001:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2023-11-12},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english},
  keywords = {/unread},
  annotation = {9996 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://direct.mit.edu/neco/article/9/8/1735-1780/6109}}
}
% == BibTeX quality report for hochreiterLongShortTermMemory1997:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{hofmannVelocityProfileBased1998,
  title = {Velocity Profile Based Recognition of Dynamic Gestures with Discrete {{Hidden Markov Models}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {Hofmann, Frank G. and Heyer, Peter and Hommel, G{\"u}nter},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Fr{\"o}hlich, Martin},
  year = {1998},
  volume = {1371},
  pages = {81--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052991},
  urldate = {2023-07-12},
  abstract = {In this paper we present a method for the recognition of dynamic gestures with discrete Hidden Markov Models (HMMs) from a continuous stream of gesture input data. The segmentation problem is addressed by extracting two velocity profiles from the gesture data and using their extrema as segmentation cues. Gestures are captured with a TUB-SensorGlove. The paper focuses on the description of the gesture recognition method (including data preprocessing) and describes experiments for the evaluation of the performance of the recognition method. The paper combines and further develops ideas from some of our previous work.},
  isbn = {978-3-540-64424-8 978-3-540-69782-4},
  keywords = {based-on:gloves,fidelity:hand,from:cite.bib,hardware:tub-sensorglove,have-read,model:adaboost,model:hmm,pdf:paywalled,tech:rgb,technique:histogram-oriented-gradients,type:paper},
  annotation = {108 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://link.springer.com/10.1007/BFb0052991}}
}
% == BibTeX quality report for hofmannVelocityProfileBased1998:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{hongCalibratingVPLDataGlove1989,
  title = {Calibrating a {{VPL DataGlove}} for Teleoperating the {{Utah}}/{{MIT}} Hand},
  author = {Hong, J. and Tan, X.},
  year = {1989},
  journal = {Proceedings, 1989 International Conference on Robotics and Automation},
  pages = {1752--1757},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Scottsdale, AZ, USA}},
  doi = {10.1109/ROBOT.1989.100228},
  urldate = {2023-07-13},
  abstract = {A system able to control the Utah/MIT hand with the VPL DataGlove has been developed. To get the actual joint angles from the DataGlove sensor values, a least-squares fit is used to find the best-fit exponential curve for each sensor, and then the correlation between the sensors is reduced by the iterative correlation elimination procedure. The calibration depends both on the wearer and the particular DataGlove being used. The first level calibration is simple and can be done in under 15 minutes with a little experience. The second level is fixed and requires no adjustments. To control the hand, a mapping from the DataGlove angles to the hand angles is applied, making the hand fingertips follow the DataGlove fingertips. The hand can successfully implement various high-level tasks under the DataGlove wearer's control.{$<<$}ETX{$>>$}},
  isbn = {9780818619380},
  keywords = {based-on:gloves,fidelity:finger,hardware:dexterous-handmaster,hardware:vpl-dataglove,have-read,pdf:paywalled,type:paper},
  annotation = {74 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://ieeexplore.ieee.org/document/100228/}}
}
% == BibTeX quality report for hongCalibratingVPLDataGlove1989:
% ? unused Library catalog ("Semantic Scholar")

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2023-05-30},
  abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
  langid = {english},
  keywords = {background,model:ffnn,model:nn,universal-approximators},
  annotation = {9990 citations (Semantic Scholar/DOI) [2023-05-30]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/0893608089900208}}
}
% == BibTeX quality report for hornikMultilayerFeedforwardNetworks1989:
% ? unused Library catalog ("Semantic Scholar")

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {17522 citations (Crossref) [2023-07-11]}
}

@article{hurrooSignLanguageRecognition2020,
  title = {Sign {{Language Recognition System}} Using {{Convolutional Neural Network}} and {{Computer Vision}}},
  author = {Hurroo, Mehreen and Elham, M.},
  year = {2020},
  month = dec,
  journal = {International journal of engineering research and technology},
  urldate = {2023-07-11},
  abstract = {Conversing to a person with hearing disability is always a major challenge. Sign language has indelibly become the ultimate panacea and is a very powerful tool for individuals with hearing and speech disability to communicate their feelings and opinions to the world. It makes the integration process between them and others smooth and less complex. However, the invention of sign language alone, is not enough . There are many strings attached to this boon.The sign gestures often get mixed and confused for someone who has never learnt it or knows it in a different language. However, this communication gap which has existed for years can now be narrowed with the introduction of various techniques to automate the detection of sign gestures . In this paper, we introduce a Sign Language recognition using American Sign Language. In this study, the user must be able to capture images of the hand gesture using web camera and the system shall predict and display the name of the captured image. We use the HSV colour algorithm to detect the hand gesture and set the background to black. The images undergo a series of processing steps which include various Computer vision techniques such as the conversion to grayscale, dilation and mask operation. And the region of interest which, in our case is the hand gesture is segmented. The features extracted are the binary pixels of the images. We make use of Convolutional Neural Network(CNN) for training and to classify the images. We are able to recognise 10 American Sign gesture alphabets with high accuracy. Our model has achieved a remarkable accuracy of above 90\%.},
  keywords = {app:american-sl,app:sign-language,based-on:vision,claims-to-be-best,classes:10,exclude:poor-quality,from:cite.bib,model:cnn,model:nn,movement:static,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Sign-Language-Recognition-System-using-Neural-and-Hurroo-Elham/5ecc9d2073755a1364433d83df2a4007e42b16e1}},
  file = {/Users/brk/Zotero/storage/IQGA9ZVK/Hurroo and Elham - 2020 - Sign Language Recognition System using Convolution.pdf}
}
% == BibTeX quality report for hurrooSignLanguageRecognition2020:
% ? unused Library catalog ("Semantic Scholar")

@article{hussainReviewCategorizationTechniques2020,
  title = {A Review and Categorization of Techniques on Device-Free Human Activity Recognition},
  author = {Hussain, Zawar and Sheng, Quan Z. and Zhang, Wei Emma},
  year = {2020},
  month = oct,
  journal = {Journal of Network and Computer Applications},
  volume = {167},
  pages = {102738},
  issn = {10848045},
  doi = {10.1016/j.jnca.2020.102738},
  urldate = {2023-07-12},
  abstract = {Human activity recognition has gained importance in recent years due to its applications in various fields such as health, security and surveillance, entertainment, and intelligent environments. A significant amount of work has been done on human activity recognition and researchers have leveraged different approaches, such as wearable, object-tagged, and devicefree, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the period 2010-2018 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything, instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the field of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interactionbased. We further divide these areas into ten different sub-topics and present the latest research work in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these sub-areas. Specifically, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to give the reader, a complete overview of the state-of-the-art techniques and trends in different sub-areas of human activity recognition. In the end, we discuss open research issues and provide future research directions in the field of human activity recognition.},
  langid = {english},
  keywords = {based-on:wifi,read-priority-1,survey-finsh:2018,survey-start:2010,tech:wifi,type:survey},
  annotation = {57 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1084804520302125}},
  file = {/Users/brk/Zotero/storage/5M8W2RNX/Hussain et al. - 2020 - A review and categorization of techniques on devic.pdf}
}
% == BibTeX quality report for hussainReviewCategorizationTechniques2020:
% ? unused Library catalog ("Semantic Scholar")

@article{hyeon-kyuleeHMMbasedThresholdModel1999,
  title = {An {{HMM-based}} Threshold Model Approach for Gesture Recognition},
  author = {{Hyeon-Kyu Lee} and Kim, J.H.},
  year = {Oct./1999},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {10},
  pages = {961--973},
  issn = {01628828},
  doi = {10.1109/34.799904},
  urldate = {2023-07-12},
  abstract = {A new method is developed using the hidden Markov model (HMM) based technique. To handle nongesture patterns, we introduce the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures in the sense that its likelihood is smaller than that of the dedicated gesture model for a given gesture. Consequently, the likelihood can be used as an adaptive threshold for selecting proper gesture model. It has, however, a large number of states and needs to be reduced because the threshold model is constructed by collecting the states of all gesture models in the system. To overcome this problem, the states with similar probability distributions are merged, utilizing the relative entropy measure. Experimental results show that the proposed method can successfully extract trained gestures from continuous hand motion with 93.14\% reliability.},
  keywords = {based-on:vision,claims-to-be-best,from:cite.bib,model:hmm,movement:dynamic,pdf:paywalled,type:paper},
  annotation = {718 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/799904/}}
}
% == BibTeX quality report for hyeon-kyuleeHMMbasedThresholdModel1999:
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Machine Intell.")
% ? unused Library catalog ("Semantic Scholar")

@misc{immersioncorporationCyberGlove2001,
  title = {Cyber {{Glove}}},
  author = {{Immersion Corporation}},
  year = {2001},
  abstract = {Immersion 3D's award-winning instrumented glove CyberGlove\textregistered{} The CyberGlove\textregistered{} is a fully instrumented glove that provides up to 22 high-accuracy joint-angle measurements. It uses proprietary resistive bend-sensing technology to accurately transform hand and finger motions into real-time digital joint-angle data. Our VirtualHand\textregistered{} Studio software converts the data into a graphical hand which mirrors the subtle movements of the physical hand. It is available in two models and for either hand.},
  howpublished = {\url{https://web.archive.org/web/20011005155130/https://www.immersion.com/products/3d/interaction/cyberglove.shtml}},
  keywords = {based-on:gloves,hardware:cyberglove,have-read,tech:flex,type:product}
}
% == BibTeX quality report for immersioncorporationCyberGlove2001:
% ? Title looks like it was stored in title-case in Zotero

@misc{immersionincImmersionIncCyberGlove2005,
  title = {Immersion {{Inc CyberGlove II}}},
  author = {{Immersion Inc}},
  year = {2005},
  howpublished = {\url{https://www.immersion.fr/en/cyberglove-ii/}},
  keywords = {based-on:gloves,from:cite.bib,have-read,type:product}
}
% == BibTeX quality report for immersionincImmersionIncCyberGlove2005:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  urldate = {2023-07-11},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  keywords = {background,batch-normalization},
  note = {\url{https://www.semanticscholar.org/paper/Batch-Normalization\%3A-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3}},
  file = {/Users/brk/Zotero/storage/ST7LJZZJ/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf}
}
% == BibTeX quality report for ioffeBatchNormalizationAccelerating2015:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{izotovRecognitionHandwrittenMNIST2021,
  title = {Recognition of Handwritten {{MNIST}} Digits on Low-Memory 2 {{Kb RAM Arduino}} Board Using {{LogNNet}} Reservoir Neural Network},
  author = {Izotov, Y A and Velichko, A A and Ivshin, A A and Novitskiy, R E},
  year = {2021},
  month = jun,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {1155},
  number = {1},
  pages = {012056},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/1155/1/012056},
  urldate = {2023-07-11},
  abstract = {Abstract             The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82\%. The algorithm was tested on a low-memory Arduino board with 2 Kb static RAM low-power microcontroller. The dependences of the accuracy and time of image recognition on the number of neurons in the reservoir have been investigated. The memory allocation demonstrates that the algorithm stores all the necessary information in RAM without using additional data storage, and operates with original images without preliminary processing. The simple structure of the algorithm, with appropriate training, can be adapted for wide practical application, for example, for creating mobile biosensors for early diagnosis of adverse events in medicine. The study results are important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing.},
  keywords = {app:resource-constrained,background,dataset:mnist,from:cite.bib,hardware:arduino,hardware:iot,model:nn},
  annotation = {5 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://iopscience.iop.org/article/10.1088/1757-899X/1155/1/012056}},
  file = {/Users/brk/Zotero/storage/ZZ94MAW3/Izotov et al. - 2021 - Recognition of handwritten MNIST digits on low-mem.pdf}
}
% == BibTeX quality report for izotovRecognitionHandwrittenMNIST2021:
% ? unused Journal abbreviation ("IOP Conf. Ser.: Mater. Sci. Eng.")
% ? unused Library catalog ("Semantic Scholar")

@article{jacobsenDesignUtahDextrous1986,
  title = {Design of the {{Utah}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}},
  author = {Jacobsen, S. and Iversen, E. and Knutti, D. and Johnson, R. and Biggers, K.},
  year = {1986},
  journal = {Proceedings. 1986 IEEE International Conference on Robotics and Automation},
  volume = {3},
  pages = {1520--1532},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/ROBOT.1986.1087395},
  urldate = {2023-07-13},
  abstract = {The Center for Engineering Design at the University of Utah, and the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology have developed a robotic end effector intended to function as a general purpose research tool for the study of machine dexterity. The high performance, multifingered hand will provide two important capabilities. First, it will permit the experimental investigation of basic concepts in manipulation theory, control system design and tactile sensing. Second, it will expand understanding required for the future design of physical machinery and will serve as a "test bed" for the development of tactile sensing systems. The paper includes: 1) a discussion of issues important to the development of manipulation machines; 2) general comments regarding design of the Utah/M.I.T. Dextrous Hand; and, 3) a detailed discussion of specific subsystems of the hand.},
  keywords = {based-on:gloves,fidelity:finger,hardware:dexterous-handmaster,have-read,tech:hall-effect,type:paper},
  annotation = {557 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://ieeexplore.ieee.org/document/1087395/}},
  file = {/Users/brk/Zotero/storage/7U79YIGP/Jacobsen et al. - DESIGN OF THE UTAHh4.I.T. DEXTROUSHAND.pdf}
}
% == BibTeX quality report for jacobsenDesignUtahDextrous1986:
% ? Possibly abbreviated journal title Proceedings. 1986 IEEE International Conference on Robotics and Automation
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("1986 IEEE International Conference on Robotics and Automation")
% ? unused Library catalog ("Semantic Scholar")

@article{jacobsenUTAHDextrousHand1984,
  title = {The {{UTAH}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}: {{Work}} in {{Progress}}},
  shorttitle = {The {{UTAH}}/{{M}}.{{I}}.{{T}}. {{Dextrous Hand}}},
  author = {Jacobsen, S.C. and {J.E. Wood} and {D.F. Knutti} and Biggers, K.B.},
  year = {1984},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {3},
  number = {4},
  pages = {21--50},
  issn = {0278-3649},
  doi = {10.1177/027836498400300402},
  urldate = {2023-07-13},
  abstract = {The Center for Biomedical Design at the University of Utah and the Artificial Intelligence Laboratory at the Massachu setts Institute of Technology are developing a tendon-oper ated multiple-degree-of-freedom dextrous hand (DH) with multichannel touch-sensing capability. Our goal is the design and fabrication of a high-performance yet well-behaved sys tem that is fast and stable and that includes considerable operational flexibility as a research tool. The paper reviews progress to date on project subtasks and discusses design issues important to hardware and control systems develop ment in terms of (1) structures that contain tendons, actua tors, joints, and sensors; (2) both pneumatic and electric tendon actuation systems; (3) optically based sensors that detect touch; (4) subcontrol systems that provide internal management of the DH; and (5) preliminary higher control systems that supervise general operation of the hand during execution of tasks and that provide integration of vision and tactile information.},
  langid = {english},
  keywords = {based-on:gloves,fidelity:finger,hardware:dexterous-handmaster,have-read,tech:hall-effect,type:paper},
  annotation = {630 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://journals.sagepub.com/doi/10.1177/027836498400300402}}
}
% == BibTeX quality report for jacobsenUTAHDextrousHand1984:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")

@article{jenningsComputergraphicModelingAnalysis1988,
  title = {Computergraphic Modeling and Analysis {{II}}: Three-Dimensional Reconstruction and Interactive Analysis},
  shorttitle = {Computergraphic Modeling and Analysis {{II}}},
  author = {Jennings, Peggy J. and Poizner, Howard},
  year = {1988},
  month = may,
  journal = {Journal of Neuroscience Methods},
  volume = {24},
  number = {1},
  pages = {45--55},
  issn = {01650270},
  doi = {10.1016/0165-0270(88)90032-5},
  urldate = {2023-07-18},
  abstract = {Semantic Scholar extracted view of "Computergraphic modeling and analysis II: three-dimensional reconstruction and interactive analysis" by P. J. Jennings et al.},
  langid = {english},
  keywords = {/unread,based-on:vision,read-priority-9,tech:rgb,type:paper},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-18]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/0165027088900325}}
}
% == BibTeX quality report for jenningsComputergraphicModelingAnalysis1988:
% ? unused Library catalog ("Semantic Scholar")

@article{jiangDevelopmentRealtimeHand2016,
  title = {Development of a Real-Time Hand Gesture Recognition Wristband Based on {{sEMG}} and {{IMU}} Sensing},
  author = {Jiang, Shuo and Lv, Bo and Sheng, Xinjun and Zhang, Chao and Wang, Haitao and Shull, Peter B.},
  year = {2016},
  month = dec,
  journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  pages = {1256--1261},
  publisher = {{IEEE}},
  address = {{Qingdao, China}},
  doi = {10.1109/ROBIO.2016.7866498},
  urldate = {2023-03-07},
  abstract = {Human computer interaction is becoming more integrated in daily life with the proliferation of mobile devices and virtual reality technology. Hand gesture recognition is a potentially promising mechanism to facilitate human computer interaction, however wrist-mounted surface electromyography (sEMG) hand gesture classification is particularly challenging given the relatively small sEMG signals as compared to traditional forearm-based sEMG sensing. This paper introduces the development of a wristband for detecting eight air gestures and four surface gestures at two different force levels through sEMG and inertial measurement unit (IMU) sensor fusion. To validate the wrist-worn device, ten healthy subjects performed hand gesture recognition experiments resulting in a total average recognition rate of 92.6\% for air gestures and 88.8\% for surface gestures. This paper demonstrates the potential of wrist-worn devices for accurate hand gesture recognition applications.},
  isbn = {9781509043644},
  keywords = {app:ambiguous,based-on:gloves,classes:24,dataset:custom,fidelity:finger,hardware:myo-armband,have-read,movement:dynamic,movement:static,participants:10,repetitions:13,segmentation:explicit,tech:emg,tech:imu,type:paper},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7866498/}},
  file = {/Users/brk/Zotero/storage/2V65QECC/Jiang et al. - 2016 - Development of a real-time hand gesture recognitio.pdf}
}
% == BibTeX quality report for jiangDevelopmentRealtimeHand2016:
% ? unused Library catalog ("Semantic Scholar")

@article{jiehuangSignLanguageRecognition2015,
  title = {Sign {{Language Recognition}} Using {{3D}} Convolutional Neural Networks},
  author = {{Jie Huang} and {Wengang Zhou} and {Houqiang Li} and {Weiping Li}},
  year = {2015},
  month = jun,
  journal = {2015 IEEE International Conference on Multimedia and Expo (ICME)},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Turin, Italy}},
  doi = {10.1109/ICME.2015.7177428},
  urldate = {2023-07-11},
  abstract = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
  isbn = {9781479970827},
  keywords = {app:sign-language,based-on:vision,claims-to-be-best,classes:25,fidelity:finger,hardware:kinect,have-read,model:cnn,model:nn,participants:9,tech:rgbd,type:paper},
  annotation = {199 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://ieeexplore.ieee.org/document/7177428}},
  file = {/Users/brk/Zotero/storage/7C8YTCA4/Jie Huang et al. - 2015 - Sign Language Recognition using 3D convolutional n.pdf}
}
% == BibTeX quality report for jiehuangSignLanguageRecognition2015:
% ? unused Library catalog ("Semantic Scholar")

@misc{johnmccarthyProposalDartmouthSummer,
  title = {A {{Proposal}} for the {{Dartmouth Summer Research Project}} on {{Artificial Intelligence}}, {{August}} 31, 1955 | {{Semantic Scholar}}},
  author = {{John McCarthy} and {Marvin Minsky} and {Nathaniel Rochester} and {Claude Shannon}},
  urldate = {2023-11-12},
  howpublished = {\url{https://www.semanticscholar.org/paper/A-Proposal-for-the-Dartmouth-Summer-Research-on-31\%2C-McCarthy-Minsky/38e61d9a65aa483ad0fb4a219fe54d4e6a2f6c36}},
  keywords = {/unread},
  file = {/Users/brk/Zotero/storage/JT7YRVIE/38e61d9a65aa483ad0fb4a219fe54d4e6a2f6c36.html}
}
% == BibTeX quality report for johnmccarthyProposalDartmouthSummer:
% ? Title looks like it was stored in title-case in Zotero

@article{jong-sungkimDynamicGestureRecognition1996,
  title = {A Dynamic Gesture Recognition System for the {{Korean}} Sign Language ({{KSL}})},
  author = {{Jong-Sung Kim} and {Won Jang} and {Zeungnam Bien}},
  year = {1996},
  month = apr,
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics)},
  volume = {26},
  number = {2},
  pages = {354--359},
  issn = {10834419},
  doi = {10.1109/3477.485888},
  urldate = {2023-07-02},
  abstract = {The sign language is a method of communication for the deaf-mute. Articulated gestures and postures of hands and fingers are commonly used for the sign language. This paper presents a system which recognizes the Korean sign language (KSL) and translates into a normal Korean text. A pair of data-gloves are used as the sensing device for detecting motions of hands and fingers. For efficient recognition of gestures and postures, a technique of efficient classification of motions is proposed and a fuzzy min-max neural network is adopted for on-line pattern recognition.},
  keywords = {app:korean-sl,app:sign-language,based-on:gloves,have-read,model:ffnn,model:hmm,model:nn,movement:dynamic,pdf:paywalled,type:paper},
  annotation = {192 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/485888/}}
}
% == BibTeX quality report for jong-sungkimDynamicGestureRecognition1996:
% ? unused Journal abbreviation ("IEEE Trans. Syst., Man, Cybern. B")
% ? unused Library catalog ("Semantic Scholar")

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2023-11-12},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1\textendash 4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\textemdash the structure prediction component of the `protein folding problem'8\textemdash has been an important open research problem for more than 50~years9. Despite recent progress10\textendash 14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {/unread,Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  annotation = {9994 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://www.nature.com/articles/s41586-021-03819-2}},
  file = {/Users/brk/Zotero/storage/TEQZK3A6/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf}
}
% == BibTeX quality report for jumperHighlyAccurateProtein2021:
% ? unused Library catalog ("www.nature.com")

@inproceedings{kadirMinimalTrainingLarge2004,
  title = {Minimal {{Training}}, {{Large Lexicon}}, {{Unconstrained Sign Language Recognition}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2004},
  author = {Kadir, T. and Bowden, R. and Ong, E. J. and Zisserman, A.},
  year = {2004},
  pages = {96.1-96.10},
  publisher = {{British Machine Vision Association}},
  address = {{Kingston}},
  doi = {10.5244/C.18.96},
  urldate = {2023-07-11},
  abstract = {This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92\% for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required.},
  isbn = {978-1-901725-25-4},
  langid = {english},
  keywords = {app:british-sl,app:sign-language,based-on:vision,classes:164,fidelity:finger,have-read,model:markov-chain,read-priority-1,tech:rgb,type:paper},
  annotation = {105 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://www.bmva.org/bmvc/2004/papers/paper_265.html}},
  file = {/Users/brk/Zotero/storage/MYCSBHHF/Kadir et al. - 2004 - Minimal Training, Large Lexicon, Unconstrained Sig.pdf}
}
% == BibTeX quality report for kadirMinimalTrainingLarge2004:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("British Machine Vision Conference 2004")
% ? unused Library catalog ("Semantic Scholar")

@article{kadousGRASPRecognitionAustralian1995,
  title = {{{GRASP}}: {{Recognition}} of {{Australian Sign Language Using Instrumented Gloves}}},
  shorttitle = {{{GRASP}}},
  author = {Kadous, Mohammed},
  year = {1995},
  month = dec,
  abstract = {Instrumented gloves -- gloves equipped with sensors for detecting finger bend, hand position and orientation -- were conceived to allow a more natural interface to computers. However, the extension of their use for recognising sign language, and in this case Auslan (Australian Sign Language), is possible. Several researchers have already explored these possibilities and have successfully achieved finger-spelling recognition with high levels of accuracy, but progress in the recognition of sign language as a whole has been limited.},
  keywords = {app:australian-sl,app:sign-language,based-on:gloves,classes:95,fidelity:finger,from:cite.bib,hardware:powerglove,have-read,model:hmm,model:instance-based-learning,model:nn,participants:5,segmentation:explicit,tech:flex,type:paper},
  file = {/Users/brk/Zotero/storage/ZVYPR4WQ/Kadous - 1995 - GRASP Recognition of Australian Sign Language Usi.pdf}
}
% == BibTeX quality report for kadousGRASPRecognitionAustralian1995:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("ResearchGate")

@inproceedings{kadousMachineRecognitionAuslan1996,
  title = {Machine {{Recognition}} of {{Auslan Signs Using PowerGloves}}: {{Towards Large-Lexicon Recognition}} of {{Sign Lan}}},
  shorttitle = {Machine {{Recognition}} of {{Auslan Signs Using PowerGloves}}},
  author = {Kadous, M. W.},
  year = {1996},
  urldate = {2023-03-07},
  abstract = {Instrumented gloves use a variety of sensors to provide information about the user's hand. They can be used for recognition of gestures; especially well-deened gesture sets such as sign languages. However, recognising gestures is a diicult task, due to intrapersonal and inter-personal variations in performing them. One approach to solving this problem is to use machine learning. In this case, samples of 95 discrete Australian Sign Language (Auslan) signs were collected using a Power-Glove. Two machine learning techniques were applied \{ instance-based learning (IBL) and decision-tree learning \{ to the data after some simple features were extracted. Accuracy of approximately 80 per cent was achieved using IBL, despite the severe limitations of the glove.\vphantom{\}\}}},
  keywords = {app:australian-sl,app:sign-language,based-on:gloves,classes:95,hardware:powerglove,have-read,model:decision-tree,model:instance-based-learning,type:paper},
  annotation = {115 Citations},
  note = {\url{https://www.semanticscholar.org/paper/Machine-Recognition-of-Auslan-Signs-Using-Towards-Kadous/1ed3e20bbcf12c60e766189987074ac2935d7140}},
  file = {/Users/brk/Zotero/storage/3FZ4HCVJ/Kadous - 1996 - Machine Recognition of Auslan Signs Using PowerGlo.pdf}
}
% == BibTeX quality report for kadousMachineRecognitionAuslan1996:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{karantonisImplementationRealTimeHuman2006,
  title = {Implementation of a {{Real-Time Human Movement Classifier Using}} a {{Triaxial Accelerometer}} for {{Ambulatory Monitoring}}},
  author = {Karantonis, D.M. and Narayanan, M.R. and Mathie, M. and Lovell, N.H. and Celler, B.G.},
  year = {2006},
  month = jan,
  journal = {IEEE Transactions on Information Technology in Biomedicine},
  volume = {10},
  number = {1},
  pages = {156--167},
  issn = {1089-7771},
  doi = {10.1109/TITB.2005.856864},
  urldate = {2023-07-11},
  abstract = {The real-time monitoring of human movement can provide valuable information regarding an individual's degree of functional ability and general level of activity. This paper presents the implementation of a real-time classification system for the types of human movement associated with the data acquired from a single, waist-mounted triaxial accelerometer unit. The major advance proposed by the system is to perform the vast majority of signal processing onboard the wearable unit using embedded intelligence. In this way, the system distinguishes between periods of activity and rest, recognizes the postural orientation of the wearer, detects events such as walking and falls, and provides an estimation of metabolic energy expenditure. A laboratory-based trial involving six subjects was undertaken, with results indicating an overall accuracy of 90.8\% across a series of 12 tasks (283 tests) involving a variety of movements related to normal daily activities. Distinction between activity and rest was performed without error; recognition of postural orientation was carried out with 94.1\% accuracy, classification of walking was achieved with less certainty (83.3\% accuracy), and detection of possible falls was made with 95.6\% accuracy. Results demonstrate the feasibility of implementing an accelerometry-based, real-time movement classifier using embedded intelligence},
  langid = {english},
  keywords = {based-on:gloves,classes:12,contains-more-references,have-read,model:decision-tree,participants:6,tech:accelerometer,tech:imu,type:paper},
  annotation = {1273 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/1573717/}},
  file = {/Users/brk/Zotero/storage/4X8QFWXU/Karantonis et al. - 2006 - Implementation of a Real-Time Human Movement Class.pdf}
}
% == BibTeX quality report for karantonisImplementationRealTimeHuman2006:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Inform. Technol. Biomed.")
% ? unused Library catalog ("Semantic Scholar")

@misc{karpathyRecipeTrainingNeural2019,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2019},
  journal = {Andrej Karpathy Blog},
  publisher = {{https://karpathy.github.io}},
  howpublished = {\url{https://karpathy.github.io/2019/04/25/recipe/}},
  keywords = {background,from:cite.bib,model:nn,type:blog}
}
% == BibTeX quality report for karpathyRecipeTrainingNeural2019:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{kaufmanToolsInteractionThree1989,
  title = {Tools for Interaction in Three Dimensions},
  author = {Kaufman, A. and Yagel, R.},
  year = {1989},
  month = sep,
  urldate = {2023-07-13},
  abstract = {Bodies, or frames, of use in the formation of modular interconnect terminals, or terminal modules are shown. The bodies are formed, characteristically, from single pieces of material to include pockets or receptacles into which electrical contact elements may be fitted. Each body, or frame, includes regions about which it may be folded to provide a four-sided box-like structure. Before being folded the body is equipped with electrical contact elements which it supports in such a way that first terminals of the contact elements are accessible from outside the folded structure. A flexible printed circuit board having conductors arranged in patterns on at least one face is folded within the box-like structure and its conductors are connected to second terminals of the contact elements to provide selected interconnections between the electrical contact elements.},
  keywords = {based-on:gloves,hardware:vpl-dataglove,have-read,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Tools-for-interaction-in-three-dimensions-Kaufman-Yagel/4bdf137539c2b6955a9cd7251b0b5839c96f6ecf}}
}
% == BibTeX quality report for kaufmanToolsInteractionThree1989:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{kelaAccelerometerbasedGestureControl2006,
  title = {Accelerometer-Based Gesture Control for a Design Environment},
  author = {Kela, Juha and Korpip{\"a}{\"a}, Panu and M{\"a}ntyj{\"a}rvi, Jani and Kallio, Sanna and Savino, Giuseppe and Jozzo, Luca and Marca, Sergio Di},
  year = {2006},
  month = aug,
  journal = {Personal and Ubiquitous Computing},
  volume = {10},
  number = {5},
  pages = {285--299},
  issn = {1617-4909, 1617-4917},
  doi = {10.1007/s00779-005-0033-8},
  urldate = {2023-06-23},
  abstract = {Accelerometer-based gesture control is studied as a supplementary or an alternative interaction modality. Gesture commands freely trainable by the user can be used for controlling external devices with handheld wireless sensor unit. Two user studies are presented. The first study concerns finding gestures for controlling a design environment (Smart Design Studio), TV, VCR, and lighting. The results indicate that different people usually prefer different gestures for the same task, and hence it should be possible to personalise them. The second user study concerns evaluating the usefulness of the gesture modality compared to other interaction modalities for controlling a design environment. The other modalities were speech, RFID-based physical tangible objects, laser-tracked pen, and PDA stylus. The results suggest that gestures are a natural modality for certain tasks, and can augment other modalities. Gesture commands were found to be natural, especially for commands with spatial association in design environment control.},
  langid = {english},
  keywords = {based-on:gloves,fidelity:arm,from:cite.bib,hardware:soapbox,have-read,read-priority-7,tech:accelerometer,type:paper},
  annotation = {201 citations (Crossref) [2023-07-11] 333 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/s00779-005-0033-8}},
  file = {/Users/brk/Zotero/storage/L5WTBMWR/Kela et al. - 2006 - Accelerometer-based gesture control for a design e.pdf}
}
% == BibTeX quality report for kelaAccelerometerbasedGestureControl2006:
% ? unused Journal abbreviation ("Pers Ubiquit Comput")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{kelloggBringingGestureRecognition2014,
  title = {Bringing {{Gesture Recognition}} to {{All Devices}}},
  booktitle = {Symposium on {{Networked Systems Design}} and {{Implementation}}},
  author = {Kellogg, Bryce and Talla, V. and Gollakota, Shyamnath},
  year = {2014},
  month = apr,
  urldate = {2023-07-12},
  abstract = {Existing gesture-recognition systems consume significant power and computational resources that limit how they may be used in low-end devices. We introduce AllSee, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries. AllSee consumes three to four orders of magnitude lower power than state-of-the-art systems and can enable always-on gesture recognition for smartphones and tablets. It extracts gesture information from existing wireless signals (e.g., TV transmissions), but does not incur the power and computational overheads of prior wireless approaches. We build AllSee prototypes that can recognize gestures on RFID tags and power-harvesting sensors. We also integrate our hardware with an off-the-shelf Nexus S phone and demonstrate gesture recognition in through-the-pocket scenarios. Our results show that AllSee achieves classification accuracies as high as 97\% over a set of eight gestures.},
  keywords = {app:activity-inference,based-on:wifi,classes:8,tech:wifi,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Bringing-Gesture-Recognition-to-All-Devices-Kellogg-Talla/c17ac67f8c74c767073e5ef09516a70ede65adcf}}
}
% == BibTeX quality report for kelloggBringingGestureRecognition2014:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{keskinRealTimeHand2003,
  title = {Real Time Hand Tracking and {{3D}} Gesture Recognition for Interactive Interfaces Using {{HMM}}},
  author = {Keskin, Cem and Erkan, A.N. and Akarun, L},
  year = {2003},
  month = jan,
  journal = {Proceedings of the joint international conference ICANN/ICONIP},
  keywords = {based-on:vision,classes:8,from:cite.bib,model:hmm,movement:dynamic,pdf:cant-find,tech:rgb,type:paper}
}

@article{kesslerEvaluationCyberGloveWholehand1995,
  title = {Evaluation of the {{CyberGlove}} as a Whole-Hand Input Device},
  author = {Kessler, G. Drew and Hodges, Larry F. and Walker, Neff},
  year = {1995},
  month = dec,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {2},
  number = {4},
  pages = {263--283},
  issn = {1073-0516, 1557-7325},
  doi = {10.1145/212430.212431},
  urldate = {2023-06-23},
  abstract = {We present a careful evaluation of the sensory characteristics of the CyberGlove model CG1801 whole-hand input device. In particular, we conducted an experimental study that investigated the level of sensitivity of the sensors, their performance in recognizing angles, and factors that affected accuracy of recognition of flexion measurements. Among our results, we show that hand size differences among the subjects of the study did not have a statistical effect on the accuracy of the device. We also analyzed the effect of different software calibration approaches on accuracy of the sensors.},
  langid = {english},
  keywords = {based-on:gloves,hardware:cyberglove,have-read,tech:flex,type:survey},
  annotation = {218 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/212430.212431}},
  file = {/Users/brk/Zotero/storage/SDEHCHUH/Kessler et al. - 1995 - Evaluation of the CyberGlove as a whole-hand input.pdf}
}
% == BibTeX quality report for kesslerEvaluationCyberGloveWholehand1995:
% ? unused Journal abbreviation ("ACM Trans. Comput.-Hum. Interact.")
% ? unused Library catalog ("Semantic Scholar")

@article{khanSurveyGestureRecognition2012,
  title = {Survey on {{Gesture Recognition}} for {{Hand Image Postures}}},
  author = {Khan, Rafiqul Zaman and Ibraheem, Noor Adnan},
  year = {2012},
  month = apr,
  journal = {Computer and Information Science},
  volume = {5},
  number = {3},
  pages = {p110},
  issn = {1913-8997, 1913-8989},
  doi = {10.5539/cis.v5n3p110},
  urldate = {2023-03-07},
  abstract = {One of the attractive methods for providing natural human-computer interaction is the use of the hand as an input device rather than the cumbersome devices such as keyboards and mice, which need the user to be located in a specific location to use these devices. Since human hand is an articulated object, it is an open issue to discuss. The most important thing in hand gesture recognition system is the input features, and the selection of good features representation. This paper presents a review study on the hand postures and gesture recognition methods, which is considered to be a challenging problem in the human-computer interaction context and promising as well. Many applications and techniques were discussed here with the explanation of system recognition framework and its main phases.},
  keywords = {based-on:vision,contains-more-references,have-read,type:survey},
  annotation = {94 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://www.ccsenet.org/journal/index.php/cis/article/view/16598}},
  file = {/Users/brk/Zotero/storage/J2T7FEZP/Khan and Ibraheem - 2012 - Survey on Gesture Recognition for Hand Image Postu.pdf}
}
% == BibTeX quality report for khanSurveyGestureRecognition2012:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("CIS")
% ? unused Library catalog ("Semantic Scholar")

@article{kimBichannelSensorFusion2008,
  title = {Bi-Channel Sensor Fusion for Automatic Sign Language Recognition},
  author = {Kim, Jonghwa and Wagner, Johannes and Rehm, Matthias and Andre, Elisabeth},
  year = {2008},
  month = sep,
  journal = {2008 8th IEEE International Conference on Automatic Face \& Gesture Recognition},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Amsterdam, Netherlands}},
  doi = {10.1109/AFGR.2008.4813341},
  urldate = {2023-07-30},
  abstract = {In this paper, we investigate the mutual-complementary functionality of accelerometer (ACC) and electromyogram (EMG) for recognizing seven word-level sign vocabularies in German sign language (GSL). Results are discussed for the single channels and for feature-level fusion for the bichannel sensor data. For the subject-dependent condition, this fusion method proves to be effective. Most relevant features for all subjects are extracted and their universal effectiveness is proven with a high average accuracy for the single subjects. Additionally, results are given for the subject-independent condition, where subjective differences do not allow for high recognition rates. Finally we discuss a problem of feature-level fusion caused by high disparity between accuracies of each single channel classification.},
  isbn = {9781424421534},
  keywords = {app:german-sl,app:sign-language,based-on:gloves,classes:7,dataset:custom,fidelity:arm,model:knn,model:svm,participants:8,reference:doi.org/10.1007/3-540-40063-X\_74,reference:doi.org/10.1016/S0004-3702(97)00043-X,reference:doi.org/10.1109/34.598226,reference:doi.org/10.1109/34.735811,reference:doi.org/10.1109/AFGR.2004.1301590,reference:doi.org/10.1109/ICSMC.1997.625741,reference:doi.org/10.1109/IEMBS.2006.259428,reference:doi.org/10.1109/ISWC.2003.1241392,reference:doi.org/10.1109/ISWC.2007.4373769,reference:doi.org/10.1145/108844.108900,tech:accelerometer,tech:emg,untagged},
  annotation = {49 citations (Semantic Scholar/DOI) [2023-07-30] --- DOIs\_of\_references: ["10.1109/ISWC.2007.4373769","10.1109/AFGR.2004.1301590","10.1109/IEMBS.2006.259428","10.1007/3-540-40063-X\_74","10.1145/122488.122499","10.1145/108844.108900","10.1016/S0004-3702(97)00043-X"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/4813341/}},
  file = {/Users/brk/Zotero/storage/Z44Z3IYN/Kim et al. - 2008 - Bi-channel sensor fusion for automatic sign langua.pdf}
}
% == BibTeX quality report for kimBichannelSensorFusion2008:
% ? unused Conference name ("Gesture Recognition (FG)")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{kimEMGbasedHandGesture2008,
  title = {{{EMG-based}} Hand Gesture Recognition for Realtime Biosignal Interfacing},
  booktitle = {Proceedings of the 13th International Conference on {{Intelligent}} User Interfaces},
  author = {Kim, Jonghwa and Mastnik, Stephan and Andr{\'e}, Elisabeth},
  year = {2008},
  month = jan,
  pages = {30--39},
  publisher = {{ACM}},
  address = {{Gran Canaria Spain}},
  doi = {10.1145/1378773.1378778},
  urldate = {2023-07-30},
  isbn = {978-1-59593-987-6},
  langid = {english},
  keywords = {/unread},
  annotation = {201 citations (Semantic Scholar/DOI) [2023-07-30]},
  note = {\url{https://dl.acm.org/doi/10.1145/1378773.1378778}}
}
% == BibTeX quality report for kimEMGbasedHandGesture2008:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("IUI08: 13th International Conference on Intelligent User Interfaces")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  journal = {CoRR},
  urldate = {2023-06-07},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {background},
  note = {\url{https://www.semanticscholar.org/paper/Adam\%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8}},
  file = {/Users/brk/Zotero/storage/SB7A4J49/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf}
}
% == BibTeX quality report for kingmaAdamMethodStochastic2014:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{klingmannAccelerometerBasedGestureRecognition2009,
  title = {Accelerometer-{{Based Gesture Recognition}} with the {{iPhone}}},
  author = {Klingmann, Marco},
  year = {2009},
  urldate = {2023-03-07},
  abstract = {The growing number of small sensors built into consumer electronic devices, such as mobile phones, allow experiments with alternative interaction methods in favour of more physical, intuitive and pervasive human computer interaction. This paper examines hand gestures as an alternative or supplementary input modality for mobile devices. The iPhone is chosen as sensing and processing device. Based on its built-in accelerometer, hand movements are detected and classified into previously trained gestures. A software library for accelerometer-based gesture recognition and a demonstration iPhone application have been developed. The system allows the training and recognition of free-from hand gestures. Discrete hidden Markov models form the core part of the gesture recognition apparatus. Five test gestures have been defined and used to evaluate the performance of the application. The evaluation shows that with 10 training repetitions, an average recognition rate of over 90 percent can be achieved.},
  keywords = {based-on:gloves,have-read,model:hmm,pdf:cant-find,tech:accelerometer,type:paper},
  annotation = {22 Citations},
  note = {\url{https://www.semanticscholar.org/paper/Accelerometer-Based-Gesture-Recognition-with-the-Klingmann/cbe0d68e8e9ec8d6b3d4129366214e98437c896b}}
}
% == BibTeX quality report for klingmannAccelerometerBasedGestureRecognition2009:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{knuthTwoNotesNotation1992,
  title = {Two {{Notes}} on {{Notation}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = may,
  journal = {The American Mathematical Monthly},
  volume = {99},
  number = {5},
  pages = {403},
  issn = {00029890},
  doi = {10.2307/2325085},
  urldate = {2023-07-11},
  abstract = {Mathematical notation evolves like all languages do. As new experiments are made, we sometimes witness the survival of the fittest, sometimes the survival of the most familiar. A healthy conservatism keeps things from changing too rapidly; a healthy radicalism keeps things in tune with new theoretical emphases. Our mathematical language continues to improve, just as "the d-ism of Leibniz overtook the dotage of Newton" in past centuries [4, Chapter 4]. In 1970 I began teaching a class at Stanford University entitled Concrete Mathematics. The students and I studied how to manipulate formulas in continuous and discrete mathematics, and the problems we investigated were often inspired by new developments in computer science. As the years went by we began to see that a few changes in notational traditions would greatly facilitate our work. The notes from that class have recently been published in a book [15], and as I wrote the final drafts of that book I learned to my surprise that two of the notations we had been using were considerably more useful than I had previously realized. The ideas "clicked" so well, in fact, that I've decided to write this article, blatantly attempting to promote these notations among the mathematicians who have no use for [15]. I hope that within five years everybody will be able to use these notations in published papers without needing to explain what they mean. The notations I'm talking about are (1) Iverson's convention for characteristic functions; and (2) the "right" notation for Stirling numbers, at last.},
  keywords = {background},
  annotation = {458 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://www.jstor.org/stable/2325085?origin=crossref}},
  file = {/Users/brk/Zotero/storage/QUCL5Y47/Knuth - 1992 - Two Notes on Notation.pdf}
}
% == BibTeX quality report for knuthTwoNotesNotation1992:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{kochRecurrentNeuralNetwork2019,
  title = {A {{Recurrent Neural Network}} for {{Hand Gesture Recognition}} Based on {{Accelerometer Data}}},
  author = {Koch, Philipp and Dreier, Mark and Maass, Marco and Bohme, Martina and Phan, Huy and Mertins, Alfred},
  year = {2019},
  month = jul,
  journal = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  pages = {5088--5091},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856844},
  urldate = {2023-06-22},
  abstract = {For many applications, hand gesture recognition systems that rely on biosignal data exclusively are mandatory. Usually, theses systems have to be affordable, reliable as well as mobile. The hand is moved due to muscle contractions that cause motions of the forearm skin. Theses motions can be captured with cheap and reliable accelerometers placed around the forearm. Since accelerometers can also be integrated into mobile systems easily, the possibility of a robust hand gesture recognition based on accelerometer signals is evaluated in this work. For this, a neural network architecture consisting of two different kinds of recurrent neural network (RNN) cells is proposed. Experiments on three databases reveal that this relatively small network outperforms by far state-of-the-art hand gesture recognition approaches that rely on multi-modal data. The combination of accelerometer data and an RNN forms a robust hand gesture classification system, i.e., the performance of the network does not vary a lot between subjects and it is outstanding for amputees. Furthermore, the proposed network uses only 5 ms short windows to classify the hand gestures. Consequently, this approach allows for a quick, and potentially delay-free hand gesture detection.},
  isbn = {9781538613115},
  keywords = {based-on:gloves,claims-to-be-best,classes:50,dataset:ninapro,fidelity:finger,fidelity:hand,have-read,model:nn,model:rnn,participants:51,reference:doi.org/10.1038/sdata.2014.53,reference:doi.org/10.1038/srep36571,reference:doi.org/10.1109/3468.925661,reference:doi.org/10.1109/EMBC.2018.8513145,reference:doi.org/10.1109/TSMCB.2012.2185843,reference:doi.org/10.1186/s12984-017-0284-4,tech:accelerometer,tech:emg,type:paper},
  annotation = {16 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1162/neco.1997.9.8.1735","10.23919/EUSIPCO.2018.8553483","10.1186/s12984-017-0284-4","10.1038/srep36571","10.1109/EMBC.2018.8513145","10.1109/TSMCB.2012.2185843","10.1109/3468.925661","10.1038/sdata.2014.53","10.1145/1891903.1891950","10.1088/1742-6596/477/1/012041","10.1109/TRO.2007.910708","10.1109/3468.925661","10.1109/TSMCB.2012.2185843","10.1038/sdata.2014.53","10.1109/TSMCA.2011.2116004","10.1109/TBME.2003.813539","10.3389/fnbot.2016.00009","10.1038/srep36571","10.1109/EMBC.2018.8513145","10.23919/EUSIPCO.2018.8553483","10.1162/neco.1997.9.8.1735","10.1186/s12984-017-0284-4"] ---},
  note = {\url{https://ieeexplore.ieee.org/document/8856844/}},
  file = {/Users/brk/Zotero/storage/ZF224WN7/Koch et al. - 2019 - A Recurrent Neural Network for Hand Gesture Recogn.pdf}
}
% == BibTeX quality report for kochRecurrentNeuralNetwork2019:
% ? unused Conference name ("2019 41st Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)")
% ? unused Library catalog ("Semantic Scholar")

@article{kohonenSelforganizedFormationTopologically1982,
  title = {Self-Organized Formation of Topologically Correct Feature Maps},
  author = {Kohonen, Teuvo},
  year = {1982},
  journal = {Biological Cybernetics},
  volume = {43},
  number = {1},
  pages = {59--69},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337288},
  urldate = {2023-07-13},
  langid = {english},
  keywords = {background},
  annotation = {7573 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://link.springer.com/10.1007/BF00337288}}
}
% == BibTeX quality report for kohonenSelforganizedFormationTopologically1982:
% ? unused Journal abbreviation ("Biol. Cybern.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{kolschKeyboardsKeyboardsSurvey2002,
  title = {Keyboards without {{Keyboards}}: {{A Survey}} of {{Virtual Keyboards}}},
  shorttitle = {Keyboards without {{Keyboards}}},
  author = {K{\"o}lsch, M. and Turk, M.},
  year = {2002},
  urldate = {2023-06-29},
  abstract = {Input to small devices is becoming an increasingly crucial factor in development for the ever-more powerful embedded market. Speech input promises to become a feasible alternative to tiny keypads, yet its limited reliability, robustness, and flexibility render it unsuitable for certain tasks and/or environments. Various attempts have been made to provide the common keyboard metaphor without the physical keyboard, to build ``virtual keyboards''. This promises to leverage our familiarity with the device without incurring the constraints of the bulky physics. This paper surveys technologies for alphanumeric input devices and methods with a strong focus on touch-typing. We analyze the characteristics of the keyboard modality and show how they contribute to making it a necessary complement to speech recognition rather than a competitor.},
  keywords = {based-on:gloves,based-on:vision,contains-more-references,have-read,read-priority-1,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/Keyboards-without-Keyboards\%3A-A-Survey-of-Virtual-K\%C3\%B6lsch-Turk/44355bea0b025ba8719a946ba6c009e0eba133f3}},
  file = {/Users/brk/Zotero/storage/MB8CKX7D/Kölsch and Turk - 2002 - Keyboards without Keyboards A Survey of Virtual K.pdf}
}
% == BibTeX quality report for kolschKeyboardsKeyboardsSurvey2002:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{kongGestureRecognitionModel2009,
  title = {Gesture Recognition Model Based on {{3D}} Accelerations},
  author = {Kong, Jun-qi and Wang, Hui and Zhang, Guang-quan},
  year = {2009},
  month = jul,
  journal = {2009 4th International Conference on Computer Science \& Education},
  pages = {66--70},
  publisher = {{IEEE}},
  address = {{Nanning}},
  doi = {10.1109/ICCSE.2009.5228524},
  urldate = {2023-03-07},
  abstract = {This paper presents a recognition model on gesture interaction, gesture recognition based on accererations, which are collected by 3D accelerometer. Unlike most other vision-based gesture recognition, our method resorting to accelerations of the gesture in three directions. Gesture is divided into small units, and each unit is modeled by an HMM, the HMM classifies the gesture unit after being well trained, the gesture identified when all of the corresponding gesture units recognized successfully, then the instruction will be triggered, and the Human-Computer Interaction achieved. Through experiments, the method proved to be effective. This recognition model can be wildly used in the field of mobile computing and remote control, and people could use computer more friendly and naturally.},
  isbn = {9781424435203},
  keywords = {based-on:gloves,have-read,model:hmm,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {17 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/5228524/}}
}
% == BibTeX quality report for kongGestureRecognitionModel2009:
% ? unused Conference name ("2009 4th International Conference on Computer Science &amp; Education (ICCSE 2009)")
% ? unused Library catalog ("Semantic Scholar")

@article{kopukluRealtimeHandGesture2019,
  title = {Real-Time {{Hand Gesture Detection}} and {{Classification Using Convolutional Neural Networks}}},
  author = {Kopuklu, Okan and Gunduz, Ahmet and Kose, Neslihan and Rigoll, Gerhard},
  year = {2019},
  month = may,
  journal = {2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Lille, France}},
  doi = {10.1109/FG.2019.8756576},
  urldate = {2023-07-11},
  abstract = {Real-time recognition of dynamic hand gestures from video streams is a challenging task since (i) there is no indication when a gesture starts and ends in the video, (ii) performed gestures should only be recognized once, and (iii) the entire architecture should be designed considering the memory and power budget. In this work, we address these challenges by proposing a hierarchical structure enabling offline-working convolutional neural network (CNN) architectures to operate online efficiently by using sliding window approach. The proposed architecture consists of two models: (1) A detector which is a lightweight CNN architecture to detect gestures and (2) a classifier which is a deep CNN to classify the detected gestures. In order to evaluate the single-time activations of the detected gestures, we propose to use Levenshtein distance as an evaluation metric since it can measure misclassifications, multiple detections, and missing detections at the same time. We evaluate our architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic Hand Gesture Datasets - which require temporal detection and classification of the performed hand gestures. ResNeXt-101 model, which is used as a classifier, achieves the state-of-the-art offline classification accuracy of 94.04\% and 83.82\% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In real-time detection and classification, we obtain considerable early detections while achieving performances close to offline operation. The codes and pretrained models used in this work are publicly available1.},
  isbn = {9781728100890},
  keywords = {based-on:vision,classes:25,classes:83,dataset:egogesture,dataset:nvidia-dynamic-hand-gesture,detector-recogniser,have-read,model:cnn,model:nn,movement:dynamic,movement:static,read-priority-1,segmentation:implicit,tech:rgb,type:paper},
  annotation = {112 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://ieeexplore.ieee.org/document/8756576/}},
  file = {/Users/brk/Zotero/storage/GE2CVBAG/Kopuklu et al. - 2019 - Real-time Hand Gesture Detection and Classificatio.pdf}
}
% == BibTeX quality report for kopukluRealtimeHandGesture2019:
% ? unused Library catalog ("Semantic Scholar")

@article{kotaruSpotFiDecimeterLevel2015,
  title = {{{SpotFi}}: {{Decimeter Level Localization Using WiFi}}},
  shorttitle = {{{SpotFi}}},
  author = {Kotaru, Manikanta and Joshi, Kiran and Bharadia, Dinesh and Katti, Sachin},
  year = {2015},
  month = aug,
  journal = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
  pages = {269--282},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/2785956.2787487},
  urldate = {2023-07-12},
  abstract = {This paper presents the design and implementation of SpotFi, an accurate indoor localization system that can be deployed on commodity WiFi infrastructure. SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems. SpotFi makes two key technical contributions. First, SpotFi incorporates super-resolution algorithms that can accurately compute the angle of arrival (AoA) of multipath components even when the access point (AP) has only three antennas. Second, it incorporates novel filtering and estimation techniques to identify AoA of direct path between the localization target and AP by assigning values for each path depending on how likely the particular path is the direct path. Our experiments in a multipath rich indoor environment show that SpotFi achieves a median accuracy of 40 cm and is robust to indoor hindrances such as obstacles and multipath.},
  isbn = {9781450335423},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {1082 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2785956.2787487}}
}
% == BibTeX quality report for kotaruSpotFiDecimeterLevel2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("SIGCOMM '15: ACM SIGCOMM 2015 Conference")
% ? unused Library catalog ("Semantic Scholar")

@article{kramerTalkingGlove1988,
  title = {The Talking Glove},
  author = {Kramer, James and Leifer, Larry},
  year = {1988},
  month = apr,
  journal = {ACM SIGCAPH Computers and the Physically Handicapped},
  number = {39},
  pages = {12--16},
  issn = {0163-5727},
  doi = {10.1145/47937.47938},
  urldate = {2023-07-13},
  abstract = {A new bi-directional communication aid is being developed which allows deaf, deaf-blind, or nonvocal individuals to interact verbally with others. The device analyzes a nonvocal person's fingerspelling hand formations and outputs the spelled words as synthesized speech. In effect, this component of the communication aid is a "talking glove". In addition, by using state-of-the-art voice recognition equipment, a deaf user is able to read incoming speech on the miniature LCD screen of a modified digital wristwatch. Similarly, a deaf-blind individual may read incoming speech on a portable braille display module.},
  langid = {english},
  keywords = {based-on:gloves,hardware:cyberglove,have-read,type:paper},
  annotation = {22 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{https://dl.acm.org/doi/10.1145/47937.47938}},
  file = {/Users/brk/Zotero/storage/E358BBT2/Kramer and Leifer - 1988 - The talking glove.pdf}
}
% == BibTeX quality report for kramerTalkingGlove1988:
% ? unused Journal abbreviation ("SIGCAPH Comput. Phys. Handicap.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{kratzWiizards3DGesture2007,
  title = {Wiizards: {{3D}} Gesture Recognition for Game Play Input},
  shorttitle = {Wiizards},
  booktitle = {Proceedings of the 2007 Conference on {{Future Play}}  - {{Future Play}} '07},
  author = {Kratz, Louis and Smith, Matthew and Lee, Frank J.},
  year = {2007},
  pages = {209},
  publisher = {{ACM Press}},
  address = {{Toronto, Canada}},
  doi = {10.1145/1328202.1328241},
  urldate = {2023-07-11},
  abstract = {Gesture based input is an emerging technology gaining widespread popularity in interactive entertainment. The use of gestures provides intuitive and natural input mechanics for games, presenting an easy to learn yet richly immersive experience. In Wiizards, we explore the use of 3D accelerometer gestures in a multiplayer, zero sum game. Hidden Markov models are constructed for gesture recognition, providing increased flexibility and fluid tolerance. Users can strategically effect the outcome via combinations of gestures with limitless scalability.},
  isbn = {978-1-59593-943-2},
  langid = {english},
  keywords = {app:gaming,based-on:gloves,classes:10,contains-more-references,fidelity:arm,hardware:wiimote,have-read,model:hmm,participants:7,tech:accelerometer,type:paper},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=1328202.1328241}},
  file = {/Users/brk/Zotero/storage/RT6LR5A7/Kratz et al. - 2007 - Wiizards 3D gesture recognition for game play inp.pdf}
}
% == BibTeX quality report for kratzWiizards3DGesture2007:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the 2007 conference")
% ? unused Library catalog ("Semantic Scholar")

@article{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  month = dec,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  urldate = {2023-11-12},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  keywords = {/unread},
  annotation = {9998 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3065386}},
  file = {/Users/brk/Zotero/storage/SJA5QSC3/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}
% == BibTeX quality report for krizhevskyImageNetClassificationDeep2012:
% ? unused Journal abbreviation ("Commun. ACM")
% ? unused Library catalog ("Semantic Scholar")

@article{kudrinkoWearableSensorBasedSign2021,
  title = {Wearable {{Sensor-Based Sign Language Recognition}}: {{A Comprehensive Review}}},
  shorttitle = {Wearable {{Sensor-Based Sign Language Recognition}}},
  author = {Kudrinko, Karly and Flavin, Emile and Zhu, Xiaodan and Li, Qingguo},
  year = {2021},
  journal = {IEEE Reviews in Biomedical Engineering},
  volume = {14},
  pages = {82--97},
  issn = {1937-3333, 1941-1189},
  doi = {10.1109/RBME.2020.3019769},
  urldate = {2023-03-07},
  abstract = {Sign language is used as a primary form of communication by many people who are Deaf, deafened, hard of hearing, and non-verbal. Communication barriers exist for members of these populations during daily interactions with those who are unable to understand or use sign language. Advancements in technology and machine learning techniques have led to the development of innovative approaches for gesture recognition. This literature review focuses on analyzing studies that use wearable sensor-based systems to classify sign language gestures. A review of 72 studies from 1991 to 2019 was performed to identify trends, best practices, and common challenges. Attributes including sign language variation, sensor configuration, classification method, study design, and performance metrics were analyzed and compared. Results from this literature review could aid in the development of user-centred and robust wearable sensor-based systems for sign language recognition.},
  keywords = {/unread,app:sign-language,based-on:gloves,survey-finsh:2019,survey-start:1991,type:survey},
  annotation = {37 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9178440/}}
}
% == BibTeX quality report for kudrinkoWearableSensorBasedSign2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Rev. Biomed. Eng.")
% ? unused Library catalog ("Semantic Scholar")

@article{kunduHandGestureRecognition2018,
  title = {Hand {{Gesture Recognition Based Omnidirectional Wheelchair Control Using IMU}} and {{EMG Sensors}}},
  author = {Kundu, Ananda Sankar and Mazumder, Oishee and Lenka, Prasanna Kumar and Bhaumik, Subhasis},
  year = {2018},
  month = sep,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {91},
  number = {3-4},
  pages = {529--541},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-017-0725-0},
  urldate = {2023-06-22},
  abstract = {This paper presents a hand gesture based control of an omnidirectional wheelchair using inertial measurement unit (IMU) and myoelectric units as wearable sensors. Seven common gestures are recognized and classified using shape based feature extraction and Dendogram Support Vector Machine (DSVM) classifier. The dynamic gestures are mapped to the omnidirectional motion commands to navigate the wheelchair. A single IMU is used to measure the wrist tilt angle and acceleration in three axis. EMG signals are extracted from two forearm muscles namely Extensor Carpi Radialis and Flexor Carpi Radialis and processed to provide Root Mean Square (RMS) signal. Initiation and termination of dynamic activities are based on autonomous identification of static to dynamic or dynamic to static transition by setting static thresholds on processed IMU and myoelectric sensor data. Classification involves recognizing the activity pattern based on periodic shape of trajectories of the triaxial wrist tilt angle and EMG-RMS from the two selected muscles. Second order Polynomial coefficients extracted from the sensor trajectory templates during specific dynamic activity cycles are used as features to classify dynamic activities. Classification algorithm and real time navigation of the wheelchair using the proposed algorithm has been tested by five healthy subjects. Classification accuracy of 94\% was achieved by DSVM classifier on `k' fold cross validation data of 5 users. Classification accuracy while operating the wheelchair was 90.5\%.},
  langid = {english},
  keywords = {/unread,based-on:gloves,model:svm,pdf:paywalled,tech:accelerometer,tech:emg,type:paper},
  annotation = {70 Citations},
  note = {\url{http://link.springer.com/10.1007/s10846-017-0725-0}}
}
% == BibTeX quality report for kunduHandGestureRecognition2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("J Intell Robot Syst")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{laviolaSurveyHandPosture1999,
  title = {A {{Survey}} of {{Hand Posture}} and {{Gesture Recognition Techniques}} and {{Technology}}},
  author = {LaViola, Jr Joseph J.},
  year = {1999},
  month = jun,
  urldate = {2023-06-23},
  abstract = {This paper surveys the use of hand postures and gestures as a mechanism for interaction with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based interaction. First, the technological requirements and limitations for using hand postures and gestures are described by discussing both glove-based and vision-based recognition systems along with advantages and disadvantages of each. Second, the various types of techniques used in recognizing hand postures and gestures are compared and contrasted. Third, the applications that have used hand posture and gesture interfaces are examined. The survey concludes with a summary and a discussion of future research directions.},
  keywords = {based-on:gloves,based-on:vision,hardware:cyberglove,hardware:powerglove,hardware:vpl-dataglove,have-read,read-priority-3,reference:doi.org/10.1001/jamafacial.2013.2127,reference:doi.org/10.1007/11739685,reference:doi.org/10.1007/3-540-63508-4\_141,reference:doi.org/10.1007/978-1-4684-2532-1\_19,reference:doi.org/10.1007/978-3-030-68799-1\_14,reference:doi.org/10.1007/BFb0053005,reference:doi.org/10.1007/BFb0053006,reference:doi.org/10.1007/BFb0053007,reference:doi.org/10.1007/s11042-014-1989-z,reference:doi.org/10.1016/0893-6080(89)90041-5,reference:doi.org/10.1016/B978-0-12-227748-1.50024-X,reference:doi.org/10.1016/c2009-0-13851-5,reference:doi.org/10.1016/c2009-0-27624-0,reference:doi.org/10.1016/j.jtct.2022.03.007,reference:doi.org/10.1016/j.molcel.2008.03.003,reference:doi.org/10.1038/s41467-022-29881-6,reference:doi.org/10.1046/j.1365-3156.2001.00822.x,reference:doi.org/10.1098/rsta.2015.0202,reference:doi.org/10.1109/5.18626,reference:doi.org/10.1109/ICNTE56631.2023.10146675,reference:doi.org/10.1109/IGARSS46834.2022.9883606,reference:doi.org/10.1109/ISMAR-Adjunct57072.2022.00118,reference:doi.org/10.1109/TASE.2014.2333754,reference:doi.org/10.1142/9789814350938\_0003,reference:doi.org/10.1145/108844.108900,reference:doi.org/10.1145/108844.108913,reference:doi.org/10.1145/122488.122499,reference:doi.org/10.1145/146022.146042,reference:doi.org/10.1145/192161.192197,reference:doi.org/10.1145/229459.229467,reference:doi.org/10.1145/800250.807503,reference:doi.org/10.1162/pres.1994.3.3.236,reference:doi.org/10.1162/pres.1995.4.4.403,reference:doi.org/10.1609/aimag.v15i3.1106,reference:doi.org/10.2307/414069,reference:doi.org/10.3233/978-1-61499-286-8-128,reference:doi.org/10.3233/978-1-61499-440-4-26,reference:doi.org/10.4995/var.2020.13343,reference:doi.org/10.5860/choice.33-1577,tech:flex,tech:rgb,type:survey},
  annotation = {253 Citations --- DOIs\_of\_references: ["10.1016/0893-6080(89)90041-5","10.1046/j.1365-3156.2001.00822.x","10.1145/192161.192197","10.3233/978-1-61499-286-8-128","10.1001/jamafacial.2013.2127","10.1007/BFb0053007","10.1007/BFb0053005","10.1109/ISMAR-Adjunct57072.2022.00118","10.1016/c2009-0-13851-5","10.3233/978-1-61499-440-4-26","10.1109/IGARSS46834.2022.9883606","10.1162/pres.1995.4.4.403","10.4995/var.2020.13343","10.1145/800250.807503","10.1016/B978-0-12-227748-1.50024-X","10.1142/9789814350938\_0003","10.1007/BFb0053006","10.1109/TASE.2014.2333754","10.1145/159544.159562","10.4324/9781315802152","10.2307/414069","10.1609/aimag.v15i3.1106","10.1098/rsta.2015.0202","10.1007/978-1-4684-2532-1\_19","10.1016/c2009-0-27624-0","10.1145/122488.122499","10.1162/pres.1994.3.3.236","10.1007/978-3-030-68799-1\_14","10.1016/j.jtct.2022.03.007","10.5860/choice.33-1577","10.1007/11739685","10.1109/ICNTE56631.2023.10146675","10.1145/108844.108900","10.1145/229459.229467","10.1038/s41467-022-29881-6","10.1145/146022.146042","10.1016/j.molcel.2008.03.003"] ---},
  note = {\url{https://www.semanticscholar.org/paper/A-Survey-of-Hand-Posture-and-Gesture-Recognition-LaViola/856d4bf0f1f5d4480ce3115d828f34d4b2782e1c}},
  file = {/Users/brk/Zotero/storage/M8XQ3QQL/LaViola - 1999 - A Survey of Hand Posture and Gesture Recognition T.pdf}
}
% == BibTeX quality report for laviolaSurveyHandPosture1999:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  urldate = {2023-11-12},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  note = {\url{https://ieeexplore.ieee.org/document/726791}}
}
% == BibTeX quality report for lecunGradientbasedLearningApplied1998:
% ? unused Library catalog ("IEEE Xplore")

@article{leeDeepLearningBased2020,
  title = {Deep {{Learning Based Real-Time Recognition}} of {{Dynamic Finger Gestures Using}} a {{Data Glove}}},
  author = {Lee, Minhyuk and Bae, Joonbum},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {219923--219933},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3039401},
  urldate = {2023-03-07},
  abstract = {In this article, a real-time dynamic finger gesture recognition using a soft sensor embedded data glove is presented, which measures the metacarpophalangeal (MCP) and proximal interphalangeal (PIP) joint angles of five fingers. In the gesture recognition field, a challenging problem is that of separating meaningful dynamic gestures from a continuous data stream. Unconscious hand motions or sudden tremors, which can easily lead to segmentation ambiguity, makes this problem difficult. Furthermore, the hand shapes and speeds of users differ when performing the same dynamic gesture, and even those made by one user often vary. To solve the problem of separating meaningful dynamic gestures, we propose a deep learning-based gesture spotting algorithm that detects the start/end of a gesture sequence in a continuous data stream. The gesture spotting algorithm takes window data and estimates a scalar value named gesture progress sequence (GPS). GPS is a quantity that represents gesture progress. Moreover, to solve the gesture variation problem, we propose a sequence simplification algorithm and a deep learning-based gesture recognition algorithm. The proposed three algorithms (gesture spotting algorithm, sequence simplification algorithm, and gesture recognition algorithm) are unified into the real-time gesture recognition system and the system was tested with 11 dynamic finger gestures in real-time. The proposed system took only 6 ms to estimate a GPS and no more than 12 ms to recognize the completed gesture in real-time.},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,classes:11,contains-more-references,dataset:custom,fidelity:finger,hardware:custom,has-picture,have-read,model:nn,movement:dynamic,participants:1,read-priority-1,reference:doi.org/10.5120/IJCA2016909103,repetitions:5,segmentation:implicit,tech:flex,type:paper},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.5120/IJCA2016909103"] ---},
  note = {\url{https://ieeexplore.ieee.org/document/9264164/}},
  file = {/Users/brk/Zotero/storage/NRM9LBZL/Lee and Bae - 2020 - Deep Learning Based Real-Time Recognition of Dynam.pdf}
}
% == BibTeX quality report for leeDeepLearningBased2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{leeSmartWearableHand2018,
  title = {Smart {{Wearable Hand Device}} for {{Sign Language Interpretation System With Sensors Fusion}}},
  author = {Lee, Boon Giin and Lee, Su Min},
  year = {2018},
  month = feb,
  journal = {IEEE Sensors Journal},
  volume = {18},
  number = {3},
  pages = {1224--1232},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2017.2779466},
  urldate = {2023-03-07},
  abstract = {Gesturing is an instinctive way of communicating to present a specific meaning or intent. Therefore, research into sign language interpretation using gestures has been explored progressively during recent decades to serve as an auxiliary tool for deaf and mute people to blend into society without barriers. In this paper, a smart sign language interpretation system using a wearable hand device is proposed to meet this purpose. This wearable system utilizes five flex-sensors, two pressure sensors, and a three-axis inertial motion sensor to distinguish the characters in the American sign language alphabet. The entire system mainly consists of three modules: 1) a wearable device with a sensor module; 2) a processing module; and 3) a display unit mobile application module. Sensor data are collected and analyzed using a built-in embedded support vector machine classifier. Subsequently, the recognized alphabet is further transmitted to a mobile device through Bluetooth low energy wireless communication. An Android-based mobile application was developed with a text-to-speech function that converts the received textinto audible voice output. Experiment results indicate that a true sign language recognition accuracy rate of 65.7\% can be achieved on average in the first version without pressure sensors. A second version of the proposed wearable system with the fusion of pressure sensors on the middle finger increased the recognition accuracy rate dramatically to 98.2\%. The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,classes:28,contains-more-references,dataset:custom,fidelity:finger,hardware:custom,have-read,impressive,model:svm,participants:12,read-priority-1,reference:doi.org/10.1007/s11042-013-1368-1,repetitions:20,segmentation:explicit,tech:accelerometer,tech:flex,tech:imu,tech:pressure,type:paper},
  annotation = {102 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1007/s11042-013-1368-1"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/8126796/}},
  file = {/Users/brk/Zotero/storage/CQT634W2/Lee and Lee - 2018 - Smart Wearable Hand Device for Sign Language Inter.pdf}
}
% == BibTeX quality report for leeSmartWearableHand2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Sensors J.")
% ? unused Library catalog ("Semantic Scholar")

@article{liang3DConvolutionalNeural2018,
  title = {{{3D Convolutional Neural Networks}} for {{Dynamic Sign Language Recognition}}},
  author = {Liang, Zhi-jie and Liao, Sheng-bin and Hu, Bing-zhang},
  editor = {Manolopoulos, Yannis},
  year = {2018},
  month = nov,
  journal = {The Computer Journal},
  volume = {61},
  number = {11},
  pages = {1724--1736},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/bxy049},
  urldate = {2023-06-22},
  abstract = {Automatic dynamic sign language recognition is even more challenging than gesture recognition due to the fact that the vocabularies are large and signs are context dependent. Previous works in this direction tend to build classifiers based on complex hand-crafted features computed from the raw inputs. As a type of deep learning model, convolutional neural networks (CNNs) have significantly advanced the accuracy of human gesture classification. However, such methods are currently used to treat video frames as 2D images and recognize gestures at the individual frame level. In this paper, we present a data driven system in which 3D-CNNs are applied to extract spatial and temporal features from video streams, and the motion information is captured by noting the variation in depth between each pair of consecutive frames. To further boost the performance, multi-modal of video streams, including infrared, contour and skeleton are used as input for the architecture and the prediction results estimated from the different sub-networks were fused together. In order to validate our method, we introduce a new challenging multi-modal dynamic sign language dataset captured with Kinect sensors. We evaluate the proposed approach on the collected dataset and achieve superior performance. Moreover, our method achieves a mean Jaccard Index score of 0.836 on the ChaLearn Looking at People Gesture datasets.},
  langid = {english},
  keywords = {app:sign-language,based-on:vision,fidelity:finger,pdf:paywalled,type:paper},
  annotation = {47 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://academic.oup.com/comjnl/article/61/11/1724/4995616}}
}
% == BibTeX quality report for liang3DConvolutionalNeural2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{liangSignLanguageRecognition1996,
  title = {A Sign Language Recognition System Using Hidden Markov Model and Context Sensitive Search},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}} - {{VRST}} '96},
  author = {Liang, Rung-Huei and Ouhyoung, Ming},
  year = {1996},
  pages = {59--66},
  publisher = {{ACM Press}},
  address = {{Hong Kong}},
  doi = {10.1145/3304181.3304194},
  urldate = {2023-07-02},
  abstract = {Hand gesture is one of the most natural and expressive ways for the hearing impaired. However, because of the complexity of dynamic gestures, most researches are focused either on static gestures, postures, or a small set of dynamic gestures. As real-time recognition of a large set of dynamic gestures is considered, some efficient algorithms and models are needed. To solve this problem in Taiwanese Sign Language, a statistics based context sensitive model is presented and both gestures and postures can be successfully recognized. A gesture is decomposed as a sequence of postures and the postures can be quickly recognized using hidden Markov model. With the probability resulted from hidden Markov model and the probability of each gesture in a lexicon, a gesture can be easily recognized in a linguistic way in real-time.},
  isbn = {978-0-89791-825-1},
  langid = {english},
  keywords = {app:sign-language,app:taiwanese-sl,based-on:gloves,classes:50,hardware:vpl-dataglove,have-read,model:hmm,tech:flex,type:paper},
  annotation = {89 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://dl.acm.org/citation.cfm?doid=3304181.3304194}},
  file = {/Users/brk/Zotero/storage/LU3L7VBE/Liang and Ouhyoung - 1996 - A sign language recognition system using hidden ma.pdf}
}
% == BibTeX quality report for liangSignLanguageRecognition1996:
% ? unused Conference name ("the ACM Symposium")
% ? unused Library catalog ("Semantic Scholar")

@article{liHandGestureRecognition2018,
  title = {Hand {{Gesture Recognition}} and {{Real-time Game Control Based}} on {{A Wearable Band}} with 6-Axis {{Sensors}}},
  author = {Li, Yande and Wang, Taiqian and {khan}, Aamir and Li, Lian and Li, Caihong and Yang, Yi and Liu, Li},
  year = {2018},
  month = jul,
  journal = {2018 International Joint Conference on Neural Networks (IJCNN)},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro}},
  doi = {10.1109/IJCNN.2018.8489743},
  urldate = {2023-03-07},
  abstract = {Human-computer interaction introduces critical open door with the proceeds with improvement of wearable gadgets. Gesture recognition through smart devices is becoming a popular research direction. This paper proposes a hand gesture recognition and real-time game control system that is capable of continues human-computer interaction in view of an off-the-rack business wearable wristband. We utilize three-axis accelerator and gyroscope sensors embedded in smart band to collect hand motion information and use Kinect camera capture video information for manual segmentation during the training model phase. A continuous gesture segmentation algorithm based on sliding window and DTW algorithm is developed to detect meaningful gestures in the real-time game control stage. In addition, an android game named Fly Birds which is controlled by gesture recognition result is presented to simulate real-time human-computer interaction. Then, we classify the data in the window using common classifiers. Finally, our experimental results show that, we can accurately identify the designed gestures during the stage of static gesture recognition, and we also achieve a perfect interactive effect in the process of dynamic real-time game control. The experiment outcomes will advance the ascent of human-PC cooperation in view of hand gesture recognition and related applications will rise in vast numbers.},
  isbn = {9781509060146},
  keywords = {app:gaming,based-on:gloves,classes:4,fidelity:arm,hardware:microsoft-band2,have-read,model:dynamic-time-warping,model:knn,participants:4,repetitions:1,segmentation:implicit,tech:accelerometer,tech:imu,type:paper,unimpressive},
  annotation = {7 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/8489743/}},
  file = {/Users/brk/Zotero/storage/GI4FZYAB/Li et al. - 2018 - Hand Gesture Recognition and Real-time Game Contro.pdf}
}
% == BibTeX quality report for liHandGestureRecognition2018:
% ? unused Library catalog ("Semantic Scholar")

@article{liIndoTrackDeviceFreeIndoor2017,
  title = {{{IndoTrack}}: {{Device-Free Indoor Human Tracking}} with {{Commodity Wi-Fi}}},
  shorttitle = {{{IndoTrack}}},
  author = {Li, Xiang and Zhang, Daqing and Lv, Qin and Xiong, Jie and Li, Shengjie and Zhang, Yue and Mei, Hong},
  year = {2017},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {1},
  number = {3},
  pages = {1--22},
  issn = {2474-9567},
  doi = {10.1145/3130940},
  urldate = {2023-07-12},
  abstract = {Indoor human tracking is fundamental to many real-world applications such as security surveillance, behavioral analysis, and elderly care. Previous solutions usually require dedicated device being carried by the human target, which is inconvenient or even infeasible in scenarios such as elderly care and break-ins. However, compared with device-based tracking, device-free tracking is particularly challenging because the much weaker reflection signals are employed for tracking. The problem becomes even more difficult with commodity Wi-Fi devices, which have limited number of antennas, small bandwidth size, and severe hardware noise.             In this work, we propose IndoTrack, a device-free indoor human tracking system that utilizes only commodity Wi-Fi devices. IndoTrack is composed of two innovative methods: (1) Doppler-MUSIC is able to extract accurate Doppler velocity information from noisy Wi-Fi Channel State Information (CSI) samples; and (2) Doppler-AoA is able to determine the absolute trajectory of the target by jointly estimating target velocity and location via probabilistic co-modeling of spatial-temporal Doppler and AoA information. Extensive experiments demonstrate that IndoTrack can achieve a 35cm median error in human trajectory estimation, outperforming the state-of-the-art systems and provide accurate location and velocity information for indoor human mobility and behavioral analysis.},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3130940}}
}
% == BibTeX quality report for liIndoTrackDeviceFreeIndoor2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.")
% ? unused Library catalog ("Semantic Scholar")

@mastersthesis{linnainmaaAlgoritminKumulatiivinenPyoristysvirhe1970,
  title = {{Algoritmin kumulatiivinen py\"oristysvirhe yksitt\"aisten py\"oristysvirheiden Taylor-kehitelm\"an\"a}},
  author = {Linnainmaa, Seppo},
  year = {1970},
  urldate = {2023-11-12},
  langid = {finnish},
  school = {University of Helsinki},
  keywords = {/unread},
  note = {\url{https://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf}}
}
% == BibTeX quality report for linnainmaaAlgoritminKumulatiivinenPyoristysvirhe1970:
% ? unused Type ("Master's")

@article{linnainmaaTaylorExpansionAccumulated1976,
  title = {Taylor Expansion of the Accumulated Rounding Error},
  author = {Linnainmaa, Seppo},
  year = {1976},
  month = jun,
  journal = {BIT},
  volume = {16},
  number = {2},
  pages = {146--160},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01931367},
  urldate = {2023-07-11},
  abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
  langid = {english},
  keywords = {background,backprop},
  annotation = {247 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://link.springer.com/10.1007/BF01931367}}
}
% == BibTeX quality report for linnainmaaTaylorExpansionAccumulated1976:
% ? unused Library catalog ("Semantic Scholar")

@article{liSurvey3DHand2019,
  title = {A Survey on {{3D}} Hand Pose Estimation: {{Cameras}}, Methods, and Datasets},
  shorttitle = {A Survey on {{3D}} Hand Pose Estimation},
  author = {Li, Rui and Liu, Zhenyu and Tan, Jianrong},
  year = {2019},
  month = sep,
  journal = {Pattern Recognition},
  volume = {93},
  pages = {251--272},
  issn = {00313203},
  doi = {10.1016/j.patcog.2019.04.026},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "A survey on 3D hand pose estimation: Cameras, methods, and datasets" by Rui Li et al.},
  langid = {english},
  keywords = {type:survey},
  annotation = {51 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320319301724}}
}
% == BibTeX quality report for liSurvey3DHand2019:
% ? unused Library catalog ("Semantic Scholar")

@article{liuDynamicGestureRecognition2021,
  title = {Dynamic {{Gesture Recognition Algorithm Based}} on {{3D Convolutional Neural Network}}},
  author = {Liu, Yuting and Jiang, Du and Duan, Haojie and Sun, Ying and Li, Gongfa and Tao, Bo and Yun, Juntong and Liu, Ying and Chen, Baojia},
  editor = {Ahmed, Syed Hassan},
  year = {2021},
  month = aug,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2021},
  pages = {1--12},
  issn = {1687-5273, 1687-5265},
  doi = {10.1155/2021/4828102},
  urldate = {2023-03-07},
  abstract = {Gesture recognition is one of the important ways of human-computer interaction, which is mainly detected by visual technology. The temporal and spatial features are extracted by convolution of the video containing gesture. However, compared with the convolution calculation of a single image, multiframe image of dynamic gestures has more computation, more complex feature extraction, and more network parameters, which affects the recognition efficiency and real-time performance of the model. To solve above problems, a dynamic gesture recognition model based on CBAM-C3D is proposed. Key frame extraction technology, multimodal joint training, and network optimization with BN layer are used for making the network performance better. The experiments show that the recognition accuracy of the proposed 3D convolutional neural network combined with attention mechanism reaches 72.4\% on EgoGesture dataset, which is improved greatly compared with the current main dynamic gesture recognition methods, and the effectiveness of the proposed algorithm is verified.},
  langid = {english},
  keywords = {based-on:vision,classes:83,dataset:egogesture,exclude:retracted,have-read,model:cnn,model:nn,tech:rgb,type:paper},
  annotation = {40 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.hindawi.com/journals/cin/2021/4828102/}},
  file = {/Users/brk/Zotero/storage/ELPL8EWG/Liu et al. - 2021 - Dynamic Gesture Recognition Algorithm Based on 3D .pdf}
}
% == BibTeX quality report for liuDynamicGestureRecognition2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{liuHandGestureRecognition2004,
  title = {Hand Gesture Recognition Using Depth Data},
  booktitle = {Sixth {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}}, 2004. {{Proceedings}}.},
  author = {Liu, Xia and Fujimura, K.},
  year = {2004},
  month = may,
  pages = {529--534},
  doi = {10.1109/AFGR.2004.1301587},
  abstract = {A method is presented for recognizing hand gestures by using a sequence of real-time depth image data acquired by an active sensing hardware. Hand posture and motion information extracted from a video is represented in a gesture space which consists of a number of aspects including hand shape, location and motion information. In this space, it is shown to be possible to recognize many types of gestures. Experimental results are shown to validate our approach and characteristics of our approach are discussed.},
  keywords = {based-on:vision,pdf:paywalled,tech:rgb,tech:rgbd,type:paper},
  annotation = {243 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/R7WNTKII/1301587.html}
}
% == BibTeX quality report for liuHandGestureRecognition2004:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("IEEE Xplore")

@article{liuTrackingVitalSigns2015,
  title = {Tracking {{Vital Signs During Sleep Leveraging Off-the-shelf WiFi}}},
  author = {Liu, Jian and Wang, Yan and Chen, Yingying and Yang, Jie and Chen, Xu and Cheng, Jerry},
  year = {2015},
  month = jun,
  journal = {Proceedings of the 16th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {267--276},
  publisher = {{ACM}},
  address = {{Hangzhou China}},
  doi = {10.1145/2746285.2746303},
  urldate = {2023-07-12},
  abstract = {Tracking human vital signs of breathing and heart rates during sleep is important as it can help to assess the general physical health of a person and provide useful clues for diagnosing possible diseases. Traditional approaches (e.g., Polysomnography (PSG)) are limited to clinic usage. Recent radio frequency (RF) based approaches require specialized devices or dedicated wireless sensors and are only able to track breathing rate. In this work, we propose to track the vital signs of both breathing rate and heart rate during sleep by using off-the-shelf WiFi without any wearable or dedicated devices. Our system re-uses existing WiFi network and exploits the fine-grained channel information to capture the minute movements caused by breathing and heart beats. Our system thus has the potential to be widely deployed and perform continuous long-term monitoring. The developed algorithm makes use of the channel information in both time and frequency domain to estimate breathing and heart rates, and it works well when either individual or two persons are in bed. Our extensive experiments demonstrate that our system can accurately capture vital signs during sleep under realistic settings, and achieve comparable or even better performance comparing to traditional and existing approaches, which is a strong indication of providing non-invasive, continuous fine-grained vital signs monitoring without any additional cost.},
  isbn = {9781450334891},
  langid = {english},
  keywords = {app:activity-inference,app:vital-signs,based-on:wifi,tech:wifi,type:paper},
  annotation = {390 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2746285.2746303}}
}
% == BibTeX quality report for liuTrackingVitalSigns2015:
% ? unused Conference name ("MobiHoc'15: The Sixteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing")
% ? unused Library catalog ("Semantic Scholar")

@article{liuUWaveAccelerometerbasedPersonalized2009,
  title = {{{uWave}}: {{Accelerometer-based}} Personalized Gesture Recognition and Its Applications},
  shorttitle = {{{uWave}}},
  author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
  year = {2009},
  month = dec,
  journal = {Pervasive and Mobile Computing},
  volume = {5},
  number = {6},
  pages = {657--675},
  issn = {15741192},
  doi = {10.1016/j.pmcj.2009.07.007},
  urldate = {2023-07-02},
  abstract = {Semantic Scholar extracted view of "uWave: Accelerometer-based personalized gesture recognition and its applications" by Jiayang Liu et al.},
  langid = {english},
  keywords = {based-on:gloves,classes:8,contains-more-references,fidelity:arm,hardware:wiimote,have-read,model:uwave,participants:8,repetitions:1,tech:accelerometer,technique:one-shot,type:paper},
  annotation = {739 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1574119209000674}},
  file = {/Users/brk/Zotero/storage/UMP9M3LQ/Liu et al. - 2009 - uWave Accelerometer-based personalized gesture re.pdf;/Users/brk/Zotero/storage/H47GNEKC/4912759.html}
}
% == BibTeX quality report for liuUWaveAccelerometerbasedPersonalized2009:
% ? unused Library catalog ("Semantic Scholar")

@article{liuWiSleepContactlessSleep2014,
  title = {Wi-{{Sleep}}: {{Contactless Sleep Monitoring}} via {{WiFi Signals}}},
  shorttitle = {Wi-{{Sleep}}},
  author = {Liu, Xuefeng and Cao, Jiannong and Tang, Shaojie and Wen, Jiaqi},
  year = {2014},
  month = dec,
  journal = {2014 IEEE Real-Time Systems Symposium},
  pages = {346--355},
  publisher = {{IEEE}},
  address = {{Rome}},
  doi = {10.1109/RTSS.2014.30},
  urldate = {2023-07-12},
  abstract = {Is it possible to leverage WiFi signals collected in bedrooms to monitor a person's sleep? In this paper, we show that with off-the-shelf WiFi devices, fine-grained sleep information like a person's respiration, sleeping postures and rollovers can be successfully extracted. We do this by introducing Wi-Sleep, the first sleep monitoring system based on WiFi signals. Wi-Sleep adopts off-the-shelf WiFi devices to continuously collect the fine-grained wireless channel state information (CSI) around a person. From the CSI, Wi-Sleep extracts rhythmic patterns associated with respiration and abrupt changes due to the body movement. Compared to existing sleep monitoring systems that usually require special devices attached to human body (i.e. Probes, head belt, and wrist band), Wi-Sleep is completely contact less. In addition, different from many vision-based sleep monitoring systems, Wi-Sleep is robust to low-light environments and does not raise privacy concerns. Preliminary testing results show that the Wi-Sleep can reliably track a person's respiration and sleeping postures in different conditions.},
  isbn = {9781479972883},
  keywords = {app:activity-inference,app:breathing-detection,app:sleep-detection,based-on:wifi,tech:wifi,type:paper},
  annotation = {243 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/7010501/}}
}
% == BibTeX quality report for liuWiSleepContactlessSleep2014:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("2014 IEEE Real-Time Systems Symposium (RTSS)")
% ? unused Library catalog ("Semantic Scholar")

@article{liWhenCSIMeets2016,
  title = {When {{CSI Meets Public WiFi}}: {{Inferring Your Mobile Phone Password}} via {{WiFi Signals}}},
  shorttitle = {When {{CSI Meets Public WiFi}}},
  author = {Li, Mengyuan and Meng, Yan and Liu, Junyi and Zhu, Haojin and Liang, Xiaohui and Liu, Yao and Ruan, Na},
  year = {2016},
  month = oct,
  journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages = {1068--1079},
  publisher = {{ACM}},
  address = {{Vienna Austria}},
  doi = {10.1145/2976749.2978397},
  urldate = {2023-07-12},
  abstract = {In this study, we present WindTalker, a novel and practical keystroke inference framework that allows an attacker to infer the sensitive keystrokes on a mobile device through WiFi-based side-channel information. WindTalker is motivated from the observation that keystrokes on mobile devices will lead to different hand coverage and the finger motions, which will introduce a unique interference to the multi-path signals and can be reflected by the channel state information (CSI). The adversary can exploit the strong correlation between the CSI fluctuation and the keystrokes to infer the user's number input. WindTalker presents a novel approach to collect the target's CSI data by deploying a public WiFi hotspot. Compared with the previous keystroke inference approach, WindTalker neither deploys external devices close to the target device nor compromises the target device. Instead, it utilizes the public WiFi to collect user's CSI data, which is easy-to-deploy and difficult-to-detect. In addition, it jointly analyzes the traffic and the CSI to launch the keystroke inference only for the sensitive period where password entering occurs. WindTalker can be launched without the requirement of visually seeing the smart phone user's input process, backside motion, or installing any malware on the tablet. We implemented Windtalker on several mobile phones and performed a detailed case study to evaluate the practicality of the password inference towards Alipay, the largest mobile payment platform in the world. The evaluation results show that the attacker can recover the key with a high successful rate.},
  isbn = {9781450341394},
  langid = {english},
  keywords = {app:activity-inference,app:password-inference,based-on:wifi,classes:10,fidelity:finger,tech:wifi,type:paper},
  annotation = {159 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2976749.2978397}}
}
% == BibTeX quality report for liWhenCSIMeets2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("CCS'16: 2016 ACM SIGSAC Conference on Computer and Communications Security")
% ? unused Library catalog ("Semantic Scholar")

@article{liWiFingerTalkYour2016,
  title = {{{WiFinger}}: Talk to Your Smart Devices with Finger-Grained Gesture},
  shorttitle = {{{WiFinger}}},
  author = {Li, Hong and Yang, Wei and Wang, Jianxin and Xu, Yang and Huang, Liusheng},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {250--261},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971738},
  urldate = {2023-06-23},
  abstract = {In recent literatures, WiFi signals have been widely used to "sense" people's locations and activities. Researchers have exploited the characteristics of wireless signals to "hear" people's talk and "see" keystrokes by human users. Inspired by the excellent work of relevant scholars, we turn to explore the field of human-computer interaction using finger-grained gestures under WiFi environment. In this paper, we present Wi-Finger - the first solution using ubiquitous wireless signals to achieve number text input in WiFi devices. We implement a prototype of WiFinger on a commercial Wi-Fi infrastructure. Our scheme is based on the key intuition that while performing a certain gesture, the fingers of a user move in a unique formation and direction and thus generate a unique pattern in the time series of Channel State Information (CSI) values. WiFinger is deigned to recognize a set of finger-grained gestures, which are further used to realize continuous text input in off-the-shelf WiFi devices. As the results show, WiFinger achieves up to 90.4\% average classification accuracy for recognizing 9 digits finger-grained gestures from American Sign Language (ASL), and its average accuracy for single individual number text input in desktop reaches 82.67\% within 90 digits.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on:wifi,read-priority-3,tech:wifi,type:paper},
  annotation = {230 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/2971648.2971738}},
  file = {/Users/brk/Zotero/storage/DT2Z2XQS/Li et al. - 2016 - WiFinger talk to your smart devices with finger-g.pdf}
}
% == BibTeX quality report for liWiFingerTalkYour2016:
% ? unused Conference name ("UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing")
% ? unused Library catalog ("Semantic Scholar")

@misc{LucidVR,
  title = {{{LucidVR}}},
  howpublished = {\url{https://github.com/LucidVR/lucidgloves}},
  keywords = {hardware:custom}
}

@article{luOneshotLearningHand2019,
  title = {One-Shot Learning Hand Gesture Recognition Based on Modified 3d Convolutional Neural Networks},
  author = {Lu, Zhi and Qin, Shiyin and Li, Xiaojie and Li, Lianwei and Zhang, Dinghao},
  year = {2019},
  month = oct,
  journal = {Machine Vision and Applications},
  volume = {30},
  number = {7-8},
  pages = {1157--1180},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-019-01043-7},
  urldate = {2023-07-11},
  abstract = {Though deep neural networks have played a very important role in the field of vision-based hand gesture recognition, however, it is challenging to acquire large numbers of annotated samples to support its deep learning or training. Furthermore, in practical applications it often encounters some case with only one single sample for a new gesture class so that conventional recognition method cannot be qualified with a satisfactory classification performance. In this paper, the methodology of transfer learning is employed to build an effective network architecture of one-shot learning so as to deal with such intractable problem. Then some useful knowledge from deep training with big dataset of relative objects can be transferred and utilized to strengthen one-shot learning hand gesture recognition (OSLHGR) rather than to train a network from scratch. According to this idea a well-designed convolutional network architecture with deeper layers, C3D (Tran et al. in: ICCV, pp 4489\textendash 4497, 2015), is modified as an effective tool to extract spatiotemporal feature by deep learning. Then continuous fine-tune training is performed on a sample of new classes to complete one-shot learning. Moreover, the test of classification is carried out by Softmax classifier and geometrical classification based on Euclidean distance. Finally, a series of experiments and tests on two benchmark datasets, VIVA (Vision for Intelligent Vehicles and Applications) and SKIG (Sheffield Kinect Gesture) are conducted to demonstrate its state-of-the-art recognition accuracy of our proposed method. Meanwhile, a special dataset of gestures, BSG, is built using SoftKinetic DS325 for the test of OSLHGR, and a series of test results verify and validate its well classification performance and real-time response speed.},
  langid = {english},
  keywords = {based-on:vision,dataset:skig,dataset:viva,model:cnn,model:nn,pdf:paywalled,repetitions:1,technique:one-shot,type:paper},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://link.springer.com/10.1007/s00138-019-01043-7}}
}
% == BibTeX quality report for luOneshotLearningHand2019:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{madadiOcclusionAwareHand2017,
  title = {Occlusion {{Aware Hand Pose Recovery}} from {{Sequences}} of {{Depth Images}}},
  booktitle = {2017 12th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}} ({{FG}} 2017)},
  author = {Madadi, Meysam and Escalera, Sergio and Carruesco, Alex and Andujar, Carlos and Bar{\'o}, Xavier and Gonz{\`a}lez, Jordi},
  year = {2017},
  month = may,
  pages = {230--237},
  doi = {10.1109/FG.2017.37},
  abstract = {State-of-the-art approaches on hand pose estimation from depth images have reported promising results under quite controlled considerations. In this paper we propose a two-step pipeline for recovering the hand pose from a sequence of depth images. The pipeline has been designed to deal with images taken from any viewpoint and exhibiting a high degree of finger occlusion. In a first step we initialize the hand pose using a part-based model, fitting a set of hand components in the depth images. In a second step we consider temporal data and estimate the parameters of a trained bilinear model consisting of shape and trajectory bases. Results on a synthetic, highly-occluded dataset demonstrate that the proposed method outperforms most recent pose recovering approaches, including those based on CNNs.},
  keywords = {based-on:vision,exclude:not-enough-information,tech:rgb,tech:rgbd,type:paper},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/Y54S29Q8/Madadi et al. - 2017 - Occlusion Aware Hand Pose Recovery from Sequences .pdf;/Users/brk/Zotero/storage/MKR4QCS5/7961746.html}
}
% == BibTeX quality report for madadiOcclusionAwareHand2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("IEEE Xplore")

@article{maHandGestureRecognition2017,
  title = {Hand Gesture Recognition with Convolutional Neural Networks for the Multimodal {{UAV}} Control},
  author = {Ma, Yuntao and Liu, Yuxuan and Jin, Ruiyang and Yuan, Xingyang and Sekha, Raza and Wilson, Samuel and Vaidyanathan, Ravi},
  year = {2017},
  month = oct,
  journal = {2017 Workshop on Research, Education and Development of Unmanned Aerial Systems (RED-UAS)},
  pages = {198--203},
  publisher = {{IEEE}},
  address = {{Linkoping}},
  doi = {10.1109/RED-UAS.2017.8101666},
  urldate = {2023-06-22},
  abstract = {We introduce a robust wearable sensor suite fusing arm motion and hand gesture recognition for operator control of UAVs. The sensor suite fuses mechanomyography (MMG) and an inertial measurement unit (IMU) to capture multi-modal (arm movement and hand gesture) command signals simultaneously. The IMU produces world referenced orientation and acceleration data while concomitant MMG tracks muscle activation through surface vibration. The use of surface muscle vibration for gesture recognition removes the need for electrical contact with the skin, which has impeded other forms of muscle measurement for gesture recognition in the field. This investigation presents hardware design, inertial recognition of arm movement, and the detailed structure of a convolutional neural network (CNN) system used for real-time hand gesture recognition based on MMG signals. The system achieved 94\% accuracy for five gestures with simple calibration for each user, thereby providing an intuitive gesture based UAV control system. To our knowledge this is the first wearable system enabling multimodal control of UAVs through intuitive gestures that does not require electrical skin contact. Future work involves testing the system with larger UAV swarms.},
  isbn = {9781538609392},
  keywords = {/unread,app:robotics,based-on:gloves,model:cnn,model:nn,pdf:paywalled,tech:imu,tech:mechanomyography,type:paper},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/8101666/}}
}
% == BibTeX quality report for maHandGestureRecognition2017:
% ? unused Library catalog ("Semantic Scholar")

@article{makaussovLowCostIMUBasedRealTime2020,
  title = {A {{Low-Cost}}, {{IMU-Based Real-Time On Device Gesture Recognition Glove}}},
  author = {Makaussov, Oleg and Krassavin, Mikhail and Zhabinets, Maxim and Fazli, Siamac},
  year = {2020},
  month = oct,
  journal = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {3346--3351},
  publisher = {{IEEE}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1109/SMC42975.2020.9283231},
  urldate = {2023-03-07},
  abstract = {This paper evaluates the possibility of performing fine gesture recognition including finger movements on a low-tech device. In particular, we present a solution with a recognition model that is small enough to fit in the memory of a low- tech device and describe related difficulties associated with this approach. Several different Machine Learning techniques are employed and their individual advantages and drawbacks are explored for the task at hand. Our results indicate an average of 95\% accuracy during real-time testing for an eight class decoding task with a custom Recurrent Neural Network approach, that runs on the low-tech device, namely an Arduino Nano 33 BLE. The novelty and strength of this research lies in the fact that we are able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures. The recognition process is conducted on the low-tech device and as a result this solution has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.},
  isbn = {9781728185262},
  keywords = {/unread,arduino-nano-ble,based-on:gloves,model:nn,model:rnn,pdf:paywalled,read-priority-1,tech:accelerometer,type:paper},
  annotation = {3 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9283231/}}
}
% == BibTeX quality report for makaussovLowCostIMUBasedRealTime2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{mantyjarviEnablingFastEffortless2004,
  title = {Enabling Fast and Effortless Customisation in Accelerometer Based Gesture Interaction},
  booktitle = {Proceedings of the 3rd International Conference on {{Mobile}} and Ubiquitous Multimedia},
  author = {M{\"a}ntyj{\"a}rvi, Jani and Kela, Juha and Korpip{\"a}{\"a}, Panu and Kallio, Sanna},
  year = {2004},
  month = oct,
  pages = {25--31},
  publisher = {{ACM}},
  address = {{College Park Maryland USA}},
  doi = {10.1145/1052380.1052385},
  urldate = {2023-03-07},
  abstract = {Accelerometer based gesture control is proposed as a complementary interaction modality for handheld devices. Predetermined gesture commands or freely trainable by the user can be used for controlling functions also in other devices. To support versatility of gesture commands in various types of personal device applications gestures should be customisable, easy and quick to train. In this paper we experiment with a procedure for training/recognizing customised accelerometer based gestures with minimum amount of user effort in training. Discrete Hidden Markov Models (HMM) are applied. Recognition results are presented for an external device, a DVD player gesture commands. A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training. For a set of eight gestures, each trained with two original gestures and with two Gaussian noise-distorted duplicates, the average recognition accuracy was 97\%, and with two original gestures and with four noise-distorted duplicates, the average recognition accuracy was 98\%, cross-validated from a total data set of 240 gestures. Use of procedure facilitates quick and effortless customisation in accelerometer based interaction.},
  isbn = {978-1-58113-981-5},
  langid = {english},
  keywords = {based-on:gloves,classes:8,contains-more-references,fidelity:arm,from:cite.bib,hardware:soapbox,have-read,model:hmm,tech:accelerometer,type:paper},
  annotation = {78 citations (Crossref) [2023-07-11] 165 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/1052380.1052385}},
  file = {/Users/brk/Zotero/storage/9GG99UUB/Mäntyjärvi et al. - 2004 - Enabling fast and effortless customisation in acce.pdf}
}
% == BibTeX quality report for mantyjarviEnablingFastEffortless2004:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("MUM04: International Conference on Mobile and Ubiquitous Multimedia")
% ? unused Library catalog ("Semantic Scholar")

@article{mantyjarviGestureInteractionSmall2005,
  title = {Gesture {{Interaction}} for {{Small Handheld Devices}} to {{Support Multimedia Applications}}},
  author = {M{\"a}ntyj{\"a}rvi, Jani and Kallio, S. and Korpip{\"a}{\"a}, Panu and Kela, J. and Plomp, J.},
  year = {2005},
  month = jun,
  journal = {J. Mobile Multimedia},
  urldate = {2023-03-07},
  abstract = {Accelerometer-based gesture control is proposed as a complementary interaction modality for small handheld devices to enable a variety of multimedia applications. The motivation for experimenting with gesture interaction is justified by the personal and public domain prototype applications developed. The challenges related to developing user-dependent and independent gesture control are presented. In this article, we experiment with methods for user-dependent gesture recognition with a low number of training repetitions, and for feasible user-independent gesture recognition from a moderately large set of gestures. The user-dependent gesture recognition performance of the continuous Hidden Markov Model (HMM) is better when compared to discrete HMM with three gesture repetitions in a training set. With continuous HMM, a recognition accuracy level of 95\% is obtained with or without tilt normalization, while for discrete HMM a best recognition accuracy of 90\% is obtained. The user-independent gesture recognition performance with continuous HMM of 89\% is considerably better compared to tests with discrete HMM, when both are obtained with cross-validation from 2,520 gestures. An important result is that the effect of using tilt normalization notably increases the user-independent gesture recognition performance by 10- 15\% depending on the method used. The chosen methods show great potential for gesture-based interaction in multimedia applications.},
  keywords = {based-on:gloves,classes:18,contains-more-references,fidelity:arm,have-read,model:hmm,participants:7,tech:accelerometer,type:paper},
  annotation = {19 Citations},
  note = {\url{https://www.semanticscholar.org/paper/Gesture-Interaction-for-Small-Handheld-Devices-to-M\%C3\%A4ntyj\%C3\%A4rvi-Kallio/4945fca3ffa1fe292e0f61ffc4fd9cc51e9c86f8}},
  file = {/Users/brk/Zotero/storage/9E9Q3KL8/Mäntyjärvi et al. - 2005 - Gesture Interaction for Small Handheld Devices to .pdf}
}
% == BibTeX quality report for mantyjarviGestureInteractionSmall2005:
% ? Possibly abbreviated journal title J. Mobile Multimedia
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{mantyjarviIdentifyingUsersPortable2005,
  title = {Identifying {{Users}} of {{Portable Devices}} from {{Gait Pattern}} with {{Accelerometers}}},
  author = {Mantyjarvi, J. and Lindholm, M. and Vildjiounaite, E. and Makela, S. and Ailisto, H.},
  year = {2005},
  journal = {Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.},
  volume = {2},
  pages = {973--976},
  publisher = {{IEEE}},
  address = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.1109/ICASSP.2005.1415569},
  urldate = {2023-07-12},
  abstract = {Identifying users of portable devices from gait signals acquired with three-dimensional accelerometers was studied. Three approaches, correlation, frequency domain and data distribution statistics, were used. Test subjects (N=36) walked with fast, normal and slow walking speeds in enrolment and test sessions on separate days wearing the accelerometer device on their belt, at back. It was shown to be possible to identify users with this novel gait recognition method. Best equal error rate (EER=7\%) was achieved with the signal correlation method, while the frequency domain method and two variations of the data distribution statistics method produced EER of 10\%, 18\% and 19\%, respectively.},
  isbn = {9780780388741},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,have-read,participants:36,tech:accelerometer},
  annotation = {434 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/1415569/}},
  file = {/Users/brk/Zotero/storage/D2FUHZWJ/Mantyjarvi et al. - 2005 - Identifying Users of Portable Devices from Gait Pa.pdf}
}
% == BibTeX quality report for mantyjarviIdentifyingUsersPortable2005:
% ? Possibly abbreviated journal title Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("(ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.")
% ? unused Library catalog ("Semantic Scholar")

@article{marasovicMotionBasedGestureRecognition2015,
  title = {Motion-{{Based Gesture Recognition Algorithms}} for {{Robot Manipulation}}},
  author = {Marasovi{\'c}, Tea and Papi{\'c}, Vladan and Marasovi{\'c}, Jadranka},
  year = {2015},
  month = may,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {12},
  number = {5},
  pages = {51},
  issn = {1729-8814, 1729-8814},
  doi = {10.5772/60077},
  urldate = {2023-03-07},
  abstract = {The prevailing trend of integrating inertial sensors in consumer electronics devices has inspired research on new forms of human-computer interaction utilizing hand gestures, which may be set-up on mobile devices themselves. At present, motion gesture recognition is intensely studied, with various recognition techniques being employed and tested. This paper provides an in-depth, unbiased comparison of different algorithms used to recognize gestures based primarily on the single 3D accelerometer recordings. The study takes two of the most popular and arguably the best recognition methods currently in use - dynamic time warping and hidden Markov model - and sets them against a relatively novel approach founded on distance metric learning. The three selected algorithms are evaluated in terms of their overall performance, accuracy, training time, execution time and storage efficacy. The optimal algorithm is further implemented in a prototype user application, aimed to serve as an interface for controlling the motion of a toy robot via gestures made with a smartphone.},
  langid = {english},
  keywords = {app:robotics,based-on:gloves,classes:9,dataset:custom,fidelity:arm,hardware:mobile-devices,have-read,model:dynamic-time-warping,model:hmm,movement:dynamic,observations:1890,participants:7,repetitions:30,segmentation:explicit,tech:accelerometer,type:paper},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://journals.sagepub.com/doi/10.5772/60077}},
  file = {/Users/brk/Zotero/storage/H6IC9DRX/Marasović et al. - 2015 - Motion-Based Gesture Recognition Algorithms for Ro.pdf}
}
% == BibTeX quality report for marasovicMotionBasedGestureRecognition2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{marceloGeFightersExperimentGesturebased2006,
  title = {{{GeFighters}}: An {{Experiment}} for {{Gesture-based Interaction Analysis}} in a {{Fighting Game}}},
  author = {Marcelo, Jo{\~a}o and Farias, Thiago and Pessoa, Saulo and Moura, Guilherme and Teichrieb, Veronica},
  year = {2006},
  month = jan,
  abstract = {This paper presents GeFighters, a 3D fighting game that supports gesture based interaction. This application has been used to test and analyze gesture interaction in the context of games that need short and reliable response times. Some inherent aspects of the application have been analyzed, like the impact that interaction causes on frame renderization and response time related to the control of game characters. In order to implement the desired interaction, an input devices management platform, named CIDA, has been used.},
  keywords = {app:gaming,based-on:vision,classes:9,fiducials,from:cite.bib,have-read,tech:rgb,type:paper},
  file = {/Users/brk/Zotero/storage/4SF4MAVE/Marcelo et al. - 2006 - GeFighters an Experiment for Gesture-based Intera.pdf}
}
% == BibTeX quality report for marceloGeFightersExperimentGesturebased2006:
% Missing required field 'journal'

@inproceedings{marcusSensingHumanHand1988,
  title = {Sensing Human Hand Motions for Controlling Dexterous Robots},
  author = {Marcus, B. and Churchill, Philip J.},
  year = {1988},
  month = nov,
  urldate = {2023-07-13},
  abstract = {The Dexterous Hand Master (DHM) system is designed to control dexterous robot hands such as the UTAH/MIT and Stanford/JPL hands. It is the first commercially available device which makes it possible to accurately and confortably track the complex motion of the human finger joints. The DHM is adaptable to a wide variety of human hand sizes and shapes, throughout their full range of motion.},
  keywords = {based-on:gloves,have-read,tech:hall-effect,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Sensing-human-hand-motions-for-controlling-robots-Marcus-Churchill/ca562cee0ce3781c0bc73e0392bc362e5a5f585c}},
  file = {/Users/brk/Zotero/storage/TKRBDT3Q/Marcus and Churchill - 1988 - Sensing human hand motions for controlling dextero.pdf}
}
% == BibTeX quality report for marcusSensingHumanHand1988:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{mardiyantoDevelopmentHandGesture2017,
  title = {Development of Hand Gesture Recognition Sensor Based on Accelerometer and Gyroscope for Controlling Arm of Underwater Remotely Operated Robot},
  author = {Mardiyanto, Ronny and Utomo, Mochamad Fajar Rinaldi and Purwanto, Djoko and Suryoatmojo, Heri},
  year = {2017},
  month = aug,
  journal = {2017 International Seminar on Intelligent Technology and Its Applications (ISITIA)},
  pages = {329--333},
  publisher = {{IEEE}},
  address = {{Surabaya}},
  doi = {10.1109/ISITIA.2017.8124104},
  urldate = {2023-03-07},
  abstract = {Hand Gesture Recognition sensor based on accelerometer and gyroscope is a sensor for capturing the positions of operator hand while controlling underwater remotely operated vehicle equipped with an arm. The proposed system has an advantage in its convenience by means of no training or exercise needed for operator before using it. The key issue here is how beginner operator could use easily the underwater remotely operated robot arm without any specific training. The conventional one uses a joystick for controlling the underwater system and it is inconvenience for beginner user as well as less precision. The proposed system consists of two main part: (1) ground station and (2) underwater remotely operated robot arm. This paper proposes the development of hand gesture recognition sensor used by operator at the ground station for controlling robot arm at the underwater robot. The proposed sensor uses accelerometers and gyroscopes installed in elbow, forearm, and wrist. These devices measure 3D position of each joints for constructing 3D position of hand. We design sensor's casing for its convenience of use by using CAD software. Each sensor is connected by Arduino Nano microcontroller having compact circuit and embedded it into sensor's casing. The sensors are connected to a microcontroller acting as master connected to microcontroller slave (sensor part). These sensors value are converted to 3D position by using forward kinematic. The forward kinematic values are sent to the underwater robot by using a wire utilizing Pulse Position Signal. Then, it converted again to servo's movement by using inverse kinematic. The result is operator able to control the underwater remotely robot arm by utilizing hand gesture directly. The last, operator could control the robot gripper based on flex sensor installed in operator's fingers. The accuracy of the sensor has been tested under laboratory condition, it has 98\% of accuracy.},
  isbn = {9781538627082},
  keywords = {app:robotics,arduino-nano,based-on:gloves,pdf:paywalled,tech:accelerometer,tech:flex,type:paper},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/8124104/}}
}
% == BibTeX quality report for mardiyantoDevelopmentHandGesture2017:
% ? unused Library catalog ("Semantic Scholar")

@misc{martinabadiTensorFlowLargeScaleMachine2015,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {{Mart\'in Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Man\'e} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi\'egas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year = {2015},
  howpublished = {\url{https://www.tensorflow.org/}},
  keywords = {from:cite.bib,type:software-lib}
}
% == BibTeX quality report for martinabadiTensorFlowLargeScaleMachine2015:
% ? Title looks like it was stored in title-case in Zotero

@article{maSignFiSignLanguage2018,
  title = {{{SignFi}}: {{Sign Language Recognition Using WiFi}}},
  shorttitle = {{{SignFi}}},
  author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan and Zhao, Hongyang and Jung, Woosub},
  year = {2018},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {2},
  number = {1},
  pages = {1--21},
  issn = {2474-9567},
  doi = {10.1145/3191755},
  urldate = {2023-07-12},
  abstract = {We propose SignFi to recognize sign language gestures using WiFi. SignFi uses Channel State Information (CSI) measured by WiFi packets as the input and a Convolutional Neural Network (CNN) as the classification algorithm. Existing WiFi-based sign gesture recognition technologies are tested on no more than 25 gestures that only involve hand and/or finger gestures. SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy. SignFi collects CSI measurements to capture wireless signal characteristics of sign gestures. Raw CSI measurements are pre-processed to remove noises and recover CSI changes over sub-carriers and sampling time. Pre-processed CSI measurements are fed to a 9-layer CNN for sign gesture classification. We collect CSI traces and evaluate SignFi in the lab and home environments. There are 8,280 gesture instances, 5,520 from the lab and 2,760 from the home, for 276 sign gestures in total. For 5-fold cross validation using CSI traces of one user, the average recognition accuracy of SignFi is 98.01\%, 98.91\%, and 94.81\% for the lab, home, and lab+home environment, respectively. We also run tests using CSI traces from 5 different users in the lab environment. The average recognition accuracy of SignFi is 86.66\% for 7,500 instances of 150 sign gestures performed by 5 different users.},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,classes:276,model:cnn,model:nn,participants:5,read-priority-1,tech:wifi,type:paper},
  annotation = {24 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3191755}}
}
% == BibTeX quality report for maSignFiSignLanguage2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.")
% ? unused Library catalog ("Semantic Scholar")

@article{maSurveyWiFiBased2016,
  title = {A {{Survey}} on {{Wi-Fi Based Contactless Activity Recognition}}},
  author = {Ma, Junyi and Wang, Hao and Zhang, Daqing and Wang, Yasha and Wang, Yuxiang},
  year = {2016},
  month = jul,
  journal = {2016 Intl IEEE Conferences on Ubiquitous Intelligence \& Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
  pages = {1086--1091},
  publisher = {{IEEE}},
  address = {{Toulouse}},
  doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0170},
  urldate = {2023-08-06},
  abstract = {Providing accurate information about human's state, activity is one of the most important elements in Ubiquitous Computing. Various applications can be enabled if one's state, activity can be recognized. Due to the low deployment cost, non-intrusive sensing nature, Wi-Fi based activity recognition has become a promising, emerging research area. In this paper, we survey the state-of-the-art of the area from four aspects ranging from historical overview, theories, models, key techniques to applications. In addition to the summary about the principles, achievements of existing work, we also highlight some open issues, research directions in this emerging area.},
  isbn = {9781509027712},
  keywords = {based-on:wifi,type:survey,untagged},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-08-06]},
  note = {\url{https://ieeexplore.ieee.org/document/7816966/}},
  file = {/Users/brk/Zotero/storage/LLIRFYDL/Ma et al. - 2016 - A Survey on Wi-Fi Based Contactless Activity Recog.pdf}
}
% == BibTeX quality report for maSurveyWiFiBased2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{materzynskaJesterDatasetLargeScale2019,
  title = {The {{Jester Dataset}}: {{A Large-Scale Video Dataset}} of {{Human Gestures}}},
  shorttitle = {The {{Jester Dataset}}},
  author = {Materzynska, Joanna and Berger, Guillaume and Bax, Ingo and Memisevic, Roland},
  year = {2019},
  month = oct,
  journal = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages = {2874--2882},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCVW.2019.00349},
  urldate = {2023-07-19},
  abstract = {Gesture recognition and its application in human-computer interfaces have been growing increasingly popular in recent years. Although many gestures can be recognized from a single image frame, to build a responsive, accurate system, that can recognize complex gestures with subtle differences between them we need large-scale real-world video datasets. In this work, we introduce the largest collection of short clips of videos of humans performing gestures in front of the camera. The dataset has been collected with the help of over 1300 different actors in their unconstrained environments. Additionally, we present an on-going gesture recognition challenge based on our dataset and the current results. We also describe how a baseline achieving over 93\% recognition accuracy can be obtained with a simple 3D convolutional neural network.},
  isbn = {9781728150239},
  keywords = {based-on:vision,classes:27,dataset:jester,observations:148092,participants:1300,tech,tech:rgb,type:dataset},
  annotation = {127 citations (Semantic Scholar/DOI) [2023-07-19]},
  note = {\url{https://ieeexplore.ieee.org/document/9022297/}},
  file = {/Users/brk/Zotero/storage/3UXRD9PT/Materzynska et al. - 2019 - The Jester Dataset A Large-Scale Video Dataset of.pdf}
}
% == BibTeX quality report for materzynskaJesterDatasetLargeScale2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{matsakisRustLanguage2014,
  title = {The {{Rust}} Language},
  booktitle = {{{ACM SIGAda Ada Letters}}},
  author = {Matsakis, Nicholas D and Klock II, Felix S},
  year = {2014},
  volume = {34},
  pages = {103--104},
  publisher = {{ACM}},
  keywords = {from:cite.bib,type:software-lib}
}
% == BibTeX quality report for matsakisRustLanguage2014:
% ? Unsure about the formatting of the booktitle
% ? unused Issue ("3")

@article{maWiFiSensingChannel2020,
  title = {{{WiFi Sensing}} with {{Channel State Information}}: {{A Survey}}},
  shorttitle = {{{WiFi Sensing}} with {{Channel State Information}}},
  author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan},
  year = {2020},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {3},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3310194},
  urldate = {2023-07-12},
  abstract = {With the high demand for wireless data traffic, WiFi networks have experienced very rapid growth, because they provide high throughput and are easy to deploy. Recently, Channel State Information (CSI) measured by WiFi networks is widely used for different sensing purposes. To get a better understanding of existing WiFi sensing technologies and future WiFi sensing trends, this survey gives a comprehensive review of the signal processing techniques, algorithms, applications, and performance results of WiFi sensing with CSI. Different WiFi sensing algorithms and signal processing techniques have their own advantages and limitations and are suitable for different WiFi sensing applications. The survey groups CSI-based WiFi sensing applications into three categories, detection, recognition, and estimation, depending on whether the outputs are binary/multi-class classifications or numerical values. With the development and deployment of new WiFi technologies, there will be more WiFi sensing opportunities wherein the targets may go beyond from humans to environments, animals, and objects. The survey highlights three challenges for WiFi sensing: robustness and generalization, privacy and security, and coexistence of WiFi sensing and networking. Finally, the survey presents three future WiFi sensing trends, i.e., integrating cross-layer network information, multi-device cooperation, and fusion of different sensors, for enhancing existing WiFi sensing capabilities and enabling new WiFi sensing opportunities.},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,impressive,read-priority-1,survey-finsh:2018,survey-start:2010,tech:wifi,type:survey},
  annotation = {277 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3310194}},
  file = {/Users/brk/Zotero/storage/JIUHXBAJ/Ma et al. - 2020 - WiFi Sensing with Channel State Information A Sur.pdf;/Users/brk/Zotero/storage/UGKERNU8/WiFiSenseSurvey_CSUR19.pdf}
}
% == BibTeX quality report for maWiFiSensingChannel2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("ACM Comput. Surv.")
% ? unused Library catalog ("Semantic Scholar")

@article{mccullochLogicalCalculusIdeas1990,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1990},
  month = jan,
  journal = {Bulletin of Mathematical Biology},
  volume = {52},
  number = {1-2},
  pages = {99--115},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/BF02459570},
  urldate = {2023-06-07},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  langid = {english},
  keywords = {background},
  annotation = {129 Citations},
  note = {\url{http://link.springer.com/10.1007/BF02459570}}
}
% == BibTeX quality report for mccullochLogicalCalculusIdeas1990:
% ? unused Journal abbreviation ("Bltn Mathcal Biology")
% ? unused Library catalog ("Semantic Scholar")

@article{MedicalGalleryBlausen2014,
  title = {Medical Gallery of {{Blausen Medical}} 2014},
  year = {2014},
  journal = {WikiJournal of Medicine},
  volume = {1},
  number = {2},
  issn = {20024436},
  doi = {10.15347/wjm/2014.010},
  urldate = {2023-07-12},
  keywords = {background,medical},
  note = {\url{https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Blausen_Medical_2014}},
  file = {/Users/brk/Zotero/storage/CI57MESF/2014 - Medical gallery of Blausen Medical 2014.pdf}
}
% == BibTeX quality report for MedicalGalleryBlausen2014:
% Missing required field 'author'
% ? unused Journal abbreviation ("Wiki J Med")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{mehdiSignLanguageRecognition2002,
  title = {Sign Language Recognition Using Sensor Gloves},
  author = {Mehdi, S.A. and Khan, Y.N.},
  year = {2002},
  journal = {Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.},
  pages = {2204-2206 vol.5},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICONIP.2002.1201884},
  urldate = {2023-07-02},
  abstract = {This paper examines the possibility of recognizing sign language gestures using sensor gloves. Previously sensor gloves are used in games or in applications with custom gestures. This paper explores their use in Sign Language recognition. This is done by implementing a project called "Talking Hands", and studying the results. The project uses a sensor glove to capture the signs of American Sign Language performed by a user and translates them into sentences of English language. Artificial neural networks are used to recognize the sensor values coming from the sensor glove. These values are then categorized in 24 alphabets of English language and two punctuation symbols introduced by the author. So, mute people can write complete sentences using this application.},
  isbn = {9789810475246},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,claims-to-be-best,classes:26,fidelity:finger,hardware:5dt-dataglove,have-read,model:ffnn,model:nn,movement:static,type:paper},
  annotation = {151 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/1201884/}},
  file = {/Users/brk/Zotero/storage/ID64UHJA/Mehdi and Khan - 2002 - Sign language recognition using sensor gloves.pdf}
}
% == BibTeX quality report for mehdiSignLanguageRecognition2002:
% ? Possibly abbreviated journal title Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.
% ? unused Conference name ("9th International Conference on Neural Information Processing")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{mehraSurveyMulticlassClassification2013,
  title = {Survey on {{Multiclass Classification Methods}}},
  author = {Mehra, Neha and Gupta, Surendra},
  year = {2013},
  urldate = {2023-07-11},
  abstract = {Supervised learning is based on the target value or the desired outputs. Various successful techniques have been proposed to solve the problem in the binary classification case. The multiclass classification case is more delicate one. In this short survey we investigate the various techniques for solving the multiclass classification problem. Various authors and research modified the multiclass classification approach such as one against one, one against all and Directed Acyclic Graph (DAG) which creates many binary classifiers and combines their results to determine the class label of a test pixel. They also describe the various extensible methods that are extended from binary class to solve the multiclass problem and also explain the method in which the classes are arranged into a tree.},
  keywords = {background,classification,multiclass},
  note = {\url{https://www.semanticscholar.org/paper/Survey-on-Multiclass-Classification-Methods-Mehra-Gupta/f0fcf860031b356a3c68b330735634c00e5d7602}},
  file = {/Users/brk/Zotero/storage/FPR3M2AA/Mehra and Gupta - 2013 - Survey on Multiclass Classification Methods.pdf}
}
% == BibTeX quality report for mehraSurveyMulticlassClassification2013:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@misc{mellisGestureRecognitionUsing,
  title = {Gesture {{Recognition Using Accelerometer}} and {{ESP}}},
  author = {{Mellis}},
  howpublished = {\url{https://create.arduino.cc/projecthub/mellis/gesture-recognition-using-accelerometer-and-esp-71faa1}},
  keywords = {hardware:arduino,hardware:esp,tech:accelerometer}
}
% == BibTeX quality report for mellisGestureRecognitionUsing:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{michahialHandGestureRecognition2015,
  title = {Hand Gesture Recognition Using Support Vector Machine},
  author = {Michahial, S. and Azeez, Beebi Hajira and Rani, R.},
  year = {2015},
  urldate = {2023-07-11},
  abstract = {The images contain measurement information of key interest for a variety of research and application areas. On the other hand, computers have become an inseparable part of our society, influencing many aspects of our daily lives in terms of communication and interaction. The main motive is to develop a system that can simplify the way humans interact with Computers. The system is designed using Canny's edge detection for edge detection and Histogram of gradients for feature extraction and the Support Vector Machine (SVM) Classifier which is widely used for classification and regression testing. SVM training algorithm builds a model that predicts whether a new example falls into one category or other. And the classifier learns from the data points in examples when they are classified belonging to their respective categories.},
  keywords = {based-on:vision,classes:20,have-read,model:svm,read-priority-9,tech:rgb,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Hand-gesture-recognition-using-support-vector-Michahial-Azeez/4deb239f4366c461ab092f17828aed9a03b52e8d}},
  file = {/Users/brk/Zotero/storage/JLWRYAFF/Michahial et al. - 2015 - Hand gesture recognition using support vector mach.pdf}
}
% == BibTeX quality report for michahialHandGestureRecognition2015:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{ming-hsuanyangRecognizingHandGesture1999,
  title = {Recognizing Hand Gesture Using Motion Trajectories},
  author = {{Ming-Hsuan Yang} and Ahuja, N.},
  year = {1999},
  journal = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
  pages = {466--472},
  publisher = {{IEEE Comput. Soc}},
  address = {{Fort Collins, CO, USA}},
  doi = {10.1109/CVPR.1999.786979},
  urldate = {2023-07-11},
  abstract = {We present an algorithm for extracting and classifying two-dimensional motion in an image sequence based on motion trajectories. First, a multiscale segmentation is performed to generate homogeneous regions in each frame. Regions between consecutive frames are then matched to obtain 2-view correspondences. Affine transformations are computed from each pair of corresponding regions to define pixel matches. Pixels matches over consecutive images pairs are concatenated to obtain pixel-level motion trajectories across the image sequence. Motion patterns are learned from the extracted trajectories using a time-delay neural network. We apply the proposed method to recognize 40 hand gestures of American Sign Language. Experimental results show that motion patterns in hand gestures can be extracted and recognized with high recognition rate using motion trajectories.},
  isbn = {9780769501499},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:40,fidelity:finger,have-read,model:nn,model:tdnn,tech:rgb,type:paper},
  annotation = {155 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/786979/}},
  file = {/Users/brk/Zotero/storage/3ZUDSQQR/Ming-Hsuan Yang and Ahuja - 1999 - Recognizing hand gesture using motion trajectories.pdf}
}
% == BibTeX quality report for ming-hsuanyangRecognizingHandGesture1999:
% ? Possibly abbreviated journal title Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)
% ? unused Conference name ("Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition")
% ? unused Library catalog ("Semantic Scholar")

@article{mitraGestureRecognitionSurvey2007,
  title = {Gesture {{Recognition}}: {{A Survey}}},
  shorttitle = {Gesture {{Recognition}}},
  author = {Mitra, Sushmita and Acharya, Tinku},
  year = {2007},
  month = may,
  journal = {IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews)},
  volume = {37},
  number = {3},
  pages = {311--324},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2007.893280},
  urldate = {2023-03-07},
  abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
  keywords = {based-on:gloves,based-on:vision,contains-more-references,have-read,model:hmm,reference:doi.org/10.1006/cviu.1998.0716,reference:doi.org/10.1007/3-540-60268-2\_326,reference:doi.org/10.1007/978-0-387-35973-1\_290,reference:doi.org/10.1007/978-3-540-24670-1\_30,reference:doi.org/10.1007/BF00133570,reference:doi.org/10.1007/BFb0015549,reference:doi.org/10.1007/BFb0055712,reference:doi.org/10.1007/s11356-020-10835-8,reference:doi.org/10.1016/0031-3203(92)90007-6,reference:doi.org/10.1016/0165-6074(93)90117-4,reference:doi.org/10.1016/0262-8856(94)90007-8,reference:doi.org/10.1016/S0031-3203(00)00096-0,reference:doi.org/10.1016/S0031-3203(02)00052-3,reference:doi.org/10.1016/S0031-3203(03)00042-6,reference:doi.org/10.1016/S0031-3203(96)00111-2,reference:doi.org/10.1016/S0031-3203(99)00175-2,reference:doi.org/10.1016/S0031-3203(99)00176-4,reference:doi.org/10.1023/B:VISI.0000029666.37597.d3,reference:doi.org/10.1109/34.254061,reference:doi.org/10.1109/34.506414,reference:doi.org/10.1109/34.598231,reference:doi.org/10.1109/34.598232,reference:doi.org/10.1109/34.643892,reference:doi.org/10.1109/34.735811,reference:doi.org/10.1109/34.895976,reference:doi.org/10.1109/34.908962,reference:doi.org/10.1109/5.18626,reference:doi.org/10.1109/5.381842,reference:doi.org/10.1109/72.536309,reference:doi.org/10.1109/AFGR.2000.840667,reference:doi.org/10.1109/CAC57257.2022.10055445,reference:doi.org/10.1109/CVPR.1992.223161,reference:doi.org/10.1109/CVPR.1996.517075,reference:doi.org/10.1109/CVPR.1999.786979,reference:doi.org/10.1109/CVPR.2004.201,reference:doi.org/10.1109/ICCV.1998.710707,reference:doi.org/10.1109/ICME.2001.1237789,reference:doi.org/10.1109/ISCV.1995.477012,reference:doi.org/10.1109/JPROC.2003.823144,reference:doi.org/10.1109/MMDBMS.1998.709526,reference:doi.org/10.1109/NAMW.1997.609859,reference:doi.org/10.1109/TIT.1967.1054010,reference:doi.org/10.1109/TPAMI.1987.4767964,reference:doi.org/10.1109/TPAMI.2004.1265861,reference:doi.org/10.1109/TPAMI.2004.77,reference:doi.org/10.1117/1.2179076,reference:doi.org/10.1117/12.965761,reference:doi.org/10.1162/jocn.1991.3.1.71,reference:doi.org/10.1201/9781420090741.ch2,reference:doi.org/10.1364/JOSAA.2.001160,reference:doi.org/10.2307/1573081,type:survey},
  annotation = {1146 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1364/JOSAA.2.001160","10.1006/cviu.1998.0716","10.1109/TPAMI.2004.77","10.1007/978-3-540-24670-1\_30","10.1109/TIT.1967.1054010","10.1109/5.381842","10.1007/BFb0015549","10.1016/0031-3203(92)90007-6","10.1162/jocn.1991.3.1.71","10.1109/ICCV.1998.710707","10.1109/34.598231","10.1109/CVPR.1992.223161","10.1016/S0031-3203(99)00176-4","10.1109/CAC57257.2022.10055445","10.1109/5.18626","10.1117/1.2179076","10.2307/1573081","10.1109/34.598232","10.1016/0165-6074(93)90117-4","10.1109/CVPR.1996.517075","10.1109/JPROC.2003.823144","10.1109/34.643892","10.1007/BF00133570","10.1016/S0031-3203(96)00111-2","10.1109/ICME.2001.1237789","10.1016/S0031-3203(99)00175-2","10.1109/34.817413","10.1109/34.254061","10.1109/ISCV.1995.477012","10.1109/CVPR.2004.201","10.1109/34.895976","10.1109/AFGR.2000.840667","10.1109/NAMW.1997.609859","10.1109/MMDBMS.1998.709526","10.1007/BFb0055712","10.1007/s11356-020-10835-8","10.1109/34.908962","10.1023/B:VISI.0000029666.37597.d3","10.1109/72.536309","10.1117/12.965761","10.1201/9781420090741.ch2","10.1007/978-0-387-35973-1\_290","10.1007/3-540-60268-2\_326","10.1016/S0031-3203(02)00052-3","10.1109/TPAMI.1987.4767964","10.1109/34.506414","10.1016/0262-8856(94)90007-8","10.1109/TPAMI.2004.1265861"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/4154947/}},
  file = {/Users/brk/Zotero/storage/32EDNKX8/Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf}
}
% == BibTeX quality report for mitraGestureRecognitionSurvey2007:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Syst., Man, Cybern. C")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{moeslundComputerVisionBasedHuman1999,
  title = {Computer {{Vision-Based Human Motion Capture}} - {{A Survey}}},
  author = {Moeslund, T. and Bajers, Fredrik},
  year = {1999},
  urldate = {2023-07-12},
  abstract = {This technical report is the documentation of a survey within computer vision-based human motion capture. The idea with this report is to present previous work in a structural manner using taxonomies at di erent levels. The report is structured in the following way. The rst chapter gives an introduction to new types of interfaces where one class is known as the "Looking at People" domain. Within this class computer vision-based human motion capture is de ned. In chapter two motion capture is described in a larger context describing the di erent sensors used for motion capture. Chapter three presents previous taxonomies which are commented and a new taxonomy is suggested. Chapter four describes a number of assumptions which are used in computer vision-based human motion capture. The next four chapters each give a detailed description of the four classes present in the taxonomy suggested in chapter three. Finally a conclusion is given in the last chapter. Throughout the descriptions of the taxonomy and its four classes a number of examples are given from the literature. Preface This technical report is the documentation of a survey within computer vision-based human motion capture. The writing of the report was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.study titled: "Multiple Cues for Model-Based Human Motion Capture". A large number of papers have been read to write this report. Approximately 2/3 of them (107) are directly concerned with computer vision-based human motion capture. Detailed summaries of these papers can be found in another concurrently written technical report: Summaries of 107 Computer Vision-Based HumanMotion Capture Papers [108]. Whenever I use 'he' throughout the report it should be read as he/she. Finally I would like to thanks Moritz Stoerring for his help editing the report. Aalborg, Denmark, March 1999 Thomas B. Moeslund (tbm@vision.auc.dk)},
  keywords = {based-on:vision,contains-more-references,have-read,survey-finsh:1998,tech:rgb,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/Computer-Vision-Based-Human-Motion-Capture-A-Survey-Moeslund-Bajers/4a59c198bdb7ce6b081f6d9477bdf1e2c4324fba}},
  file = {/Users/brk/Zotero/storage/L3S4NTTE/Moeslund and Bajers - 1999 - Computer Vision-Based Human Motion Capture - A Sur.pdf}
}
% == BibTeX quality report for moeslundComputerVisionBasedHuman1999:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{moeslundSummaries107Computer1999,
  title = {Summaries of 107 {{Computer Vision-Based Human Motion Capture Papers}}},
  author = {Moeslund, T. and Bajers, Fredrik},
  year = {1999},
  urldate = {2023-07-12},
  abstract = {This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. The survey gives a taxonomy for the papers described in this report. The summaries are presented in alphabetic order, with respect to the surname of the rst Author. I have tried to give objective summaries of the di erent papers and only give subjective critique in an item, comments, reserved for this purpose. I have also tried to use the same amount of energy and space on the di erent summaries. But large variations between the length of the di erent summaries can be observed. This is mainly due to the length of the papers, but also due to my interest in the individual papers. Preface This technical report contains summaries of 107 papers concerned with computer vision-based human motion capture. The report can be seen as an appendix to a survey, Computer Vision-Based Human Motion Capture A Survey, which I wrote in parallel to this report. This report has been written as a separate report due to its size alone. The writing of the two reports was carried out in the winter 98/99 while the reading of papers was done during the summer and fall of 1998. The work presented is the rst step in my Ph.D.-study titled: "Multiple Cues for Model-Based Human Motion Capture". Whenever I use 'he' throughout the report is should be read as he/she. Aalborg, Denmark, March 1999. Thomas B. Moeslund (tbm@vision.auc.dk)},
  keywords = {based-on:vision,contains-more-references,have-read,tech:rgb,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/Summaries-of-107-Computer-Vision-Based-Human-Motion-Moeslund-Bajers/4f0e8c2b9fd0d6852b773b405e81aa768e449813}},
  file = {/Users/brk/Zotero/storage/64W2WM2U/Moeslund and Bajers - 1999 - Summaries of 107 Computer Vision-Based Human Motio.pdf}
}
% == BibTeX quality report for moeslundSummaries107Computer1999:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{moeslundSurveyAdvancesVisionbased2006,
  title = {A Survey of Advances in Vision-Based Human Motion Capture and Analysis},
  author = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker},
  year = {2006},
  month = nov,
  journal = {Computer Vision and Image Understanding},
  volume = {104},
  number = {2-3},
  pages = {90--126},
  issn = {10773142},
  doi = {10.1016/j.cviu.2006.08.002},
  urldate = {2023-07-11},
  abstract = {This survey reviews advances in human motion capture and analysis from 2000 to 2006, following a previous survey of papers up to 2000 [247]. Human motion capture continues to be an increasingly active research area in computer vision with over 350 publications over this period. A number of significant research advances are identified together with novel methodologies for automatic initialization, tracking, pose estimation and movement recognition. Recent research has addressed reliable tracking and pose estimation in natural scenes. Progress has also been made towards automatic understanding of human actions and behavior. This survey reviews recent trends in video based human capture and analysis, as well as discussing open problems for future research to achieve automatic visual analysis of human movement.},
  langid = {english},
  keywords = {based-on:vision,contains-more-references,have-read,survey-finsh:2006,survey-start:2000,tech:rgb,type:survey},
  annotation = {2830 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1077314206001263}},
  file = {/Users/brk/Zotero/storage/72PUWBZ4/Moeslund et al. - 2006 - A survey of advances in vision-based human motion .pdf}
}
% == BibTeX quality report for moeslundSurveyAdvancesVisionbased2006:
% ? unused Library catalog ("Semantic Scholar")

@article{moeslundSurveyComputerVisionBased2001,
  title = {A {{Survey}} of {{Computer Vision-Based Human Motion Capture}}},
  author = {Moeslund, Thomas B. and Granum, Erik},
  year = {2001},
  month = mar,
  journal = {Computer Vision and Image Understanding},
  volume = {81},
  number = {3},
  pages = {231--268},
  issn = {10773142},
  doi = {10.1006/cviu.2000.0897},
  urldate = {2023-07-12},
  abstract = {A comprehensive survey of computer vision-based human motion capture literature from the past two decades is presented. The focus is on a general overview based on a taxonomy of system functionalities, broken down into four processes: initialization, tracking, pose estimation, and recognition. Each process is discussed and divided into subprocesses and/or categories of methods to provide a reference to describe and compare the more than 130 publications covered by the survey. References are included throughout the paper to exemplify important issues and their relations to the various methods. A number of general assumptions used in this research field are identified and the character of these assumptions indicates that the research field is still in an early stage of development. To evaluate the state of the art, the major application areas are identified and performances are analyzed in light of the methods presented in the survey. Finally, suggestions for future research directions are offered.},
  langid = {english},
  keywords = {based-on:vision,have-read,survey-finsh:2000,tech:rgb,type:survey},
  annotation = {2025 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S107731420090897X}},
  file = {/Users/brk/Zotero/storage/E36FVISY/Moeslund and Granum - 2001 - A Survey of Computer Vision-Based Human Motion Cap.pdf}
}
% == BibTeX quality report for moeslundSurveyComputerVisionBased2001:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{mohammedDeepLearningBasedEndtoEnd2019,
  title = {A {{Deep Learning-Based End-to-End Composite System}} for {{Hand Detection}} and {{Gesture Recognition}}},
  author = {Mohammed, Adam Ahmed Qaid and Lv, Jiancheng and Islam, Md. Sajjatul},
  year = {2019},
  month = nov,
  journal = {Sensors},
  volume = {19},
  number = {23},
  pages = {5282},
  issn = {1424-8220},
  doi = {10.3390/s19235282},
  urldate = {2023-06-22},
  abstract = {Recent research on hand detection and gesture recognition has attracted increasing interest due to its broad range of potential applications, such as human-computer interaction, sign language recognition, hand action analysis, driver hand behavior monitoring, and virtual reality. In recent years, several approaches have been proposed with the aim of developing a robust algorithm which functions in complex and cluttered environments. Although several researchers have addressed this challenging problem, a robust system is still elusive. Therefore, we propose a deep learning-based architecture to jointly detect and classify hand gestures. In the proposed architecture, the whole image is passed through a one-stage dense object detector to extract hand regions, which, in turn, pass through a lightweight convolutional neural network (CNN) for hand gesture recognition. To evaluate our approach, we conducted extensive experiments on four publicly available datasets for hand detection, including the Oxford, 5-signers, EgoHands, and Indian classical dance (ICD) datasets, along with two hand gesture datasets with different gesture vocabularies for hand gesture recognition, namely, the LaRED and TinyHands datasets. Here, experimental results demonstrate that the proposed architecture is efficient and robust. In addition, it outperforms other approaches in both the hand detection and gesture classification tasks.},
  langid = {english},
  keywords = {based-on:vision,classes:7,classes:81,contains-more-references,dataset:5-signers,dataset:egohands,dataset:indian-classical-dance,dataset:lared,dataset:oxford,dataset:tinyhands,have-read,model:cnn,model:nn,participants:10,participants:40,read-priority-1,tech:rgb,type:paper},
  annotation = {34 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/1424-8220/19/23/5282}},
  file = {/Users/brk/Zotero/storage/V5WL63KJ/Mohammed et al. - 2019 - A Deep Learning-Based End-to-End Composite System .pdf}
}
% == BibTeX quality report for mohammedDeepLearningBasedEndtoEnd2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{mohandesAutomationArabicSign2004,
  title = {Automation of the Arabic Sign Language Recognition},
  author = {Mohandes, M. and {A-Buraiky}, S. and Halawani, T. and {Al-Baiyat}, S.},
  year = {2004},
  journal = {Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.},
  pages = {479--480},
  publisher = {{IEEE}},
  address = {{Damascus, Syria}},
  doi = {10.1109/ICTTA.2004.1307840},
  urldate = {2023-03-07},
  abstract = {This paper introduces a system to recognize the Arabic sign language using an instrumented glove and a machine learning method. Interfaces in sign language systems can be categorized as direct-device or vision-based. The direct-device approach uses measurement devices that are in direct contact with the hand such as instrumented gloves, flexion sensors, styli and position-tracking devices. On the other hand, the vision-based approach captures the movement of the singer's hand using a camera that is sometimes aided by making the signer wear a glove that has painted areas indicating the positions of the fingers or knuckles. The proposed system basically consists of a PowerGlove that is connected through the serial port to a workstation running the support vector machine algorithm. Obtained results are promising even though a simple and cheap glove with limited sensors was utilized.},
  isbn = {9780780384828},
  keywords = {app:arabic-sl,app:sign-language,based-on:gloves,hardware:powerglove,have-read,model:svm,pdf:paywalled,type:paper},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/1307840/}},
  file = {/Users/brk/Zotero/storage/HX626BIS/Mohandes et al. - 2004 - Automation of the arabic sign language recognition.pdf}
}
% == BibTeX quality report for mohandesAutomationArabicSign2004:
% ? Possibly abbreviated journal title Proceedings. 2004 International Conference on Information and Communication Technologies: From Theory to Applications, 2004.
% ? unused Library catalog ("Semantic Scholar")

@article{mohandesRecognitionTwoHandedArabic2013,
  title = {Recognition of {{Two-Handed Arabic Signs Using}} the {{CyberGlove}}},
  author = {Mohandes, Mohamed A.},
  year = {2013},
  month = mar,
  journal = {Arabian Journal for Science and Engineering},
  volume = {38},
  number = {3},
  pages = {669--677},
  issn = {1319-8025, 2191-4281},
  doi = {10.1007/s13369-012-0378-z},
  urldate = {2023-07-13},
  abstract = {Sign language maps letters, words, and expressions of a certain language to a set of hand gestures enabling an individual to communicate by using hands and gestures rather than by speaking. Systems capable of recognizing sign-language symbols can be used for communication with the hearing-impaired. This paper represents the first attempt to recognize two-handed signs from the Unified Arabic Sign Language Dictionary using the CyberGlove and support vector machines (SVMs). 20 samples from each of 100 two-handed signs were collected from two adult signers. Because the signs are of different lengths, time division is used to standardize sign length. The duration of every sign is divided into a specific number of segments, and the mean and standard deviation of each segment are used to represent the signal in the segment. After pre-processing, principal component analysis is used for feature extraction. For recognition, a SVM is trained on 15 samples from each sign. The performance is obtained by testing the trained SVM on the remaining five samples from each sign. A recognition rate of 99.6\% on the testing data is obtained.},
  langid = {english},
  keywords = {app:arabic-sl,app:sign-language,based-on:gloves,classes:100,dataset:custom,fidelity:finger,hardware:cyberglove,have-read,model:svm,participants:1,reference:doi.org/10.1109/3477.485888,reference:doi.org/10.1109/86.413199,reference:doi.org/10.1109/ICOSP.1998.770847,reference:doi.org/10.1145/238218.238310,repetitions:20,type:paper,unimpressive},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-13] --- DOIs\_of\_references: ["10.1109/3477.485888","10.1145/238218.238310","10.1109/38.250916","10.1109/86.413199","10.1109/ICOSP.1998.770847","10.1109/34.908974"] ---},
  note = {\url{http://link.springer.com/10.1007/s13369-012-0378-z}},
  file = {/Users/brk/Zotero/storage/D6I7QCWG/Mohandes - 2013 - Recognition of Two-Handed Arabic Signs Using the C.pdf}
}
% == BibTeX quality report for mohandesRecognitionTwoHandedArabic2013:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Arab J Sci Eng")
% ? unused Library catalog ("Semantic Scholar")

@article{moinWearableBiosensingSystem2020,
  title = {A Wearable Biosensing System with In-Sensor Adaptive Machine Learning for Hand Gesture Recognition},
  author = {Moin, Ali and Zhou, Andy and Rahimi, Abbas and Menon, Alisha and Benatti, Simone and Alexandrov, George and Tamakloe, Senam and Ting, Jonathan and Yamamoto, Natasha and Khan, Yasser and Burghardt, Fred and Benini, Luca and Arias, Ana C. and Rabaey, Jan M.},
  year = {2020},
  month = dec,
  journal = {Nature Electronics},
  volume = {4},
  number = {1},
  pages = {54--63},
  issn = {2520-1131},
  doi = {10.1038/s41928-020-00510-8},
  urldate = {2023-03-07},
  abstract = {Wearable devices that monitor muscle activity based on surface electromyography could be of use in the development of hand gesture recognition applications. Such devices typically use machine-learning models, either locally or externally, for gesture classification. However, most devices with local processing cannot offer training and updating of the machine-learning model during use, resulting in suboptimal performance under practical conditions. Here we report a wearable surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities. Our system implements a neuro-inspired hyperdimensional computing algorithm locally for real-time gesture classification, as well as model training and updating under variable conditions such as different arm positions and sensor replacement. The system can classify 13 hand gestures with 97.12\% accuracy for two participants when training with a single trial per gesture. A high accuracy (92.87\%) is preserved on expanding to 21 gestures, and accuracy is recovered by 9.5\% by implementing model updates in response to varying conditions, without additional computation on an external device. A surface electromyography biosensing system that is based on a screen-printed, conformal electrode array and has in-sensor adaptive learning capabilities can classify human gestures in real time and with high accuracy.},
  langid = {english},
  keywords = {based-on:gloves,classes:13,classes:21,contains-more-references,fidelity:finger,hardware:custom,has-picture,have-read,impressive,model:custom,movement:dynamic,participants:2,read-priority-1,repetitions:1,tech:emg,technique:embedded,technique:one-shot,type:paper},
  annotation = {222 citations (Crossref) [2023-07-02] 0 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.nature.com/articles/s41928-020-00510-8}},
  file = {/Users/brk/Zotero/storage/NMEFHCRT/Moin et al. - 2020 - A wearable biosensing system with in-sensor adapti.pdf}
}
% == BibTeX quality report for moinWearableBiosensingSystem2020:
% ? unused Journal abbreviation ("Nat Electron")
% ? unused Library catalog ("Semantic Scholar")

@article{moniHMMBasedHand2009,
  title = {{{HMM}} Based Hand Gesture Recognition: {{A}} Review on Techniques and Approaches},
  shorttitle = {{{HMM}} Based Hand Gesture Recognition},
  author = {Moni, M. A. and Ali, A. B. M. Shawkat},
  year = {2009},
  journal = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
  pages = {433--437},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/ICCSIT.2009.5234536},
  urldate = {2023-07-02},
  abstract = {Gesture is one of the most natural and expressive ways of communications between human and computer in a virtual reality system. We naturally use various gestures to express our own intentions in everyday life. Hand gesture is one of the important methods of non-verbal communication for human beings for its freer in movements and much more expressive than any other body parts. Hand gesture recognition has a number of potential applications in human-computer interaction, machine vision, virtual reality, machine control in industry, and so on. As a gesture is a continuous motion on a sequential time series, the HMMs (Hidden Markov Models) must be a prominent recognition tool. The most important thing in hand gesture recognition is what the input features are that best represent the characteristics of the moving hand gesture.This paper presents part of literature review on ongoing research and findings on different technique and approaches in gesture recognition using HMMs for vision-based approach.},
  isbn = {9781424445196},
  keywords = {based-on:vision,have-read,model:hmm,read-priority-1,tech:rgb,type:survey},
  annotation = {76 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/5234536/}},
  file = {/Users/brk/Zotero/storage/8F5BHH48/Moni and Ali - 2009 - HMM based hand gesture recognition A review on te.pdf}
}
% == BibTeX quality report for moniHMMBasedHand2009:
% ? unused Library catalog ("Semantic Scholar")

@article{mujahidRealTimeHandGesture2021,
  title = {Real-{{Time Hand Gesture Recognition Based}} on {{Deep Learning YOLOv3 Model}}},
  author = {Mujahid, Abdullah and Awan, Mazhar Javed and Yasin, Awais and Mohammed, Mazin Abed and Dama{\v s}evi{\v c}ius, Robertas and Maskeli{\=u}nas, Rytis and Abdulkareem, Karrar Hameed},
  year = {2021},
  month = may,
  journal = {Applied Sciences},
  volume = {11},
  number = {9},
  pages = {4164},
  issn = {2076-3417},
  doi = {10.3390/app11094164},
  urldate = {2023-03-07},
  abstract = {Using gestures can help people with certain disabilities in communicating with other people. This paper proposes a lightweight model based on YOLO (You Only Look Once) v3 and DarkNet-53 convolutional neural networks for gesture recognition without additional preprocessing, image filtering, and enhancement of images. The proposed model achieved high accuracy even in a complex environment, and it successfully detected gestures even in low-resolution picture mode. The proposed model was evaluated on a labeled dataset of hand gestures in both Pascal VOC and YOLO format. We achieved better results by extracting features from the hand and recognized hand gestures of our proposed YOLOv3 based model with accuracy, precision, recall, and an F-1 score of 97.68, 94.88, 98.66, and 96.70\%, respectively. Further, we compared our model with Single Shot Detector (SSD) and Visual Geometry Group (VGG16), which achieved an accuracy between 82 and 85\%. The trained model can be used for real-time detection, both for static hand images and dynamic gestures recorded on a video.},
  langid = {english},
  keywords = {based-on:vision,classes:5,have-read,model:cnn,model:nn,movement:dynamic,movement:static,tech:rgb,type:paper},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/2076-3417/11/9/4164}},
  file = {/Users/brk/Zotero/storage/VWFYIVN9/Mujahid et al. - 2021 - Real-Time Hand Gesture Recognition Based on Deep L.pdf}
}
% == BibTeX quality report for mujahidRealTimeHandGesture2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{mummadiRealTimeEmbeddedDetection2018,
  title = {Real-{{Time}} and {{Embedded Detection}} of {{Hand Gestures}} with an {{IMU-Based Glove}}},
  author = {Mummadi, Chaithanya and Leo, Frederic and Verma, Keshav and Kasireddy, Shivaji and Scholl, Philipp and Kempfle, Jochen and Laerhoven, Kristof},
  year = {2018},
  month = jun,
  journal = {Informatics},
  volume = {5},
  number = {2},
  pages = {28},
  issn = {2227-9709},
  doi = {10.3390/informatics5020028},
  urldate = {2023-03-07},
  abstract = {This article focuses on the use of data gloves for human-computer interaction concepts, where external sensors cannot always fully observe the user's hand. A good concept hereby allows to intuitively switch the interaction context on demand by using different hand gestures. The recognition of various, possibly complex hand gestures, however, introduces unintentional overhead to the system. Consequently, we present a data glove prototype comprising a glove-embedded gesture classifier utilizing data from Inertial Measurement Units (IMUs) in the fingertips. In an extensive set of experiments with 57 participants, our system was tested with 22 hand gestures, all taken from the French Sign Language (LSF) alphabet. Results show that our system is capable of detecting the LSF alphabet with a mean accuracy score of 92\% and an F1 score of 91\%, using complementary filter with a gyroscope-to-accelerometer ratio of 93\%. Our approach has also been compared to the local fusion algorithm on an IMU motion sensor, showing faster settling times and less delays after gesture changes. Real-time performance of the recognition is shown to occur within 63 milliseconds, allowing fluent use of the gestures via Bluetooth-connected systems.},
  langid = {english},
  keywords = {app:french-sl,app:sign-language,based-on:gloves,classes:22,dataset:custom,fidelity:finger,hardware:custom,have-read,impressive,model:naive-bayes,model:nn,model:random-forests,model:svm,movement:static,participants:57,read-priority-1,reference:doi.org/10.1007/978-3-540-76652-0\_9,reference:doi.org/10.1007/978-3-642-02710-9\_21,reference:doi.org/10.1016/j.neucom.2006.04.016,reference:doi.org/10.1049/icp.2022.0614,reference:doi.org/10.1056/NEJMoa1304839,reference:doi.org/10.1109/38.250916,reference:doi.org/10.1109/AFGR.2008.4813341,reference:doi.org/10.1109/AIM.2014.6878066,reference:doi.org/10.1109/BSN.2011.13,reference:doi.org/10.1109/BSN.2012.17,reference:doi.org/10.1109/ICIEA.2015.7334442,reference:doi.org/10.1109/ICOSP.1998.770847,reference:doi.org/10.1109/IE.2011.51,reference:doi.org/10.1109/ISWC.1999.806717,reference:doi.org/10.1109/SYSOSE.2008.4724149,reference:doi.org/10.1109/THMS.2015.2406692,reference:doi.org/10.1109/TSMCC.2008.923862,reference:doi.org/10.1142/9789814415958\_0095,reference:doi.org/10.1145/29933.275628,reference:doi.org/10.1145/3134230.3134236,repetitions:5,segmentation:explicit,tech:accelerometer,technique:embedded,type:paper},
  annotation = {--- DOIs\_of\_references: ["10.1109/AFGR.2008.4813341","10.1109/BSN.2012.17","10.1109/TSMCC.2008.923862","10.1109/BSN.2011.13","10.1007/978-3-540-76652-0\_9","10.1109/THMS.2015.2406692","10.1145/29933.275628","10.1109/ICIEA.2015.7334442","10.1109/ICICISYS.2010.5658608","10.5555/1953048.2078195","10.1016/j.neucom.2006.04.016","10.1142/9789814415958\_0095","10.1049/icp.2022.0614","10.1056/NEJMoa1304839"] ---},
  note = {\url{http://www.mdpi.com/2227-9709/5/2/28}},
  file = {/Users/brk/Zotero/storage/VFSAQLD7/Mummadi et al. - 2018 - Real-Time and Embedded Detection of Hand Gestures .pdf}
}
% == BibTeX quality report for mummadiRealTimeEmbeddedDetection2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{murakamiGestureRecognitionUsing1991,
  title = {Gesture Recognition Using Recurrent Neural Networks},
  author = {Murakami, Kouichi and Taguchi, Hitomi},
  year = {1991},
  journal = {Proceedings of the SIGCHI conference on Human factors in computing systems Reaching through technology - CHI '91},
  pages = {237--242},
  publisher = {{ACM Press}},
  address = {{New Orleans, Louisiana, United States}},
  doi = {10.1145/108844.108900},
  urldate = {2023-07-02},
  abstract = {A gesture recognition method for Japanese sign language is presented. We have developed a posture recognition system using neural networks which could recognize a finger alphabet of 42 symbols. We then developed a gesture recognition system where each gesture specifies a word. Gesture recognition is more difficult than posture recognition because it has to handle dynamic processes. To deal with dynamic processes we use a recurrent neural network. Here, we describe a gesture recognition method which can recognize continuous gesture. We then discuss the results of our research.},
  isbn = {9780897913836},
  langid = {english},
  keywords = {app:japanese-sl,app:sign-language,based-on:gloves,classes:42,hardware:vpl-dataglove,have-read,model:nn,model:rnn,referenced,tech:flex,type:paper},
  annotation = {405 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=108844.108900}},
  file = {/Users/brk/Zotero/storage/GHGUU73Y/Murakami and Taguchi - 1991 - Gesture recognition using recurrent neural network.pdf}
}
% == BibTeX quality report for murakamiGestureRecognitionUsing1991:
% ? unused Conference name ("the SIGCHI conference")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{muraseGestureKeyboardMachine2012,
  title = {Gesture Keyboard with a Machine Learning Requiring Only One Camera},
  booktitle = {Proceedings of the 3rd {{Augmented Human International Conference}}},
  author = {Murase, Taichi and Moteki, Atsunori and Suzuki, Genta and Nakai, Takahiro and Hara, Nobuyuki and Matsuda, Takahiro},
  year = {2012},
  month = mar,
  pages = {1--2},
  publisher = {{ACM}},
  address = {{Meg\`eve France}},
  doi = {10.1145/2160125.2160154},
  urldate = {2023-07-12},
  abstract = {In this paper, the authors propose a novel gesture-based virtual keyboard (Gesture Keyboard) that uses a standard QWERTY keyboard layout, and requires only one camera, and employs a machine learning technique. Gesture Keyboard tracks the user's fingers and recognizes finger motions to judge keys input in the horizontal direction. Real-Adaboost (Adaptive Boosting), a machine learning technique, uses HOG (Histograms of Oriented Gradients) features in an image of the user's hands to estimate keys in the depth direction. Each virtual key follows a corresponding finger, so it is possible to input characters at the user's preferred hand position even if the user displaces his hands while inputting data. Additionally, because Gesture Keyboard requires only one camera, keyboard-less devices can implement this system easily. We show the effectiveness of utilizing a machine learning technique for estimating depth.},
  isbn = {978-1-4503-1077-2},
  langid = {english},
  keywords = {based-on:vision,model:adaboost,pdf:paywalled,tech:rgb,technique:histogram-oriented-gradients,type:paper},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2160125.2160154}}
}
% == BibTeX quality report for muraseGestureKeyboardMachine2012:
% ? unused Conference name ("AH '12: Augmented Human International Conference")
% ? unused Library catalog ("Semantic Scholar")

@article{myersComparativeStudySeveral1981,
  title = {A Comparative Study of Several Dynamic Time-Warping Algorithms for Connected-Word Recognition},
  author = {Myers, C. S. and Rabiner, L. R.},
  year = {1981},
  journal = {The Bell System Technical Journal},
  volume = {60},
  number = {7},
  pages = {1389--1409},
  doi = {10.1002/j.1538-7305.1981.tb00272.x},
  keywords = {background,from:cite.bib,model:dynamic-time-warping},
  annotation = {274 citations (Crossref) [2023-07-11] 481 citations (Semantic Scholar/DOI) [2023-07-11]}
}

@book{myronw.kruegerArtificialRealityII1991,
  title = {Artificial Reality {{II}}},
  author = {{Myron W. Krueger}},
  year = {1991},
  publisher = {{Addison-Wesley}},
  urldate = {2023-07-18},
  collaborator = {{Internet Archive}},
  isbn = {978-0-201-52260-0},
  langid = {english},
  keywords = {/unread,background,based-on:vision,read-priority-9,Technology and the arts.,type:book,untagged},
  note = {\url{http://archive.org/details/artificialrealit00krue}}
}
% == BibTeX quality report for myronw.kruegerArtificialRealityII1991:
% ? unused Library catalog ("Internet Archive")
% ? unused Number of pages ("322")

@inproceedings{naidooSouthAfricanSign2010,
  title = {South {{African}} Sign Language Recognition Using Feature Vectors and {{Hidden Markov Models}}},
  author = {Naidoo, Nathan Lyle},
  year = {2010},
  urldate = {2023-07-02},
  abstract = {This thesis presents a system for performing whole gesture recognition for South African Sign Language. The system uses feature vectors combined with Hidden Markov models. In order to constuct a feature vector, dynamic segmentation must occur to extract the signer's hand movements. Techniques and methods for normalising variations that occur when recording a signer performing a gesture, are investigated. The system has a classification rate of 69\%.},
  keywords = {based-on:vision,classes:20,have-read,model:hmm,tech:rgb,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/South-African-sign-language-recognition-using-and-Naidoo/be177e5e05b402da3bd53db0ba8445f87596f7d3}},
  file = {/Users/brk/Zotero/storage/JX268W5D/Naidoo - 2010 - South African sign language recognition using feat.pdf}
}
% == BibTeX quality report for naidooSouthAfricanSign2010:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{nairRectifiedLinearUnits2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  month = jun,
  series = {{{ICML}}'10},
  pages = {807--814},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  urldate = {2023-11-12},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  isbn = {978-1-60558-907-7},
  keywords = {/unread}
}
% == BibTeX quality report for nairRectifiedLinearUnits2010:
% ? unused Library catalog ("ACM Digital Library")

@article{nealPatternRecognitionMachine2007,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Neal, Radford M},
  year = {2007},
  month = aug,
  journal = {Technometrics},
  volume = {49},
  number = {3},
  pages = {366--366},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/tech.2007.s518},
  urldate = {2023-05-30},
  abstract = {the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.},
  langid = {english},
  keywords = {background},
  annotation = {9985 citations (Semantic Scholar/DOI) [2023-05-30]},
  note = {\url{http://www.tandfonline.com/doi/abs/10.1198/tech.2007.s518}}
}
% == BibTeX quality report for nealPatternRecognitionMachine2007:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{nelIntegratedSignLanguage2013,
  title = {An Integrated Sign Language Recognition System},
  booktitle = {Proceedings of the {{South African Institute}} for {{Computer Scientists}} and {{Information Technologists Conference}}},
  author = {Nel, Warren and Ghaziasgar, Mehrdad and Connan, James},
  year = {2013},
  month = oct,
  pages = {179--185},
  publisher = {{ACM}},
  address = {{East London South Africa}},
  doi = {10.1145/2513456.2513491},
  urldate = {2023-07-02},
  abstract = {The South African Sign Language research group at the University of the Western Cape has created several systems to recognize Sign Language gestures using single parameters. Research has shown that five parameters are required to recognize any sign language gesture: hand shape, location, orientation and motion, as well as facial expressions. Using a single parameter can cause conflicts in recognition of signs that are similarly signed. This paper pioneers research at the group towards combining multiple parameters to better distinguish between similar signs. This eventually aims to enable the recognition of a large SASL vocabulary. The proposed methodology combines hand location and hand shape recognition into one combined recognition system. The recognition approach is applied to 12 SASL signs that consist of six pairs of signs with the same hand shape performed at two different locations. It is shown that the approach is able to achieve a high average recognition accuracy of 79\% across all signs and distinguish between the signs effectively. It is also shown to be robust to variations in test subjects.},
  isbn = {978-1-4503-2112-9},
  langid = {english},
  keywords = {app:sign-language,app:south-african-sl,based-on:vision,classes:50,fidelity:finger,have-read,type:paper},
  annotation = {3 citations (Crossref) [2023-07-21] some extra data --- DOIs\_of\_references: ["10.1016/S0167-8655(03)00146-6","10.1214/SS/1177010638","10.1016/j.patrec.2006.01.007","10.1145/360881.360919","10.1109/ICPR.2004.1334577","10.1109/TMI.2002.806569","10.21236/ada444419","10.2478/jaiscr-2020-0006","10.1007/978-3-642-33564-8\_35","10.1007/BFb0052992","10.1155/2009/184617","10.1109/AFGR.2004.1301591","10.1109/CRV.2010.10","10.1016/j.patrec.2006.01.007","10.1109/ICASSP.2005.1415381","10.1109/ROBOT.2005.1570335","10.1109/CVPRW.2006.29","10.1109/AFGR.2000.840667","10.1007/978-3-642-38628-2\_10","10.1109/IGARSS.2013.6723376","10.1109/TIT.1975.1055330","10.5244/C.16.75","10.5815/IJIGSP.2012.04.05","10.1109/ICSMC.2004.1400815","10.1109/TSMCA.2006.886347","10.1558/JIRCD.V4I1.71","10.1109/CVPR.2007.383459","10.1109/ROBOT.2008.4543701","10.1109/93.556459","10.1109/CVPR.1996.517075","10.1109/ICIP.2010.5653220","10.1007/BF00116037","10.1109/CVPR.2004.345","10.1145/2513456.2513493","10.1007/978-3-540-72927-3\_4","10.1088/1742-6596/1732/1/012074","10.1109/SIBGRAPI.2006.23","10.1093/DEAFED/ENI001","10.1109/ICIAP.2007.4362788","10.1109/ACV.2002.1182194","10.1109/CVPR.2000.854761","10.1007/978-3-540-69905-7\_43","10.1007/11744085\_9","10.1117/12.526886","10.1145/1389586.1389687","10.1109/RATFG.2001.938906","10.1007/978-3-642-13681-8\_24","10.1023/A:1012450327387","10.1007/s11042-018-6005-6","10.1109/FGR.2006.39","10.1109/ICIAP.1999.797615","10.1109/RATFG.1999.799224","10.1023/A:1022627411411","10.5842/52-0-727","10.3745/JIPS.2009.5.2.041","10.1007/978-3-540-73040-8\_5","10.1145/1961189.1961199","10.1109/AFGR.1998.671007","10.1109/34.982883","10.5244/C.19.50","10.1006/dspr.1999.0361","10.1109/ICCIMA.2007.208","10.1109/CyberC.2011.27","10.23919/BIOSIG.2018.8553251","10.1142/S0218001406005010","10.1023/B:VISI.0000013087.49260.fb","10.1007/11492429\_63","10.1109/ICPR.2004.1333992"] ---},
  note = {\url{https://dl.acm.org/doi/10.1145/2513456.2513491}},
  file = {/Users/brk/Zotero/storage/R5QLW7FJ/Nel et al. - 2013 - An integrated sign language recognition system.pdf}
}
% == BibTeX quality report for nelIntegratedSignLanguage2013:
% ? unused Conference name ("SAICSIT '13: 2013 South African Institute for Computer Scientists and Information Technologists")
% ? unused Library catalog ("Semantic Scholar")

@article{netoHighLevelProgramming2010,
  title = {High-level Programming and Control for Industrial Robotics: Using a Hand-held Accelerometer-based Input Device for Gesture and Posture Recognition},
  shorttitle = {High-level Programming and Control for Industrial Robotics},
  author = {Neto, Pedro and Norberto Pires, J. and Paulo Moreira, A.},
  year = {2010},
  month = mar,
  journal = {Industrial Robot: An International Journal},
  volume = {37},
  number = {2},
  pages = {137--147},
  issn = {0143-991X},
  doi = {10.1108/01439911011018911},
  urldate = {2023-07-03},
  abstract = {Purpose               Most industrial robots are still programmed using the typical teaching process, through the use of the robot teach pendant. This is a tedious and time-consuming task that requires some technical expertise, and hence new approaches to robot programming are required. The purpose of this paper is to present a robotic system that allows users to instruct and program a robot with a high-level of abstraction from the robot language.                                         Design/methodology/approach               The paper presents in detail a robotic system that allows users, especially non-expert programmers, to instruct and program a robot just showing it what it should do, in an intuitive way. This is done using the two most natural human interfaces (gestures and speech), a force control system and several code generation techniques. Special attention will be given to the recognition of gestures, where the data extracted from a motion sensor (three-axis accelerometer) embedded in the Wii remote controller was used to capture human hand behaviours. Gestures (dynamic hand positions) as well as manual postures (static hand positions) are recognized using a statistical approach and artificial neural networks.                                         Findings               It is shown that the robotic system presented is suitable to enable users without programming expertise to rapidly create robot programs. The experimental tests showed that the developed system can be customized for different users and robotic platforms.                                         Research limitations/implications               The proposed system is tested on two different robotic platforms. Since the options adopted are mainly based on standards, it can be implemented with other robot controllers without significant changes. Future work will focus on improving the recognition rate of gestures and continuous gesture recognition.                                         Practical implications               The key contribution of this paper is that it offers a practical method to program robots by means of gestures and speech, improving work efficiency and saving time.                                         Originality/value               This paper presents an alternative to the typical robot teaching process, extending the concept of human-robot interaction and co-worker scenario. Since most companies do not have engineering resources to make changes or add new functionalities to their robotic manufacturing systems, this system constitutes a major advantage for small- to medium-sized enterprises.},
  langid = {english},
  keywords = {app:robotics,based-on:gloves,classes:12,fidelity:arm,hardware:wiimote,have-read,model:ffnn,model:nn,movement:dynamic,movement:static,participants:4,repetitions:100,tech:accelerometer,type:paper,unimpressive},
  annotation = {95 citations (Semantic Scholar/DOI) [2023-07-03]},
  note = {\url{https://www.emerald.com/insight/content/doi/10.1108/01439911011018911/full/html}},
  file = {/Users/brk/Zotero/storage/GM28KZG6/Neto et al. - 2010 - High‐level programming and control for industrial .pdf}
}
% == BibTeX quality report for netoHighLevelProgramming2010:
% ? unused Library catalog ("Semantic Scholar")

@misc{nickgillianGestureRecognitionToolkit,
  title = {Gesture {{Recognition Toolkit}} ({{GRT}})},
  author = {{Nick Gillian}},
  abstract = {The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.},
  howpublished = {\url{https://github.com/nickgillian/grt}},
  keywords = {type:software-lib}
}
% == BibTeX quality report for nickgillianGestureRecognitionToolkit:
% ? Title looks like it was stored in title-case in Zotero

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {background},
  note = {\url{http://neuralnetworksanddeeplearning.com}}
}
% == BibTeX quality report for nielsenNeuralNetworksDeep2015:
% Missing required field 'journal'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("neuralnetworksanddeeplearning.com")

@misc{norvigHowWriteSpelling2007,
  title = {How to {{Write}} a {{Spelling Corrector}}},
  author = {Norvig, Peter},
  year = {2007},
  urldate = {2023-05-25},
  howpublished = {\url{https://norvig.com/spell-correct.html}},
  keywords = {autocorrect,background},
  file = {/Users/brk/Zotero/storage/ERETGY5C/spell-correct.html}
}
% == BibTeX quality report for norvigHowWriteSpelling2007:
% ? Title looks like it was stored in title-case in Zotero

@article{ongAutomaticSignLanguage2005,
  title = {Automatic {{Sign Language Analysis}}: {{A Survey}} and the {{Future}} beyond {{Lexical Meaning}}},
  shorttitle = {Automatic {{Sign Language Analysis}}},
  author = {Ong, S.C.W. and Ranganath, S.},
  year = {2005},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {27},
  number = {6},
  pages = {873--891},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2005.112},
  urldate = {2023-07-02},
  abstract = {Research in automatic analysis of sign language has largely focused on recognizing the lexical (or citation) form of sign gestures as they appear in continuous signing, and developing algorithms that scale well to large vocabularies. However, successful recognition of lexical signs is not sufficient for a full understanding of sign language communication. Nonmanual signals and grammatical processes which result in systematic variations in sign appearance are integral aspects of this communication but have received comparatively little attention in the literature. In this survey, we examine data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures. These are discussed with respect to issues such as modeling transitions between signs in continuous signing, modeling inflectional processes, signer independence, and adaptation. We further examine works that attempt to analyze nonmanual signals and discuss issues related to integrating these with (hand) sign gestures. We also discuss the overall progress toward a true test of sign recognition systems--dealing with natural signing by native signers. We suggest some future directions for this research and also point to contributions it can make to other fields of research. Web-based supplemental materials (appendicies) which contain several illustrative examples and videos of signing can be found at www.computer.org/publications/dlib.},
  langid = {english},
  keywords = {app:sign-language,based-on:gloves,based-on:vision,contains-more-references,have-read,type:survey},
  annotation = {595 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/1432718/}},
  file = {/Users/brk/Zotero/storage/7IUZNIGX/Ong and Ranganath - 2005 - Automatic Sign Language Analysis A Survey and the.pdf}
}
% == BibTeX quality report for ongAutomaticSignLanguage2005:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Machine Intell.")
% ? unused Library catalog ("Semantic Scholar")

@misc{openaiDotaLargeScale2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  month = dec,
  number = {arXiv:1912.06680},
  eprint = {1912.06680},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.06680},
  urldate = {2023-11-12},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1912.06680}},
  keywords = {/unread,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {1279 citations (Semantic Scholar/arXiv) [2023-11-12]},
  file = {/Users/brk/Zotero/storage/TA958HB6/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf;/Users/brk/Zotero/storage/YWMD4S2L/1912.html}
}
% == BibTeX quality report for openaiDotaLargeScale2019:
% ? Title looks like it was stored in title-case in Zotero

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2023-11-12},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/2303.08774}},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {1689 citations (Semantic Scholar/arXiv) [2023-11-12]},
  file = {/Users/brk/Zotero/storage/9AHNFB33/OpenAI - 2023 - GPT-4 Technical Report.pdf;/Users/brk/Zotero/storage/BREWXA7N/2303.html}
}
% == BibTeX quality report for openaiGPT4TechnicalReport2023:
% ? Title looks like it was stored in title-case in Zotero

@article{oudahHandGestureRecognition2020,
  title = {Hand {{Gesture Recognition Based}} on {{Computer Vision}}: {{A Review}} of {{Techniques}}},
  shorttitle = {Hand {{Gesture Recognition Based}} on {{Computer Vision}}},
  author = {Oudah, Munir and {Al-Naji}, Ali and Chahl, Javaan},
  year = {2020},
  month = aug,
  journal = {Journal of Imaging},
  volume = {6},
  number = {8},
  pages = {73},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2313-433X},
  doi = {10.3390/jimaging6080073},
  urldate = {2023-06-23},
  abstract = {Hand gestures are a form of nonverbal communication that can be used in several fields such as communication between deaf-mute people, robot control, human\textendash computer interaction (HCI), home automation and medical applications. Research papers based on hand gestures have adopted many different techniques, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be classified under many headings, such as posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review of the literature on hand gesture techniques and introduces their merits and limitations under different circumstances. In addition, it tabulates the performance of these methods, focusing on computer vision techniques that deal with the similarity and difference points, technique of hand segmentation used, classification algorithms and drawbacks, number and types of gestures, dataset used, detection range (distance) and type of camera used. This paper is a thorough general overview of hand gesture methods with a brief discussion of some possible applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {based-on:vision,tech:rgb,type:survey},
  annotation = {178 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/2313-433X/6/8/73}},
  file = {/Users/brk/Zotero/storage/WREJRICA/Oudah et al. - 2020 - Hand Gesture Recognition Based on Computer Vision.pdf}
}
% == BibTeX quality report for oudahHandGestureRecognition2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("www.mdpi.com")

@article{pageContinuousInspectionSchemes1954,
  title = {Continuous {{Inspection Schemes}}},
  author = {Page, E. S.},
  year = {1954},
  month = jun,
  journal = {Biometrika},
  volume = {41},
  number = {1-2},
  pages = {100--115},
  issn = {0006-3444},
  doi = {10.1093/biomet/41.1-2.100},
  urldate = {2023-05-30},
  keywords = {background,model:cusum},
  annotation = {4816 citations (Semantic Scholar/DOI) [2023-05-30]},
  note = {\url{https://doi.org/10.1093/biomet/41.1-2.100}},
  file = {/Users/brk/Zotero/storage/2CCAB6JV/456627.html;/Users/brk/Zotero/storage/886PZALL/456627.html}
}
% == BibTeX quality report for pageContinuousInspectionSchemes1954:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Silverchair")

@article{pageCumulativeSumCharts1961,
  title = {Cumulative {{Sum Charts}}},
  author = {Page, E. S.},
  year = {1961},
  month = feb,
  journal = {Technometrics},
  volume = {3},
  number = {1},
  pages = {1--9},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1961.10489922},
  urldate = {2023-05-08},
  abstract = {This paper, presented orally to the Gordon Research Conference on Statistics in Chemistry in July 1960, traces the development of process inspection schemes from the original methods of Shewhart to the new charts using cumulative sums, and surveys the present practice in the operation of schemes based on cumulative sums. In spitc of the completely different appearance of the visual records kept for Shewhart and cumulative sum charts, a continuous sequence of development from the one type of scheme to the other can be established. The criteria by which a particular process inspection scheme is chosen are also developed and the practical choice of schemes is described.},
  keywords = {background,model:cusum},
  annotation = {250 citations (Semantic Scholar/DOI) [2023-05-08]},
  note = {\url{https://www.tandfonline.com/doi/abs/10.1080/00401706.1961.10489922}}
}
% == BibTeX quality report for pageCumulativeSumCharts1961:
% ? Title looks like it was stored in title-case in Zotero
% ? unused extra: _eprint ("https://www.tandfonline.com/doi/pdf/10.1080/00401706.1961.10489922")
% ? unused Library catalog ("Taylor and Francis+NEJM")

@article{paoTransformationHumanHand1989,
  title = {Transformation of Human Hand Positions for Robotic Hand Control},
  author = {Pao, L. and Speeter, T.H.},
  year = {1989},
  journal = {Proceedings, 1989 International Conference on Robotics and Automation},
  pages = {1758--1763},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Scottsdale, AZ, USA}},
  doi = {10.1109/ROBOT.1989.100229},
  urldate = {2023-07-13},
  abstract = {A method for using the human hand as a multidegree-of-freedom teaching device is described. The algorithm is based on a functional analysis of the human hand and results in an algebraic transformation of human hand positions to corresponding positions in a target domain. The target domain should be of lower dimensionality (fewer degrees of freedom) than the human hand but is not constrained in any other way. The target described here is a sixteen-degree-of-freedom robotic hand, with four fingers of four joints each. The target need not, however be a handlike device but, for each use, should have a kinematic structure with poses similar in functionality to natural poses of the human hand.{$<<$}ETX{$>>$}},
  isbn = {9780818619380},
  keywords = {based-on:gloves,fidelity:finger,hardware:dexterous-handmaster,have-read,type:paper},
  annotation = {85 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://ieeexplore.ieee.org/document/100229/}},
  file = {/Users/brk/Zotero/storage/A3QUL974/Pao and Speeter - 1989 - Transformation of human hand positions for robotic.pdf}
}
% == BibTeX quality report for paoTransformationHumanHand1989:
% ? unused Library catalog ("Semantic Scholar")

@article{parkAdvancedMachineLearning2019,
  title = {Advanced {{Machine Learning}} for {{Gesture Learning}} and {{Recognition Based}} on {{Intelligent Big Data}} of {{Heterogeneous Sensors}}},
  author = {Park, Jisun and Jin, Yong and Cho, Seoungjae and Sung, Yunsick and Cho, Kyungeun},
  year = {2019},
  month = jul,
  journal = {Symmetry},
  volume = {11},
  number = {7},
  pages = {929},
  issn = {2073-8994},
  doi = {10.3390/sym11070929},
  urldate = {2023-06-26},
  abstract = {With intelligent big data, a variety of gesture-based recognition systems have been developed to enable intuitive interaction by utilizing machine learning algorithms. Realizing a high gesture recognition accuracy is crucial, and current systems learn extensive gestures in advance to augment their recognition accuracies. However, the process of accurately recognizing gestures relies on identifying and editing numerous gestures collected from the actual end users of the system. This final end-user learning component remains troublesome for most existing gesture recognition systems. This paper proposes a method that facilitates end-user gesture learning and recognition by improving the editing process applied on intelligent big data, which is collected through end-user gestures. The proposed method realizes the recognition of more complex and precise gestures by merging gestures collected from multiple sensors and processing them as a single gesture. To evaluate the proposed method, it was used in a shadow puppet performance that could interact with on-screen animations. An average gesture recognition rate of 90\% was achieved in the experimental evaluation, demonstrating the efficacy and intuitiveness of the proposed method for editing visualized learning gestures.},
  langid = {english},
  keywords = {type:survey},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://www.mdpi.com/2073-8994/11/7/929}},
  file = {/Users/brk/Zotero/storage/ALFHQ8X4/Park et al. - 2019 - Advanced Machine Learning for Gesture Learning and.pdf}
}
% == BibTeX quality report for parkAdvancedMachineLearning2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{parsaniSingleAccelerometerBased2009,
  title = {A {{Single Accelerometer}} Based {{Wireless Embedded System}} for {{Predefined Dynamic Gesture Recognition}}},
  booktitle = {Proceedings of the {{First International Conference}} on {{Intelligent Human Computer Interaction}}},
  author = {Parsani, Rahul and Singh, Karandeep},
  editor = {Tiwary, U. S. and Siddiqui, Tanveer J. and Radhakrishna, M. and Tiwari, M. D.},
  year = {2009},
  pages = {195--201},
  publisher = {{Springer India}},
  address = {{New Delhi}},
  doi = {10.1007/978-81-8489-203-1_18},
  urldate = {2023-03-07},
  abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction. A complete embedded system which facilitates the data acquisition, analysis, recognition, and the transmission wirelessly, of human dynamic gestures to a computer, is described. An intuitive algorithm for processing the accelerometer data was implemented and tested. This method permits all the analysis to be done by the embedded system processor. The system is capable of recognizing gestures involving a combination of straight line motions in three dimensions. These gestures are then used to control a host computer which executes tasks based on the gesture received. A sample application showing how the gestures can be mapped to the English alphabet is also shown.},
  isbn = {978-81-8489-404-2 978-81-8489-203-1},
  langid = {english},
  keywords = {based-on:gloves,have-read,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {4 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-81-8489-203-1_18}}
}
% == BibTeX quality report for parsaniSingleAccelerometerBased2009:
% ? unused Library catalog ("Semantic Scholar")

@article{patilHandwritingRecognitionFree2016,
  title = {Handwriting {{Recognition}} in {{Free Space Using WIMU-Based Hand Motion Analysis}}},
  author = {Patil, Shashidhar and Kim, Dubeom and Park, Seongsill and Chai, Youngho},
  year = {2016},
  journal = {Journal of Sensors},
  volume = {2016},
  pages = {1--10},
  issn = {1687-725X, 1687-7268},
  doi = {10.1155/2016/3692876},
  urldate = {2023-07-11},
  abstract = {We present a wireless-inertial-measurement-unit- (WIMU-) based hand motion analysis technique for handwriting recognition in three-dimensional (3D) space. The proposed handwriting recognition system is not bounded by any limitations or constraints; users have the freedom and flexibility to write characters in free space. It uses hand motion analysis to segment hand motion data from a WIMU device that incorporates magnetic, angular rate, and gravity sensors (MARG) and a sensor fusion algorithm to automatically distinguish segments that represent handwriting from nonhandwriting data in continuous hand motion data. Dynamic time warping (DTW) recognition algorithm is used to recognize handwriting in real-time. We demonstrate that a user can freely write in air using an intuitive WIMU as an input and hand motion analysis device to recognize the handwriting in 3D space. The experimental results for recognizing handwriting in free space show that the proposed method is effective and efficient for other natural interaction techniques, such as in computer games and real-time hand gesture recognition applications.},
  langid = {english},
  keywords = {app:gaming,app:writing,based-on:gloves,classes:36,dataset:custom,fidelity:arm,have-read,model:dynamic-time-warping,movement:dynamic,observations:2000,participants:20,repetitions:5,segmentation:explicit,tech:imu,type:paper},
  annotation = {21 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://www.hindawi.com/journals/js/2016/3692876/}},
  file = {/Users/brk/Zotero/storage/T68RSGXI/Patil et al. - 2016 - Handwriting Recognition in Free Space Using WIMU-B.pdf}
}
% == BibTeX quality report for patilHandwritingRecognitionFree2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{patilMarathiSignLanguage2022,
  title = {Marathi {{Sign Language Hand Gesture Recognition Using Accelerometer}} and {{3D Printed Gloves}}},
  author = {Patil, Sachin and Joshi, Sarang and Kulkarni, Hrushikesh B. and Hagawane, Pradnesh and Shinde, Pradnya},
  year = {2022},
  month = dec,
  journal = {2022 14th International Conference on Computational Intelligence and Communication Networks (CICN)},
  pages = {72--77},
  publisher = {{IEEE}},
  address = {{Al-Khobar, Saudi Arabia}},
  doi = {10.1109/CICN56167.2022.10008290},
  urldate = {2023-03-07},
  abstract = {Due to the communication abilities impairment, deaf \& dumb peoples are more or less isolated from mainstream societal activities. NGOs, social workers \& Governments in India are putting efforts with multiple initiatives to increase the involvement of these peoples in the mainstream activities happening in the rest of world. This paper proposes to establish an alternative method to streamline the communications among normal \& speech impaired persons by using smart hand glove system equipped with 3D accelerometer modules at the fingertips. The proposed system is delivering the measurement results in very proficient way. The experiments were carried out over 15 persons using the sign gestures, providing good co-relation between the Posed gestures versus system mapped gestures.},
  isbn = {9781665487719},
  keywords = {/unread,app:marathi-sl,app:sign-language,based-on:gloves,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/10008290/}}
}
% == BibTeX quality report for patilMarathiSignLanguage2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{pavlovicVisualInterpretationHand1997,
  title = {Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review},
  shorttitle = {Visual Interpretation of Hand Gestures for Human-Computer Interaction},
  author = {Pavlovic, Vladimir I. and Sharma, R. and Huang, T.S.},
  year = {1997},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {19},
  number = {7},
  pages = {677--695},
  issn = {01628828},
  doi = {10.1109/34.598226},
  urldate = {2023-06-23},
  abstract = {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3Dmodel of the human hand or an image appearancemodel of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient ``purposive'' approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of humancomputer interaction},
  keywords = {based-on:vision,contains-more-references,have-read,type:survey},
  annotation = {1979 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/598226/}},
  file = {/Users/brk/Zotero/storage/7SQ99MKT/Pavlovic et al. - 1997 - Visual interpretation of hand gestures for human-c.pdf}
}
% == BibTeX quality report for pavlovicVisualInterpretationHand1997:
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Machine Intell.")
% ? unused Library catalog ("Semantic Scholar")

@article{pengyuhongGestureModelingRecognition2000,
  title = {Gesture Modeling and Recognition Using Finite State Machines},
  author = {{Pengyu Hong} and Turk, M. and Huang, T.S.},
  year = {2000},
  journal = {Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
  pages = {410--415},
  publisher = {{IEEE Comput. Soc}},
  address = {{Grenoble, France}},
  doi = {10.1109/AFGR.2000.840667},
  urldate = {2023-07-11},
  abstract = {We propose a state-based approach to gesture learning and recognition. Using spatial clustering and temporal alignment, each gesture is defined to be an ordered sequence of states in spatial-temporal space. The 2D image positions of the centers of the head and both hands of the user are used as features; these are located by a color-based tracking method. From training data of a given gesture, we first learn the spatial information and then group the data into segments that are automatically aligned temporally. The temporal information is further integrated to build a finite state machine (FSM) recognizer. Each gesture has a FSM corresponding to it. The computational efficiency of the FSM recognizers allows us to achieve real-time on-line performance. We apply this technique to build an experimental system that plays a game of "Simon Says" with the user.},
  isbn = {9780769505800},
  keywords = {based-on:vision,classes:4,have-read,model:finite-state-machine,tech:rgb,type:paper},
  annotation = {276 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/840667/}},
  file = {/Users/brk/Zotero/storage/G3CD967K/Pengyu Hong et al. - 2000 - Gesture modeling and recognition using finite stat.pdf}
}
% == BibTeX quality report for pengyuhongGestureModelingRecognition2000:
% ? Possibly abbreviated journal title Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)
% ? unused Conference name ("Fourth International Conference on Automatic Face and Gesture Recognition")
% ? unused Library catalog ("Semantic Scholar")

@article{plouffeStaticDynamicHand2016,
  title = {Static and {{Dynamic Hand Gesture Recognition}} in {{Depth Data Using Dynamic Time Warping}}},
  author = {Plouffe, Guillaume and Cretu, Ana-Maria},
  year = {2016},
  month = feb,
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {65},
  number = {2},
  pages = {305--316},
  issn = {0018-9456, 1557-9662},
  doi = {10.1109/TIM.2015.2498560},
  urldate = {2023-07-20},
  note = {\url{http://ieeexplore.ieee.org/document/7332940/}}
}
% == BibTeX quality report for plouffeStaticDynamicHand2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Instrum. Meas.")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{prekopcsakAccelerometerBasedRealTime2008,
  title = {Accelerometer {{Based Real-Time Gesture Recognition}}},
  author = {Prekopcs{\'a}k, Zolt{\'a}n},
  year = {2008},
  urldate = {2023-03-07},
  abstract = {Gesture is a natural expression form for humans, but its recognition is a similarly hard problem as speech recognition. In this paper, I present a real-time hand gesture recognition system, which identifies relevant parts in the continous sensor data stream, and classifies them to the most probable gesture. Instead of the usual button-based segmentation, I have created an automatic segmentation method, which makes the interface more natural. The results showed that the two different classifiers reach 97.4\% and 96\% accuracy on a personalized gesture set, and these results can be improved for certain gesture sets with the combination of the two algorithms. Furthermore, the system has great performance and low response time, so the user experience is much better than with previous gesture recognizers.},
  keywords = {based-on:gloves,classes:10,fidelity:arm,fidelity:hand,hardware:mobile-devices,have-read,model:hmm,model:svm,participants:4,repetitions:10,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Accelerometer-Based-Real-Time-Gesture-Recognition-Prekopcs\%C3\%A1k/4e0b17d35696db6aa1cd0ea3565b47ff317e3ad3}},
  file = {/Users/brk/Zotero/storage/Q44WBJN9/Prekopcsák - 2008 - Accelerometer Based Real-Time Gesture Recognition.pdf}
}
% == BibTeX quality report for prekopcsakAccelerometerBasedRealTime2008:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{premaratneAustralianSignLanguage2013,
  title = {Australian {{Sign Language Recognition Using Moment Invariants}}},
  booktitle = {Intelligent {{Computing Theories}} and {{Technology}}},
  author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Huang, De-Shuang and Jo, Kang-Hyun and Zhou, Yong-Quan and Han, Kyungsook},
  year = {2013},
  volume = {7996},
  pages = {509--514},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39482-9_59},
  urldate = {2023-03-07},
  abstract = {Human Computer Interaction is geared towards seamless human machine integration without the need for LCDs, Keyboards or Gloves. Systems have already been developed to react to limited hand gestures especially in gaming and in consumer electronics control. Yet, it is a monumental task in bridging the well-developed sign languages in different parts of the world with a machine to interpret the meaning. One reason is the sheer extent of the vocabulary used in sign language and the sequence of gestures needed to communicate different words and phrases. Auslan the Australian Sign Language is comprised of numbers, finger spelling for words used in common practice and a medical dictionary. There are 7415 words listed in Auslan website. This research article tries to implement recognition of numerals using a computer using the static hand gesture recognition system developed for consumer electronics control at the University of Wollongong in Australia. The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures. The system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
  isbn = {978-3-642-39481-2 978-3-642-39482-9},
  keywords = {app:sign-language,based-on:gloves,pdf:paywalled,type:paper},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-3-642-39482-9_59}}
}
% == BibTeX quality report for premaratneAustralianSignLanguage2013:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{puWholehomeGestureRecognition2013,
  title = {Whole-Home Gesture Recognition Using Wireless Signals},
  author = {Pu, Qifan and Gupta, Sidhant and Gollakota, Shyamnath and Patel, Shwetak},
  year = {2013},
  journal = {Proceedings of the 19th annual international conference on Mobile computing \& networking - MobiCom '13},
  pages = {27},
  publisher = {{ACM Press}},
  address = {{Miami, Florida, USA}},
  doi = {10.1145/2500423.2500436},
  urldate = {2023-06-23},
  abstract = {This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two- bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94\%.},
  isbn = {9781450319997},
  langid = {english},
  keywords = {based-on:wifi,tech:wifi,type:paper},
  annotation = {1010 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://dl.acm.org/citation.cfm?doid=2500423.2500436}},
  file = {/Users/brk/Zotero/storage/DRDJ9SJ8/Pu et al. - 2013 - Whole-home gesture recognition using wireless sign.pdf}
}
% == BibTeX quality report for puWholehomeGestureRecognition2013:
% ? unused Conference name ("the 19th annual international conference")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{pylvanainenAccelerometerBasedGesture2005,
  title = {Accelerometer {{Based Gesture Recognition Using Continuous HMMs}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {Pylv{\"a}n{\"a}inen, Timo},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Marques, Jorge S. and {P{\'e}rez de la Blanca}, Nicol{\'a}s and Pina, Pedro},
  year = {2005},
  volume = {3522},
  pages = {639--646},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11492429_77},
  urldate = {2023-03-07},
  abstract = {This paper presents a gesture recognition system based on continuous hidden Markov models. Gestures here are hand movements which are recorded by a 3D accelerometer embedded in a handheld device. In addition to standard hidden Markov model classifier, the recognition system has a preprocessing step which removes the effect of device orientation from the data. The performance of the recognizer is evaluated in both user dependent and user independent cases. The effects of sample resolution and sampling rate are studied in the user dependent case.},
  isbn = {978-3-540-26153-7 978-3-540-32237-5},
  keywords = {based-on:gloves,have-read,model:hmm,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {150 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/11492429_77}}
}
% == BibTeX quality report for pylvanainenAccelerometerBasedGesture2005:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{qaroushSmartComfortableWearable2021,
  title = {Smart, Comfortable Wearable System for Recognizing {{Arabic Sign Language}} in Real-Time Using {{IMUs}} and Features-Based Fusion},
  author = {Qaroush, Aziz and Yassin, Sara and {Al-Nubani}, Ali and Alqam, Ameer},
  year = {2021},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {184},
  pages = {115448},
  issn = {09574174},
  doi = {10.1016/j.eswa.2021.115448},
  urldate = {2023-03-07},
  abstract = {Semantic Scholar extracted view of "Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion" by Aziz Qaroush et al.},
  langid = {english},
  keywords = {/unread,app:sign-language,based-on:gloves,pdf:paywalled,tech:imu,type:paper},
  annotation = {2 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0957417421008629}}
}
% == BibTeX quality report for qaroushSmartComfortableWearable2021:
% ? unused Library catalog ("Semantic Scholar")

@article{qianWidar2PassiveHuman2018,
  title = {Widar2.0: {{Passive Human Tracking}} with a {{Single Wi-Fi Link}}},
  shorttitle = {Widar2.0},
  author = {Qian, Kun and Wu, Chenshu and Zhang, Yi and Zhang, Guidong and Yang, Zheng and Liu, Yunhao},
  year = {2018},
  month = jun,
  journal = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
  pages = {350--361},
  publisher = {{ACM}},
  address = {{Munich Germany}},
  doi = {10.1145/3210240.3210314},
  urldate = {2023-07-12},
  abstract = {This paper presents Widar2.0, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices. Previous works based on either specialized or commercial hardware all require multiple links, preventing their wide adoption in scenarios like homes where typically only one single AP is installed. The key insight underlying Widar2.0 to circumvent the use of multiple links is to leverage multi-dimensional signal parameters from one single link. To this end, we build a unified model accounting for Angle-of-Arrival, Time-of-Flight, and Doppler shifts together and devise an efficient algorithm for their joint estimation. We then design a pipeline to translate the erroneous raw parameters into precise locations, which first finds parameters corresponding to the reflections of interests, then refines range estimates, and ultimately outputs target locations. Our implementation and evaluation on commodity WiFi devices demonstrate that Widar2.0 achieves better or comparable performance to state-of-the-art localization systems, which either use specialized hardwares or require 2 to 40 Wi-Fi links.},
  isbn = {9781450357203},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,claims-to-be-best,tech:wifi,type:paper},
  annotation = {246 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3210240.3210314}}
}
% == BibTeX quality report for qianWidar2PassiveHuman2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("MobiSys '18: The 16th Annual International Conference on Mobile Systems, Applications, and Services")
% ? unused Library catalog ("Semantic Scholar")

@article{qianWidarDecimeterLevelPassive2017,
  title = {Widar: {{Decimeter-Level Passive Tracking}} via {{Velocity Monitoring}} with {{Commodity Wi-Fi}}},
  shorttitle = {Widar},
  author = {Qian, Kun and Wu, Chenshu and Yang, Zheng and Liu, Yunhao and Jamieson, Kyle},
  year = {2017},
  month = jul,
  journal = {Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Chennai India}},
  doi = {10.1145/3084041.3084067},
  urldate = {2023-07-12},
  abstract = {Various pioneering approaches have been proposed for Wi-Fi-based sensing, which usually employ learning-based techniques to seek appropriate statistical features, yet do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we propose Widar, a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and the user's location and velocity. On this basis, we propose novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity. We implement Widar on commercial Wi-Fi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 25 cm given initial positions and 38 cm without them and a median relative velocity error of 13\%.},
  isbn = {9781450349123},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {215 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3084041.3084067}}
}
% == BibTeX quality report for qianWidarDecimeterLevelPassive2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("Mobihoc '17: The Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing")
% ? unused Library catalog ("Semantic Scholar")

@article{qiMultiSensorGuidedHand2021,
  title = {Multi-{{Sensor Guided Hand Gesture Recognition}} for a {{Teleoperated Robot Using}} a {{Recurrent Neural Network}}},
  author = {Qi, Wen and Ovur, Salih Ertug and Li, Zhijun and Marzullo, Aldo and Song, Rong},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  pages = {6039--6045},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3089999},
  urldate = {2023-03-07},
  abstract = {Touch-free guided hand gesture recognition for human-robot interactions plays an increasingly significant role in teleoperated surgical robot systems. Indeed, despite depth cameras provide more practical information for recognition accuracy enhancement, the instability and computational burden of depth data represent a tricky problem. In this letter, we propose a novel multi-sensor guided hand gesture recognition system for surgical robot teleoperation. A multi-sensor data fusion model is designed for performing interference in the presence of occlusions. A multilayer Recurrent Neural Network (RNN) consisting of a Long Short-Term Memory (LSTM) module and a dropout layer (LSTM-RNN) is proposed for multiple hand gestures classification. Detected hand gestures are used to perform a set of human-robot collaboration tasks on a surgical robot platform. Classification performance and prediction time is compared among the LSTM-RNN model and several traditional Machine Learning (ML) algorithms, such as k-Nearest Neighbor (k-NN) and Support Vector Machines (SVM). Results show that the proposed LSTM-RNN classifier is able to achieve a higher recognition rate and faster inference speed. In addition, the present adaptive data fusion system shows a strong anti-interference capability for hand gesture recognition in real-time.},
  keywords = {app:robotics,app:surgery,based-on:vision,classes:11,have-read,model:knn,model:lstm,model:nn,model:rnn,model:svm,movement:dynamic,tech:radar,tech:rgbd,type:paper},
  annotation = {62 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9457168/}},
  file = {/Users/brk/Zotero/storage/8MHHF5ZG/Qi et al. - 2021 - Multi-Sensor Guided Hand Gesture Recognition for a.pdf}
}
% == BibTeX quality report for qiMultiSensorGuidedHand2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Robot. Autom. Lett.")
% ? unused Library catalog ("Semantic Scholar")

@article{quamExperimentalDeterminationHuman1989,
  title = {An {{Experimental Determination}} of {{Human Hand Accuracy}} with a {{DataGlove}}},
  author = {Quam, David L. and Williams, George B. and Agnew, Jeffery R. and Browne, Patricia C.},
  year = {1989},
  month = oct,
  journal = {Proceedings of the Human Factors Society Annual Meeting},
  volume = {33},
  number = {5},
  pages = {315--319},
  issn = {0163-5182},
  doi = {10.1177/154193128903300517},
  urldate = {2023-07-14},
  abstract = {A three-part experiment was conducted to determine the accuracy, repeatability and linearity of a human hand manipulating the DataGlove. Accuracy and repeatability of finger flexure were investigated with repeated measurements of three calibration positions. Linearity of finger flexure was investigated with steady finger and thumb curling motions. Accuracy and repeatability of hand location and orientation were investigated with repeated measurements of six hand positions. Finger flexure mean accuracy was 6\textordmasculine{} for the four fingers and 11\textordmasculine{} for the thumb, repeatability was 3\textordmasculine{} for the four fingers and 9\textordmasculine{} for the thumb, and linearity varied from 2 to 5\textordmasculine. Although the mean location accuracy was 1 inch and the mean orientation accuracy was 17\textordmasculine, the position and orientation receiver was observed to twist on the glove back. Across all subjects, the location repeatability was 0.5 inch, while the orientation repeatability was 9\textordmasculine. However, the within-subject location repeatability was 0.13 inches, while the orientation repeatability was 2\textordmasculine.},
  langid = {english},
  keywords = {based-on:gloves,hardware:vpl-dataglove,have-read,pdf:paywalled,read-priority-5,type:paper},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-14]},
  note = {\url{http://journals.sagepub.com/doi/10.1177/154193128903300517}}
}
% == BibTeX quality report for quamExperimentalDeterminationHuman1989:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{quinlanC4ProgramsMachine1992,
  title = {C4.5: {{Programs}} for {{Machine Learning}}},
  author = {Quinlan, J. Ross},
  year = {1992},
  keywords = {background,from:cite.bib,model:c4.5}
}
% == BibTeX quality report for quinlanC4ProgramsMachine1992:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  year = {Feb./1989},
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {00189219},
  doi = {10.1109/5.18626},
  urldate = {2023-11-13},
  abstract = {The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting.},
  keywords = {/unread},
  note = {\url{http://ieeexplore.ieee.org/document/18626/}}
}
% == BibTeX quality report for rabinerTutorialHiddenMarkov1989:
% ? unused Journal abbreviation ("Proc. IEEE")
% ? unused Library catalog ("Semantic Scholar")

@article{ramamoorthyRecognitionDynamicHand2003,
  title = {Recognition of Dynamic Hand Gestures},
  author = {Ramamoorthy, Aditya and Vaswani, Namrata and Chaudhury, Santanu and Banerjee, Subhashis},
  year = {2003},
  month = sep,
  journal = {Pattern Recognition},
  volume = {36},
  number = {9},
  pages = {2069--2081},
  issn = {00313203},
  doi = {10.1016/S0031-3203(03)00042-6},
  urldate = {2023-07-11},
  abstract = {This paper is concerned with the problem of recognition of dynamic hand gestures. We have considered gestures which are sequences of distinct hand poses. In these gestures hand poses can undergo motion and discrete changes. However, continuous deformations of the hand shapes are not permitted. We have developed a recognition engine which can reliably recognize these gestures despite individual variations. The engine also has the ability to detect start and end of gesture sequences in an automated fashion. The recognition strategy uses a combination of static shape recognition (performed using contour discriminant analysis), Kalman filter based hand tracking and a HMM based temporal characterization scheme. The system is fairly robust to background clutter and uses skin color for static shape recognition and tracking. A real time implementation on standard hardware is developed. Experimental results establish the effectiveness of the approach.},
  langid = {english},
  keywords = {app:robotics,based-on:vision,classes:3,have-read,model:hmm,movement:dynamic,tech:rgb,type:paper},
  annotation = {169 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320303000426}},
  file = {/Users/brk/Zotero/storage/C424ZSCP/Ramamoorthy et al. - 2003 - Recognition of dynamic hand gestures.pdf}
}
% == BibTeX quality report for ramamoorthyRecognitionDynamicHand2003:
% ? unused Library catalog ("Semantic Scholar")

@article{rashidWearableTechnologiesHand2019,
  title = {Wearable Technologies for Hand Joints Monitoring for Rehabilitation: {{A}} Survey},
  shorttitle = {Wearable Technologies for Hand Joints Monitoring for Rehabilitation},
  author = {Rashid, Adnan and Hasan, Osman},
  year = {2019},
  month = jun,
  journal = {Microelectronics Journal},
  volume = {88},
  pages = {173--183},
  issn = {00262692},
  doi = {10.1016/j.mejo.2018.01.014},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "Wearable technologies for hand joints monitoring for rehabilitation: A survey" by Adnan Rashid et al.},
  langid = {english},
  keywords = {/unread,app:medical,based-on:gloves,type:survey},
  annotation = {58 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0026269217305207}}
}
% == BibTeX quality report for rashidWearableTechnologiesHand2019:
% ? unused Library catalog ("Semantic Scholar")

@article{rautarayVisionBasedHand2015,
  title = {Vision Based Hand Gesture Recognition for Human Computer Interaction: A Survey},
  shorttitle = {Vision Based Hand Gesture Recognition for Human Computer Interaction},
  author = {Rautaray, Siddharth S. and Agrawal, Anupam},
  year = {2015},
  month = jan,
  journal = {Artificial Intelligence Review},
  volume = {43},
  number = {1},
  pages = {1--54},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-012-9356-9},
  urldate = {2023-06-26},
  abstract = {As computers become more pervasive in society, facilitating natural human\textendash computer interaction (HCI) will have a positive impact on their use. Hence, there has been growing interest in the development of new approaches and technologies for bridging the human\textendash computer barrier. The ultimate aim is to bring HCI to a regime where interactions with computers will be as natural as an interaction between humans, and to this end, incorporating gestures in HCI is an important research area. Gestures have long been considered as an interaction technique that can potentially deliver more natural, creative and intuitive methods for communicating with our computers. This paper provides an analysis of comparative surveys done in this area. The use of hand gestures as a natural interface serves as a motivating force for research in gesture taxonomies, its representations and recognition techniques, software platforms and frameworks which is discussed briefly in this paper. It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition. Different application which employs hand gestures for efficient interaction has been discussed under core and advanced application domains. This paper also provides an analysis of existing literature related to gesture recognition systems for human computer interaction by categorizing it under different key parameters. It further discusses the advances that are needed to further improvise the present hand gesture recognition systems for future perspective that can be widely used for efficient human computer interaction. The main goal of this survey is to provide researchers in the field of gesture based HCI with a summary of progress achieved to date and to help identify areas where further research is needed.},
  langid = {english},
  keywords = {based-on:vision,have-read,reference:doi.org/10.1007/0-8176-4481-4,reference:doi.org/10.1007/s41870-022-00950-9,reference:doi.org/10.1017/rep.2021.34,reference:doi.org/10.1017/S0022226700004643,reference:doi.org/10.1109/EURCON.2007.4400657,reference:doi.org/10.1109/FG.2013.6553694,reference:doi.org/10.1109/ICME.2000.871479,reference:doi.org/10.1109/WACV56688.2023.00575,reference:doi.org/10.1136/annrheumdis-2022-eular.1741,reference:doi.org/10.1145/306774,reference:doi.org/10.1145/3411764.3445076,reference:doi.org/10.1145/3489849,reference:doi.org/10.1145/3491102,reference:doi.org/10.1145/571985,reference:doi.org/10.1145/97924,reference:doi.org/10.1162/coli.2000.26.2.277,reference:doi.org/10.1201/9781420064995,reference:doi.org/10.14236/EWIC/HCI2018.9,reference:doi.org/10.14358/PERS.84.11.733,reference:doi.org/10.18782/2320-7051.4046,reference:doi.org/10.25073/2588-1086/VNUCSCE.236,type:survey},
  annotation = {1303 Citations --- DOIs\_of\_references: ["10.1162/coli.2000.26.2.277","10.1017/S0022226700004643","10.1109/ICME.2000.871479","10.1109/FG.2013.6553694","10.1145/3491102","10.1007/s41870-022-00950-9","10.1145/306774","10.1145/306774","10.1109/WACV56688.2023.00575","10.1109/EURCON.2007.4400657","10.1007/0-8176-4481-4","10.1145/3411764.3445076","10.25073/2588-1086/VNUCSCE.236","10.1145/97924","10.14236/EWIC/HCI2018.9","10.1145/571985","10.1201/9781420064995","10.1017/rep.2021.34","10.1109/FG.2013.6553694"] ---},
  note = {\url{http://link.springer.com/10.1007/s10462-012-9356-9}},
  file = {/Users/brk/Zotero/storage/78UXQTNQ/Rautaray and Agrawal - 2015 - Vision based hand gesture recognition for human co.pdf}
}
% == BibTeX quality report for rautarayVisionBasedHand2015:
% ? unused Journal abbreviation ("Artif Intell Rev")
% ? unused Library catalog ("Semantic Scholar")

@article{raysarkarHandGestureRecognition2013,
  title = {Hand {{Gesture Recognition Systems}}: {{A Survey}}},
  shorttitle = {Hand {{Gesture Recognition Systems}}},
  author = {RaySarkar, Arpita and Sanyal, G. and Majumder, S.},
  year = {2013},
  month = jun,
  journal = {International Journal of Computer Applications},
  volume = {71},
  number = {15},
  pages = {25--37},
  issn = {09758887},
  doi = {10.5120/12435-9123},
  urldate = {2023-07-12},
  abstract = {Gesture was the first mode of communication for the primitive cave men. Later on human civilization has developed the verbal communication very well. But still nonverbal communication has not lost its weightage. Such non \textendash{} verbal communication are being used not only for the physically challenged people, but also for different applications in diversified areas, such as aviation, surveying, music direction etc. It is the best method to interact with the computer without using other peripheral devices, such as keyboard, mouse. Researchers around the world are actively engaged in development of robust and efficient gesture recognition system, more specially, hand gesture recognition system for various applications. The major steps associated with the hand gesture recognition system are; data acquisition, gesture modeling, feature extraction and hand gesture recognition. There are several sub-steps and methodologies associated with the above steps. Different researchers have followed different algorithm or sometimes have devised their own algorithm. The current research work reviews the work carried out in last twenty years and a brief comparison has been performed to analyze the difficulties encountered by these systems, as well as the limitation. Finally the desired characteristics of a robust and efficient hand gesture recognition system have been described. General Terms Hand gesture recognition, comparison},
  keywords = {based-on:gloves,based-on:vision,contains-more-references,have-read,reference:doi.org/10.1016/j.cviu.2006.10.012,reference:doi.org/10.1016/j.engappai.2009.03.008,reference:doi.org/10.1016/j.imavis.2011.11.003,reference:doi.org/10.1016/S0031-3203(00)00096-0,reference:doi.org/10.1109/34.598226,reference:doi.org/10.1109/COGINF.2006.365596,reference:doi.org/10.1109/ICED.2008.4786633,reference:doi.org/10.1109/ICIINFS.2011.6038061,reference:doi.org/10.1109/ICONIP.2002.1199054,reference:doi.org/10.1109/ICPR.2002.1044576,reference:doi.org/10.1109/ICSESS.2012.6269439,reference:doi.org/10.1109/ICSIPA.2011.6144163,reference:doi.org/10.1109/ICSMC.1993.390870,reference:doi.org/10.1109/ICSMC.1997.637364,reference:doi.org/10.1109/ICSMC.1999.812376,reference:doi.org/10.1109/icss.2013.40,reference:doi.org/10.1109/IFOST.2012.6357626,reference:doi.org/10.1109/ISACC.2015.7377319,reference:doi.org/10.1109/ISSPIT.2007.4458209,reference:doi.org/10.1109/ISSPIT.2010.5711749,reference:doi.org/10.1109/MFI.2012.6343032,reference:doi.org/10.1109/RTEICT.2016.7808052,reference:doi.org/10.1109/SIU.2006.1659822,reference:doi.org/10.1109/SPCOM.2010.5560535,reference:doi.org/10.1109/TGRS.2021.3122332,reference:doi.org/10.1109/TIM.2011.2161140,reference:doi.org/10.1117/12.2666559,reference:doi.org/10.1145/2166966.2166983,reference:doi.org/10.2478/v10006-012-0033-6,reference:doi.org/10.5120/6349-8695,reference:doi.org/10.5121/IJU.2012.3103,reference:doi.org/10.5281/ZENODO.42817,reference:doi.org/10.7763/IJET.2012.V4.427,tech:accelerometer,tech:rgb,tech:rgbd,type:survey},
  annotation = {69 citations (Semantic Scholar/DOI) [2023-07-12] --- DOIs\_of\_references: ["10.1109/icss.2013.40","10.5281/ZENODO.42817","10.1016/J.CVIU.2006.08.002","10.1109/SPCOM.2010.5560535","10.1016/j.engappai.2009.03.008","10.1109/TIM.2011.2161140","10.1016/j.imavis.2011.11.003","10.1109/SIU.2006.1659822","10.1109/ICONIP.2002.1199054","10.1109/ICPR.2008.4761080","10.1109/ICIINFS.2011.6038061","10.5121/IJAIA.2012.3412","10.1109/FUZZ.2002.1006741","10.1109/RTEICT.2016.7808052","10.2478/v10006-012-0033-6","10.1109/ISACC.2015.7377319","10.1117/12.2666559","10.1007/s00500-020-04803-0","10.1109/MFI.2012.6343032","10.1109/IADCC.2010.5423024","10.1109/34.598226","10.1145/2166966.2166983","10.1109/COGINF.2006.365596","10.1016/j.cviu.2006.10.012","10.5121/IJU.2012.3103"] ---},
  note = {\url{http://research.ijcaonline.org/volume71/number15/pxc3889123.pdf}},
  file = {/Users/brk/Zotero/storage/T734BQCQ/RaySarkar et al. - 2013 - Hand Gesture Recognition Systems A Survey.pdf}
}
% == BibTeX quality report for raysarkarHandGestureRecognition2013:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJCA")
% ? unused Library catalog ("Semantic Scholar")

@article{rekimotoGestureWristGesturePadUnobtrusive2001,
  title = {{{GestureWrist}} and {{GesturePad}}: Unobtrusive Wearable Interaction Devices},
  shorttitle = {{{GestureWrist}} and {{GesturePad}}},
  author = {Rekimoto, Jun},
  year = {2001},
  journal = {Proceedings Fifth International Symposium on Wearable Computers},
  pages = {21--27},
  publisher = {{IEEE Comput. Soc}},
  address = {{Zurich, Switzerland}},
  doi = {10.1109/ISWC.2001.962092},
  urldate = {2023-06-23},
  abstract = {In this paper we introduce two input devices for wearable computers, called GestureWrist and GesturePad. Both devices allow users to interact with wearable or nearby computers by using gesture-based commands. Both are designed to be as unobtrusive as possible, so they can be used under various social contexts. The first device, called GestureWrist, is a wristband-type input device that recognizes hand gestures and forearm movements. Unlike DataGloves or other hand gesture-input devices, all sensing elements are embedded in a normal wristband. The second device, called GesturePad, is a sensing module that can be attached on the inside of clothes, and users can interact with this module from the outside. It transforms conventional clothes into an interactive device without changing their appearance.},
  isbn = {9780769513188},
  keywords = {based-on:gloves,classes:6,have-read,tech:accelerometer,tech:capacitive,type:paper},
  annotation = {450 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/962092/}},
  file = {/Users/brk/Zotero/storage/CAUJY4HG/Rekimoto - 2001 - GestureWrist and GesturePad unobtrusive wearable .pdf}
}
% == BibTeX quality report for rekimotoGestureWristGesturePadUnobtrusive2001:
% ? unused Conference name ("Fifth International Symposium on Wearable Computers")
% ? unused Library catalog ("Semantic Scholar")

@article{rempelEffectWristPosture2008,
  title = {Effect of Wrist Posture on Carpal Tunnel Pressure While Typing},
  author = {Rempel, David M. and Keir, Peter J. and Bach, Joel M.},
  year = {2008},
  journal = {Journal of Orthopaedic Research},
  volume = {26},
  number = {9},
  pages = {1269--1273},
  doi = {10.1002/jor.20599},
  keywords = {background,carpal tunnel syndrome,from:cite.bib,keyboard,medical,neuropathy,occupation,overuse},
  annotation = {62 citations (Crossref) [2023-07-11]},
  note = {\url{https://onlinelibrary.wiley.com/doi/abs/10.1002/jor.20599}}
}
% == BibTeX quality report for rempelEffectWristPosture2008:
% ? unused DOI ("https://doi.org/10.1002/jor.20599")
% ? unused extra: _eprint ("https://onlinelibrary.wiley.com/doi/pdf/10.1002/jor.20599")

@inproceedings{rigollHighPerformanceRealtime1998,
  title = {High Performance Real-Time Gesture Recognition Using {{Hidden Markov Models}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {Rigoll, Gerhard and Kosmala, Andreas and Eickeler, Stefan},
  editor = {Wachsmuth, Ipke and Fr{\"o}hlich, Martin},
  year = {1998},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {69--80},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052990},
  abstract = {An advanced real-time system for gesture recognition is presented, which is able to recognize complex dynamic gestures, such as ''hand waving'', ''spin'', ''pointing'', and ''head moving''. The recognition is based on global motion features, extracted from each difference image of the image sequence. The system uses Hidden Markov Models (HMMs) as statistical classifier. These HMMs are trained on a database of 24 isolated gestures, performed by 14 different people. With the use of global motion features, a recognition rate of 92.9\% is achieved for a person and background independent recognition.},
  isbn = {978-3-540-69782-4},
  langid = {english},
  keywords = {based-on:vision,classes:24,from:cite.bib,have-read,model:hmm,participants:14,tech:rgb,type:paper},
  annotation = {27 citations (Crossref) [2023-07-11] 122 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/BBCJSZZM/Rigoll et al. - 1998 - High performance real-time gesture recognition usi.pdf}
}
% == BibTeX quality report for rigollHighPerformanceRealtime1998:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Springer Link")

@article{rigollNewImprovedFeature1997,
  title = {New Improved Feature Extraction Methods for Real-Time High Performance Image Sequence Recognition},
  author = {Rigoll, G. and Kosmala, A.},
  year = {1997},
  journal = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  volume = {4},
  pages = {2901--2904},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Munich, Germany}},
  doi = {10.1109/ICASSP.1997.595396},
  urldate = {2023-07-11},
  abstract = {This paper describes new feature extraction methods which can be used very effectively in combination with statistical methods for image sequence recognition. Although these feature extraction methods can be used for a wide variety of image sequence processing applications, the target application presented in this paper is gesture recognition. The novel feature extraction methods have been integrated into an HMM-based gesture recognition system and led to substantial improvements for this system. It turned out that the new features are not only able to describe the gesture characteristics much better than the old features, but additionally they also led to a dramatic reduction in dimensionality of the feature vector used for representing each frame of the image sequence. This resulted in the fact that it was possible to use the novel features in combination with a new architecture for statistical image sequence recognition. The result of this investigation is a high performance gesture recognition system with significantly improved recognition rates and real-time capabilities.},
  isbn = {9780818679193},
  keywords = {model:hmm},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/595396/}}
}
% == BibTeX quality report for rigollNewImprovedFeature1997:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{riveraRecognitionHumanHand2017,
  title = {Recognition of {{Human Hand Activities Based}} on a {{Single Wrist IMU Using Recurrent Neural Networks}}},
  booktitle = {International {{Journal}} of {{Pharma Medicine}} and {{Biological Sciences}}},
  author = {Rivera, Patricio and Valarezo, Edwin and Choi, Mun-Taek and Kim, Tae-Seong},
  year = {2017},
  volume = {6},
  pages = {114--118},
  issn = {22785221},
  doi = {10.18178/ijpmbs.6.4.114-118},
  urldate = {2023-07-11},
  abstract = {Recognition of hand activities could provide new information towards daily human activity logging and gesture interface applications. However, there is a technical challenge due to delicate hand motions and complex movement contexts. In this work, we proposed hand activity recognition (HAR) based on a single inertial measurement unit (IMU) sensor at one wrist via deep learning recurrent neural network. The proposed HAR works directly with signals from a tri-axial accelerometer, gyroscope, and magnetometer sensors within one IMU. We evaluated the performance of our HAR with a public human hand activity database for six hand activities including Open Door, Close Door, Open Fridge, Close Fridge, Clean Table and Drink from Cup. Our results show an overall recognition accuracy of 80.09\% with discrete standard epochs and 74.92\% with noise-added epochs. With continuous time series epochs, the accuracy of 71.75\% was obtained. },
  keywords = {app:ambiguous,based-on:gloves,classes:6,contains-more-references,dataset:opportunity,fidelity:arm,have-read,model:nn,model:rnn,observations:nan,participants:4,reference:doi.org/10.1007/11682127\_8,reference:doi.org/10.1016/j.patrec.2012.12.014,reference:doi.org/10.1109/IPIN.2016.7743581,reference:doi.org/10.1109/ISDA.2012.6416645,reference:doi.org/10.1145/2499621,reference:doi.org/10.1162/153244303768966139,reference:doi.org/10.1162/neco.1997.9.8.1735,reference:doi.org/10.1371/journal.pone.0272764,reference:doi.org/10.4108/ICST.MOBICASE.2014.257786,repetitions:nan,tech:imu,type:paper},
  annotation = {19 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1109/ISDA.2012.6416645","10.1145/2499621","10.1016/j.patrec.2012.12.014","10.1007/11682127\_8","10.1162/153244303768966139","10.1109/IPIN.2016.7743581","10.1371/journal.pone.0272764","10.1162/neco.1997.9.8.1735","10.4108/ICST.MOBICASE.2014.257786"] ---},
  note = {\url{http://www.ijpmbs.com/uploadfile/2017/1227/20171227050020234.pdf}},
  file = {/Users/brk/Zotero/storage/38I9KL9J/Dept. of Biomedical Engineering, Kyung Hee University, Republic of Korea et al. - 2017 - Recognition of Human Hand Activities Based on a Si.pdf}
}
% == BibTeX quality report for riveraRecognitionHumanHand2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Issue ("4")
% ? unused Journal abbreviation ("IJPMBS")
% ? unused Library catalog ("Semantic Scholar")

@article{rollEffectivenessOccupationalTherapy2016,
  title = {Effectiveness of {{Occupational Therapy Interventions}} for {{Adults With Musculoskeletal Conditions}} of the {{Forearm}}, {{Wrist}}, and {{Hand}}: {{A Systematic Review}}},
  author = {Roll, Shawn C. and Hardison, Mark E.},
  year = {2016},
  month = dec,
  journal = {The American Journal of Occupational Therapy},
  volume = {71},
  number = {1},
  pages = {7101180010p1-7101180010p12},
  issn = {0272-9490},
  doi = {10.5014/ajot.2017.023234},
  keywords = {background,from:cite.bib,medical},
  annotation = {38 citations (Crossref) [2023-07-11] 45 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://doi.org/10.5014/ajot.2017.023234}}
}
% == BibTeX quality report for rollEffectivenessOccupationalTherapy2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused extra: _eprint ("https://research.aota.org/ajot/article-pdf/71/1/7101180010p1/68296/7101180010p1.pdf")

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2023-11-12},
  abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
  langid = {english},
  keywords = {/unread},
  annotation = {9980 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519}}
}
% == BibTeX quality report for rosenblattPerceptronProbabilisticModel1958:
% ? unused Library catalog ("Semantic Scholar")

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2023-06-08},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  langid = {english},
  keywords = {background,backpropogation},
  annotation = {9997 citations (Semantic Scholar/DOI) [2023-06-08]},
  note = {\url{https://www.nature.com/articles/323533a0}}
}
% == BibTeX quality report for rumelhartLearningRepresentationsBackpropagating1986:
% ? unused Library catalog ("Semantic Scholar")

@article{rung-hueiliangRealtimeContinuousGesture1998,
  title = {A Real-Time Continuous Gesture Recognition System for Sign Language},
  author = {{Rung-Huei Liang} and {Ming Ouhyoung}},
  year = {1998},
  journal = {Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition},
  pages = {558--567},
  publisher = {{IEEE Comput. Soc}},
  address = {{Nara, Japan}},
  doi = {10.1109/AFGR.1998.671007},
  urldate = {2023-07-02},
  abstract = {In this paper, a large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a DataGloveTM. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to 4 parameters in a gesture : posture, position, orientation, and motion. We have implemented a prototype system with a lexicon of 250 vocabularies in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signerdependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%. A large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a data glove. Sign language, which is usually known as a set of natural language with formal semantic definitions and syntactic rules, is a large set of hand gestures that are daily used to communicate with the hearing impaired. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to four parameters in a gesture: posture, position, orientation, and motion. The authors have implemented a prototype system with a lexicon of 250 vocabularies and collected 196 training sentences in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signer-dependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4\%,.},
  isbn = {9780818683442},
  keywords = {app:sign-language,app:taiwanese-sl,based-on:gloves,classes:65,hardware:vpl-dataglove,have-read,model:hmm,tech:flex,type:paper},
  annotation = {498 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/671007/}},
  file = {/Users/brk/Zotero/storage/8I9EZ3VC/Rung-Huei Liang and Ming Ouhyoung - 1998 - A real-time continuous gesture recognition system .pdf}
}
% == BibTeX quality report for rung-hueiliangRealtimeContinuousGesture1998:
% ? unused Conference name ("Third IEEE International Conference on Automatic Face and Gesture Recognition")
% ? unused Library catalog ("Semantic Scholar")

@article{ryuFeatureBasedHandGesture2018,
  title = {Feature-{{Based Hand Gesture Recognition Using}} an {{FMCW Radar}} and Its {{Temporal Feature Analysis}}},
  author = {Ryu, Si-Jung and Suh, Jun-Seuk and Baek, Seung-Hwan and Hong, Songcheol and Kim, Jong-Hwan},
  year = {2018},
  month = sep,
  journal = {IEEE Sensors Journal},
  volume = {18},
  number = {18},
  pages = {7593--7602},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2018.2859815},
  urldate = {2023-07-20},
  note = {\url{https://ieeexplore.ieee.org/document/8419726/}}
}
% == BibTeX quality report for ryuFeatureBasedHandGesture2018:
% ? unused Journal abbreviation ("IEEE Sensors J.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{sagayamHandPostureGesture2017,
  title = {Hand Posture and Gesture Recognition Techniques for Virtual Reality Applications: A Survey},
  shorttitle = {Hand Posture and Gesture Recognition Techniques for Virtual Reality Applications},
  author = {Sagayam, K. Martin and Hemanth, D. Jude},
  year = {2017},
  month = jun,
  journal = {Virtual Reality},
  volume = {21},
  number = {2},
  pages = {91--107},
  issn = {1359-4338, 1434-9957},
  doi = {10.1007/s10055-016-0301-0},
  urldate = {2023-07-02},
  abstract = {Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined.},
  langid = {english},
  keywords = {app:virtual-reality,based-on:vision,model:hmm,type:survey},
  annotation = {9 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/s10055-016-0301-0}}
}
% == BibTeX quality report for sagayamHandPostureGesture2017:
% ? unused Library catalog ("Semantic Scholar")

@article{sahooRealTimeHandGesture2022,
  title = {Real-{{Time Hand Gesture Recognition Using Fine-Tuned Convolutional Neural Network}}},
  author = {Sahoo, Jaya Prakash and Prakash, Allam Jaya and P{\l}awiak, Pawe{\l} and Samantray, Saunak},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {3},
  pages = {706},
  issn = {1424-8220},
  doi = {10.3390/s22030706},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is one of the most effective modes of interaction between humans and computers due to being highly flexible and user-friendly. A real-time hand gesture recognition system should aim to develop a user-independent interface with high recognition performance. Nowadays, convolutional neural networks (CNNs) show high recognition rates in image classification problems. Due to the unavailability of large labeled image samples in static hand gesture images, it is a challenging task to train deep CNN networks such as AlexNet, VGG-16 and ResNet from scratch. Therefore, inspired by CNN performance, an end-to-end fine-tuning method of a pre-trained CNN model with score-level fusion technique is proposed here to recognize hand gestures in a dataset with a low number of gesture images. The effectiveness of the proposed technique is evaluated using leave-one-subject-out cross-validation (LOO CV) and regular CV tests on two benchmark datasets. A real-time American sign language (ASL) recognition system is developed and tested using the proposed technique.},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:36,dataset:hust-asl,fidelity:finger,have-read,model:cnn,model:nn,movement:static,participants:10,tech:rgbd,type:paper},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://www.mdpi.com/1424-8220/22/3/706}},
  file = {/Users/brk/Zotero/storage/CUKP99QW/Sahoo et al. - 2022 - Real-Time Hand Gesture Recognition Using Fine-Tune.pdf}
}
% == BibTeX quality report for sahooRealTimeHandGesture2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{salibaCompactGloveInput2004,
  title = {A {{Compact Glove Input Device}} to {{Measure Human Hand}}, {{Wrist}} and {{Forearm Joint Positions}} for {{Teleoperation Applications}}},
  author = {Saliba, M. and Farrugia, F. and Giordmaina, A.},
  year = {2004},
  urldate = {2023-03-07},
  abstract = {In this work, we have developed a new glove input device that is able to measure the angular joint positions of two fingers and of the thumb on the human hand, as well as the pitch position of the wrist and the roll position of the radio-ulnar joint of the human forearm. The glove has various new features, including the measurement of forearm roll position, that are not found in other glove input devices described in the literature. The glove contains a number of flexible, plastic bands whose displacement, during joint rotation, is measured using linear potentiometers. The new glove is light, compact, easy to wear and use, robust, and inexpensive, and is intended for use in teleoperation applications in conjunction with a remotely located robot hand/wrist. Another property is that it can be easily adjusted to fit a wide range of human hand sizes. Preliminary testing of the glove has shown that it can achieve an accuracy in position measurement that compares well to that of a number of commercially-produced gloves that are presently in use. Keywords\textemdash glove input device; whole hand input device; teleoperation; human joint position sensing.},
  keywords = {based-on:gloves,hardware:custom,have-read,model:none,participants:1,tech:potentiometer,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/A-Compact-Glove-Input-Device-to-Measure-Human-Hand\%2C-Saliba-Farrugia/f99389f732525d8a5603da09b2d3de257b763e43}},
  file = {/Users/brk/Zotero/storage/446F9VJ8/Saliba et al. - 2004 - A Compact Glove Input Device to Measure Human Hand.pdf}
}
% == BibTeX quality report for salibaCompactGloveInput2004:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{samuelStudiesMachineLearning1959,
  title = {Some {{Studies}} in {{Machine Learning Using}} the {{Game}} of {{Checkers}}},
  author = {Samuel, A. L.},
  year = {1959},
  month = jul,
  journal = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.33.0210},
  urldate = {2023-11-12},
  abstract = {Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called ``alpha-beta'' pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the},
  keywords = {type:seminal},
  annotation = {4289 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{http://ieeexplore.ieee.org/document/5392560/}},
  file = {/Users/brk/Zotero/storage/SG4Y7Q29/Samuel - 1959 - Some Studies in Machine Learning Using the Game of.pdf}
}
% == BibTeX quality report for samuelStudiesMachineLearning1959:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IBM J. Res. & Dev.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{schlomerGestureRecognitionWii2008,
  title = {Gesture Recognition with a {{Wii}} Controller},
  booktitle = {Proceedings of the 2nd International Conference on {{Tangible}} and Embedded Interaction},
  author = {Schl{\"o}mer, Thomas and Poppinga, Benjamin and Henze, Niels and Boll, Susanne},
  year = {2008},
  month = feb,
  pages = {11--14},
  publisher = {{ACM}},
  address = {{Bonn Germany}},
  doi = {10.1145/1347390.1347395},
  urldate = {2023-03-07},
  abstract = {In many applications today user interaction is moving away from mouse and pens and is becoming pervasive and much more physical and tangible. New emerging interaction technologies allow developing and experimenting with new interaction methods on the long way to providing intuitive human computer interaction. In this paper, we aim at recognizing gestures to interact with an application and present the design and evaluation of our sensor-based gesture recognition. As input device we employ the Wii-controller (Wiimote) which recently gained much attention world wide. We use the Wiimote's acceleration sensor independent of the gaming console for gesture recognition. The system allows the training of arbitrary gestures by users which can then be recalled for interacting with systems like photo browsing on a home TV. The developed library exploits Wii-sensor data and employs a hidden Markov model for training and recognizing user-chosen gestures. Our evaluation shows that we can already recognize gestures with a small number of training samples. In addition to the gesture recognition we also present our experiences with the Wii-controller and the implementation of the gesture recognition. The system forms the basis for our ongoing work on multimodal intuitive media browsing and are available to other researchers in the field.},
  isbn = {978-1-60558-004-3},
  langid = {english},
  keywords = {based-on:gloves,classes:5,fidelity:arm,hardware:wiimote,have-read,model:hmm,model:k-means,tech:accelerometer,type:paper},
  annotation = {560 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/1347390.1347395}},
  file = {/Users/brk/Zotero/storage/QUVGMHED/Schlömer et al. - 2008 - Gesture recognition with a Wii controller.pdf}
}
% == BibTeX quality report for schlomerGestureRecognitionWii2008:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("TEI08: Conference on Tangible and Embedded Interaction 2008")
% ? unused Library catalog ("Semantic Scholar")

@misc{scotts.fisherVIRTUALENVIRONMENTSPERSONAL1991,
  title = {{{VIRTUAL ENVIRONMENTS}}, {{PERSONAL SIMULATION}} \& {{TELEPRESENCE}}},
  author = {{Scott S. Fisher}},
  year = {1991},
  abstract = {For most people, "duplicating reality" is an assumed, if not obvious goal for any contemporary imaging technology. The proof of the 'ideal' picture is not being able to discern object from representation - to be convinced that one is looking at the real thing. At best, this judgement is usually based on a first order evaluation of 'ease of identification'; i.e. realistic pictures should resemble what they represent. But resemblance is only part of the effect. In summing up prevailing theories on realism in images, Perkins comments: " Pictures inform by packaging information in light in essentially the same form that real objects and scenes package it, and the perceiver unwraps that package in essentially the same way." [2]},
  howpublished = {\url{https://www.academia.edu/download/76150375/VirtualSimPresence-pdfrev.pdf}},
  keywords = {referenced}
}
% == BibTeX quality report for scotts.fisherVIRTUALENVIRONMENTSPERSONAL1991:
% ? Title looks like it was stored in title-case in Zotero

@article{segenFastAccurate3D1998,
  title = {Fast and Accurate {{3D}} Gesture Recognition Interface},
  author = {Segen, J. and Kumar, S.},
  year = {1998},
  journal = {Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)},
  volume = {1},
  pages = {86--91},
  publisher = {{IEEE Comput. Soc}},
  address = {{Brisbane, Qld., Australia}},
  doi = {10.1109/ICPR.1998.711086},
  urldate = {2023-07-12},
  abstract = {A video-based gesture recognition system can serve as a natural and accurate 3D user input device. We describe a two-camera system, that recognizes three gesture classes: two static and one dynamic. For one of these gestures (pointing), the system estimates five parameters of 3D pose: position and pointing direction. The recognition is robust, independent of the user and fast (60 Hz), and the estimated pose is very stable. We describe some of the interface applications that demonstrate the benefits of the system: control of a video game, piloting a virtual reality fly-through, and interaction with a 3D scene editor.},
  isbn = {9780818685125},
  keywords = {based-on:vision,classes:3,have-read,movement:dynamic,movement:static,tech:rgb,type:paper},
  annotation = {41 citations (Semantic Scholar/DOI) [2023-07-12]

,- \{"text":"R. Kjeldsen and J. Kender.  Towards the use of Gesture in Traditional User Interfaces.  In Proc. International Conference on Automatic  Face and Gesuture Recognition , pages 151\textendash 156. October 1996.","identifiers":\{\},"authors":["R. Kjeldsen and J. Kender."],"year":"1996","title":"Towards the use of Gesture in Traditional User Interfaces","publicationVenue":"In Proc. International Conference on Automatic Face and Gesuture Recognition","type":"journalArticle"\} ,- \{"text":"M. W. Krueger. Artificial Reality II . Addison-Wesley, 1991.","identifiers":\{\},"authors":["M. W. Krueger."],"year":"1991","title":"Artificial Reality II","publicationVenue":"Addison-Wesley","type":"journalArticle"\} ,- \{"text":"C. Maggioni. GestureComputer - New Ways of Operating a Computer.  In Proc. International Conference on Automatic Face and Gesuture Recognition , pages 166\textendash 171. June 1995.","identifiers":\{\},"authors":["C. Maggioni."],"year":"1995","title":"GestureComputer - New Ways of Operating a Computer","publicationVenue":"In Proc. International Conference on Automatic Face and Gesuture Recognition","type":"journalArticle"\} ,- \{"text":"V. I. Pavlovic,  Sharma, and T. S. Huang.  Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review. IEEE Transactions on Pattern Recognition and Machine Intelligence , 19(7):677\textendash 695, July 1997.","identifiers":\{\},"authors":["V. I. Pavlovic, Sharma, and T. S. Huang."],"year":"1997","title":"Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review","publicationVenue":"IEEE Transactions on Pattern Recognition and Machine Intelligence","type":"journalArticle"\} ,- \{"text":"J. M. Rehg and T. Kanade.  DigitalEyes:  Vision Based Human Hand Tracking. In CMU Tech Report CMU-CS-93-220 . 1993.","identifiers":\{\},"authors":["J. M. Rehg and T. Kanade."],"year":"1993","title":"DigitalEyes: Vision Based Human Hand Tracking","publicationVenue":"In CMU Tech Report CMU-CS-","type":"journalArticle"\} ,- \{"text":"J. Segen.  Controlling Computers with Gloveless Gesture s. In Proceedings of Virtual Reality Systems . 1993.","identifiers":\{\},"authors":["J. Segen."],"year":"1993","title":"Controlling Computers with Gloveless Gesture s","publicationVenue":"In Proceedings of Virtual Reality Systems .","type":"journalArticle"\} ,- \{"text":"J. Segen.  GEST: A Learning Computer Vision System that Recognizes Hand Gestures. In R. S. Michalski and G. Tecuci, editors, Machine Learning IV . Morgan Kaufmann, 1994.","identifiers":\{\},"authors":["J. Segen."],"year":"1994","title":"GEST: A Learning Computer Vision System that Recognizes Hand Gestures","publicationVenue":"In R. S. Michalski and G. Tecuci","type":"journalArticle"\} ,- \{"text":"J. Triesch and  C. von  der Malsburg.   A Gesture  Interface for Human-Robot-Interaction.  In Proc. International Conference on Automatic Face and Gesture Recognition , pages 546\textendash 551. April 1998.","identifiers":\{\},"authors":["J. Triesch and C. von der Malsburg."],"year":"1998","title":"A Gesture Interface for Human-Robot-Interaction","publicationVenue":"In Proc. International Conference on Automatic Face and Gesture Recognition","type":"journalArticle"\} ,- \{"text":"V. J. Vincent.  Dwelving in the depth of the mind.  In Proc. Interface to Real and Virtual Worlds . 1991.","identifiers":\{\},"authors":["V. J. Vincent. Dwelving in the depth of the mind. In Proc."],"year":"1991","title":"Interface to Real and Virtual Worlds","publicationVenue":"","type":"journalArticle"\} ,- \{"text":"C. R. Wren, A. Azarbayejani, Darrell,  and A. P. Pentland . Pfinder:   Real-Time  Tracking of  the  Human  Body. IEEE Transactions on  Pattern  Recognition and  Machine  Intelligence , 19(7):780\textendash 785, July 1997. (a) (b) (c)","identifiers":\{\},"authors":["C. R. Wren, A. Azarbayejani, Darrell, and A. P. Pentland ."],"year":"1997","title":"Pfinder: Real-Time Tracking of the Human Body","publicationVenue":"IEEE Transactions on Pattern Recognition and Machine Intelligence","type":"journalArticle"\}

,- \{"identifiers":\{"DOI":"10.1109/AFGR.1996.557257"\},"authors":[null],"text":"key: 2; doi-asserted-by: publisher; DOI: 10.1109/AFGR.1996.557257","type":"journalArticle","url":"https://doi.org/10.1109/AFGR.1996.557257"\} ,- \{"identifiers":\{\},"title":"Dwelving in the depth of the mind","authors":["vincent"],"year":"1991","text":"vincent et al., 1991, Dwelving in the depth of the mind","type":"journalArticle"\} ,- \{"identifiers":\{"DOI":"10.1109/AFGR.1998.671012"\},"authors":[null],"text":"key: 1; doi-asserted-by: publisher; DOI: 10.1109/AFGR.1998.671012","type":"journalArticle","url":"https://doi.org/10.1109/AFGR.1998.671012"\} ,- \{"identifiers":\{\},"title":"Controlling computers with gloveless gestures","authors":["segen"],"year":"1993","text":"segen et al., 1993, Controlling computers with gloveless gestures","type":"journalArticle"\} ,- \{"identifiers":\{\},"title":"DigitalEyes vision based human hand tracking","authors":["rehg"],"year":"1993","text":"rehg et al., 1993, DigitalEyes vision based human hand tracking","type":"journalArticle"\} ,- \{"identifiers":\{"DOI":"10.1109/34.598226"\},"authors":[null],"text":"key: 5; doi-asserted-by: publisher; DOI: 10.1109/34.598226","type":"journalArticle","url":"https://doi.org/10.1109/34.598226"\} ,- \{"identifiers":\{\},"title":"GestureComputer-New ways of operating a computer","authors":["maggioni"],"year":"1995","text":"maggioni et al., 1995, GestureComputer-New ways of operating a computer","type":"journalArticle"\} ,- \{"identifiers":\{"DOI":"10.1109/AFGR.1998.671005"\},"authors":[null],"text":"key: 9; doi-asserted-by: publisher; DOI: 10.1109/AFGR.1998.671005","type":"journalArticle","url":"https://doi.org/10.1109/AFGR.1998.671005"\} ,- \{"identifiers":\{\},"authors":["segen"],"year":"1994","text":"key: 8; author: segen; year: 1994; journal-title: GEST A Learning Computer Vision System That RecognizesHandGestures","type":"journalArticle"\} ,- \{"identifiers":\{"DOI":"10.1109/34.598236"\},"authors":[null],"text":"key: 11; doi-asserted-by: publisher; DOI: 10.1109/34.598236","type":"journalArticle","url":"https://doi.org/10.1109/34.598236"\}

,- \{"text":"R. Kjeldsen and J. Kender.  Towards the use of Gesture in Traditional User Interfaces.  In Proc. International Conference on Automatic  Face and Gesuture Recognition , pages 151\textendash 156. October 1996.","identifiers":\{\},"authors":["R. Kjeldsen and J. Kender."],"year":"1996","title":"Towards the use of Gesture in Traditional User Interfaces","publicationVenue":"In Proc. International Conference on Automatic Face and Gesuture Recognition","type":"journalArticle"\} ,- \{"text":"M. W. Krueger. Artificial Reality II . Addison-Wesley, 1991.","identifiers":\{\},"authors":["M. W. Krueger."],"year":"1991","title":"Artificial Reality II","publicationVenue":"Addison-Wesley","type":"journalArticle"\} ,- \{"text":"C. Maggioni. GestureComputer - New Ways of Operating a Computer.  In Proc. International Conference on Automatic Face and Gesuture Recognition , pages 166\textendash 171. June 1995.","identifiers":\{\},"authors":["C. Maggioni."],"year":"1995","title":"GestureComputer - New Ways of Operating a Computer","publicationVenue":"In Proc. International Conference on Automatic Face and Gesuture Recognition","type":"journalArticle"\} ,- \{"text":"V. I. Pavlovic,  Sharma, and T. S. Huang.  Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review. IEEE Transactions on Pattern Recognition and Machine Intelligence , 19(7):677\textendash 695, July 1997.","identifiers":\{\},"authors":["V. I. Pavlovic, Sharma, and T. S. Huang."],"year":"1997","title":"Visual Interpretation of Hand Gestures for Human-Computer Interaction: A Review","publicationVenue":"IEEE Transactions on Pattern Recognition and Machine Intelligence","type":"journalArticle"\} ,- \{"text":"J. M. Rehg and T. Kanade.  DigitalEyes:  Vision Based Human Hand Tracking. In CMU Tech Report CMU-CS-93-220 . 1993.","identifiers":\{\},"authors":["J. M. Rehg and T. Kanade."],"year":"1993","title":"DigitalEyes: Vision Based Human Hand Tracking","publicationVenue":"In CMU Tech Report CMU-CS-","type":"journalArticle"\} ,- \{"text":"J. Segen.  Controlling Computers with Gloveless Gesture s. In Proceedings of Virtual Reality Systems . 1993.","identifiers":\{\},"authors":["J. Segen."],"year":"1993","title":"Controlling Computers with Gloveless Gesture s","publicationVenue":"In Proceedings of Virtual Reality Systems .","type":"journalArticle"\} ,- \{"text":"J. Segen.  GEST: A Learning Computer Vision System that Recognizes Hand Gestures. In R. S. Michalski and G. Tecuci, editors, Machine Learning IV . Morgan Kaufmann, 1994.","identifiers":\{\},"authors":["J. Segen."],"year":"1994","title":"GEST: A Learning Computer Vision System that Recognizes Hand Gestures","publicationVenue":"In R. S. Michalski and G. Tecuci","type":"journalArticle"\} ,- \{"text":"J. Triesch and  C. von  der Malsburg.   A Gesture  Interface for Human-Robot-Interaction.  In Proc. International Conference on Automatic Face and Gesture Recognition , pages 546\textendash 551. April 1998.","identifiers":\{\},"authors":["J. Triesch and C. von der Malsburg."],"year":"1998","title":"A Gesture Interface for Human-Robot-Interaction","publicationVenue":"In Proc. International Conference on Automatic Face and Gesture Recognition","type":"journalArticle"\} ,- \{"text":"V. J. Vincent.  Dwelving in the depth of the mind.  In Proc. Interface to Real and Virtual Worlds . 1991.","identifiers":\{\},"authors":["V. J. Vincent. Dwelving in the depth of the mind. In Proc."],"year":"1991","title":"Interface to Real and Virtual Worlds","publicationVenue":"","type":"journalArticle"\} ,- \{"text":"C. R. Wren, A. Azarbayejani, Darrell,  and A. P. Pentland . Pfinder:   Real-Time  Tracking of  the  Human  Body. IEEE Transactions on  Pattern  Recognition and  Machine  Intelligence , 19(7):780\textendash 785, July 1997. (a) (b) (c)","identifiers":\{\},"authors":["C. R. Wren, A. Azarbayejani, Darrell, and A. P. Pentland ."],"year":"1997","title":"Pfinder: Real-Time Tracking of the Human Body","publicationVenue":"IEEE Transactions on Pattern Recognition and Machine Intelligence","type":"journalArticle"\}},
  note = {\url{http://ieeexplore.ieee.org/document/711086/}},
  file = {/Users/brk/Zotero/storage/LRWUAUKB/Segen and Kumar - 1998 - Fast and accurate 3D gesture recognition interface.pdf}
}
% == BibTeX quality report for segenFastAccurate3D1998:
% ? Possibly abbreviated journal title Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)
% ? unused Conference name ("Fourteenth International Conference on Pattern Recognition")
% ? unused Library catalog ("Semantic Scholar")
% ? unused References ("- {"text":"C. Hummels and P. J. Stappers. Meaningful Gestures for Hu man Computer Interaction: Beyond Hand Postures. In Proc. International Conference on  Automatic  Face  and  Gesture Recognition , pages 591–596. April 1998.","identifiers":{},"authors":["C. Hummels and P. J. Stappers."],"year":"1998","title":"Meaningful Gestures for Hu man Computer Interaction: Beyond Hand Postures","publicationVenue":"In Proc. International Conference on Automatic Face and Gesture Recognition","type":"journalArticle"}")

@article{segenHumancomputerInteractionUsing1998,
  title = {Human-Computer Interaction Using Gesture Recognition and {{3D}} Hand Tracking},
  author = {Segen, J. and Kumar, S.},
  year = {1998},
  journal = {Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)},
  volume = {3},
  pages = {188--192},
  publisher = {{IEEE Comput. Soc}},
  address = {{Chicago, IL, USA}},
  doi = {10.1109/ICIP.1998.727164},
  urldate = {2023-07-12},
  abstract = {This paper describes a real time system for human-computer interaction through gesture recognition and three dimensional hand-tracking. Using two cameras that are focused at the user's hand the system recognizes three gestures and tracks the hand in three dimensions. The system can simultaneously track two fingers (thumb and pointing finger) and output their poses. The pose for each finger consists of three positional coordinates and two angles (azimuth and elevation). By moving the thumb and the pointing finger in 3D, the user can control 10 degrees of freedom in a smooth and natural fashion. We have used this system as a multi-dimensional input-interface to computer games, terrain navigation software and graphical editors. In addition to providing 10 degrees of control, our system is much more natural and intuitive to use compared to traditional input devices. The system is user independent, and operates at the rate of 60 Hz.},
  isbn = {9780818688218},
  keywords = {based-on:vision,pdf:paywalled,tech:rgb,type:paper},
  annotation = {60 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/727164/}}
}
% == BibTeX quality report for segenHumancomputerInteractionUsing1998:
% ? Possibly abbreviated journal title Proceedings 1998 International Conference on Image Processing. ICIP98 (Cat. No.98CB36269)
% ? unused Conference name ("IPCIP'98 International Conference on Image Processing")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{seninDynamicTimeWarping2008,
  title = {Dynamic {{Time Warping Algorithm Review}}},
  author = {Senin, Pavel},
  year = {2008},
  keywords = {background,from:cite.bib,model:dynamic-time-warping,type:survey}
}
% == BibTeX quality report for seninDynamicTimeWarping2008:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero

@misc{SensorGlove,
  title = {Sensor {{Glove}}},
  howpublished = {\url{https://github.com/SensorGlove/SensorGlove}},
  keywords = {based-on:gloves,hardware:custom,tech:accelerometer,type:github-project}
}
% == BibTeX quality report for SensorGlove:
% ? Title looks like it was stored in title-case in Zotero

@article{sethujanakiRealTimeRecognition2013,
  title = {Real Time Recognition of {{3D}} Gestures in Mobile Devices},
  author = {Sethu Janaki, V M and Babu, Satish and Sreekanth, S S},
  year = {2013},
  month = dec,
  journal = {2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},
  pages = {149--152},
  publisher = {{IEEE}},
  address = {{Trivandrum, India}},
  doi = {10.1109/RAICS.2013.6745463},
  urldate = {2023-03-07},
  abstract = {Gesture-based user interaction is increasingly relevant today as the use of personal computing devices becomes widespread. Smartphones have several inbuilt sensors like accelerometer, orientation sensor and gyroscope, which are able to provide data on motion of the device in 3D space. This paper proposes a mechanism for real-time recognition of 3D gestures using sensors in mobile devices. 3D gestures are space-drawn gestures computed from 3-axial accelerometer readings. The algorithms discussed in this paper include single value decomposition, dynamic time warping and Mahalanobis distance.},
  isbn = {9781479921782 9781479921775},
  keywords = {based-on:gloves,model:dynamic-time-warping,model:knn,model:svd,pdf:cant-find,tech:accelerometer,type:paper},
  annotation = {8 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/6745463/}}
}
% == BibTeX quality report for sethujanakiRealTimeRecognition2013:
% ? unused Library catalog ("Semantic Scholar")

@article{shangRobustSignLanguage2017,
  title = {A {{Robust Sign Language Recognition System}} with {{Multiple Wi-Fi Devices}}},
  author = {Shang, Jiacheng and Wu, Jie},
  year = {2017},
  month = aug,
  journal = {Proceedings of the Workshop on Mobility in the Evolving Internet Architecture},
  pages = {19--24},
  publisher = {{ACM}},
  address = {{Los Angeles CA USA}},
  doi = {10.1145/3097620.3097624},
  urldate = {2023-07-12},
  abstract = {Sign language is important since it provides a way for us to the deaf culture and more opportunities to communicate with those who are deaf or hard of hearing. Since sign language chiefly uses body languages to convey meaning, Human Activity Recognition (HAR) techniques can be used to recognize them for some sign language translation applications. In this paper, we show for the first time that Wi-Fi signals can be used to recognize sign language. The key intuition is that different hand and arm motions introduce different multi-path distortions in Wi-Fi signals and generate different unique patterns in the time-series of Channel State Information (CSI). More specifically, we propose a Wi-Fi signal-based sign language recognition system called WiSign. Different from existing Wi-Fi signal-based human activity recognition systems, WiSign uses 3 Wi-Fi devices to improve the recognition performance. We implemented the WiSign using a TP-Link TL-WR1043ND Wi-Fi router and two Lenovo X100e laptops. The evaluation results show that our system can achieve a mean prediction accuracy of 93.8\% and mean false positive of 1.55\%.},
  isbn = {9781450350594},
  langid = {english},
  keywords = {app:sign-language,based-on:wifi,claims-to-be-best,classes:5,tech:wifi,type:paper},
  annotation = {39 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3097620.3097624}}
}
% == BibTeX quality report for shangRobustSignLanguage2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("SIGCOMM '17: ACM SIGCOMM 2017 Conference")
% ? unused Library catalog ("Semantic Scholar")

@article{sharmaASL3DCNNAmericanSign2021,
  title = {{{ASL-3DCNN}}: {{American}} Sign Language Recognition Technique Using 3-{{D}} Convolutional Neural Networks},
  shorttitle = {{{ASL-3DCNN}}},
  author = {Sharma, Shikhar and Kumar, Krishan},
  year = {2021},
  month = jul,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {17},
  pages = {26319--26331},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-021-10768-5},
  urldate = {2023-06-22},
  abstract = {The communication between a person from the impaired community with a person who does not understand sign language could be a tedious task. Sign language is the art of conveying messages using hand gestures. Recognition of dynamic hand gestures in American Sign Language (ASL) became a very important challenge that is still unresolved. In order to resolve the challenges of dynamic ASL recognition, a more advanced successor of the Convolutional Neural Networks (CNNs) called 3-D CNNs is employed, which can recognize the patterns in volumetric data like videos. The CNN is trained for classification of 100 words on Boston ASL (Lexicon Video Dataset) LVD dataset with more than 3300 English words signed by 6 different signers. 70\% of the dataset is used for Training while the remaining 30\% dataset is used for testing the model. The proposed work outperforms the existing state-of-art models in terms of precision (3.7\%), recall (4.3\%), and f-measure (3.9\%). The computing time (0.19 seconds per frame) of the proposed work shows that the proposal may be used in real-time applications.},
  langid = {english},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:100,contains-more-references,fidelity:finger,have-read,model:cnn,model:nn,movement:dynamic,participants:6,type:paper},
  annotation = {44 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://link.springer.com/10.1007/s11042-021-10768-5}},
  file = {/Users/brk/Zotero/storage/5S684PID/Sharma and Kumar - 2021 - ASL-3DCNN American sign language recognition tech.pdf}
}
% == BibTeX quality report for sharmaASL3DCNNAmericanSign2021:
% ? unused Journal abbreviation ("Multimed Tools Appl")
% ? unused Library catalog ("Semantic Scholar")

@article{sharmaMultimodalHumancomputerInterface1998,
  title = {Toward Multimodal Human-Computer Interface},
  author = {Sharma, R. and Pavlovic, V.I. and Huang, T.S.},
  year = {1998},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {5},
  pages = {853--869},
  issn = {00189219},
  doi = {10.1109/5.664275},
  urldate = {2023-06-23},
  abstract = {Recent advances in various signal processing technologies, coupled with an explosion in the available computing power, have given rise to a number of novel human-computer interaction (HCI) modalities: speech, vision-based gesture recognition, eye tracking, electroencephalograph, etc. Successful embodiment of these modalities into an interface has the potential of easing the HCI bottleneck that has become noticeable with the advances in computing and communication. It has also become increasingly evident that the difficulties encountered in the analysis and interpretation of individual sensing modalities may be overcome by integrating them into a multimodal human-computer interface. We examine several promising directions toward achieving multimodal HCI. We consider some of the emerging novel input modalities for HCI and the fundamental issues in integrating them at various levels, from early signal level to intermediate feature level to late decision level. We discuss the different computational approaches that may be applied at the different levels of modality integration. We also briefly review several demonstrated multimodal HCI systems and applications. Despite all the recent developments, it is clear that further research is needed for interpreting and fitting multiple sensing modalities in the context of HCI. This research can benefit from many disparate fields of study that increase our understanding of the different human communication modalities and their potential role in HCI.},
  keywords = {based-on:vision,contains-more-references,have-read,type:survey},
  annotation = {341 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/664275/}},
  file = {/Users/brk/Zotero/storage/UDDF98WU/Sharma et al. - 1998 - Toward multimodal human-computer interface.pdf}
}
% == BibTeX quality report for sharmaMultimodalHumancomputerInterface1998:
% ? unused Journal abbreviation ("Proc. IEEE")
% ? unused Library catalog ("Semantic Scholar")

@article{shengDeepSpatialTemporal2020,
  title = {Deep {{Spatial}}\textendash{{Temporal Model Based Cross-Scene Action Recognition Using Commodity WiFi}}},
  author = {Sheng, Biyun and Xiao, Fu and Sha, Letian and Sun, Lijuan},
  year = {2020},
  month = apr,
  journal = {IEEE Internet of Things Journal},
  volume = {7},
  number = {4},
  pages = {3592--3601},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2020.2973272},
  urldate = {2023-07-12},
  abstract = {With the popularization of Internet-of-Things (IoT) systems, passive action recognition on channel state information (CSI) has attracted much attention. Most conventional work under the machine-learning framework utilizes handcrafted features (e.g., statistic features) that are unable to sufficiently describe the sequence data and heavily rely on designers' experiences. Therefore, how to automatically learn abundant spatial\textendash temporal information from CSI data is a topic worthy of study. In this article, we propose a deep learning framework that integrates spatial features learned from the convolutional neural network (CNN) into the temporal model multilayer bidirectional long short-term memory (Bi-LSTM). Specifically, CSI streams are segmented into a series of patches, from which spatial features are extracted by our designed CNN structure. Considering long-term dependencies between adjacent sequences, the fully connected layer of CNN for each patch is taken as the Bi-LSTM sequential input to further capture temporal features. Our model is appealing in that it can simultaneously learn temporal dynamics and convolutional perceptual representations. To the best of our knowledge, this is the first work to explore deep spatial\textendash temporal features for CSI-based action recognition. Furthermore, in order to solve the problem that the trained model fully fails with environmental changes, we use the off-the-shelf model as the pretrained model and fine-tune it in the new scenario. The transfer method is able to realize cross-scene action recognition with low computational consumption and satisfactory accuracy. We carry out experiments on indoor data and the experimental results validate the effectiveness of our algorithm.},
  keywords = {based-on:wifi,model:cnn,model:lstm,model:nn,tech:wifi,type:paper},
  annotation = {36 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/8993738/}}
}
% == BibTeX quality report for shengDeepSpatialTemporal2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Internet Things J.")
% ? unused Library catalog ("Semantic Scholar")

@article{shenWiPassCSIbasedKeystroke2020,
  title = {{{WiPass}}: {{CSI-based Keystroke Recognition}} for {{Numerical Keypad}} of {{Smartphones}}},
  shorttitle = {{{WiPass}}},
  author = {Shen, Xingfa and Yan, Guo and Yang, Jian and Xu, Sheng},
  year = {2020},
  month = oct,
  journal = {2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
  pages = {276--283},
  publisher = {{IEEE}},
  address = {{Zhanjiang, China}},
  doi = {10.1109/YAC51587.2020.9337673},
  urldate = {2023-07-12},
  abstract = {Nowadays, smartphones are everywhere. They play an indispensable role in our lives and makes people convenient to communicate, pay, socialize, etc. However, they also bring a lot of security and privacy risks. Keystroke operations of numeric keypad are often required when users input password to perform mobile payment or input other privacy-sensitive information. Different keystrokes may cause different finger movements that will bring different interference to WiFi signal, which may be reflected by channel state information (CSI). In this paper, we propose WiPass, a password-keystroke recognition system for numerical keypad input on smartphones, which especially occurs frequently in mobile payment APPs. Based on only a public WiFi hotspot deployed in the victim payment scenario, WiPass would extracts and analyzes the CSI data generated by the password-keystroke operation of the smartphone user, and infers the user's payment password by comparing the CSI waveforms of different keystrokes. We implemented the WiPass system by using COTS WiFi AP devices and smartphones. The average keystroke segmentation accuracy was 80.45\%, and the average keystroke recognition accuracy was 74.24\%.},
  isbn = {9781728176840},
  keywords = {app:activity-inference,app:password-inference,based-on:wifi,fidelity:finger,tech:wifi,type:paper},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/9337673/}}
}
% == BibTeX quality report for shenWiPassCSIbasedKeystroke2020:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{shyamalaSURVEYVISIONBASED2014,
  title = {A {{SURVEY OF VISION BASED HAND GESTURE RECOGNITION}}},
  author = {Shyamala, M.},
  year = {2014},
  urldate = {2023-07-11},
  abstract = {Gesture recognition is to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. Hand Gestures have greater importance in designing an intelligent and efficient human\textendash computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. This exploratory survey aims to provide a progress report on static and dynamic hand gesture recognition, gesture taxonomies, representations. Vision based Gesture recognition has the potential to be a natural and powerful tool supporting efficient and intuitive interaction between the human and the computer. Visual interpretation of hand gestures can help in achieving the ease and naturalness desired for Human Computer Interaction (HCI). It focuses on the three main phases of hand gesture recognition i.e. detection, tracking and recognition.},
  keywords = {based-on:vision,movement:dynamic,movement:static,tech:rgb,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/A-SURVEY-OF-VISION-BASED-HAND-GESTURE-RECOGNITION-Shyamala/01fb6988163209f16021dec3320956de74a8f45d}}
}
% == BibTeX quality report for shyamalaSURVEYVISIONBASED2014:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2023-11-12},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  langid = {english},
  keywords = {/unread},
  annotation = {9998 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://www.nature.com/articles/nature16961}}
}
% == BibTeX quality report for silverMasteringGameGo2016:
% ? unused Library catalog ("Semantic Scholar")

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2023-11-12},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo.},
  langid = {english},
  keywords = {/unread},
  annotation = {7681 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://www.nature.com/articles/nature24270}},
  file = {/Users/brk/Zotero/storage/ZAE7II58/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}
% == BibTeX quality report for silverMasteringGameGo2017:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{songAntLearningAlgorithm2013,
  title = {An {{Ant Learning Algorithm}} for {{Accelerometer-based Gesture Recognition}}},
  author = {Song, Sichao},
  year = {2013},
  urldate = {2023-07-17},
  abstract = {Today's emerging gesture recognition techniques have enriched the ways of human machine interaction. With the popularity of smart devices such as iPhone and iPod Touch, accelerometer-based gesture recognition for facilitating such interactions is becoming even more pervasive and promising. Accelerometer-based gesture recognition systems have been extensively discussed in many previous related work. Currently, there are several techniques being applied for recognizing gestures, most well-known algorithms are Hidden Markov Model (HMM) and Dynamic Time Warping (DTW). However, they do have shortcomings: 1) HMM requires a sizeable amount of training data, and suffers from the high computational overhead for both training and classification. 2) The processing time of DTW depends on both the length and number of templates. In this thesis, we introduce a novel gesture recognition algorithm named the Ant Learning Algorithm (ALA), which aims at addressing some of the limitations with the currently two leading algorithms, especially HMM. It takes advantage of the pheromone mechanism from ant colony optimization and uses pheromone tables to represent gestures, which scales well with gesture complexity. ALA requires minimal training instances and greatly reduces the computational overhead required by both training and classification. The experimental results show that ALA can achieve a high recognition accuracy of over 90\% with only one training instance and exhibits good generalization.},
  keywords = {based-on:gloves,claims-to-be-best,classes:5,dataset:custom,fidelity:arm,hardware:ipod-touch-4th-gen,have-read,model:ant-learning-algorithm,reference:doi.org/10.1007/11492429\_77,reference:doi.org/10.1007/978-3-540-28646-2\_29,reference:doi.org/10.1007/978-3-642-02830-4\_4,reference:doi.org/10.1007/978-3-642-12553-9\_7,reference:doi.org/10.1016/j.cviu.2010.07.012,reference:doi.org/10.1016/j.neunet.2005.03.006,reference:doi.org/10.1016/j.pmcj.2009.07.007,reference:doi.org/10.1109/3477.484436,reference:doi.org/10.1109/38.674971,reference:doi.org/10.1109/5.18626,reference:doi.org/10.1109/AFRCON.2009.5308175,reference:doi.org/10.1109/CEC.2013.6557929,reference:doi.org/10.1109/icss.2013.40,reference:doi.org/10.1109/TEVC.2006.890229,reference:doi.org/10.1118/1.597428,reference:doi.org/10.1145/1347390.1347395,reference:doi.org/10.1145/1830761.1830899,reference:doi.org/10.1145/2072298.2072443,reference:doi.org/10.1609/aimag.v23i3.1656,reference:doi.org/10.2307/2670189,reference:doi.org/10.3390/A13100250,tech:accelerometer,technique:one-shot,type:msc},
  annotation = {--- DOIs\_of\_references: ["10.1109/3477.484436","10.1109/AFRCON.2009.5308175","10.5281/zenodo.1178029","10.1609/aimag.v23i3.1656","10.1007/11492429\_77","10.1007/978-3-642-02830-4\_4","10.1109/38.674971","10.1109/icss.2013.40","10.1145/2072298.2072443","10.1109/TEVC.2006.890229","10.2307/2670189","10.1016/j.pmcj.2009.07.007","10.1145/1830761.1830899","10.1016/j.neunet.2005.03.006"] ---},
  note = {\url{https://www.semanticscholar.org/paper/An-Ant-Learning-Algorithm-for-Accelerometer-based-Song/997f3ddb75fba7c59397a049ce515846f3cfd2b4}},
  file = {/Users/brk/Zotero/storage/94PMMIZT/Song - 2013 - An Ant Learning Algorithm for Accelerometer-based .pdf}
}
% == BibTeX quality report for songAntLearningAlgorithm2013:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{SPARC2023,
  title = {{{SPARC}}},
  year = {2023},
  month = jun,
  journal = {Wikipedia},
  urldate = {2023-07-18},
  abstract = {SPARC (Scalable Processor Architecture) is a reduced instruction set computer (RISC) instruction set architecture originally developed by Sun Microsystems. Its design was strongly influenced by the experimental Berkeley RISC system developed in the early 1980s. First developed in 1986 and released in 1987, SPARC was one of the most successful early commercial RISC systems, and its success led to the introduction of similar RISC designs from many vendors through the 1980s and 1990s. The first implementation of the original 32-bit architecture (SPARC V7) was used in Sun's Sun-4 computer workstation and server systems, replacing their earlier Sun-3 systems based on the Motorola 68000 series of processors. SPARC V8 added a number of improvements that were part of the SuperSPARC series of processors released in 1992. SPARC V9, released in 1993, introduced a 64-bit architecture and was first released in Sun's UltraSPARC processors in 1995. Later, SPARC processors were used in symmetric multiprocessing (SMP) and non-uniform memory access (CC-NUMA) servers produced by Sun, Solbourne, and Fujitsu, among others. The design was turned over to the SPARC International trade group in 1989, and since then its architecture has been developed by its members. SPARC International is also responsible for licensing and promoting the SPARC architecture, managing SPARC trademarks (including SPARC, which it owns), and providing conformance testing. SPARC International was intended to grow the SPARC architecture to create a larger ecosystem; SPARC has been licensed to several manufacturers, including Atmel, Bipolar Integrated Technology, Cypress Semiconductor, Fujitsu, Matsushita and Texas Instruments. Due to SPARC International, SPARC is fully open, non-proprietary and royalty-free. As of September 2017, the latest commercial high-end SPARC processors are Fujitsu's SPARC64 XII (introduced in 2017 for its SPARC M12 server) and Oracle's SPARC M8 introduced in September 2017 for its high-end servers. On Friday, September 1, 2017, after a round of layoffs that started in Oracle Labs in November 2016, Oracle terminated SPARC design after completing the M8. Much of the processor core development group in Austin, Texas, was dismissed, as were the teams in Santa Clara, California, and Burlington, Massachusetts.Fujitsu will also discontinue their SPARC production (has already shifted to producing their own ARM-based CPUs), after two "enhanced" versions of Fujitsu's older SPARC M12 server in 2020\textendash 22 (formerly planned for 2021) and again in 2026\textendash 27, end-of-sale in 2029, of UNIX servers and a year later for their mainframe and end-of-support in 2034 "to promote customer modernization".},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  keywords = {/unread,background,untagged},
  annotation = {Page Version ID: 1158839839},
  note = {\url{https://en.wikipedia.org/w/index.php?title=SPARC&oldid=1158839839}},
  file = {/Users/brk/Zotero/storage/AJ6CMS5B/SPARC.html}
}
% == BibTeX quality report for SPARC2023:
% Missing required field 'author'

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {J. Mach. Learn. Res.},
  volume = {15},
  pages = {1929--1958},
  keywords = {background,dropout,from:cite.bib}
}
% == BibTeX quality report for srivastavaDropoutSimpleWay2014:
% ? Possibly abbreviated journal title J. Mach. Learn. Res.

@article{starnerRealtimeAmericanSign1995,
  title = {Real-Time {{American Sign Language}} Recognition from Video Using Hidden {{Markov}} Models},
  author = {Starner, T. and Pentland, A.},
  year = {1995},
  journal = {Proceedings of International Symposium on Computer Vision - ISCV},
  pages = {265--270},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Coral Gables, FL, USA}},
  doi = {10.1109/ISCV.1995.477012},
  urldate = {2023-06-22},
  abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
  isbn = {9780818671906},
  keywords = {app:american-sl,app:sign-language,author:starner,based-on:vision,claims-to-be-best,classes:40,fidelity:finger,have-read,model:hmm,tech:rgb,type:seminal},
  annotation = {994 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/477012/}},
  file = {/Users/brk/Zotero/storage/MI8H8BJY/Starner and Pentland - 1995 - Real-time American Sign Language recognition from .pdf}
}
% == BibTeX quality report for starnerRealtimeAmericanSign1995:
% ? unused Conference name ("International Symposium on Computer Vision - ISCV")
% ? unused Library catalog ("Semantic Scholar")

@article{starnerRealtimeAmericanSign1998,
  title = {Real-Time {{American}} Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, T. and Weaver, J. and Pentland, A.},
  year = {Dec./1998},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1371--1375},
  issn = {01628828},
  doi = {10.1109/34.735811},
  urldate = {2023-06-22},
  abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  keywords = {app:american-sl,app:sign-language,author:starner,based-on:vision,classes:40,fidelity:finger,have-read,model:hmm,type:seminal},
  annotation = {1394 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/735811/}},
  file = {/Users/brk/Zotero/storage/JITL4ER8/Starner et al. - 1998 - Real-time American sign language recognition using.pdf}
}
% == BibTeX quality report for starnerRealtimeAmericanSign1998:
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Machine Intell.")
% ? unused Library catalog ("Semantic Scholar")

@article{starnerVisualRecognitionAmerican1995,
  title = {Visual Recognition of American Sign Language Using Hidden Markov Models},
  author = {Starner, Thad and Pentland, Alex},
  year = {1995},
  month = may,
  abstract = {Abstract\textemdash We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American Sign Language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  keywords = {app:american-sl,app:sign-language,based-on:vision,classes:40,fidelity:finger,from:cite.bib,have-read,model:hmm,tech:rgb,type:paper},
  file = {/Users/brk/Zotero/storage/DE69PP87/Starner and Pentland - 1995 - Visual recognition of american sign language using.pdf}
}
% == BibTeX quality report for starnerVisualRecognitionAmerican1995:
% Missing required field 'journal'

@article{sturmanDesignMethodWholehand1993,
  title = {A Design Method for ``Whole-Hand'' Human-Computer Interaction},
  author = {Sturman, David J. and Zeltzer, David},
  year = {1993},
  month = jul,
  journal = {ACM Transactions on Information Systems},
  volume = {11},
  number = {3},
  pages = {219--238},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/159161.159159},
  urldate = {2023-07-02},
  abstract = {A disciplined investigation of ``whole-hand interfaces (often glove based, currently) and their appropriate use for the control of complex task domains is embodied by the design method for whole-hand input. This is a series of procedures\textemdash including a common basis for the description, design, and evaluation of whole-hand input, together with an accompanying taxonomy\textemdash that enumerates key issues and points for consideration in the development of whole-hand input. The method helps designers focus on task requirements, isolate problem areas, and choose appropriate whole-hand input strategies for their specified tasks. Several experiments were conducted to validate and demonstrate the use of the design method. The results of the experiments are summarized and discussed.},
  langid = {english},
  keywords = {based-on:gloves,evaluation-of-whole-hand-input,have-read,type:seminal},
  annotation = {30 citations (Crossref) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/159161.159159}},
  file = {/Users/brk/Zotero/storage/65J52Q6P/Sturman and Zeltzer - 1993 - A design method for “whole-hand” human-computer in.pdf}
}
% == BibTeX quality report for sturmanDesignMethodWholehand1993:
% ? unused Journal abbreviation ("ACM Trans. Inf. Syst.")
% ? unused Library catalog ("Semantic Scholar")

@article{sturmanSurveyGlovebasedInput1994,
  title = {A Survey of Glove-Based Input},
  author = {Sturman, D.J. and Zeltzer, D.},
  year = {1994},
  month = jan,
  journal = {IEEE Computer Graphics and Applications},
  volume = {14},
  number = {1},
  pages = {30--39},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/38.250916},
  urldate = {2023-06-23},
  abstract = {Clumsy intermediary devices constrain our interaction with computers and their applications. Glove-based input devices let us apply our manual dexterity to the task. We provide a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input. The bulk of development in glove-based input has taken place very recently, and not all of it is easily accessible in the literature. We present a cross-section of the field to date. Hand-tracking devices may use the following technologies: position tracking, optical tracking, marker systems, silhouette analysis, magnetic tracking or acoustic tracking. Actual glove technologies on the market include: Sayre glove, MIT LED glove, Digital Data Entry Glove, DataGlove, Dexterous HandMaster, Power Glove, CyberGlove and Space Glove. Various applications of glove technologies include projects into the pursuit of natural interfaces, systems for understanding signed languages, teleoperation and robotic control, computer-based puppetry, and musical performance.{$<<$}ETX{$>>$}},
  keywords = {based-on:gloves,have-read,read-priority-1,reference:doi.org/10.1007/978-4-431-55861-3\_10,reference:doi.org/10.1038/35079748,reference:doi.org/10.1109/2.84835,reference:doi.org/10.1109/case49439.2021.9551664,reference:doi.org/10.1109/IV.2002.1028752,reference:doi.org/10.1109/JSEN.2020.3040114,reference:doi.org/10.1117/3.374903.CH1,reference:doi.org/10.1145/29933,reference:doi.org/10.1145/67449.67495,reference:doi.org/10.2514/6.2012-2552,reference:doi.org/10.4108/ICST.PERVASIVEHEALTH.2014.255384,type:seminal,type:survey},
  annotation = {853 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1109/JSEN.2020.3040114","10.1145/67449.67495","10.1109/2.84835","10.4108/ICST.PERVASIVEHEALTH.2014.255384","10.2514/6.2012-2552","10.1145/29933","10.1117/3.374903.CH1","10.1109/mc.2020.2987515","10.1007/978-4-431-55861-3\_10","10.1109/IV.2002.1028752","10.1038/35079748"] ---},
  note = {\url{https://ieeexplore.ieee.org/document/250916/}},
  file = {/Users/brk/Zotero/storage/UDDK6KNT/Sturman and Zeltzer - 1994 - A survey of glove-based input.pdf}
}
% == BibTeX quality report for sturmanSurveyGlovebasedInput1994:
% ? unused Journal abbreviation ("IEEE Comput. Grap. Appl.")
% ? unused Library catalog ("Semantic Scholar")

@misc{sturmanWholehandInput1992,
  title = {Whole-Hand {{Input}}},
  author = {Sturman, David Joel},
  year = {1992},
  urldate = {2023-06-28},
  abstract = {This dissertation examines whole-hand input: the full and direct use of the hand's capabilities for the control of computer-mediated tasks. It presents the subject as a distinct study, independent of specific application or interface device. It includes a comprehensive discussion of the ideas, issues, and technologies relevant to the field. Whole-hand input is a powerful tool for the real-time control of complex computer-mediated tasks that require the manipulation and coordination of many degrees of freedom. By taking advantage of the innate naturalness, adaptability, and dexterity of the hand, whole-hand input techniques can provide performance superior to that of conventional devices (such as dials, mice, and joysticks) when applied to complex tasks.The important problems of whole-hand input involve appropriateness of use, control design, and device selection. The dissertation addresses these with a design method for whole-hand input by which an interface designer can discuss, develop, and evaluate techniques and devices for using whole-hand input in a particular application. Three experiments illustrate use of the design method and validate the principles of the thesis.A testbed and software library for investigating whole-hand input techniques is described. The testbed allows easy development and testing of whole-hand input with application simulations. The library is based on an abstract whole-hand input device type providing a standard interface to different physical whole-hand input devices. It features techniques for device calibration, posture recognition, and gesture recognition.Three prototype applications using the testbed, and one musical performance application demonstrate a variety of whole-hand input techniques including master-slave control, controlling task variables with hand shape, and gestural command input.The text concludes with detailed recommendations for future work to forward the understanding of the direct use of the hand as an input device.An accompanying videotape demonstrates the three experiments, the prototype applications, and shows a short section of the musical performance. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.) ftn*This work was supported in part by NHK (Japan Broadcasting Company), Defense Advanced Research Projects Agency-RADC Contract \#F30602-89-C-0022, and equipment grants from Hewlett-Packard, Inc},
  howpublished = {\url{https://scholar.googleusercontent.com/scholar?q=cache:Yk_AfzsiGZsJ:scholar.google.com/+sturman+whole+hand+input&hl=en&as_sdt=0,5}},
  keywords = {based-on:gloves,have-read,read-priority-5,type:seminal},
  file = {/Users/brk/Zotero/storage/I4VQXME4/Whole-hand Input.pdf;/Users/brk/Zotero/storage/RG78DTFU/scholar.html}
}

@inproceedings{suarezHandGestureRecognition2012,
  title = {Hand Gesture Recognition with Depth Images: {{A}} Review},
  shorttitle = {Hand Gesture Recognition with Depth Images},
  booktitle = {2012 {{IEEE RO-MAN}}: {{The}} 21st {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}}},
  author = {Suarez, Jesus and Murphy, Robin R.},
  year = {2012},
  month = sep,
  pages = {411--417},
  issn = {1944-9437},
  doi = {10.1109/ROMAN.2012.6343787},
  abstract = {This paper presents a literature review on the use of depth for hand tracking and gesture recognition. The survey examines 37 papers describing depth-based gesture recognition systems in terms of (1) the hand localization and gesture classification methods developed and used, (2) the applications where gesture recognition has been tested, and (3) the effects of the low-cost Kinect and OpenNI software libraries on gesture recognition research. The survey is organized around a novel model of the hand gesture recognition process. In the reviewed literature, 13 methods were found for hand localization and 11 were found for gesture classification. 24 of the papers included real-world applications to test a gesture recognition system, but only 8 application categories were found (and three applications accounted for 18 of the papers). The papers that use the Kinect and the OpenNI libraries for hand tracking tend to focus more on applications than on localization and classification methods, and show that the OpenNI hand tracking method is good enough for the applications tested thus far. However, the limitations of the Kinect and other depth sensors for gesture recognition have yet to be tested in challenging applications and environments.},
  keywords = {based-on:vision,have-read,tech:rgb,tech:rgbd,type:survey},
  annotation = {364 citations (Semantic Scholar/DOI) [2023-07-02]},
  file = {/Users/brk/Zotero/storage/HZA3IQ8T/Suarez and Murphy - 2012 - Hand gesture recognition with depth images A revi.pdf;/Users/brk/Zotero/storage/EYLG8BUK/6343787.html}
}
% == BibTeX quality report for suarezHandGestureRecognition2012:
% ? unused Library catalog ("IEEE Xplore")

@article{sunWiDrawEnablingHandsfree2015,
  title = {{{WiDraw}}: {{Enabling Hands-free Drawing}} in the {{Air}} on {{Commodity WiFi Devices}}},
  shorttitle = {{{WiDraw}}},
  author = {Sun, Li and Sen, Souvik and Koutsonikolas, Dimitrios and Kim, Kyu-Han},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {77--89},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790129},
  urldate = {2023-07-12},
  abstract = {This paper demonstrates that it is possible to leverage WiFi signals from commodity mobile devices to enable hands-free drawing in the air. While prior solutions require the user to hold a wireless transmitter, or require custom wireless hardware, or can only determine a pre-defined set of hand gestures, this paper introduces WiDraw, the first hand motion tracking system using commodity WiFi cards, and without any user wearables. WiDraw harnesses the Angle-of-Arrival values of incoming wireless signals at the mobile device to track the user's hand trajectory. We utilize the intuition that whenever the user's hand occludes a signal coming from a certain direction, the signal strength of the angle representing the same direction will experience a drop. Our software prototype using commodity wireless cards can track the user's hand with a median error lower than 5 cm. We use WiDraw to implement an in-air handwriting application that allows the user to draw letters, words, and sentences, and achieves a mean word recognition accuracy of 91\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,app:writing,based-on:wifi,tech:wifi,type:paper},
  annotation = {261 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2789168.2790129}}
}
% == BibTeX quality report for sunWiDrawEnablingHandsfree2015:
% ? unused Conference name ("MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking")
% ? unused Library catalog ("Semantic Scholar")

@article{supancicDepthBasedHandPose2015,
  title = {Depth-{{Based Hand Pose Estimation}}: {{Data}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Depth-{{Based Hand Pose Estimation}}},
  author = {Supancic, James S. and Rogez, Gregory and Yang, Yi and Shotton, Jamie and Ramanan, Deva},
  year = {2015},
  month = dec,
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
  pages = {1868--1876},
  publisher = {{IEEE}},
  address = {{Santiago}},
  doi = {10.1109/ICCV.2015.217},
  urldate = {2023-06-26},
  abstract = {Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.},
  isbn = {9781467383912},
  keywords = {based-on:vision,type:survey},
  annotation = {155 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7410574/}},
  file = {/Users/brk/Zotero/storage/HAGU84CM/Supancic et al. - 2015 - Depth-Based Hand Pose Estimation Data, Methods, a.pdf}
}
% == BibTeX quality report for supancicDepthBasedHandPose2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{takahashiHandGestureCoding1991,
  title = {Hand Gesture Coding Based on Experiments Using a Hand Gesture Interface Device},
  author = {Takahashi, Tomoichi and Kishino, Fumio},
  year = {1991},
  month = mar,
  journal = {ACM SIGCHI Bulletin},
  volume = {23},
  number = {2},
  pages = {67--74},
  issn = {0736-6906},
  doi = {10.1145/122488.122499},
  urldate = {2023-07-13},
  abstract = {It would be ideal for computer-human interaction if a computer could understand human gestures. Hand gestures are one means of interaction between computers and humans[1][2]. A hand gesture interface device, the VPL Data Glove               TM               , provides real-time information on a user's hand movement[3].},
  langid = {english},
  keywords = {app:japanese-sl,app:sign-language,based-on:gloves,classes:46,hardware:vpl-dataglove,have-read,model:pca,tech:flex,type:paper},
  annotation = {150 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{https://dl.acm.org/doi/10.1145/122488.122499}},
  file = {/Users/brk/Zotero/storage/WL8YGNIS/Takahashi and Kishino - 1991 - Hand gesture coding based on experiments using a h.pdf}
}
% == BibTeX quality report for takahashiHandGestureCoding1991:
% ? unused Journal abbreviation ("SIGCHI Bull.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{takemuraEvaluation3DObject1989,
  title = {An Evaluation of 3-{{D}} Object Pointing Using a Field Sequential Stereoscopic Display},
  author = {Takemura, H. and Tomono, A. and Kobayashi, Yukio},
  year = {1989},
  month = dec,
  urldate = {2023-07-13},
  abstract = {Experiments to measure user performance in 3-D object pointing using a field sequential stereoscopic display are described . First , huma.n performance in adjusting a random dot stereogram depth is measured to determine the possibility of pointing at a 3-D object. This experiment shows that the image displayed on the field sequential stereoscopic display can give enough depth information to its user who inputs 3-D coordinate with a mouse. Next, user performance in pointing at 3-D objects using a mouse as an input device is measured . The target for 3-D pointing and an arrow shaped 3-D cursor are in the form of wire frames and displayed stereoscopically. Finally, user performance using a 3-D magnetic tracking input device is tested. These experiments show that it is possible to point at a 3-D object on a field sequential stereoscopic display with relatively high accuracy. However, pointing at objects in depth takes much more time than pointing at objects in a plane. When a mouse is used , the required pointing time heavily depends on the direction of pointing. This tendency is due to the difficulties of depth detection using the field sequential stereoscopic display, and of manipulating a 3-D cursor with a 2-D input device. For the task tested, the 3-D magnetic tracking device was better in terms of the task completion time and error rate.},
  keywords = {have-read,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/An-evaluation-of-3-D-object-pointing-using-a-field-Takemura-Tomono/c92768bd2508b400fd7ba6be012fc3eb46caea6e}},
  file = {/Users/brk/Zotero/storage/94XM6AL6/Takemura et al. - 1989 - An evaluation of 3-D object pointing using a field.pdf}
}
% == BibTeX quality report for takemuraEvaluation3DObject1989:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{tanWiFingerLeveragingCommodity2016,
  title = {{{WiFinger}}: Leveraging Commodity {{WiFi}} for Fine-Grained Finger Gesture Recognition},
  shorttitle = {{{WiFinger}}},
  author = {Tan, Sheng and Yang, Jie},
  year = {2016},
  month = jul,
  journal = {Proceedings of the 17th ACM International Symposium on Mobile Ad Hoc Networking and Computing},
  pages = {201--210},
  publisher = {{ACM}},
  address = {{Paderborn Germany}},
  doi = {10.1145/2942358.2942393},
  urldate = {2023-07-12},
  abstract = {Gesture recognition has become increasingly important in human-computer interaction (HCI) and can support a broad array of emerging applications, such as smart home, virtual reality, and mobile gaming. Traditional approaches usually rely on dedicated sensors that are worn by the user or cameras that require line of sight. In this paper, we present fine-grained finger gesture recognition by using a single commodity WiFi device without requiring user to wear any sensors. Our low-cost system, WiFinger, takes advantages of the fine-grained Channel State Information (CSI) available from commodity WiFi devices and the prevalence of WiFi network infrastructures. It senses and identifies subtle movements of finger gestures by examining the unique patterns exhibited in the detailed CSI. In WiFigner, we devise environmental noise removal mechanism to mitigate the effect of signal dynamic due to the environment changes. Moreover, we propose to capture the intrinsic gesture behavior to deal with individual diversity and gesture inconsistency. Our experimental evaluation in both home and office environments demonstrates that our system can achieve over 93\% recognition accuracy and is robust to both environment changes and individual diversity. Results also show that our system can work with WiFi beacon signals and provides accurate gesture recognition under NLOS scenarios.},
  isbn = {9781450341844},
  langid = {english},
  keywords = {based-on:wifi,model:dynamic-time-warping,tech:wifi,type:paper},
  annotation = {234 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2942358.2942393}}
}
% == BibTeX quality report for tanWiFingerLeveragingCommodity2016:
% ? unused Conference name ("MobiHoc'16: The Seventeenth ACM International Symposium on Mobile Ad Hoc Networking and Computing")
% ? unused Library catalog ("Semantic Scholar")

@misc{texasinstrumentsCD74HC4067DataSheet2003,
  title = {{{CD74HC4067}} Data Sheet, Product Information and Support | {{TI}}.Com},
  author = {{Texas Instruments}},
  year = {2003},
  month = jul,
  journal = {CD74HC4067 data sheet, product information and support | TI.com},
  publisher = {{www.ti.com}},
  howpublished = {\url{https://www.ti.com/product/CD74HC4067}},
  keywords = {from:cite.bib,type:datasheet}
}
% == BibTeX quality report for texasinstrumentsCD74HC4067DataSheet2003:
% ? Possibly abbreviated journal title CD74HC4067 data sheet, product information and support | TI.com

@article{thariqahmedDeviceFreeHuman2020,
  title = {Device Free Human Gesture Recognition Using {{Wi-Fi CSI}}: {{A}} Survey},
  shorttitle = {Device Free Human Gesture Recognition Using {{Wi-Fi CSI}}},
  author = {Thariq Ahmed, Hasmath Farhana and Ahmad, Hafisoh and C.V., Aravind},
  year = {2020},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {87},
  pages = {103281},
  issn = {09521976},
  doi = {10.1016/j.engappai.2019.103281},
  urldate = {2023-07-12},
  abstract = {Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices.},
  langid = {english},
  keywords = {based-on:wifi,pdf:paywalled,tech:wifi,type:survey},
  annotation = {53 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0952197619302441}}
}
% == BibTeX quality report for thariqahmedDeviceFreeHuman2020:
% ? unused Library catalog ("Semantic Scholar")

@misc{thepandasdevelopmentteamPandasdevPandasPandas2020,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {{The Pandas Development Team}},
  year = {2020},
  month = feb,
  doi = {10.5281/zenodo.3509134},
  howpublished = {Zenodo},
  keywords = {from:cite.bib,type:software-lib},
  note = {\url{https://doi.org/10.5281/zenodo.3509134}}
}
% == BibTeX quality report for thepandasdevelopmentteamPandasdevPandasPandas2020:
% ? unused Version number ("latest")

@misc{thomasa.defantiUSNEAR60341631977,
  title = {{{US NEA R60-34-163 FINAL PROJECT REPORT}}},
  author = {{Thomas A. DeFanti} and {Daniel J. Sandin}},
  year = {1977},
  publisher = {{University of Illinois at Chicago Circle}},
  keywords = {based-on:gloves,hardware:sayre-glove,have-read,tech:optical-tubes,type:paper},
  file = {/Users/brk/Zotero/storage/V95YJFCN/Thomas A. DeFanti and Daniel J. Sandin - 1977 - US NEA R60-34-163 FINAL PROJECT REPORT.pdf}
}
% == BibTeX quality report for thomasa.defantiUSNEAR60341631977:
% ? Title looks like it was stored in title-case in Zotero

@misc{todo,
  author = {{TODO}}
}

@inproceedings{torrenceMartinNewellOriginal2006,
  title = {Martin {{Newell}}'s Original Teapot: {{Copyright}} Restrictions Prevent {{ACM}} from Providing the Full Text for This Work.},
  shorttitle = {Martin {{Newell}}'s Original Teapot},
  booktitle = {{{ACM SIGGRAPH}} 2006 {{Teapot}} on   - {{SIGGRAPH}} '06},
  author = {Torrence, Ann},
  year = {2006},
  pages = {29},
  publisher = {{ACM Press}},
  address = {{Boston, Massachusetts}},
  doi = {10.1145/1180098.1180128},
  urldate = {2023-08-01},
  isbn = {978-1-59593-364-5},
  langid = {english},
  keywords = {/unread,background},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-08-01]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=1180098.1180128}}
}
% == BibTeX quality report for torrenceMartinNewellOriginal2006:
% ? unused Conference name ("ACM SIGGRAPH 2006 Teapot")
% ? unused Library catalog ("DOI.org (Crossref)")

@inproceedings{tuulariSoapBoxPlatformUbiquitous2002,
  title = {{{SoapBox}}: {{A Platform}} for {{Ubiquitous Computing Research}} and {{Applications}}},
  shorttitle = {{{SoapBox}}},
  booktitle = {Pervasive {{Computing}}},
  author = {Tuulari, Esa and {Ylisaukko-oja}, Arto},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Mattern, Friedemann and Naghshineh, Mahmoud},
  year = {2002},
  volume = {2414},
  pages = {125--138},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45866-2_11},
  urldate = {2023-07-11},
  abstract = {Designing, implementing and evaluating prototypes is a normal way of doing technical research. In recent years we have seen lots of research prototypes specifically designed for context awareness, future user interfaces and intelligent environment research. The problem with this type of specialised prototypes is that their lifetime is rather short and the valuable work done for them is not easily reusable. Our approach has been different as we have deliberately aimed towards a multipurpose platform that would be suitable for various ubiquitous computing related research themes. In this article we present the design and implementation of the platform that is named as SoapBox (Sensing, Operating and Activating Peripheral Box). Its main features are wired and wireless communications, in-built sensors, small size and low power consumption. We also introduce some results of research projects that have already used the platform successfully. Finally we conclude the paper with application scenarios for further work.},
  isbn = {978-3-540-44060-4 978-3-540-45866-1},
  keywords = {based-on:gloves,from:cite.bib,hardware:custom,have-read,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {74 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://link.springer.com/10.1007/3-540-45866-2_11}}
}
% == BibTeX quality report for tuulariSoapBoxPlatformUbiquitous2002:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@inproceedings{umedaRecognitionMultiFontPrinted1982,
  title = {Recognition of {{Multi-Font Printed Chinese Characters}}},
  author = {Umeda, M.},
  year = {1982},
  urldate = {2023-07-18},
  abstract = {Semantic Scholar extracted view of "Recognition of Multi-Font Printed Chinese Characters" by M. Umeda},
  keywords = {/unread,background},
  note = {\url{https://www.semanticscholar.org/paper/Recognition-of-Multi-Font-Printed-Chinese-Umeda/1a2c1f0f5c3f788a92754b29ad1bfdd4d1c37ab5}}
}
% == BibTeX quality report for umedaRecognitionMultiFontPrinted1982:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{urrehmanDynamicHandGesture2022,
  title = {Dynamic {{Hand Gesture Recognition Using 3D-CNN}} and {{LSTM Networks}}},
  author = {Ur Rehman, Muneeb and Ahmed, Fawad and Attique Khan, Muhammad and Tariq, Usman and Abdulaziz Alfouzan, Faisal and M. Alzahrani, Nouf and Ahmad, Jawad},
  year = {2022},
  journal = {Computers, Materials \& Continua},
  volume = {70},
  number = {3},
  pages = {4675--4690},
  issn = {1546-2226},
  doi = {10.32604/cmc.2022.019586},
  urldate = {2023-06-22},
  abstract = {: Recognition of dynamic hand gestures in real-time is a difficult task because the system can never know when or from where the gesture starts and ends in a video stream. Many researchers have been working on vision-based gesture recognition due to its various applications. This paper proposes a deep learning architecture based on the combination of a 3D Convolutional Neural Network (3D-CNN) and a Long Short-Term Memory (LSTM) network. The proposed architecture extracts spatial-temporal information from video sequences input while avoiding extensive computation. The 3D-CNN is used for the extraction of spectral and spatial features which are then given to the LSTM network through which classification is carried out. The proposed model is a light-weight architecture with only 3.7 million training parameters. The model has been evaluated on 15 classes from the 20BN-jester dataset available publicly. The model was trained on 2000 video-clips per class which were separated into 80\% training and 20\% validation sets. An accuracy of 99\% and 97\% was achieved on training and testing data, respectively. We further show that the combination of 3D-CNN with LSTM gives superior results as compared to MobileNetv2 + LSTM.},
  langid = {english},
  keywords = {based-on:vision,classes:15,dataset:jester,have-read,model:cnn,model:lstm,model:nn,movement:dynamic,tech:rgb,type:paper},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.3390/s19245429","10.1109/CVPRW.2016.100","10.1109/ICCVW.2019.00349","10.1016/j.cma.2020.113226","10.1002/aisy.201900088","10.1109/ICCVW.2017.365","10.1016/j.patcog.2019.107037","10.1109/ICIT.2019.8755038","10.1109/ICVRV.2017.00091","10.2478/acss-2020-0007","10.1007/s00500-020-04860-5","10.1016/j.imavis.2017.01.010","10.1007/s11042-020-08852-3","10.1007/978-3-319-15554-8\_73","10.1016/j.imavis.2020.104090","10.15625/2525-2518/58/4/14742","10.1002/jemt.23447","10.1007/s10489-014-0629-7","10.1109/FG.2019.8756576","10.1109/ICCV.2015.510"] ---},
  note = {\url{https://www.techscience.com/cmc/v70n3/44942}},
  file = {/Users/brk/Zotero/storage/7NE5RC22/Ur Rehman et al. - 2022 - Dynamic Hand Gesture Recognition Using 3D-CNN and .pdf}
}
% == BibTeX quality report for urrehmanDynamicHandGesture2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{vamplewRecognitionSignLanguage2007,
  title = {Recognition of Sign Language Gestures Using Neural Networks},
  booktitle = {Neuropsychological {{Trends}}},
  author = {Vamplew, Simon},
  year = {2007},
  month = apr,
  number = {1},
  pages = {4},
  issn = {1970321X, 19703201},
  doi = {10.7358/neur-2007-001-vamp},
  urldate = {2023-07-02},
  abstract = {This paper describes the structure and performance of the SLARTI sign language recognition system developed at the University of Tasmania. SLARTI uses a modular architecture consisting of multiple feature-recognition neural networks and a nearest-neighbour classifier to recognise Australian sign language (Auslan) hand gestures.},
  keywords = {app:australian-sl,app:sign-language,model:ffnn,model:knn,model:nn,movement:dynamic,pdf:cant-find},
  annotation = {93 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://www.ledonline.it/NeuropsychologicalTrends/}}
}
% == BibTeX quality report for vamplewRecognitionSignLanguage2007:
% ? Unsure about the formatting of the booktitle
% ? unused Library catalog ("Semantic Scholar")

@book{vanrossumPythonReferenceManual2009,
  title = {Python 3 {{Reference Manual}}},
  author = {Van Rossum, Guido and Drake, Fred L.},
  year = {2009},
  publisher = {{CreateSpace}},
  address = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7},
  keywords = {from:cite.bib,type:software-lib}
}
% == BibTeX quality report for vanrossumPythonReferenceManual2009:
% ? Title looks like it was stored in title-case in Zotero

@article{vasconezHandGestureRecognition2022,
  title = {Hand {{Gesture Recognition Using EMG-IMU Signals}} and {{Deep Q-Networks}}},
  author = {V{\'a}sconez, Juan Pablo and Barona L{\'o}pez, Lorena Isabel and Valdivieso Caraguay, {\'A}ngel Leonardo and Benalc{\'a}zar, Marco E.},
  year = {2022},
  month = dec,
  journal = {Sensors},
  volume = {22},
  number = {24},
  pages = {9613},
  issn = {1424-8220},
  doi = {10.3390/s22249613},
  urldate = {2023-03-07},
  abstract = {Hand gesture recognition systems (HGR) based on electromyography signals (EMGs) and inertial measurement unit signals (IMUs) have been studied for different applications in recent years. Most commonly, cutting-edge HGR methods are based on supervised machine learning methods. However, the potential benefits of reinforcement learning (RL) techniques have shown that these techniques could be a viable option for classifying EMGs. Methods based on RL have several advantages such as promising classification performance and online learning from experience. In this work, we developed an HGR system made up of the following stages: pre-processing, feature extraction, classification, and post-processing. For the classification stage, we built an RL-based agent capable of learning to classify and recognize eleven hand gestures\textemdash five static and six dynamic\textemdash using a deep Q-network (DQN) algorithm based on EMG and IMU information. The proposed system uses a feed-forward artificial neural network (ANN) for the representation of the agent policy. We carried out the same experiments with two different types of sensors to compare their performance, which are the Myo armband sensor and the G-force sensor. We performed experiments using training, validation, and test set distributions, and the results were evaluated for user-specific HGR models. The final accuracy results demonstrated that the best model was able to reach up to 97.50\%{$\pm$}1.13\% and 88.15\%{$\pm$}2.84\% for the classification and recognition, respectively, with regard to static gestures, and 98.95\%{$\pm$}0.62\% and 90.47\%{$\pm$}4.57\% for the classification and recognition, respectively, with regard to dynamic gestures with the Myo armband sensor. The results obtained in this work demonstrated that RL methods such as the DQN are capable of learning a policy from online experience to classify and recognize static and dynamic gestures using EMG and IMU signals.},
  langid = {english},
  keywords = {app:ambiguous,based-on:gloves,classes:12,dataset:custom,fidelity:arm,fidelity:hand,hardware:myo-armband,has-picture,have-read,model:deep-q-network,model:ffnn,model:nn,movement:dynamic,movement:static,participants:85,reference:doi.org/10.1002/ajmg.a.62747,reference:doi.org/10.1038/nature14236,reference:doi.org/10.1109/CBS.2018.8612213,reference:doi.org/10.1109/ETCM53643.2021.9590677,reference:doi.org/10.1109/ICOIN.2018.8343257,reference:doi.org/10.1109/JSEN.2019.2904595,reference:doi.org/10.1109/METROI4.2018.8439044,reference:doi.org/10.1109/TCYB.2020.3007173,reference:doi.org/10.1109/TII.2017.2779814,reference:doi.org/10.1145/3163080.3163117,reference:doi.org/10.1609/aaai.v30i1.10295,reference:doi.org/10.3233/JIFS-169795,reference:doi.org/10.3390/s19183827,reference:doi.org/10.3390/s20072106,reference:doi.org/10.3390/s20092467,reference:doi.org/10.3390/s22155855,reference:doi.org/10.3390/s22176327,repetitions:15,segmentation:implicit,tech:emg,tech:imu,technique:reinforcement-learning,type:paper},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.23919/EUSIPCO.2017.8081366","10.1007/s10846-017-0725-0","10.1109/ETCM.2017.8247458","10.1109/TBME.2003.813539","10.1109/TCYB.2020.3007173","10.3390/s22207984","10.3390/s22218535","10.3390/s20072106","10.3390/s22176327","10.1109/JSEN.2019.2904595","10.1109/METROI4.2018.8439044","10.3390/s22155855","10.3390/s20092467","10.1007/978-3-030-97672-9\_32","10.1002/ajmg.a.62747"] ---},
  note = {\url{https://www.mdpi.com/1424-8220/22/24/9613}},
  file = {/Users/brk/Zotero/storage/AJ2WMLPJ/Vásconez et al. - 2022 - Hand Gesture Recognition Using EMG-IMU Signals and.pdf}
}
% == BibTeX quality report for vasconezHandGestureRecognition2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{vasishtDecimeterLevelLocalizationSingle2016,
  title = {Decimeter-{{Level Localization}} with a {{Single WiFi Access Point}}},
  booktitle = {Symposium on {{Networked Systems Design}} and {{Implementation}}},
  author = {Vasisht, Deepak and Kumar, Swarun and Katabi, D.},
  year = {2016},
  month = mar,
  urldate = {2023-07-12},
  abstract = {We present Chronos, a system that enables a single WiFi access point to localize clients to within tens of centimeters. Such a system can bring indoor positioning to homes and small businesses which typically have a single access point.    The key enabler underlying Chronos is a novel algorithm that can compute sub-nanosecond time-of-flight using commodity WiFi cards. By multiplying the time-of-flight with the speed of light, a MIMO access point computes the distance between each of its antennas and the client, hence localizing it. Our implementation on commodity WiFi cards demonstrates that Chronos's accuracy is comparable to state-of-the-art localization systems, which use four or five access points.},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Decimeter-Level-Localization-with-a-Single-WiFi-Vasisht-Kumar/3e204852e9315efe3df10b831246471cc52a8655}}
}
% == BibTeX quality report for vasishtDecimeterLevelLocalizationSingle2016:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  urldate = {2023-11-12},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1\textendash 3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  langid = {english},
  keywords = {/unread},
  annotation = {2646 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://www.nature.com/articles/s41586-019-1724-z}}
}
% == BibTeX quality report for vinyalsGrandmasterLevelStarCraft2019:
% ? unused Library catalog ("Semantic Scholar")

@article{virmaniPositionOrientationAgnostic2017,
  title = {Position and {{Orientation Agnostic Gesture Recognition Using WiFi}}},
  author = {Virmani, Aditya and Shahzad, Muhammad},
  year = {2017},
  month = jun,
  journal = {Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services},
  pages = {252--264},
  doi = {10.1145/3081333.3081340},
  urldate = {2023-07-12},
  abstract = {WiFi based gesture recognition systems have recently proliferated due to the ubiquitous availability of WiFi in almost every modern building. The key limitation of existing WiFi based gesture recognition systems is that they require the user to be in the same configuration (i.e., at the same position and in same orientation) when performing gestures at runtime as when providing training samples, which significantly restricts their practical usability. In this paper, we propose a WiFi based gesture recognition system, namely WiAG, which recognizes the gestures of the user irrespective of his/her configuration. The key idea behind WiAG is that it first requests the user to provide training samples for all gestures in only one configuration and then automatically generates virtual samples for all gestures in all possible configurations by applying our novel translation function on the training samples. Next, for each configuration, it generates a classification model using virtual samples corresponding to that configuration. To recognize gestures of a user at runtime, as soon as the user performs a gesture, WiAG first automatically estimates the configuration of the user and then evaluates the gesture against the classification model corresponding to that estimated configuration. Our evaluation results show that when user's configuration is not the same at runtime as at the time of providing training samples, WiAG significantly improves the gesture recognition accuracy from just 51.4\% to 91.4\%.},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,classes:6,tech:wifi,type:paper},
  annotation = {163 citations (Semantic Scholar/DOI) [2023-08-06] 144 citations (Crossref) [2023-08-06]},
  note = {\url{https://dl.acm.org/doi/10.1145/3081333.3081340}},
  file = {/Users/brk/Zotero/storage/XLDUCJ4U/Virmani and Shahzad - 2017 - Position and Orientation Agnostic Gesture Recognit.pdf}
}
% == BibTeX quality report for virmaniPositionOrientationAgnostic2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{viterbiErrorBoundsConvolutional1967,
  title = {Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm},
  author = {Viterbi, A.},
  year = {1967},
  month = apr,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {2},
  pages = {260--269},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.1967.1054010},
  urldate = {2023-07-11},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R\_\{0\} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R\_\{0\} and whose performance bears certain similarities to that of sequential decoding algorithms.},
  keywords = {background,model:hmm,type:seminal,viterbi},
  annotation = {5898 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/1054010/}}
}
% == BibTeX quality report for viterbiErrorBoundsConvolutional1967:
% ? unused Journal abbreviation ("IEEE Trans. Inform. Theory")
% ? unused Library catalog ("Semantic Scholar")

@article{vuleticSystematicLiteratureReview2019,
  title = {Systematic Literature Review of Hand Gestures Used in Human Computer Interaction Interfaces},
  author = {Vuletic, Tijana and Duffy, Alex and Hay, Laura and McTeague, Chris and Campbell, Gerard and Grealy, Madeleine},
  year = {2019},
  month = sep,
  journal = {International Journal of Human-Computer Studies},
  volume = {129},
  pages = {74--94},
  issn = {10715819},
  doi = {10.1016/j.ijhcs.2019.03.011},
  urldate = {2023-06-26},
  abstract = {Semantic Scholar extracted view of "Systematic literature review of hand gestures used in human computer interaction interfaces" by T. Vuletic et al.},
  langid = {english},
  keywords = {have-read,type:survey},
  annotation = {82 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S1071581918305676}},
  file = {/Users/brk/Zotero/storage/7VJ7GZXV/Vuletic et al. - 2019 - Systematic literature review of hand gestures used.pdf}
}
% == BibTeX quality report for vuleticSystematicLiteratureReview2019:
% ? unused Library catalog ("Semantic Scholar")

@article{waibelPhonemeRecognitionUsing1989,
  title = {Phoneme Recognition Using Time-Delay Neural Networks},
  author = {Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K.J.},
  year = {1989},
  month = mar,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {37},
  number = {3},
  pages = {328--339},
  issn = {00963518},
  doi = {10.1109/29.21701},
  urldate = {2023-07-18},
  abstract = {The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5\% correct while the rate obtained by the best of the HMMs was only 93.7\%. {$>$}},
  keywords = {/unread,background,model:nn,model:tdnn},
  annotation = {2938 citations (Semantic Scholar/DOI) [2023-07-18]},
  note = {\url{http://ieeexplore.ieee.org/document/21701/}}
}
% == BibTeX quality report for waibelPhonemeRecognitionUsing1989:
% ? unused Journal abbreviation ("IEEE Trans. Acoust., Speech, Signal Processing")
% ? unused Library catalog ("Semantic Scholar")

@article{wanExploreEfficientLocal2016,
  title = {Explore {{Efficient Local Features}} from {{RGB-D Data}} for {{One-Shot Learning Gesture Recognition}}},
  author = {Wan, Jun and Guo, Guodong and Li, Stan Z.},
  year = {2016},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {8},
  pages = {1626--1639},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2015.2513479},
  urldate = {2023-06-22},
  abstract = {Availability of handy RGB-D sensors has brought about a surge of gesture recognition research and applications. Among various approaches, one shot learning approach is advantageous because it requires minimum amount of data. Here, we provide a thorough review about one-shot learning gesture recognition from RGB-D data and propose a novel spatiotemporal feature extracted from RGB-D data, namely mixed features around sparse keypoints (MFSK). In the review, we analyze the challenges that we are facing, and point out some future research directions which may enlighten researchers in this field. The proposed MFSK feature is robust and invariant to scale, rotation and partial occlusions. To alleviate the insufficiency of one shot training samples, we augment the training samples by artificially synthesizing versions of various temporal scales, which is beneficial for coping with gestures performed at varying speed. We evaluate the proposed method on the Chalearn gesture dataset (CGD). The results show that our approach outperforms all currently published approaches on the challenging data of CGD, such as translated, scaled and occluded subsets. When applied to the RGB-D datasets that are not one-shot (e.g., the Cornell Activity Dataset-60 and MSR Daily Activity 3D dataset), the proposed feature also produces very promising results under leave-one-out cross validation or one-shot learning.},
  keywords = {app:medical,app:robotics,app:surgery,based-on:vision,dataset:chalearn,dataset:chalearn-gesture,dataset:cornell-activity-60,dataset:msr-daily-activity-3d,fidelity:finger,pdf:paywalled,tech:rgbd,type:paper},
  annotation = {93 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/7368923/}}
}
% == BibTeX quality report for wanExploreEfficientLocal2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Mach. Intell.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{wangchunliRealTimeLargeVocabulary2001,
  title = {A {{Real-Time Large Vocabulary Continuous Recognition System}} for {{Chinese Sign Language}}},
  booktitle = {Advances in {{Multimedia Information Processing}} \textemdash{} {{PCM}} 2001},
  author = {{Wang Chunli} and {Wen Gao} and {Zhaoguo Xuan}},
  editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan and Shum, Heung-Yeung and Liao, Mark and Chang, Shih-Fu},
  year = {2001},
  volume = {2195},
  pages = {150--157},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45453-5_20},
  urldate = {2023-07-02},
  abstract = {In this paper, a real-time system designed for recognizing continuous Chinese Sign Language (CSL) sentences with a 4800 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. The worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including states tying, still frame detecting and fast search algorithm. Experiments were carried out, and for real-time continuous sign recognition, the correct rate is over 90\%.},
  isbn = {978-3-540-42680-6 978-3-540-45453-3},
  keywords = {app:chinese-sl,app:sign-language,based-on:gloves,classes:4800,contains-more-references,hardware:cyberglove,have-read,model:hmm,pdf:paywalled,read-priority-1,type:paper},
  annotation = {31 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/3-540-45453-5_20}}
}
% == BibTeX quality report for wangchunliRealTimeLargeVocabulary2001:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@inproceedings{wangchunliRealTimeLargeVocabulary2002,
  title = {A {{Real-Time Large Vocabulary Recognition System}} for {{Chinese Sign Language}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human-Computer Interaction}}},
  author = {{Wang Chunli} and {Gao Wen} and {Ma Jiyong}},
  editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Wachsmuth, Ipke and Sowa, Timo},
  year = {2002},
  volume = {2298},
  pages = {86--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-47873-6_9},
  urldate = {2023-07-02},
  abstract = {The major challenge that faces Sign Language recognition now is to develop methods that will scale well with increasing vocabulary size. In this paper, a real-time system designed for recognizing Chinese Sign Language (CSL) signs with a 5100 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. An algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed. Then the worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including modifying the transferring probability, clustering the Gaussians and fast matching algorithm. Experiments show that techniques proposed in this paper are efficient on either recognition speed or recognition performance.},
  isbn = {978-3-540-43678-2 978-3-540-47873-7},
  keywords = {app:chinese-sl,app:sign-language,based-on:gloves,classes:5100,contains-more-references,hardware:cyberglove,have-read,model:hmm,type:paper},
  annotation = {38 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/3-540-47873-6_9}},
  file = {/Users/brk/Zotero/storage/QZ3Y7L3J/Chunli et al. - 2002 - A Real-Time Large Vocabulary Recognition System fo.pdf}
}
% == BibTeX quality report for wangchunliRealTimeLargeVocabulary2002:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{wangCSIbasedHumanSensing2021,
  title = {{{CSI-based}} Human Sensing Using Model-Based Approaches: A Survey},
  shorttitle = {{{CSI-based}} Human Sensing Using Model-Based Approaches},
  author = {Wang, Zhengjie and Huang, Zehua and Zhang, Chengming and Dou, Wenwen and Guo, Yinjing and Chen, Da},
  year = {2021},
  month = apr,
  journal = {Journal of Computational Design and Engineering},
  volume = {8},
  number = {2},
  pages = {510--523},
  issn = {2288-5048},
  doi = {10.1093/jcde/qwab003},
  urldate = {2023-07-12},
  abstract = {Abstract             Currently, human sensing draws much attention in the field of ubiquitous computing, and human sensing based on WiFi CSI (channel state information) becomes a hot research topic due to the easy deployment and availability of WiFi devices. Although various human sensing applications based on the CSI signal model are emerging, the model-based approach has not been studied thoroughly. This paper provides a comprehensive survey of the latest model-based human sensing methods and their applications. First, the CSI signal and framework of model-based human sensing methods are introduced. Then, related models and fundamental signal preprocessing techniques are described. Next, typical human sensing applications are investigated, and the crucial characteristics are summarized. Finally, the advantages, limitations, and future research trends of model-based human sensing methods are concluded in this paper.},
  langid = {english},
  keywords = {based-on:wifi,reference:doi.org/10.1016/j.engappai.2019.103281,reference:doi.org/10.1109/ACCESS.2019.2927644,reference:doi.org/10.1109/ACCESS.2019.2931088,reference:doi.org/10.1109/ACCESS.2019.2933987,reference:doi.org/10.1109/HPCC-SmartCity-DSS.2016.0085,reference:doi.org/10.1109/ICCV.2019.00096,reference:doi.org/10.1109/ISPA/IUCC.2017.00158,reference:doi.org/10.1109/JIOT.2015.2511805,reference:doi.org/10.1109/JIOT.2018.2822818,reference:doi.org/10.1109/JIOT.2019.2953488,reference:doi.org/10.1109/JIOT.2020.2989426,reference:doi.org/10.1109/JSYST.2020.2994062,reference:doi.org/10.1109/MASS.2018.00017,reference:doi.org/10.1109/MCOM.2017.1700143,reference:doi.org/10.1109/MCOM.2018.1700144,reference:doi.org/10.1109/PERCOM.2017.7917861,reference:doi.org/10.1109/PERCOM.2017.7917871,reference:doi.org/10.1109/TVT.2018.2810307,reference:doi.org/10.1109/TWC.2019.2914194,reference:doi.org/10.1145/2968219,reference:doi.org/10.1145/2971648.2971658,reference:doi.org/10.1145/2973750.2973764,reference:doi.org/10.1145/3084041,reference:doi.org/10.1145/3191785,reference:doi.org/10.1145/3264953,reference:doi.org/10.1145/3314420,tech:wifi,type:survey},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-12] --- DOIs\_of\_references: ["10.1109/TCOMM.2018.2874941","10.1109/PERCOM.2017.7917871","10.1145/3411816","10.1109/WoWMoM49955.2020.00056","10.1109/MASS.2018.00017","10.1109/ICCV.2019.00096","10.1109/JIOT.2020.2989426","10.1109/PERCOM.2017.7917861","10.1145/3084041","10.1109/JIOT.2015.2511805","10.1109/ACCESS.2019.2933987","10.1145/1851182.1851203","10.1145/2789168.2790093","10.1109/JIOT.2020.2990314","10.1109/CVPR.2018.00768","10.1145/2816795.2818072","10.1145/3130937","10.1109/JIOT.2019.2953488","10.1145/2973750.2973764","10.1109/MCOM.2017.1700143","10.1145/3314420","10.1145/2971648.2971658","10.1109/TWC.2019.2914194","10.1145/2968219","10.1145/3264953","10.1109/ACCESS.2019.2931088","10.1109/TVT.2018.2810307"] ---},
  note = {\url{https://academic.oup.com/jcde/article/8/2/510/6137731}},
  file = {/Users/brk/Zotero/storage/JW8EU7FV/Wang et al. - 2021 - CSI-based human sensing using model-based approach.pdf}
}
% == BibTeX quality report for wangCSIbasedHumanSensing2021:
% ? unused Library catalog ("Semantic Scholar")

@article{wangDeviceFreeHumanActivity2017,
  title = {Device-{{Free Human Activity Recognition Using Commercial WiFi Devices}}},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
  year = {2017},
  month = may,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {35},
  number = {5},
  pages = {1118--1131},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2017.2679658},
  urldate = {2023-07-12},
  abstract = {Since human bodies are good reflectors of wireless signals, human activities can be recognized by monitoring changes in WiFi signals. However, existing WiFi-based human activity recognition systems do not build models that can quantify the correlation between WiFi signal dynamics and human activities. In this paper, we propose a Channel State Information (CSI)-based human Activity Recognition and Monitoring system (CARM). CARM is based on two theoretical models. First, we propose a CSI-speed model that quantifies the relation between CSI dynamics and human movement speeds. Second, we propose a CSI-activity model that quantifies the relation between human movement speeds and human activities. Based on these two models, we implemented the CARM on commercial WiFi devices. Our experimental results show that the CARM achieves recognition accuracy of 96\% and is robust to environmental changes.},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {293 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7875148/}}
}
% == BibTeX quality report for wangDeviceFreeHumanActivity2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE J. Select. Areas Commun.")
% ? unused Library catalog ("Semantic Scholar")

@article{wangEeyesDevicefreeLocationoriented2014,
  title = {E-Eyes: Device-Free Location-Oriented Activity Identification Using Fine-Grained {{WiFi}} Signatures},
  shorttitle = {E-Eyes},
  author = {Wang, Yan and Liu, Jian and Chen, Yingying and Gruteser, Marco and Yang, Jie and Liu, Hongbo},
  year = {2014},
  month = sep,
  journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
  pages = {617--628},
  publisher = {{ACM}},
  address = {{Maui Hawaii USA}},
  doi = {10.1145/2639108.2639143},
  urldate = {2023-07-12},
  abstract = {Activity monitoring in home environments has become increasingly important and has the potential to support a broad array of applications including elder care, well-being management, and latchkey child safety. Traditional approaches involve wearable sensors and specialized hardware installations. This paper presents device-free location-oriented activity identification at home through the use of existing WiFi access points and WiFi devices (e.g., desktops, thermostats, refrigerators, smartTVs, laptops). Our low-cost system takes advantage of the ever more complex web of WiFi links between such devices and the increasingly fine-grained channel state information that can be extracted from such links. It examines channel features and can uniquely identify both in-place activities and walking movements across a home by comparing them against signal profiles. Signal profiles construction can be semi-supervised and the profiles can be adaptively updated to accommodate the movement of the mobile devices and day-to-day signal calibration. Our experimental evaluation in two apartments of different size demonstrates that our approach can achieve over 96\% average true positive rate and less than 1\% average false positive rate to distinguish a set of in-place and walking activities with only a single WiFi access point. Our prototype also shows that our system can work with wider signal band (802.11ac) with even higher accuracy.},
  isbn = {9781450327831},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {746 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2639108.2639143}}
}
% == BibTeX quality report for wangEeyesDevicefreeLocationoriented2014:
% ? unused Conference name ("MobiCom'14: The 20th Annual International Conference on Mobile Computing and Networking")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{wangEvaluationLocalSpatiotemporal2009,
  title = {Evaluation of Local Spatio-Temporal Features for Action Recognition},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2009},
  author = {Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  year = {2009},
  pages = {124.1-124.11},
  publisher = {{British Machine Vision Association}},
  address = {{London}},
  doi = {10.5244/C.23.124},
  urldate = {2023-07-11},
  abstract = {Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.},
  isbn = {978-1-901725-39-1},
  langid = {english},
  keywords = {based-on:vision,classes:25,have-read,model:svm,survey-finsh:2011,type:seminal,type:survey},
  annotation = {1494 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html}},
  file = {/Users/brk/Zotero/storage/QFB8QYCI/Wang et al. - 2009 - Evaluation of local spatio-temporal features for a.pdf}
}
% == BibTeX quality report for wangEvaluationLocalSpatiotemporal2009:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("British Machine Vision Conference 2009")
% ? unused Library catalog ("Semantic Scholar")

@article{wangGaitRecognitionUsing2016,
  title = {Gait Recognition Using Wifi Signals},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {363--373},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971670},
  urldate = {2023-07-12},
  abstract = {In this paper, we propose WifiU, which uses commercial WiFi devices to capture fine-grained gait patterns to recognize humans. The intuition is that due to the differences in gaits of different people, the WiFi signal reflected by a walking human generates unique variations in the Channel State Information (CSI) on the WiFi receiver. To profile human movement using CSI, we use signal processing techniques to generate spectrograms from CSI measurements so that the resulting spectrograms are similar to those generated by specifically designed Doppler radars. To extract features from spectrograms that best characterize the walking pattern, we perform autocorrelation on the torso reflection to remove imperfection in spectrograms. We evaluated WifiU on a dataset with 2,800 gait instances collected from 50 human subjects walking in a room with an area of 50 square meters. Experimental results show that WifiU achieves top-1, top-2, and top-3 recognition accuracies of 79.28\%, 89.52\%, and 93.05\%, respectively.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,based-on:wifi,participants:50,tech:wifi,type:paper},
  annotation = {420 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2971648.2971670}},
  file = {/Users/brk/Zotero/storage/MK6DA5HU/Wang et al. - 2016 - Gait recognition using wifi signals.pdf}
}
% == BibTeX quality report for wangGaitRecognitionUsing2016:
% ? unused Conference name ("UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing")
% ? unused Library catalog ("Semantic Scholar")

@article{wangHumanRespirationDetection2016,
  title = {Human Respiration Detection with Commodity Wifi Devices: Do User Location and Body Orientation Matter?},
  shorttitle = {Human Respiration Detection with Commodity Wifi Devices},
  author = {Wang, Hao and Zhang, Daqing and Ma, Junyi and Wang, Yasha and Wang, Yuxiang and Wu, Dan and Gu, Tao and Xie, Bing},
  year = {2016},
  month = sep,
  journal = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages = {25--36},
  publisher = {{ACM}},
  address = {{Heidelberg Germany}},
  doi = {10.1145/2971648.2971744},
  urldate = {2023-07-12},
  abstract = {Recent research has demonstrated the feasibility of detecting human respiration rate non-intrusively leveraging commodity WiFi devices. However, is it always possible to sense human respiration no matter where the subject stays and faces? What affects human respiration sensing and what's the theory behind? In this paper, we first introduce the Fresnel model in free space, then verify the Fresnel model for WiFi radio propagation in indoor environment. Leveraging the Fresnel model and WiFi radio propagation properties derived, we investigate the impact of human respiration on the receiving RF signals and develop the theory to relate one's breathing depth, location and orientation to the detectability of respiration. With the developed theory, not only when and why human respiration is detectable using WiFi devices become clear, it also sheds lights on understanding the physical limit and foundation of WiFi-based sensing systems. Intensive evaluations validate the developed theory and case studies demonstrate how to apply the theory to the respiration monitoring system design.},
  isbn = {9781450344616},
  langid = {english},
  keywords = {app:activity-inference,app:breathing-detection,based-on:wifi,tech:wifi,type:paper},
  annotation = {339 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2971648.2971744}}
}
% == BibTeX quality report for wangHumanRespirationDetection2016:
% ? unused Conference name ("UbiComp '16: The 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing")
% ? unused Library catalog ("Semantic Scholar")

@article{wangMEMSDevicesBasedHand2023,
  title = {{{MEMS Devices-Based Hand Gesture Recognition}} via {{Wearable Computing}}},
  author = {Wang, Huihui and Ru, Bo and Miao, Xin and Gao, Qin and Habib, Masood and Liu, Long and Qiu, Sen},
  year = {2023},
  month = apr,
  journal = {Micromachines},
  volume = {14},
  number = {5},
  pages = {947},
  issn = {2072-666X},
  doi = {10.3390/mi14050947},
  urldate = {2023-07-19},
  abstract = {Gesture recognition has found widespread applications in various fields, such as virtual reality, medical diagnosis, and robot interaction. The existing mainstream gesture-recognition methods are primarily divided into two categories: inertial-sensor-based and camera-vision-based methods. However, optical detection still has limitations such as reflection and occlusion. In this paper, we investigate static and dynamic gesture-recognition methods based on miniature inertial sensors. Hand-gesture data are obtained through a data glove and preprocessed using Butterworth low-pass filtering and normalization algorithms. Magnetometer correction is performed using ellipsoidal fitting methods. An auxiliary segmentation algorithm is employed to segment the gesture data, and a gesture dataset is constructed. For static gesture recognition, we focus on four machine learning algorithms, namely support vector machine (SVM), backpropagation neural network (BP), decision tree (DT), and random forest (RF). We evaluate the model prediction performance through cross-validation comparison. For dynamic gesture recognition, we investigate the recognition of 10 dynamic gestures using Hidden Markov Models (HMM) and Attention-Biased Mechanisms for Bidirectional Long- and Short-Term Memory Neural Network Models (Attention-BiLSTM). We analyze the differences in accuracy for complex dynamic gesture recognition with different feature datasets and compare them with the prediction results of the traditional long- and short-term memory neural network model (LSTM). Experimental results demonstrate that the random forest algorithm achieves the highest recognition accuracy and shortest recognition time for static gestures. Moreover, the addition of the attention mechanism significantly improves the recognition accuracy of the LSTM model for dynamic gestures, with a prediction accuracy of 98.3\%, based on the original six-axis dataset.},
  langid = {english},
  keywords = {/unread,untagged},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-07-19]},
  note = {\url{https://www.mdpi.com/2072-666X/14/5/947}},
  file = {/Users/brk/Zotero/storage/D6NZCZSG/Wang et al. - 2023 - MEMS Devices-Based Hand Gesture Recognition via We.pdf}
}
% == BibTeX quality report for wangMEMSDevicesBasedHand2023:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{wangPlacementMattersUnderstanding2022,
  title = {Placement {{Matters}}: {{Understanding}} the {{Effects}} of {{Device Placement}} for {{WiFi Sensing}}},
  shorttitle = {Placement {{Matters}}},
  author = {Wang, Xuanzhi and Niu, Kai and Xiong, Jie and Qian, Bochong and Yao, Zhiyun and Lou, Tairong and Zhang, Daqing},
  year = {2022},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {6},
  number = {1},
  pages = {1--25},
  issn = {2474-9567},
  doi = {10.1145/3517237},
  urldate = {2023-07-12},
  abstract = {WiFi-based contactless sensing has found numerous applications in the fields of smart home and health care owning to its low-cost, non-intrusive and privacy-preserving characteristics. While promising in many aspects, the limited sensing range and interference issues still exist, hindering the adoption of WiFi sensing in real world. In this paper, inspired by the SNR (signal-to-noise ratio) metric in communication theory, we propose a new metric named SSNR (sensing-signal-to-noise-ratio) to quantify the sensing capability of WiFi systems. We theoretically model the effect of transmitter-receiver distance on sensing coverage. We show that in LoS scenario, the sensing coverage area increases first from a small oval to a maximal one and then decreases. When the transmitter-receiver distance further increases, the coverage area is separated into two ovals located around the two transceivers respectively. We demonstrate that, instead of applying complex signal processing scheme or advanced hardware, by just properly placing the transmitter and receiver, the two well-known issues in WiFi sensing (i.e., small range and severe interference) can be greatly mitigated. Specifically, by properly placing the transmitter and receiver, the coverage of human walking sensing can be expanded by around 200\%. By increasing the transmitter-receiver distance, a target's fine-grained respiration can still be accurately sensed with one interferer sitting just 0.5 m away.},
  langid = {english},
  keywords = {based-on:wifi,tech:wifi,type:paper},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3517237}}
}
% == BibTeX quality report for wangPlacementMattersUnderstanding2022:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.")
% ? unused Library catalog ("Semantic Scholar")

@article{wangRealtimeHandtrackingColor2009,
  title = {Real-Time Hand-Tracking with a Color Glove},
  author = {Wang, Robert Y. and Popovi{\'c}, Jovan},
  year = {2009},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {28},
  number = {3},
  pages = {1--8},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/1531326.1531369},
  urldate = {2023-07-20},
  abstract = {Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applications enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality.},
  langid = {english},
  keywords = {/unread},
  note = {\url{https://dl.acm.org/doi/10.1145/1531326.1531369}}
}
% == BibTeX quality report for wangRealtimeHandtrackingColor2009:
% ? unused Journal abbreviation ("ACM Trans. Graph.")
% ? unused Library catalog ("DOI.org (Crossref)")

@article{wangRTFallRealTimeContactless2017,
  title = {{{RT-Fall}}: {{A Real-Time}} and {{Contactless Fall Detection System}} with {{Commodity WiFi Devices}}},
  shorttitle = {{{RT-Fall}}},
  author = {Wang, Hao and Zhang, Daqing and Wang, Yasha and Ma, Junyi and Wang, Yuxiang and Li, Shengjie},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {16},
  number = {2},
  pages = {511--526},
  issn = {1536-1233},
  doi = {10.1109/TMC.2016.2557795},
  urldate = {2023-07-12},
  abstract = {This paper presents the design and implementation of RT-Fall, a real-time, contactless, low-cost yet accurate indoor fall detection system using the commodity WiFi devices. RT-Fall exploits the phase and amplitude of the fine-grained Channel State Information (CSI) accessible in commodity WiFi devices, and for the first time fulfills the goal of segmenting and detecting the falls automatically in real-time, which allows users to perform daily activities naturally and continuously without wearing any devices on the body. This work makes two key technical contributions. First, we find that the CSI phase difference over two antennas is a more sensitive base signal than amplitude for activity recognition, which can enable very reliable segmentation of fall and fall-like activities. Second, we discover the sharp power profile decline pattern of the fall in the time-frequency domain and further exploit the insight for new feature extraction and accurate fall segmentation/detection. Experimental results in four indoor scenarios demonstrate that RT-fall consistently outperforms the state-of-the-art approach WiFall with 14~percent higher sensitivity and 10~percent higher specificity on average.},
  keywords = {app:activity-inference,app:fall-detection,based-on:wifi,tech:wifi,type:paper},
  annotation = {390 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7458198/}}
}
% == BibTeX quality report for wangRTFallRealTimeContactless2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. on Mobile Comput.")
% ? unused Library catalog ("Semantic Scholar")

@misc{wangTimeSeriesClassification2016,
  title = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}: {{A Strong Baseline}}},
  shorttitle = {Time {{Series Classification}} from {{Scratch}} with {{Deep Neural Networks}}},
  author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  year = {2016},
  month = dec,
  number = {arXiv:1611.06455},
  eprint = {1611.06455},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.06455},
  urldate = {2023-05-13},
  abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
  archiveprefix = {arxiv},
  howpublished = {\url{http://arxiv.org/abs/1611.06455}},
  keywords = {background,time-series},
  annotation = {1024 citations (Semantic Scholar/arXiv) [2023-05-13]},
  file = {/Users/brk/Zotero/storage/F8CWCQYK/Wang et al. - 2016 - Time Series Classification from Scratch with Deep .pdf;/Users/brk/Zotero/storage/NUUI7HZK/1611.html}
}
% == BibTeX quality report for wangTimeSeriesClassification2016:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{wangTrafficPoliceGesture2008,
  title = {Traffic {{Police Gesture Recognition}} Using {{Accelerometers}}},
  author = {Wang, Ben},
  year = {2008},
  urldate = {2023-07-11},
  abstract = {When an automatic traffic light system is not used due to too heavy traffic, the traffic would be controlled by traffic police gesture. This paper is about the design of a system so that the traffic lights can follow the traffic police gestures. To simplify the system, a unique mapping between the traffic police gestures and the orientation and movement of hands is defined. The hand motion characters are extracted by fixing a 3-axis accelerometer on the back of each hand. A 2level hierarchical classifier is used to recognize the gestures. First the gestures are categorized into three groups, according to the movement of each hand. Then a gesture is recognized by comparing it with the predefined templates. This real-time recognition algorithm is implemented by a micro-controller. It is envisaged that this will help drivers.},
  keywords = {app:traffic-police,based-on:gloves,classes:9,fidelity:arm,have-read,model:decision-tree,tech:accelerometer,type:paper},
  note = {\url{https://www.semanticscholar.org/paper/Traffic-Police-Gesture-Recognition-using-Wang/59715d7a74376e7a21ada77f3a7af6b854a35545}},
  file = {/Users/brk/Zotero/storage/69LXNATR/Wang - 2008 - Traffic Police Gesture Recognition using Accelerom.pdf}
}
% == BibTeX quality report for wangTrafficPoliceGesture2008:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{wangUnderstandingModelingWiFi2015,
  title = {Understanding and {{Modeling}} of {{WiFi Signal Based Human Activity Recognition}}},
  author = {Wang, Wei and Liu, Alex X. and Shahzad, Muhammad and Ling, Kang and Lu, Sanglu},
  year = {2015},
  month = sep,
  journal = {Proceedings of the 21st Annual International Conference on Mobile Computing and Networking},
  pages = {65--76},
  publisher = {{ACM}},
  address = {{Paris France}},
  doi = {10.1145/2789168.2790093},
  urldate = {2023-07-12},
  abstract = {Some pioneer WiFi signal based human activity recognition systems have been proposed. Their key limitation lies in the lack of a model that can quantitatively correlate CSI dynamics and human activities. In this paper, we propose CARM, a CSI based human Activity Recognition and Monitoring system. CARM has two theoretical underpinnings: a CSI-speed model, which quantifies the correlation between CSI value dynamics and human movement speeds, and a CSI-activity model, which quantifies the correlation between the movement speeds of different human body parts and a specific human activity. By these two models, we quantitatively build the correlation between CSI value dynamics and a specific human activity. CARM uses this correlation as the profiling mechanism and recognizes a given activity by matching it to the best-fit profile. We implemented CARM using commercial WiFi devices and evaluated it in several different environments. Our results show that CARM achieves an average accuracy of greater than 96\%.},
  isbn = {9781450336192},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,tech:wifi,type:paper},
  annotation = {790 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2789168.2790093}},
  file = {/Users/brk/Zotero/storage/CCUH4UTT/Wang et al. - 2015 - Understanding and Modeling of WiFi Signal Based Hu.pdf}
}
% == BibTeX quality report for wangUnderstandingModelingWiFi2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("MobiCom'15: The 21th Annual International Conference on Mobile Computing and Networking")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{wangUserindependentAccelerometerbasedGesture2013,
  title = {User-Independent Accelerometer-Based Gesture Recognition for Mobile Devices},
  booktitle = {{{ADCAIJ}}: {{Advances}} in {{Distributed Computing}} and {{Artificial Intelligence Journal}}},
  author = {Wang, Xian and Tarr{\'i}o, Paula and Bernardos, Ana Mar{\'i}a and Metola, Eduardo and Casar, Jos{\'e} Ram{\'o}n},
  year = {2013},
  month = jul,
  volume = {1},
  pages = {11--25},
  issn = {2255-2863},
  doi = {10.14201/ADCAIJ20121311125},
  urldate = {2023-03-07},
  abstract = {Many mobile devices embed nowadays inertial sensors. This enables new forms of human-computer interaction through the use of gestures (movements performed with the mobile device) as a way of communication. This paper presents an accelerometer-based gesture recognition system for mobile devices which is able to recognize a collection of 10 different hand gestures. The system was conceived to be light and to operate in a user-independent manner in real time. The recognition system was implemented in a smart phone and evaluated through a collection of user tests, which showed a recognition accuracy similar to other state-of-the art techniques and a lower computational complexity. The system was also used to build a human-robot interface that enables controlling a wheeled robot with the gestures made with the mobile phone},
  keywords = {app:robotics,based-on:gloves,classes:10,contains-more-references,fidelity:arm,hardware:mobile-devices,have-read,model:dynamic-time-warping,reference:doi.org/10.1002/bjs.1800630637,reference:doi.org/10.1017/S0021859600084422,reference:doi.org/10.1021/acs.chemrev.9b00463,reference:doi.org/10.1038/s41586-020-2405-7,reference:doi.org/10.1111/j.1420-9101.2004.00730.x,reference:doi.org/10.1112/PLMS/S3-68.3.641,reference:doi.org/10.1126/science.abm0829,reference:doi.org/10.1130/1052-5173(2004)014{$<$}0004:APAOMO{$>$}2.0.CO;2,reference:doi.org/10.1177/0261927X15610793,reference:doi.org/10.25073/2588-1132/vnumps.4289,tech:accelerometer,type:paper,unimpressive},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.25073/2588-1132/vnumps.4289","10.1017/S0021859600084422","10.1017/S0021859600084422","10.1130/1052-5173(2004)014{$<$}0004:APAOMO{$>$}2.0.CO;2","10.1021/acs.chemrev.9b00463","10.1177/0261927X15610793","10.1111/j.1420-9101.2004.00730.x","10.1126/science.abm0829","10.1112/PLMS/S3-68.3.641","10.1038/s41586-020-2405-7","10.1130/1052-5173(2004)014{$<$}0004:APAOMO{$>$}2.0.CO;2","10.1038/s41586-020-2405-7","10.1112/PLMS/S3-68.3.641","10.1111/j.1420-9101.2004.00730.x","10.1017/S0021859600084422","10.1017/S0021859600084422","10.25073/2588-1132/vnumps.4289","10.1126/science.abm0829","10.1021/acs.chemrev.9b00463","10.1002/bjs.1800630637","10.1177/0261927X15610793","10.1126/science.abm0829"] ---},
  note = {\url{https://revistas.usal.es/index.php/2255-2863/article/view/ADCAIJ20121311125}},
  file = {/Users/brk/Zotero/storage/2YFHA534/Wang et al. - 2013 - User-independent accelerometer-based gesture recog.pdf}
}
% == BibTeX quality report for wangUserindependentAccelerometerbasedGesture2013:
% ? unused Issue ("3")
% ? unused Journal abbreviation ("ADCAIJ")
% ? unused Library catalog ("Semantic Scholar")

@article{wangWeCanHear2014,
  title = {We Can Hear You with {{Wi-Fi}}!},
  author = {Wang, Guanhua and Zou, Yongpan and Zhou, Zimu and Wu, Kaishun and Ni, Lionel M.},
  year = {2014},
  month = sep,
  journal = {Proceedings of the 20th annual international conference on Mobile computing and networking},
  pages = {593--604},
  publisher = {{ACM}},
  address = {{Maui Hawaii USA}},
  doi = {10.1145/2639108.2639112},
  urldate = {2023-07-12},
  abstract = {Recent literature advances Wi-Fi signals to ``see'' people's motions and locations. This paper asks the following question: Can Wi-Fi ``hear'' our talks? We present WiHear, which enables Wi-Fi signals to ``hear'' our talks without deploying any devices. To achieve this, WiHear needs to detect and analyze fine-grained radio reflections from mouth movements. WiHear solves this micro-movement detection problem by introducing Mouth Motion Profile that leverages partial multipath effects and wavelet packet transformation. Since Wi-Fi signals do not require line-of-sight, WiHear can ``hear'' people talks within the radio range. Further, WiHear can simultaneously ``hear'' multiple people's talks leveraging MIMO technology. We implement WiHear on both USRP N210 platform and commercial Wi-Fi infrastructure. Results show that within our pre-defined vocabulary, WiHear can achieve detection accuracy of 91 percent on average for single individual speaking no more than six words and up to 74 percent for no more than three people talking simultaneously. Moreover, the detection accuracy can be further improved by deploying multiple receivers from different angles.},
  isbn = {9781450327831},
  langid = {english},
  keywords = {app:activity-inference,app:speech-detection,based-on:wifi,read-priority-1,tech:wifi,type:paper},
  annotation = {410 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2639108.2639112}},
  file = {/Users/brk/Zotero/storage/C48EEYJB/Wang et al. - 2014 - We can hear you with Wi-Fi!.pdf}
}
% == BibTeX quality report for wangWeCanHear2014:
% ? unused Conference name ("MobiCom'14: The 20th Annual International Conference on Mobile Computing and Networking")
% ? unused Library catalog ("Semantic Scholar")

@article{wangWiFallDeviceFreeFall2017,
  title = {{{WiFall}}: {{Device-Free Fall Detection}} by {{Wireless Networks}}},
  shorttitle = {{{WiFall}}},
  author = {Wang, Yuxi and Wu, Kaishun and Ni, Lionel M.},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {16},
  number = {2},
  pages = {581--594},
  issn = {1536-1233},
  doi = {10.1109/TMC.2016.2557792},
  urldate = {2023-07-12},
  abstract = {The world population is in the midst of a unique and irreversible process of aging. Fall, which is one of the major health threats and obstacles to independent living of elders, will aggravate the global pressure in elders' health care and injury rescue. Thus, automatic fall detection is highly in need. Current proposed fall detection systems either need hardware installation or disrupt people's daily life. These limitations make it hard to widely deploy fall detection systems in residential settings. In this work, we analyze the wireless signal propagation model considering human activities influence. We then propose a novel and truly unobtrusive detection method based on the advanced wireless technologies, which we call as WiFall. WiFall employs the time variability and special diversity of Channel State Information (CSI) as the indicator of human activities. As CSI is readily available in prevalent in-use wireless infrastructures, WiFall withdraws the need for hardware modification, environmental setup and worn or taken devices. We implement WiFall on laptops equipped with commercial 802.11n NICs. Two typical indoor scenarios and several layout schemes are examined. As demonstrated by the experimental results, WiFall yielded 87\% detection precision with false alarm rate of 18\% in average.},
  keywords = {app:activity-inference,app:fall-detection,based-on:wifi,tech:wifi,type:paper},
  annotation = {721 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7458186/}}
}
% == BibTeX quality report for wangWiFallDeviceFreeFall2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. on Mobile Comput.")
% ? unused Library catalog ("Semantic Scholar")

@article{warrens.mccullochLogicalCalculusIdeas1944,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {{Warren S. McCulloch} and {Walter Pitts}},
  year = {1944},
  month = jun,
  journal = {The Journal of Symbolic Logic},
  volume = {9},
  number = {2},
  pages = {49--50},
  publisher = {{Cambridge University Press}},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2268029},
  urldate = {2023-11-12},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0022481200062460/resource/name/firstPage-S0022481200062460a.jpg},
  langid = {english},
  keywords = {/unread},
  annotation = {29 citations (Semantic Scholar/DOI) [2023-11-12]},
  note = {\url{https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/warren-s-mcculloch-and-walter-pitts-a-logical-calculus-of-the-ideas-immanent-in-nervous-activity-bulletin-of-mathematical-biophysics-vol-5-1943-pp-115133/7DFDC43EC1E5BD05E9DA85E1C41A01BD}}
}

@article{waskomSeabornStatisticalData2021,
  title = {Seaborn: Statistical Data Visualization},
  author = {Waskom, Michael L.},
  year = {2021},
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {60},
  pages = {3021},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.03021},
  keywords = {from:cite.bib,type:software-lib},
  annotation = {1622 citations (Crossref) [2023-07-11] 1607 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://doi.org/10.21105/joss.03021}}
}

@inproceedings{watsonSurveyGestureRecognition1993,
  title = {A {{Survey}} of {{Gesture Recognition Techniques}}},
  author = {Watson, R.},
  year = {1993},
  urldate = {2023-07-02},
  abstract = {Processing speeds have increased dramatically bitmapped displays allow graph ics to be rendered and updated at increasing rates and in general computers have advanced to the point where they can assist humans in complex tasks Yet input technologies seem to cause the major bottleneck in performing these tasks under utilising the available resources and restricting the expressiveness of application use We use our hands constantly to interact with things pick them up move them transform their shape or activate them in some way In the same uncon scious way we gesticulate in communicating fundamental ideas stop come closer over there no agreed and so on Gestures are thus a natural and intuitive form of both interaction and communication This report develops the motivations for gestural input and surveys current gesture recognition techniques A recognition technique under development at TCD as part of the GLAD IN ART EP project is also introduced},
  keywords = {based-on:gloves,based-on:vision,have-read,type:survey},
  note = {\url{https://www.semanticscholar.org/paper/A-Survey-of-Gesture-Recognition-Techniques-Watson/23ec699d9a8c19e3807c53f85c564b9cfa172fff}},
  file = {/Users/brk/Zotero/storage/EHEKTWGH/Watson - 1993 - A Survey of Gesture Recognition Techniques.pdf}
}
% == BibTeX quality report for watsonSurveyGestureRecognition1993:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{weimerSyntheticVisualEnvironment1989,
  title = {A Synthetic Visual Environment with Hand Gesturing and Voice Input},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems {{Wings}} for the Mind - {{CHI}} '89},
  author = {Weimer, D. and Ganapathy, S. K.},
  year = {1989},
  pages = {235--240},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/67449.67495},
  urldate = {2023-07-13},
  abstract = {This paper describes a practical synthetic visual environment for use in CAD and teleoperation. Instead of using expensive head mounted display systems, we use a standard display and compute smooth shaded images using an AT\&T Pixel Machine. The interface uses a VPL DataGlove [9] to track the hand, bringing the synthetic world into the same space as the hand. Hand gesturing is used to implement a virtual control panel, and some 3D modeling tasks. When simple speech recognition was added it markedly improved the interface. We also outline what extensions might be needed for using this kind of interface for teleoperation.},
  isbn = {978-0-89791-301-0},
  langid = {english},
  keywords = {based-on:gloves,hardware:vpl-dataglove,have-read,type:paper},
  annotation = {138 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=67449.67495}}
}
% == BibTeX quality report for weimerSyntheticVisualEnvironment1989:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the SIGCHI conference")
% ? unused Library catalog ("Semantic Scholar")

@article{wengaoChineseSignLanguage2004,
  title = {A {{Chinese}} Sign Language Recognition System Based on {{SOFM}}/{{SRN}}/{{HMM}}},
  author = {{Wen Gao} and {Gaolin Fang} and {Debin Zhao} and {Yiqiang Chen}},
  year = {2004},
  month = dec,
  journal = {Pattern Recognition},
  volume = {37},
  number = {12},
  pages = {2389--2402},
  issn = {00313203},
  doi = {10.1016/S0031-3203(04)00165-7},
  urldate = {2023-07-02},
  abstract = {Semantic Scholar extracted view of "A Chinese sign language recognition system based on SOFM/SRN/HMM" by Wen Gao et al.},
  langid = {english},
  keywords = {app:chinese-sl,app:sign-language,based-on:gloves,classes:5113,contains-more-references,fidelity:finger,hardware:cyberglove,hardware:polhemus,have-read,model:hmm,model:nn,model:rnn,model:som,participants:6,repetitions:2,segmentation:implicit,type:paper},
  annotation = {128 citations},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320304001657}},
  file = {/Users/brk/Zotero/storage/X6ZWMRXU/Gao et al. - 2004 - A Chinese sign language recognition system based o.pdf}
}
% == BibTeX quality report for wengaoChineseSignLanguage2004:
% ? unused Library catalog ("Semantic Scholar")

@article{wenMachineLearningGlove2020,
  title = {Machine {{Learning Glove Using Self}}-{{Powered Conductive Superhydrophobic Triboelectric Textile}} for {{Gesture Recognition}} in {{VR}}/{{AR Applications}}},
  author = {Wen, Feng and Sun, Zhongda and He, Tianyiyi and Shi, Qiongfeng and Zhu, Minglu and Zhang, Zixuan and Li, Lianhui and Zhang, Ting and Lee, Chengkuo},
  year = {2020},
  month = jul,
  journal = {Advanced Science},
  volume = {7},
  number = {14},
  pages = {2000261},
  issn = {2198-3844, 2198-3844},
  doi = {10.1002/advs.202000261},
  urldate = {2023-03-07},
  abstract = {The rapid progress of Internet of things (IoT) technology raises an imperative demand on human machine interfaces (HMIs) which provide a critical linkage between human and machines. Using a glove as an intuitive and low-cost HMI can expediently track the motions of human fingers, resulting in a straightforward communication media of human\textendash machine interactions. When combining several triboelectric textile sensors and proper machine learning technique, it has great potential to realize complex gesture recognition with the minimalist-designed glove for the comprehensive control in both real and virtual space. However, humidity or sweat may negatively affect the triboelectric output as well as the textile itself. Hence, in this work, a facile carbon nanotubes/thermoplastic elastomer (CNTs/TPE) coating approach is investigated in detail to achieve superhydrophobicity of the triboelectric textile for performance improvement. With great energy harvesting and human motion sensing capabilities, the glove using the superhydrophobic textile realizes a low-cost and self-powered interface for gesture recognition. By leveraging machine learning technology, various gesture recognition tasks are done in real time by using gestures to achieve highly accurate virtual reality/augmented reality (VR/AR) controls including gun shooting, baseball pitching, and flower arrangement, with minimized effect from sweat during operation.},
  langid = {english},
  keywords = {app:augmented-reality,app:virtual-reality,based-on:gloves,classes:11,contains-more-references,dataset:custom,fidelity:finger,have-read,model:cnn,model:nn,movement:dynamic,read-priority-1,reference:doi.org/10.1002/ADFM.201906269,reference:doi.org/10.1007/s11605-014-2708-9,reference:doi.org/10.1016/j.bbi.2020.04.020,reference:doi.org/10.1016/J.NANOEN.2019.01.016,reference:doi.org/10.1016/J.NANOEN.2019.06.035,reference:doi.org/10.1016/J.NANOEN.2019.06.046,reference:doi.org/10.1016/j.nanoen.2019.104239,reference:doi.org/10.1016/j.scitotenv.2019.06.059,reference:doi.org/10.1021/acsnano.8b00282,reference:doi.org/10.1038/S41598-017-07625-7,reference:doi.org/10.1073/pnas.1907557116,reference:doi.org/10.1080/02786826.2017.1335390,reference:doi.org/10.1093/ehjci/ehaa946.1527,reference:doi.org/10.1109/TIT.2016.2621748,reference:doi.org/10.1126/SCIENCE.362.6417.905-H,reference:doi.org/10.1126/science.366.6471.1325-e,reference:doi.org/10.1158/1538-7445.AM2016-5246,reference:doi.org/10.1177/102490791602300511,reference:doi.org/10.1177/2349301120170130,reference:doi.org/10.18458/kb.2019.2.1,reference:doi.org/10.18458/kb.2019.3.1,reference:doi.org/10.23926/rpd.2023.v8.n1.e23020.id1710,reference:doi.org/10.32604/CMES.2019.07807,reference:doi.org/10.3390/jcm9113440,reference:doi.org/10.4314/BCSE.V33I2.5,repetitions:200,segmentation:explicit,tech:flex,tech:triboelectric-textile,type:paper},
  annotation = {189 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1093/ehjci/ehaa946.1527","10.1021/acsnano.8b00282","10.32604/CMES.2019.07807","10.1016/J.NANOEN.2019.06.035","10.18458/kb.2019.2.1","10.1158/1538-7445.AM2019-3151","10.18458/kb.2019.3.1","10.1126/SCIENCE.362.6417.905-H","10.4314/BCSE.V33I2.5","10.1073/pnas.1907557116","10.1021/acsnano.8b00282","10.1002/ADFM.201906269","10.23926/rpd.2023.v8.n1.e23020.id1710","10.1002/ADFM.201906269","10.3390/jcm9113440","10.1080/02786826.2017.1335390","10.1002/ADFM.201906269","10.1038/S41598-017-07625-7","10.1016/j.bbi.2020.04.020","10.1016/j.scitotenv.2019.06.059","10.1158/1538-7445.AM2016-5246","10.1016/J.NANOEN.2019.01.016","10.1109/TIT.2016.2621748","10.1126/science.366.6471.1325-e","10.32604/CMES.2019.07807","10.1016/J.GCA.2016.05.016","10.1002/jia2.25187","10.1177/2349301120170130","10.1002/ADFM.201906269","10.1016/j.nanoen.2019.104239","10.1016/J.NANOEN.2019.06.046","10.1016/j.nanoen.2019.104239","10.1177/102490791602300511"] ---},
  note = {\url{https://onlinelibrary.wiley.com/doi/10.1002/advs.202000261}},
  file = {/Users/brk/Zotero/storage/ETVGGB62/Wen et al. - 2020 - Machine Learning Glove Using Self‐Powered Conducti.pdf}
}
% == BibTeX quality report for wenMachineLearningGlove2020:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("Adv. Sci.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{werbosRegressionNewTools1974,
  title = {Beyond {{Regression}} : "{{New Tools}} for {{Prediction}} and {{Analysis}} in the {{Behavioral Sciences}}},
  shorttitle = {Beyond {{Regression}}},
  author = {Werbos, P.},
  year = {1974},
  urldate = {2023-11-12},
  abstract = {Semantic Scholar extracted view of "Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral Sciences" by P. Werbos},
  keywords = {/unread},
  note = {\url{https://www.semanticscholar.org/paper/Beyond-Regression-\%3A-\%22New-Tools-for-Prediction-and-Werbos/56623a496727d5c71491850e04512ddf4152b487}}
}
% == BibTeX quality report for werbosRegressionNewTools1974:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@book{werbosRootsBackpropagationOrdered1994,
  title = {The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting},
  shorttitle = {The Roots of Backpropagation},
  author = {Werbos, Paul J.},
  year = {1994},
  series = {Adaptive and Learning Systems for Signal Processing, Communications, and Control},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-59897-8},
  lccn = {QA76.87 .W43 1994},
  keywords = {/unread,Neural networks (Computer science),Prediction theory,Regression analysis}
}
% == BibTeX quality report for werbosRootsBackpropagationOrdered1994:
% ? unused Library catalog ("Library of Congress ISBN")
% ? unused Number of pages ("319")

@article{wheatlandStateArtHand2015,
  title = {State of the {{Art}} in {{Hand}} and {{Finger Modeling}} and {{Animation}}},
  author = {Wheatland, Nkenge and Wang, Yingying and Song, Huaguang and Neff, Michael and Zordan, Victor and J{\"o}rg, Sophie},
  year = {2015},
  month = may,
  journal = {Computer Graphics Forum},
  volume = {34},
  number = {2},
  pages = {735--760},
  issn = {01677055},
  doi = {10.1111/cgf.12595},
  urldate = {2023-07-12},
  abstract = {The human hand is a complex biological system able to perform numerous tasks with impressive accuracy and dexterity. Gestures furthermore play an important role in our daily interactions, and humans are particularly skilled at perceiving and interpreting detailed signals in communications. Creating believable hand motions for virtual characters is an important and challenging task. Many new methods have been proposed in the Computer Graphics community within the last years, and significant progress has been made towards creating convincing, detailed hand and finger motions. This state of the art report presents a review of the research in the area of hand and finger modeling and animation. Starting with the biological structure of the hand and its implications for how the hand moves, we discuss current methods in motion capturing hands, data-driven and physics-based algorithms to synthesize their motions, and techniques to make the appearance of the hand model surface more realistic. We then focus on areas in which detailed hand motions are crucial such as manipulation and communication. Our report concludes by describing emerging trends and applications for virtual hand animation.},
  langid = {english},
  keywords = {background,hand-modelling},
  annotation = {71 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://onlinelibrary.wiley.com/doi/10.1111/cgf.12595}},
  file = {/Users/brk/Zotero/storage/B7NNE55B/Wheatland et al. - 2015 - State of the Art in Hand and Finger Modeling and A.pdf}
}
% == BibTeX quality report for wheatlandStateArtHand2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{whiteheadGestureRecognitionAccelerometers2014,
  title = {Gesture {{Recognition}} with {{Accelerometers}} for {{Game Controllers}}, {{Phones}} and {{Wearables}}},
  author = {Whitehead, A.D.},
  year = {2014},
  month = apr,
  journal = {GSTF Journal on Computing (JoC)},
  volume = {3},
  doi = {10.7603/s40601-013-0042-9},
  abstract = {Hidden Markov Models have been effectively used in time series based pattern recognition problems in the past. This work explores using Hidden Markov Models (HMM) to do 3D gesture recognition from accelerometer data. Our work differs from much of the previous work in that we examine the use of discreet HMMs rather than continuous HMMs. An interesting side effect of this is that method is therefore theoretically transportable to other devices that have a 3D sensor output system. In essence this brings us a mechanism to use the HMM model across a series of different sensor devices for gesture recognition. We achieve recognition results with accuracy rates approaching 90 percent for users who are not in the training samples. The speed of our system is also of interest as we are able to classify gestures at a rate of several hundred times per second. As long as the sen-sor system is capable of outputting information about the 3 axes of motion, and the outputs can be discretized to volumetrically equivalent cubic sub-spaces; that information can then be used in this generic model for accurate, high speed gesture recognition.},
  keywords = {app:gaming,based-on:gloves,classes:7,contains-more-references,dataset:custom,fidelity:arm,from:cite.bib,hardware:custom,have-read,model:hmm,movement:dynamic,participants:5,reference:doi.org/10.1007/s10916-010-9571-3,reference:doi.org/10.1109/38.674971,reference:doi.org/10.1109/HAVE.2005.1545662,reference:doi.org/10.1109/ICIP.1998.727164,reference:doi.org/10.1109/ICPR.1998.711914,reference:doi.org/10.1145/1052380.1052385,reference:doi.org/10.1145/1125451.1125679,repetitions:60,segmentation:explicit,tech:accelerometer,type:paper},
  annotation = {0 citations (Crossref) [2023-07-11] 1 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1109/38.674971","10.1145/1052380.1052385","10.1145/1125451.1125679","10.1109/ICIP.1998.727164","10.1109/ICPR.1998.711914","10.1109/HAVE.2005.1545662","10.1007/s10916-010-9571-3","10.1007/s10916-010-9571-3","10.1109/HAVE.2005.1545662","10.1109/ICIP.1998.727164","10.1109/38.674971","10.1145/1052380.1052385","10.1145/1125451.1125679","10.1109/ICPR.1998.711914"] ---},
  file = {/Users/brk/Zotero/storage/26N5WRXC/Whitehead - 2014 - Gesture Recognition with Accelerometers for Game C.pdf}
}
% == BibTeX quality report for whiteheadGestureRecognitionAccelerometers2014:
% ? Title looks like it was stored in title-case in Zotero

@article{whitePrinciplesNeurodynamicsPerceptrons1963,
  title = {Principles of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  shorttitle = {Principles of {{Neurodynamics}}},
  author = {White, B. W. and Rosenblatt, Frank},
  year = {1963},
  month = dec,
  journal = {The American Journal of Psychology},
  volume = {76},
  number = {4},
  pages = {705},
  issn = {00029556},
  doi = {10.2307/1419730},
  urldate = {2023-06-07},
  abstract = {Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.},
  keywords = {background,perceptrons},
  annotation = {2231 citations (Semantic Scholar/DOI) [2023-06-07]},
  note = {\url{https://www.jstor.org/stable/1419730?origin=crossref}}
}
% == BibTeX quality report for whitePrinciplesNeurodynamicsPerceptrons1963:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{wilsonParametricHiddenMarkov1999,
  title = {Parametric Hidden {{Markov}} Models for Gesture Recognition},
  author = {Wilson, A.D. and Bobick, A.F.},
  year = {Sept./1999},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {9},
  pages = {884--900},
  issn = {01628828},
  doi = {10.1109/34.790429},
  urldate = {2023-06-22},
  abstract = {A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction.},
  keywords = {based-on:vision,claims-to-be-best,classes:2,hardware:polhemus,hardware:stive,have-read,model:hmm,movement:dynamic,repetitions:40,tech:rgb,type:paper},
  annotation = {675 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/790429/}},
  file = {/Users/brk/Zotero/storage/P9TEDP8E/Wilson and Bobick - 1999 - Parametric hidden Markov models for gesture recogn.pdf}
}
% == BibTeX quality report for wilsonParametricHiddenMarkov1999:
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Machine Intell.")
% ? unused Library catalog ("Semantic Scholar")

@article{wiseEvaluationFiberOptic1990,
  title = {Evaluation of a Fiber Optic Glove for Semi-Automated Goniometric Measurements},
  author = {Wise, Sam and Gardner, William and Sabelman, Eric and Valainis, Erik and Wong, Yuriko and Glass, Karen and Drace, John and Rosen, Joseph M.},
  year = {1990},
  journal = {The Journal of Rehabilitation Research and Development},
  volume = {27},
  number = {4},
  pages = {411},
  issn = {0748-7711},
  doi = {10.1682/JRRD.1990.10.0411},
  urldate = {2023-07-13},
  abstract = {Normal subjects were used to evaluate a fiber optic instrumented glove for semi-automated goniometric measurement. The glove electronically records and transmits hand and finger position to a host computer by measuring the amount of joint flexion. The glove was put through a series of range-of-motion (ROM) tests with five subjects. Metacarpal (MP) and proximal interphalangeal (PIP) joint angles of the five digits were compared during repetitive standardized motions to evaluate the glove's repeatability. The results showed an overall error of 5.6 degrees, as compared to an error of between 5 and 8 degrees with manual measurement. Additional tests were done to determine factors such as fit, grip force, and wrist motion that may contribute to the overall error. The glove should have applicability to some aspects of hand evaluation as a semi-automated goniometric measurement device.},
  langid = {english},
  keywords = {based-on:gloves,hardware:vpl-dataglove,have-read,referenced,tech:fibre-optic,tech:flex,type:paper},
  annotation = {141 citations (Semantic Scholar/DOI) [2023-07-13]},
  note = {\url{http://www.rehab.research.va.gov/jour/90/27/4/pdf/wise.pdf}},
  file = {/Users/brk/Zotero/storage/NISVG4S7/Wise et al. - 1990 - Evaluation of a fiber optic glove for semi-automat.pdf}
}
% == BibTeX quality report for wiseEvaluationFiberOptic1990:
% ? unused Journal abbreviation ("JRRD")
% ? unused Library catalog ("Semantic Scholar")

@article{wongMultiFeaturesCapacitiveHand2021,
  title = {Multi-{{Features Capacitive Hand Gesture Recognition Sensor}}: {{A Machine Learning Approach}}},
  shorttitle = {Multi-{{Features Capacitive Hand Gesture Recognition Sensor}}},
  author = {Wong, W. K. and Juwono, Filbert H. and Khoo, Brendan Teng Thiam},
  year = {2021},
  month = mar,
  journal = {IEEE Sensors Journal},
  volume = {21},
  number = {6},
  pages = {8441--8450},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2021.3049273},
  urldate = {2023-03-07},
  abstract = {Gesture recognition technology enables machines to understand human gestures. The technology is considered as a key enabler for gaming and virtual reality applications. In this paper, we propose an effective, low-cost capacitive sensor device to recognize hand gestures. In particular, we designed a prototype of a wearable capacitive sensor unit to capture the capacitance values from the electrodes placed on finger phalanges. The sensor captures finger capacitance values. Each gesture has specific finger capacitance values. We applied a running median filter to the output of the sensor and extracted 15 features for gesture classification training and testing tasks. Subsequently, various analyses were performed to provide more insights into the sensing data. We applied and compared two machine learning algorithms: Error Correction Output Code Support Vector Machines (ECOC-SVM) and \$\{K\}\$ -Nearest Neighbour (KNN) classifiers. The training and testing recognition rates were observed for both intra-participant and inter-participant data sets. Further, we introduced a feature compression approach derived from correlation analysis to reduce the complexity of the machine learning algorithms. Using cross validation, we achieved a classification rate of approximately 99\% for intra-participant data. We achieved a lower recognition rate of 97\% (average cross validation testing) for compressed feature data set using both machine learning approaches. For the inter-participant data, the recognition rate was 99\% (normalized feature data) using KNN and 97\% using ECOC-SVM. The research findings show that our recognition system is competitive and has an immense potential for further study.},
  keywords = {/unread,based-on:gloves,model:knn,model:svm,pdf:paywalled,tech:capacitive,type:paper},
  annotation = {35 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9314169/}}
}
% == BibTeX quality report for wongMultiFeaturesCapacitiveHand2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Sensors J.")
% ? unused Library catalog ("Semantic Scholar")

@article{wuDeepDynamicNeural2016,
  title = {Deep {{Dynamic Neural Networks}} for {{Multimodal Gesture Segmentation}} and {{Recognition}}},
  author = {Wu, Di and Pigou, Lionel and Kindermans, Pieter-Jan and Le, Nam Do-Hoang and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
  year = {2016},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {8},
  pages = {1583--1597},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2016.2537340},
  urldate = {2023-06-22},
  abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
  keywords = {based-on:vision,classes:20,dataset:chalearn,dataset:chalearn-lap,fidelity:finger,have-read,model:cnn,model:ffnn,model:hmm,model:nn,reference-in-results,tech:rgbd,type:paper},
  annotation = {373 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/7423804/}},
  file = {/Users/brk/Zotero/storage/UNZIHXE3/Wu et al. - 2016 - Deep Dynamic Neural Networks for Multimodal Gestur.pdf}
}
% == BibTeX quality report for wuDeepDynamicNeural2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Mach. Intell.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{wuGestureRecognition3D2009,
  title = {Gesture {{Recognition}} with a 3-{{D Accelerometer}}},
  booktitle = {Ubiquitous {{Intelligence}} and {{Computing}}},
  author = {Wu, Jiahui and Pan, Gang and Zhang, Daqing and Qi, Guande and Li, Shijian},
  editor = {Zhang, Daqing and Portmann, Marius and Tan, Ah-Hwee and Indulska, Jadwiga},
  year = {2009},
  volume = {5585},
  pages = {25--38},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02830-4_4},
  urldate = {2023-03-07},
  abstract = {Gesture-based interaction, as a natural way for human-computer interaction, has a wide range of applications in ubiquitous computing environment. This paper presents an acceleration-based gesture recognition approach, called FDSVM ( Frame-based Descriptor and multi-class SVM), which needs only a wearable 3-dimensional accelerometer. With FDSVM, firstly, the acceleration data of a gesture is collected and represented by a frame-based descriptor, to extract the discriminative information. Then a SVM-based multi-class gesture classifier is built for recognition in the nonlinear gesture feature space. Extensive experimental results on a data set with 3360 gesture samples of 12 gestures over weeks demonstrate that the proposed FDSVM approach significantly outperforms other four methods: DTW, Naive Bayes, C4.5 and HMM. In the user-dependent case, FDSVM achieves the recognition rate of 99.38\% for the 4 direction gestures and 95.21\% for all the 12 gestures. In the user-independent case, it obtains the recognition rate of 98.93\% for 4 gestures and 89.29\% for 12 gestures. Compared to other accelerometer-based gesture recognition approaches reported in literature FDSVM gives the best resulrs for both user-dependent and user-independent cases.},
  isbn = {978-3-642-02829-8 978-3-642-02830-4},
  keywords = {based-on:gloves,classes:12,fidelity:arm,from:cite.bib,hardware:wiimote,have-read,model:c4.5,model:dynamic-time-warping,model:hmm,model:naive-bayes,model:svm,read-priority-1,tech:accelerometer,type:paper},
  annotation = {133 citations (Crossref) [2023-07-11] 240 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://link.springer.com/10.1007/978-3-642-02830-4_4}},
  file = {/Users/brk/Zotero/storage/MF2YEJ85/Wu et al. - 2009 - Gesture Recognition with a 3-D Accelerometer.pdf}
}
% == BibTeX quality report for wuGestureRecognition3D2009:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Lecture Notes in Computer Science")

@article{wuNonInvasiveDetectionMoving2015,
  title = {Non-{{Invasive Detection}} of {{Moving}} and {{Stationary Human With WiFi}}},
  author = {Wu, Chenshu and Yang, Zheng and Zhou, Zimu and Liu, Xuefeng and Liu, Yunhao and Cao, Jiannong},
  year = {2015},
  month = nov,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {33},
  number = {11},
  pages = {2329--2342},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2015.2430294},
  urldate = {2023-07-12},
  abstract = {Non-invasive human sensing based on radio signals has attracted a great deal of research interest and fostered a broad range of innovative applications of localization, gesture recognition, smart health-care, etc., for which a primary primitive is to detect human presence. Previous works have studied the detection of moving humans via signal variations caused by human movements. For stationary people, however, existing approaches often employ a prerequisite scenario-tailored calibration of channel profile in human-free environments. Based on in-depth understanding of human motion induced signal attenuation reflected by PHY layer channel state information (CSI), we propose DeMan, a unified scheme for non-invasive detection of moving and stationary human on commodity WiFi devices. DeMan takes advantage of both amplitude and phase information of CSI to detect moving targets. In addition, DeMan considers human breathing as an intrinsic indicator of stationary human presence and adopts sophisticated mechanisms to detect particular signal patterns caused by minute chest motions, which could be destroyed by significant whole-body motion or hidden by environmental noises. By doing this, DeMan is capable of simultaneously detecting moving and stationary people with only a small number of prior measurements for model parameter determination, yet without the cumbersome scenario-specific calibration. Extensive experimental evaluation in typical indoor environments validates the great performance of DeMan in various human poses and locations and diverse channel conditions. Particularly, DeMan provides a detection rate of around 95\% for both moving and stationary people, while identifies human-free scenarios by 96\%, all of which outperforms existing methods by about 30\%.},
  keywords = {app:activity-inference,app:breathing-detection,based-on:wifi,tech:wifi,type:paper},
  annotation = {266 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7102722/}}
}
% == BibTeX quality report for wuNonInvasiveDetectionMoving2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE J. Select. Areas Commun.")
% ? unused Library catalog ("Semantic Scholar")

@article{wuWearableSystemRecognizing2016,
  title = {A {{Wearable System}} for {{Recognizing American Sign Language}} in {{Real-Time Using IMU}} and {{Surface EMG Sensors}}},
  author = {Wu, Jian and Sun, Lu and Jafari, Roozbeh},
  year = {2016},
  month = sep,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  volume = {20},
  number = {5},
  pages = {1281--1290},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2016.2598302},
  urldate = {2023-03-07},
  abstract = {A sign language recognition system translates signs performed by deaf individuals into text/speech in real time. Inertial measurement unit and surface electromyography (sEMG) are both useful modalities to detect hand/arm gestures. They are able to capture signs and the fusion of these two complementary sensor modalities will enhance system performance. In this paper, a wearable system for recognizing American Sign Language (ASL) in real time is proposed, fusing information from an inertial sensor and sEMG sensors. An information gain-based feature selection scheme is used to select the best subset of features from a broad range of well-established features. Four popular classification algorithms are evaluated for 80 commonly used ASL signs on four subjects. The experimental results show 96.16\% and 85.24\% average accuracies for intra-subject and intra-subject cross session evaluation, respectively, with the selected feature subset and a support vector machine classifier. The significance of adding sEMG for ASL recognition is explored and the best channel of sEMG is highlighted.},
  keywords = {app:american-sl,app:sign-language,based-on:gloves,claims-to-be-best,classes:80,fidelity:finger,hardware:custom,have-read,model:decision-tree,model:knn,model:naive-bayes,model:svm,movement:dynamic,observations:24000,participants:4,read-priority-1,reference:doi.org/10.1109/IEMBS.2002.1134383,repetitions:25,segmentation:implicit,tech:accelerometer,tech:emg,tech:imu,type:paper},
  annotation = {166 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1109/IEMBS.2002.1134383"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/7552525/}},
  file = {/Users/brk/Zotero/storage/2J63995L/Wu et al. - 2016 - A Wearable System for Recognizing American Sign La.pdf}
}
% == BibTeX quality report for wuWearableSystemRecognizing2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE J. Biomed. Health Inform.")
% ? unused Library catalog ("Semantic Scholar")

@article{wuWiTrajRobustIndoor2023,
  title = {{{WiTraj}}: {{Robust Indoor Motion Tracking With WiFi Signals}}},
  shorttitle = {{{WiTraj}}},
  author = {Wu, Dan and Zeng, Youwei and Gao, Ruiyang and Li, Shenjie and Li, Yang and Shah, Rahul C. and Lu, Hong and Zhang, Daqing},
  year = {2023},
  month = may,
  journal = {IEEE Transactions on Mobile Computing},
  volume = {22},
  number = {5},
  pages = {3062--3078},
  issn = {1536-1233, 1558-0660, 2161-9875},
  doi = {10.1109/TMC.2021.3133114},
  urldate = {2023-07-12},
  abstract = {WiFi-based device-free motion tracking systems track persons without requiring them to carry any device. Existing work has explored signal parameters such as time-of-flight (ToF), angle-of-arrival (AoA), and Doppler-frequency-shift (DFS) extracted from WiFi channel state information (CSI) to locate and track people in a room. However, they are not robust due to unreliable estimation of signal parameters. ToF and AoA estimations are not accurate for current standards-compliant WiFi devices that typically have only two antennas and limited channel bandwidth. On the other hand, DFS can be extracted relatively easily on current devices but is susceptible to the high noise level and random phase offset in CSI measurement, which results in a speed-sign-ambiguity problem and renders ambiguous walking speeds. This paper proposes WiTraj, a device-free indoor motion tracking system using commodity WiFi devices. WiTraj improves tracking robustness from three aspects: 1) It significantly improves DFS estimation quality by using the ratio of the CSI from two antennas of each receiver, 2) To better track human walking, it leverages multiple receivers placed at different viewing angles to capture human walking and then intelligently combines the best views to achieve a robust trajectory reconstruction, and, 3) It differentiates walking from in-place activities, which are typically interleaved in daily life, so that non-walking activities do not cause tracking errors. Experiments show that WiTraj can significantly improve tracking accuracy in typical environments compared to existing DFS-based systems. Evaluations across 9 participants and 3 different environments show that the median tracking error {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$<$}2.5\textbackslash\%\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mo{$><<$}/mml:mo{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}mml:mo{$>$}.{$<$}/mml:mo{$><$}mml:mn{$>$}5{$<$}/mml:mn{$><$}mml:mo{$>\%<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="wu-ieq1-3133114.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} for typical room-sized trajectories.},
  keywords = {app:activity-inference,based-on:wifi,participants:9,tech:wifi,type:paper},
  annotation = {13 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/9645160/}},
  file = {/Users/brk/Zotero/storage/CHVHRXH7/Wu et al. - 2023 - WiTraj Robust Indoor Motion Tracking With WiFi Si.pdf}
}
% == BibTeX quality report for wuWiTrajRobustIndoor2023:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. on Mobile Comput.")
% ? unused Library catalog ("Semantic Scholar")

@article{wynantsThreeMythsRisk2019,
  title = {Three Myths about Risk Thresholds for Prediction Models},
  author = {Wynants, Laure and {van Smeden}, Maarten and McLernon, David J. and Timmerman, Dirk and Steyerberg, Ewout W. and Van Calster, Ben and {on behalf of the Topic Group `Evaluating diagnostic tests and prediction models' of the STRATOS initiative}},
  year = {2019},
  month = oct,
  journal = {BMC Medicine},
  volume = {17},
  number = {1},
  pages = {192},
  issn = {1741-7015},
  doi = {10.1186/s12916-019-1425-3},
  urldate = {2023-03-24},
  abstract = {Clinical prediction models are useful in estimating a patient's risk of having a certain disease or experiencing an event in the future based on their current characteristics. Defining an appropriate risk threshold to recommend intervention is a key challenge in bringing a risk prediction model to clinical application; such risk thresholds are often defined in an ad hoc way. This is problematic because tacitly assumed costs of false positive and false negative classifications may not be clinically sensible. For example, when choosing the risk threshold that maximizes the proportion of patients correctly classified, false positives and false negatives are assumed equally costly. Furthermore, small to moderate sample sizes may lead to unstable optimal thresholds, which requires a particularly cautious interpretation of results.},
  keywords = {background},
  annotation = {68 citations (Semantic Scholar/DOI) [2023-03-24]},
  note = {\url{https://doi.org/10.1186/s12916-019-1425-3}},
  file = {/Users/brk/Zotero/storage/R6RTYDFE/Wynants et al. - 2019 - Three myths about risk thresholds for prediction m.pdf;/Users/brk/Zotero/storage/V9FAH7AE/s12916-019-1425-3.html}
}
% == BibTeX quality report for wynantsThreeMythsRisk2019:
% ? unused Library catalog ("BioMed Central")

@inproceedings{xieAccelerometerGestureRecognition2014,
  title = {Accelerometer {{Gesture Recognition}}},
  author = {Xie, Michael},
  year = {2014},
  urldate = {2023-03-07},
  abstract = {Our goal is to make gesture-based input for smartphones and smartwatches accurate and feasible to use. With a custom Android application to record accelerometer data for 5 gestures, we developed a highly accurate SVM classifier using only 1 training example per class. Our novel Dynamic-Threshold Truncation algorithm during preprocessing improved accuracy on 1 training example per class by 14\% and the addition of axis-wise Discrete Fourier Transform coefficient features improved accuracy on 1 training example per class by 5\%. With 5 gesture classes, 1 training example for each class, and 30 test examples for each class, our classifier achieves 96\% accuracy. With 5 training examples per class, the classifier achieves 98\% accuracy, which is greater than the 10-example accuracy of other efforts using HMM's[1, 2]. This makes it feasible for a real-time implementation of accelerometer-based gesture recognition to identify user-defined gestures with high accuracy while requiring little training effort from the user.},
  keywords = {app:ambiguous,based-on:gloves,classes:5,dataset:custom,fidelity:arm,hardware:mobile-devices,have-read,model:svm,movement:dynamic,participants:1,reference:doi.org/10.1016/j.pmcj.2009.07.007,reference:doi.org/10.1145/2451176.2451211,repetitions:31,segmentation:explicit,tech:accelerometer,technique:one-shot,type:paper},
  annotation = {--- DOIs\_of\_references: ["10.1016/j.pmcj.2009.07.007","10.1145/2451176.2451211","10.1016/j.pmcj.2009.07.007","10.1145/2451176.2451211"] ---},
  note = {\url{https://www.semanticscholar.org/paper/Accelerometer-Gesture-Recognition-Xie/c8bb38e46b7d97cc1a4ccd4a9d6df9717c1c3ff7}},
  file = {/Users/brk/Zotero/storage/EC67HB44/Xie - 2014 - Accelerometer Gesture Recognition.pdf}
}
% == BibTeX quality report for xieAccelerometerGestureRecognition2014:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{xingHandGestureRecognition2018,
  title = {Hand {{Gesture Recognition Based}} on {{Deep Learning Method}}},
  booktitle = {2018 {{IEEE Third International Conference}} on {{Data Science}} in {{Cyberspace}} ({{DSC}})},
  author = {Xing, Kai and Ding, Zhen and Jiang, Shuai and Ma, Xueyan and Yang, Kai and Yang, Chifu and Li, Xiang and Jiang, Feng},
  year = {2018},
  month = jun,
  pages = {542--546},
  publisher = {{IEEE}},
  address = {{Guangzhou}},
  doi = {10.1109/DSC.2018.00087},
  urldate = {2023-07-20},
  isbn = {978-1-5386-4210-8},
  keywords = {/unread},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-20]},
  note = {\url{https://ieeexplore.ieee.org/document/8411908/}}
}
% == BibTeX quality report for xingHandGestureRecognition2018:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("DOI.org (Crossref)")

@article{xuFingerwritingSmartwatchCase2015,
  title = {Finger-Writing with {{Smartwatch}}: {{A Case}} for {{Finger}} and {{Hand Gesture Recognition}} Using {{Smartwatch}}},
  shorttitle = {Finger-Writing with {{Smartwatch}}},
  author = {Xu, Chao and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2015},
  month = feb,
  journal = {Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications},
  pages = {9--14},
  publisher = {{ACM}},
  address = {{Santa Fe New Mexico USA}},
  doi = {10.1145/2699343.2699350},
  urldate = {2023-07-11},
  abstract = {Smartwatch is becoming one of the most popular wearable device with many major smartphone manufacturers such as Samsung and Apple releasing their smartwatches recently. Apart from the fitness applications, the smartwatch provides a rich user interface that has enabled many applications like instant messaging and email. Since the smartwatch is worn on the wrist, it introduces a unique opportunity to understand user's arm, hand and possibly finger movements using its accelerometer and gyroscope sensors. Although user's arm and hand gestures are likely to be identified with ease using the smartwatch sensors, it is not clear how much of user's finger gestures can be recognized. In this paper, we show that motion energy measured at the smartwatch is sufficient to uniquely identify user's hand and finger gestures. We identify essential features of accelerometer and gyroscope data that reflect the movements of tendons (passing through the wrist) when performing a finger or a hand gesture. With these features, we build a classifier that can uniquely identify 37 (13 finger, 14 hand and 10 arm) gestures with an accuracy of 98\textbackslash\%. We further extend our gesture recognition to identify the characters written by the user with her index finger on a surface, and show that such finger-writing can also be accurately recognized with nearly 95\% accuracy. Our presented results will enable many novel applications like remote control and finger-writing-based input to devices using smartwatch.},
  isbn = {9781450333917},
  langid = {english},
  keywords = {based-on:gloves,classes:37,fidelity:hand,hardware:smartwatch,pdf:cant-find,tech:imu,type:paper},
  annotation = {228 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://dl.acm.org/doi/10.1145/2699343.2699350}}
}
% == BibTeX quality report for xuFingerwritingSmartwatchCase2015:
% ? unused Conference name ("HotMobile '15: The 16th International Workshop on Mobile Computing Systems and Applications")
% ? unused Library catalog ("Semantic Scholar")

@article{xuHandGestureInteraction2006,
  title = {Hand {{Gesture Interaction}} for {{Virtual Training}} of {{SPG}}},
  author = {Xu, Deyou and Yao, Wuyun and Zhang, Yongliang},
  year = {2006},
  month = nov,
  journal = {16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)},
  pages = {672--676},
  publisher = {{IEEE}},
  address = {{Hangzhou}},
  doi = {10.1109/ICAT.2006.68},
  urldate = {2023-07-02},
  abstract = {We develop a virtual reality based driving training system of self-propelled gun (SPG). In order to make the interface of the system more powerful and natural, hand gesture interaction need to be incorporated into the system's interface. This paper discusses the use of hand gestures for interaction with the virtual training environment. We employ static hand gestures which coupled with hand translations and rotations as the method of interacting with the virtual training environment. An 18-sensor data glove is chosen for monitoring the movements of the fingers and the wrist. The feed-forward neural network is developed for recognizing gestures for use in virtual training application of artillery self-propelled gun (SPG). We present our approach for the algorithm design and implementation, and the use of the gestures in our application. The presented hand gesture interaction method can be effectively used in our virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way},
  isbn = {9780769527543},
  keywords = {model:ffnn,model:nn,movement:static,pdf:paywalled},
  annotation = {14 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/4089336/}}
}
% == BibTeX quality report for xuHandGestureInteraction2006:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{xuzhangFrameworkHandGesture2011,
  title = {A {{Framework}} for {{Hand Gesture Recognition Based}} on {{Accelerometer}} and {{EMG Sensors}}},
  author = {{Xu Zhang} and {Xiang Chen} and {Yun Li} and Lantz, V. and {Kongqiao Wang} and {Jihai Yang}},
  year = {2011},
  month = nov,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  volume = {41},
  number = {6},
  pages = {1064--1076},
  issn = {1083-4427, 1558-2426},
  doi = {10.1109/TSMCA.2011.2116004},
  urldate = {2023-03-07},
  abstract = {This paper presents a framework for hand gesture recognition based on the information fusion of a three-axis accelerometer (ACC) and multichannel electromyography (EMG) sensors. In our framework, the start and end points of meaningful gesture segments are detected automatically by the intensity of the EMG signals. A decision tree and multistream hidden Markov models are utilized as decision-level fusion to get the final results. For sign language recognition (SLR), experimental results on the classification of 72 Chinese Sign Language (CSL) words demonstrate the complementary functionality of the ACC and EMG sensors and the effectiveness of our framework. Additionally, the recognition of 40 CSL sentences is implemented to evaluate our framework for continuous SLR. For gesture-based control, a real-time interactive system is built as a virtual Rubik's cube game using 18 kinds of hand gestures as control commands. While ten subjects play the game, the performance is also examined in user-specific and user-independent classification. Our proposed framework facilitates intelligent and natural control in gesture-based interaction.},
  keywords = {app:chinese-sl,app:sign-language,based-on:gloves,classes:72,contains-more-references,dataset:custom,fidelity:finger,hardware:custom,have-read,model:decision-tree,model:hmm,participants:10,reference:doi.org/10.1007/s00779-005-0033-8,reference:doi.org/10.1016/j.clinph.2003.12.030,reference:doi.org/10.1016/j.patrec.2009.08.009,reference:doi.org/10.1109/10.914793,reference:doi.org/10.1109/34.735811,reference:doi.org/10.1109/ICASSP.2002.5743873,reference:doi.org/10.1109/ICCV.1998.710744,reference:doi.org/10.1109/IEMBS.2006.259428,reference:doi.org/10.1109/IEMBS.2007.4353509,reference:doi.org/10.1109/TSMCA.2010.2093883,reference:doi.org/10.1145/1052380.1052385,reference:doi.org/10.1145/1054972.1055039,reference:doi.org/10.1145/1452392.1452442,reference:doi.org/10.1186/s12913-020-05181-x,reference:doi.org/10.4236/jilsa.2010.21003,reference:doi.org/10.5120/IJCA2016909103,tech:accelerometer,tech:emg,type:paper},
  annotation = {518 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1109/ICASSP.2002.5743873","10.1145/1052380.1052385","10.1109/ICCV.1998.710744","10.1109/TBME.2005.856295","10.1145/1452392.1452442","10.1109/TSMCA.2010.2093883","10.4236/jilsa.2010.21003","10.1109/ICASSP.2002.5745027","10.1109/IEMBS.2004.1404221","10.1007/s00779-005-0033-8","10.1145/1378773.1378778","10.1109/TSMCA.2004.824852","10.1145/1647314.1647387","10.1109/IEMBS.2002.1134383","10.1109/ISWC.2003.1241392","10.1007/11492429\_77","10.1109/TSMCC.2006.875418","10.1016/j.patrec.2009.08.009","10.1109/34.735811","10.1186/s12913-020-05181-x","10.5120/IJCA2016909103"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/5735233/}},
  file = {/Users/brk/Zotero/storage/BP5X9NIT/Xu Zhang et al. - 2011 - A Framework for Hand Gesture Recognition Based on .pdf}
}
% == BibTeX quality report for xuzhangFrameworkHandGesture2011:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Syst., Man, Cybern. A")
% ? unused Library catalog ("Semantic Scholar")

@article{yamatoRecognizingHumanAction1992,
  title = {Recognizing Human Action in Time-Sequential Images Using Hidden {{Markov}} Model},
  author = {Yamato, J. and Ohya, J. and Ishii, K.},
  year = {1992},
  journal = {Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages = {379--385},
  publisher = {{IEEE Comput. Soc. Press}},
  address = {{Champaign, IL, USA}},
  doi = {10.1109/CVPR.1992.223161},
  urldate = {2023-07-11},
  abstract = {A human action recognition method based on a hidden Markov model (HMM) is proposed. It is a feature-based bottom-up approach that is characterized by its learning capability and time-scale invariability. To apply HMMs, one set of time-sequential images is transformed into an image feature vector sequence, and the sequence is converted into a symbol sequence by vector quantization. In learning human action categories, the parameters of the HMMs, one per category, are optimized so as to best describe the training sequences from the category. To recognize an observed sequence, the HMM which best matches the sequence is chosen. Experimental results for real time-sequential images of sports scenes show recognition rates higher than 90\%. The recognition rate is improved by increasing the number of people used to generate the training data, indicating the possibility of establishing a person-independent action recognizer.{$<<$}ETX{$>>$}},
  isbn = {9780818628559},
  keywords = {based-on:vision,claims-to-be-best,from:cite.bib,have-read,model:hmm,model:vector-quantization,movement:dynamic,tech:rgb,type:paper},
  annotation = {610 citations (Crossref) [2023-07-11] 1566 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/223161/}},
  file = {/Users/brk/Zotero/storage/M2J27ASX/Yamato et al. - 1992 - Recognizing human action in time-sequential images.pdf}
}
% == BibTeX quality report for yamatoRecognizingHumanAction1992:
% ? unused Conference name ("1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition")
% ? unused Library catalog ("Semantic Scholar")

@article{yangDynamicHandGesture2012,
  title = {Dynamic Hand Gesture Recognition Using Hidden {{Markov}} Models},
  author = {Yang, Zhong and Li, Yi and Chen, Weidong and Zheng, Yang},
  year = {2012},
  month = jul,
  journal = {2012 7th International Conference on Computer Science \& Education (ICCSE)},
  pages = {360--365},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/ICCSE.2012.6295092},
  urldate = {2023-07-11},
  abstract = {Hand gesture has become a powerful means for human-computer interaction. Traditional gesture recognition just consider hand trajectory. For some specific applications, such as virtual reality, more natural gestures are needed, which are complex and contain movement in 3-D space. In this paper, we introduce an HMM-based method to recognize complex single hand gestures. Gesture images are gained by a common web camera. Skin color is used to segment hand area from the image to form a hand image sequence. Then we put forward a state-based spotting algorithm to split continuous gestures. After that, feature extraction is executed on each gesture. Features used in the system contain hand position, velocity, size, and shape. We raise a data aligning algorithm to align feature vector sequences for training. Then an HMM is trained alone for each gesture. The recognition results demonstrate that our methods are effective and accurate.},
  isbn = {9781467302425 9781467302418 9781467302401},
  keywords = {based-on:vision,model:hmm,pdf:paywalled,tech:rgb,type:paper},
  annotation = {68 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/6295092/}}
}
% == BibTeX quality report for yangDynamicHandGesture2012:
% ? unused Conference name ("2012 7th International Conference on Computer Science & Education (ICCSE 2012)")
% ? unused Library catalog ("Semantic Scholar")

@article{yanminzhuVisionBasedHand2013,
  title = {Vision {{Based Hand Gesture Recognition}}},
  author = {{Yanmin Zhu} and {Zhibo Yang} and {Bo Yuan}},
  year = {2013},
  month = apr,
  journal = {2013 International Conference on Service Sciences (ICSS)},
  pages = {260--265},
  publisher = {{IEEE}},
  address = {{Shenzhen}},
  doi = {10.1109/ICSS.2013.40},
  urldate = {2023-07-11},
  abstract = {With the development of ubiquitous computing, current user interaction approaches with keyboard, mouse and pen are not sufficient. Due to the limitation of these devices the useable command set is also limited. Direct use of hands as an input device is an attractive method for providing natural Human Computer Interaction which has evolved from text-based interfaces through 2D graphical-based interfaces, multimedia-supported interfaces, to fully fledged multi-participant Virtual Environment (VE) systems. Imagine the human-computer interaction of the future: A 3D- application where you can move and rotate objects simply by moving and rotating your hand - all without touching any input device. In this paper a review of vision based hand gesture recognition is presented. The existing approaches are categorized into 3D model based approaches and appearance based approaches, highlighting their advantages and shortcomings and identifying the open issues.},
  isbn = {9781467362580 9780769549729},
  keywords = {based-on:vision,tech:rgb,type:survey},
  annotation = {28 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{http://ieeexplore.ieee.org/document/6519802/}},
  file = {/Users/brk/Zotero/storage/9VX9QTMU/Yanmin Zhu et al. - 2013 - Vision Based Hand Gesture Recognition.pdf}
}
% == BibTeX quality report for yanminzhuVisionBasedHand2013:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("2013 International Conference on Service Sciences (ICSS 2013)")
% ? unused Library catalog ("Semantic Scholar")

@article{yeasinVisualUnderstandingDynamic2000,
  title = {Visual Understanding of Dynamic Hand Gestures},
  author = {Yeasin, M. and Chaudhuri, S.},
  year = {2000},
  month = nov,
  journal = {Pattern Recognition},
  volume = {33},
  number = {11},
  pages = {1805--1817},
  issn = {00313203},
  doi = {10.1016/S0031-3203(99)00175-2},
  urldate = {2023-07-11},
  abstract = {Analysis of a dynamic hand gesture requires processing a spatio-temporal image sequence. The actual length of the sequence varies with each instantiation of the gesture. The key idea behind solving the problem is to translate the richness of the human gestural communication power to a machine for a better man\vphantom\{\}machine interaction. We propose a novel vision-based system for automatic interpretation of a limited set of dynamic hand gestures. This involves extracting the temporal signature of the hand motion from the performed gesture. The concept of motion energy is used to estimate the dominant motion from an image sequence. To achieve the desired result, we introduce the concept of modeling the dynamic hand gesture using a "nite state machine. The temporal signature is subsequently analyzed by the "nite state machine to interpret automatically the performed gesture. ( 2000 Pattern Recognition Society. Published by Elsevier},
  langid = {english},
  keywords = {based-on:vision,claims-to-be-best,classes:5,contains-more-references,have-read,model:finite-state-machine,movement:dynamic,tech:rgb,type:paper},
  annotation = {107 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320399001752}},
  file = {/Users/brk/Zotero/storage/3YFE7NYK/Yeasin and Chaudhuri - 2000 - Visual understanding of dynamic hand gestures.pdf}
}
% == BibTeX quality report for yeasinVisualUnderstandingDynamic2000:
% ? unused Library catalog ("Semantic Scholar")

@article{yoonHandGestureRecognition2001,
  title = {Hand Gesture Recognition Using Combined Features of Location, Angle and Velocity},
  author = {Yoon, Ho-Sub and Soh, Jung and Bae, Younglae J. and Seung Yang, Hyun},
  year = {2001},
  month = jan,
  journal = {Pattern Recognition},
  volume = {34},
  number = {7},
  pages = {1491--1501},
  issn = {00313203},
  doi = {10.1016/S0031-3203(00)00096-0},
  urldate = {2023-07-11},
  abstract = {The use of hand gesture provides an attractive alternative to cumbersome interface devices for human\textendash computer interaction (HCI). Many hand gesture recognition methods using visual analysis have been proposed: syntactical analysis, neural networks, the hidden Markov model (HMM). In our research, an HMM is proposed for various types of hand gesture recognition. In the preprocessing stage, this approach consists of three different procedures for hand localization, hand tracking and gesture spotting. The hand location procedure detects hand candidate regions on the basis of skin-color and motion. The hand tracking algorithm finds the centroids of the moving hand regions, connects them, and produces a hand trajectory. The gesture spotting algorithm divides the trajectory into real and meaningless segments. To construct a feature database, this approach uses a combined and weighted location, angle and velocity feature codes, and employs a k-means clustering algorithm for the HMM codebook. In our experiments, 2400 trained gestures and 2400 untrained gestures are used for training and testing, respectively. Those experimental results demonstrate that the proposed approach yields a satisfactory and higher recognition rate for user images of different hand size, shape and skew angle.},
  langid = {english},
  keywords = {based-on:vision,classes:48,have-read,model:hmm,tech:rgb,type:paper},
  annotation = {272 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320300000960}},
  file = {/Users/brk/Zotero/storage/AVUVID9X/Yoon et al. - 2001 - Hand gesture recognition using combined features o.pdf}
}
% == BibTeX quality report for yoonHandGestureRecognition2001:
% ? unused Library catalog ("Semantic Scholar")

@article{yousefiSurveyBehaviorRecognition2017,
  title = {A {{Survey}} on {{Behavior Recognition Using WiFi Channel State Information}}},
  author = {Yousefi, Siamak and Narui, Hirokazu and Dayal, Sankalp and Ermon, Stefano and Valaee, Shahrokh},
  year = {2017},
  month = oct,
  journal = {IEEE Communications Magazine},
  volume = {55},
  number = {10},
  pages = {98--104},
  issn = {0163-6804},
  doi = {10.1109/MCOM.2017.1700082},
  urldate = {2023-07-12},
  abstract = {In this article, we present a survey of recent advances in passive human behavior recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems. The movement of the human body parts cause changes in the wireless signal reflections, which result in variations in the CSI. By analyzing the data streams of CSIs for different activities and comparing them against stored models, human behavior can be recognized. This is done by extracting features from CSI data streams and using machine learning techniques to build models and classifiers. The techniques from the literature that are presented herein have great performance; however, instead of the machine learning techniques employed in these works, we propose to use deep learning techniques such as long-short term memory (LSTM) recurrent neural networking (RNN) and show the improved performance. We also discuss different challenges such as environment change, frame rate selection, and the multi-user scenario; and finally suggest possible directions for future work.},
  keywords = {based-on:wifi,model:lstm,model:nn,model:rnn,pdf:paywalled,tech:wifi,type:survey},
  annotation = {245 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/8067693/}}
}
% == BibTeX quality report for yousefiSurveyBehaviorRecognition2017:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Commun. Mag.")
% ? unused Library catalog ("Semantic Scholar")

@article{yuanHandGestureRecognition2020,
  title = {Hand {{Gesture Recognition}} Using {{Deep Feature Fusion Network}} Based on {{Wearable Sensors}}},
  author = {Yuan, Guan and Liu, Xiao and Yan, Qiuyan and Qiao, Shaojie and Wang, Zhixiao and Yuan, Li},
  year = {2020},
  journal = {IEEE Sensors Journal},
  pages = {1--1},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2020.3014276},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition is an important way for human machine interaction, and it is widely used in many areas, such as health care, smart home, virtual reality as well as other areas. While many valuable efforts have been made, it still lacks efficient ways to capture fine grain hand gesture as well as track data of long distance dependency in complex gesture. In this paper, we firstly design a novel data glove with two arm rings and a specially integrated three-dimensional flex sensor to capture fine grain motion from full arm and all knuckles. Secondly, an improved deep feature fusion network is proposed to detect long distance dependency in complex hand gestures. In order to track detailed motion features, a convolutional neural network based feature fusion strategy is given to fuse data from multi-sensors by extracting both shallow and deep features. Moreover, a residual module is introduced to avoid over fitting and gradient vanishing during deepening the neural network. Thirdly, a long short term memory (LSTM) model with fused feature vector as input is introduced to classify complex hand motions into corresponding categories. Results of comprehensive experiments demonstrate that our work performs better than related algorithms, especially in America sign language (with the precise of 99.93\%) and Chinese sign language (with the precise of 96.1\%).},
  keywords = {/unread,app:american-sl,app:chinese-sl,app:sign-language,based-on:gloves,model:cnn,model:lstm,model:nn,pdf:paywalled,tech:flex,type:paper},
  annotation = {32 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://ieeexplore.ieee.org/document/9158332/}}
}
% == BibTeX quality report for yuanHandGestureRecognition2020:
% ? unused Journal abbreviation ("IEEE Sensors J.")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{zabulisVisionBasedHandGesture2009,
  title = {Vision-{{Based Hand Gesture Recognition}} for {{Human-Computer Interaction}}},
  booktitle = {The {{Universal Access Handbook}}},
  author = {Zabulis, Xenophon and Baltzakis, Haris and Argyros, Antonis},
  editor = {Stephanidis, Constantine},
  year = {2009},
  month = jun,
  volume = {20091047},
  pages = {1--30},
  publisher = {{CRC Press}},
  doi = {10.1201/9781420064995-c34},
  urldate = {2023-06-26},
  abstract = {In recent years, research efforts seeking to provide more natural, human-centered means of interacting with computers have gained growing interest. A particularly important direction is that of perceptive user interfaces, where the computer is endowed with perceptive capabilities that allow it to acquire both implicit and explicit information about the user and the environment. Vision has the potential of carrying a wealth of information in a non-intrusive manner and at a low cost, therefore it constitutes a very attractive sensing modality for developing perceptive user interfaces. Proposed approaches for vision-driven interactive user interfaces resort to technologies such as head tracking, face and facial expression recognition, eye tracking and gesture recognition. In this paper, we focus our attention to vision-based recognition of hand gestures. The first part of the paper provides an overview of the current state of the art regarding the recognition of hand gestures as these are observed and recorded by typical video cameras. In order to make the review of the related literature tractable, this paper does not discuss:},
  isbn = {978-0-8058-6280-5 978-1-4200-6499-5},
  langid = {english},
  keywords = {based-on:vision,classes:4,contains-more-references,have-read,movement:static,type:survey},
  annotation = {171 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://www.crcnetbase.com/doi/abs/10.1201/9781420064995-c34}},
  file = {/Users/brk/Zotero/storage/24YMVRP3/Zabulis et al. - 2009 - Vision-Based Hand Gesture Recognition for Human-Co.pdf}
}
% == BibTeX quality report for zabulisVisionBasedHandGesture2009:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")
% ? unused Series title ("Human Factors and Ergonomics")

@article{zamankhanHandGestureRecognition2012,
  title = {Hand {{Gesture Recognition}}: {{A Literature Review}}},
  shorttitle = {Hand {{Gesture Recognition}}},
  author = {Zaman Khan, Rafiqul},
  year = {2012},
  month = jul,
  journal = {International Journal of Artificial Intelligence \& Applications},
  volume = {3},
  number = {4},
  pages = {161--174},
  issn = {09762191},
  doi = {10.5121/ijaia.2012.3412},
  urldate = {2023-07-03},
  abstract = {Hand gesture recognition system received great attention in the recent few years because of its manifoldness applications and the ability to interact with machine efficiently through human computer interaction. In this paper a survey of recent hand gesture recognition systems is presented. Key issues of hand gesture recognition system are presented with challenges of gesture system. Review methods of recent postures and gestures recognition system presented as well. Summary of research results of hand gesture methods, databases, and comparison between main gesture recognition phases are also given. Advantages and drawbacks of the discussed systems are explained finally.},
  keywords = {based-on:vision,have-read,model:fuzzy,model:hmm,model:nn,tech:rgb,type:survey},
  annotation = {165 citations (Semantic Scholar/DOI) [2023-07-03]},
  note = {\url{http://www.airccse.org/journal/ijaia/papers/3412ijaia12.pdf}},
  file = {/Users/brk/Zotero/storage/LTRX4QL9/Zaman Khan - 2012 - Hand Gesture Recognition A Literature Review.pdf}
}
% == BibTeX quality report for zamankhanHandGestureRecognition2012:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJAIA")
% ? unused Library catalog ("Semantic Scholar")

@article{zamankhanHandGestureRecognition2012a,
  title = {Hand {{Gesture Recognition}}: {{A Literature Review}}},
  shorttitle = {Hand {{Gesture Recognition}}},
  author = {Zaman Khan, Rafiqul},
  year = {2012},
  month = jul,
  journal = {International Journal of Artificial Intelligence \& Applications},
  volume = {3},
  number = {4},
  pages = {161--174},
  issn = {09762191},
  doi = {10.5121/ijaia.2012.3412},
  urldate = {2023-08-02},
  abstract = {Semantic Scholar extracted view of "Hand Gesture Recognition: A Literature Review" by R. Khan},
  keywords = {based-on:gloves,survey-finsh:2012,type:survey,untagged},
  annotation = {--- DOIs\_of\_references: ["10.5121/IJCSIT.2011.3203","10.1007/978-3-540-88513-9\_85","10.1177/00405175211034245","10.1109/ICONIP.2002.1199054","10.21307/IJSSIS-2017-283","10.5539/cis.v5n3p110","10.1109/ICADIWT.2008.4664396","10.1006/cviu.2000.0897","10.4103/jodp.jodp\_66\_22","10.1109/ICUMT57764.2022.9943504","10.1007/s001380050095","10.5120/IJCA2016909103","10.1109/icss.2013.40","10.1016/j.engappai.2009.03.008","10.1109/ICSMC.1997.637364","10.1108/edi-10-2018-0198","10.1016/j.eswa.2021.114683"] ---},
  note = {\url{http://www.airccse.org/journal/ijaia/papers/3412ijaia12.pdf}},
  file = {/Users/brk/Zotero/storage/GSRK2XM4/Zaman Khan - 2012 - Hand Gesture Recognition A Literature Review.pdf}
}
% == BibTeX quality report for zamankhanHandGestureRecognition2012a:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IJAIA")
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{zeltzerIntegratedGraphicalSimulation1989,
  title = {An Integrated Graphical Simulation Platform},
  author = {Zeltzer, D. and Pieper, S. and Sturman, D.},
  year = {1989},
  urldate = {2023-07-13},
  abstract = {This paper describes an integrated graphical simulation platform (IGSP) which provides a framework for constructing interactive simulations, specifically those oriented towards task level animation that is, animation for which the user specifies the tasks to be performed and the system determines the correct sequence of events and selection of tools to use in accomplishing that task . Our prototype system, which we call bolio, allows diverse applications to interact within a run-time environment, displaying their results on a common 3D graphics platform. Here we describe aspects of bolio 's design that allow applications to interact through a common network of constraints, including an integrated suite of tools that simulate kinematic , dynamic, and event-driven processes in virtual worlds. In addition , the IGSP architecture allows us to easily integrate gestural input from a new device the DataGlove that allows non-expert users to manipulate virtual objects directly in these microworlds.},
  keywords = {based-on:gloves,fidelity:finger,hardware:vpl-dataglove,have-read,type:paper,untagged},
  note = {\url{https://www.semanticscholar.org/paper/An-integrated-graphical-simulation-platform-Zeltzer-Pieper/fc652d52ab473d2315f9431d43e1df84eb829637}},
  file = {/Users/brk/Zotero/storage/54FTNNNV/Zeltzer et al. - 1989 - An integrated graphical simulation platform.pdf}
}
% == BibTeX quality report for zeltzerIntegratedGraphicalSimulation1989:
% Missing required field 'booktitle'
% ? unused Library catalog ("Semantic Scholar")

@article{zengAnalyzingShopperBehavior2015,
  title = {Analyzing {{Shopper}}'s {{Behavior}} through {{WiFi Signals}}},
  author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2015},
  month = may,
  journal = {Proceedings of the 2nd workshop on Workshop on Physical Analytics},
  pages = {13--18},
  publisher = {{ACM}},
  address = {{Florence Italy}},
  doi = {10.1145/2753497.2753508},
  urldate = {2023-07-12},
  abstract = {Substantial progress in WiFi-based indoor localization has proven that pervasiveness of WiFi can be exploited beyond its traditional use of internet access to enable a variety of sensing applications. Understanding shopper's behavior through physical analytics can provide crucial insights to the business owner in terms of effectiveness of promotions, arrangement of products and efficiency of services. However, analyzing shopper's behavior and browsing patterns is challenging. Since video surveillance can not used due to high cost and privacy concerns, it is necessary to design novel techniques that can provide accurate and efficient view of shopper's behavior. In this work, we propose WiFi-based sensing of shopper's behavior in a retail store. Specifically, we show that various states of a shopper such as standing near the entrance to view a promotion or walking quickly to proceed towards the intended item can be accurately classified by profiling Channel State Information (CSI) of WiFi. We recognize a few representative states of shopper's behavior at the entrance and inside the store, and show how CSI-based profile can be used to detect that a shopper is in one of the states with very high accuracy ({$\approx$} 90\%). We discuss the potential and limitations of CSI-based sensing of shopper's behavior and physical analytics in general.},
  isbn = {9781450334983},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,classes:5,tech:wifi,type:paper},
  annotation = {104 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2753497.2753508}}
}
% == BibTeX quality report for zengAnalyzingShopperBehavior2015:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Conference name ("MobiSys'15: The 13th Annual International Conference on Mobile Systems, Applications, and Services")
% ? unused Library catalog ("Semantic Scholar")

@article{zengFarSensePushingRange2019,
  title = {{{FarSense}}: {{Pushing}} the {{Range Limit}} of {{WiFi-based Respiration Sensing}} with {{CSI Ratio}} of {{Two Antennas}}},
  shorttitle = {{{FarSense}}},
  author = {Zeng, Youwei and Wu, Dan and Xiong, Jie and Yi, Enze and Gao, Ruiyang and Zhang, Daqing},
  year = {2019},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {3},
  number = {3},
  pages = {1--26},
  issn = {2474-9567},
  doi = {10.1145/3351279},
  urldate = {2023-07-12},
  abstract = {The past few years have witnessed the great potential of exploiting channel state information retrieved from commodity WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. On the other hand, most home environments only have one WiFi access point and it may not be located in the same room as the target. This sensing range constraint greatly limits the application of the proposed approaches in real life.             This paper presents FarSense--the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair. FarSense works well even when one of the transceivers is located in another room, moving a big step towards real-life deployment. We propose two novel schemes to achieve this goal: (1) Instead of applying the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range; (2) The division operation further enables us to utilize the phase information which is not usable with one single antenna for sensing. The orthogonal amplitude and phase are elaborately combined to address the "blind spots" issue and further increase the sensing range. Extensive experiments show that FarSense is able to accurately monitor human respiration even when the target is 8 meters away from the transceiver pair, increasing the sensing range by more than 100\%.1 We believe this is the first system to enable through-wall respiration sensing with commodity WiFi devices and the proposed method could also benefit other sensing applications.},
  langid = {english},
  keywords = {based-on:wifi,tech:wifi,type:paper},
  annotation = {20 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/3351279}},
  file = {/Users/brk/Zotero/storage/9DWYS5JZ/Zeng et al. - 2019 - FarSense Pushing the Range Limit of WiFi-based Re.pdf}
}
% == BibTeX quality report for zengFarSensePushingRange2019:
% ? unused Journal abbreviation ("Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.")
% ? unused Library catalog ("Semantic Scholar")

@article{zengWiWhoWiFiBasedPerson2016,
  title = {{{WiWho}}: {{WiFi-Based Person Identification}} in {{Smart Spaces}}},
  shorttitle = {{{WiWho}}},
  author = {Zeng, Yunze and Pathak, Parth H. and Mohapatra, Prasant},
  year = {2016},
  month = apr,
  journal = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
  pages = {1--12},
  publisher = {{IEEE}},
  address = {{Vienna}},
  doi = {10.1109/IPSN.2016.7460727},
  urldate = {2023-07-12},
  abstract = {There has been a growing interest in equipping the objects and environment surrounding the user with sensing capabilities. Smart indoor spaces such as smart homes and offices can implement the sensing and processing functionality, relieving users from the need of wearing or carrying smart devices. Enabling such smart spaces requires device-free effortless sensing of user's identity and activities. Device-free sensing using WiFi has shown great potential in such scenarios, however, fundamental questions such as person identification have remained unsolved. In this paper, we present WiWho, a framework that can identify a person from a small group of people in a device-free manner using WiFi. We show that Channel State Information (CSI) used in recent WiFi can identify a person's steps and walking gait. The walking gait being distinguishing characteristics for different people, WiWho uses CSI-based gait for person identification. We demonstrate how step and walk analysis can be used to identify a person's walking gait from CSI, and how this information can be used to identify a person. WiWho does not require a person to carry any device and is effortless since it only requires the person to walk for a few steps (e.g. entering a home or an office). We evaluate WiWho using experiments at multiple locations with a total of 20 volunteers, and show that it can identify a person with average accuracy of 92\% to 80\% from a group of 2 to 6 people. We also show that in most cases walking as few as 2-3 meters is sufficient to recognize a person's gait and identify the person. We discuss the potential and challenges of WiFi- based person identification with respect to smart space applications.},
  isbn = {9781509008025},
  keywords = {app:activity-inference,app:gait-inference,app:human-identification,based-on:wifi,participants:20,tech:wifi,type:paper},
  annotation = {303 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://ieeexplore.ieee.org/document/7460727/}}
}
% == BibTeX quality report for zengWiWhoWiFiBasedPerson2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{zhangEgoGestureNewDataset2018,
  title = {{{EgoGesture}}: {{A New Dataset}} and {{Benchmark}} for {{Egocentric Hand Gesture Recognition}}},
  shorttitle = {{{EgoGesture}}},
  author = {Zhang, Yifan and Cao, Congqi and Cheng, Jian and Lu, Hanqing},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Multimedia},
  volume = {20},
  number = {5},
  pages = {1038--1050},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2018.2808769},
  urldate = {2023-07-02},
  abstract = {Gesture is a natural interface in human\textendash computer interaction, especially interacting with wearable devices, such as VR/AR helmet and glasses. However, in the gesture recognition community, it lacks of suitable datasets for developing egocentric (first-person view) gesture recognition methods, in particular in the deep learning era. In this paper, we introduce a new benchmark dataset named EgoGesture with sufficient size, variation, and reality to be able to train deep neural networks. This dataset contains more than 24~000 gesture samples and 3~000~000 frames for both color and depth modalities from 50 distinct subjects. We design 83 different static and dynamic gestures focused on interaction with wearable devices and collect them from six diverse indoor and outdoor scenes, respectively, with variation in background and illumination. We also consider the scenario when people perform gestures while they are walking. The performances of several representative approaches are systematically evaluated on two tasks: gesture classification in segmented data and gesture spotting and recognition in continuous data. Our empirical study also provides an in-depth analysis on input modality selection and domain adaptation between different scenes.},
  keywords = {classes:83,dataset:egogesture,movement:dynamic,movement:static,observations:24000,participants:50,tech:rgbd,type:dataset},
  annotation = {143 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/8299578/}},
  file = {/Users/brk/Zotero/storage/K5WPV7AJ/Zhang et al. - 2018 - EgoGesture A New Dataset and Benchmark for Egocen.pdf}
}
% == BibTeX quality report for zhangEgoGestureNewDataset2018:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Multimedia")
% ? unused Library catalog ("Semantic Scholar")

@article{zhangGestureRecognitionBased2020,
  title = {Gesture Recognition Based on Deep Deformable {{3D}} Convolutional Neural Networks},
  author = {Zhang, Yifan and Shi, Lei and Wu, Yi and Cheng, Ke and Cheng, Jian and Lu, Hanqing},
  year = {2020},
  month = nov,
  journal = {Pattern Recognition},
  volume = {107},
  pages = {107416},
  issn = {00313203},
  doi = {10.1016/j.patcog.2020.107416},
  urldate = {2023-07-11},
  abstract = {Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jester's official leader-board until the submission time. The code and the trained models are released for better communication and future works.},
  langid = {english},
  keywords = {based-on:vision,classes:27,contains-more-references,dataset:chalearn,dataset:chalearn-isogd,dataset:egogesture,dataset:jester,fidelity:hand,have-read,model:cnn,model:nn,movement:dynamic,read-priority-1,type:paper},
  annotation = {23 citations (Semantic Scholar/DOI) [2023-07-11] --- DOIs\_of\_references: ["10.1109/ICCVW.2019.00349","10.1109/TITS.2014.2337331","10.1109/TCSVT.2017.2749509","10.58484/ssegl.v28i12757"] ---},
  note = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0031320320302193}},
  file = {/Users/brk/Zotero/storage/TH3LXEXM/Zhang et al. - 2020 - Gesture recognition based on deep deformable 3D co.pdf}
}
% == BibTeX quality report for zhangGestureRecognitionBased2020:
% ? unused Library catalog ("Semantic Scholar")

@article{zhangHandGestureRecognition2009,
  title = {Hand Gesture Recognition and Virtual Game Control Based on {{3D}} Accelerometer and {{EMG}} Sensors},
  author = {Zhang, Xu and Chen, Xiang and Wang, Wen-hui and Yang, Ji-hai and Lantz, Vuokko and Wang, Kong-qiao},
  year = {2009},
  month = feb,
  journal = {Proceedings of the 14th international conference on Intelligent user interfaces},
  pages = {401--406},
  publisher = {{ACM}},
  address = {{Sanibel Island Florida USA}},
  doi = {10.1145/1502650.1502708},
  urldate = {2023-03-07},
  abstract = {This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7\% in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.},
  isbn = {9781605581682},
  langid = {english},
  keywords = {based-on:gloves,classes:18,fidelity:arm,hardware:delsys-myomonitor-iv,have-read,model:hmm,model:nearest-conflicting-neighbors,participants:5,reference:doi.org/10.1109/ICASSP.2004.1326121,reference:doi.org/10.1109/IEMBS.2002.1134383,reference:doi.org/10.1109/ISWC.2003.1241392,reference:doi.org/10.1145/1054972.1055039,reference:doi.org/10.1145/1378773.1378778,reference:doi.org/10.5120/IJCA2016909103,tech:accelerometer,tech:emg,type:paper},
  annotation = {229 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1145/1054972.1055039","10.1109/ICASSP.2004.1326121","10.1109/ISWC.2003.1241392","10.5120/IJCA2016909103","10.1109/ISWC.2003.1241392","10.1145/1054972.1055039","10.1145/1378773.1378778","10.5120/IJCA2016909103","10.1109/IEMBS.2002.1134383","10.1109/ICASSP.2004.1326121"] ---},
  note = {\url{https://dl.acm.org/doi/10.1145/1502650.1502708}},
  file = {/Users/brk/Zotero/storage/E3IQUGNT/Zhang et al. - 2009 - Hand gesture recognition and virtual game control .pdf}
}
% == BibTeX quality report for zhangHandGestureRecognition2009:
% ? unused Conference name ("IUI09: 14th International Conference on Intelligent User Interfaces")
% ? unused Library catalog ("Semantic Scholar")

@article{zhangMudraUserfriendlyFinegrained2016,
  title = {Mudra: {{User-friendly Fine-grained Gesture Recognition}} Using {{WiFi Signals}}},
  shorttitle = {Mudra},
  author = {Zhang, Ouyang and Srinivasan, Kannan},
  year = {2016},
  month = dec,
  journal = {Proceedings of the 12th International on Conference on emerging Networking EXperiments and Technologies},
  pages = {83--96},
  publisher = {{ACM}},
  address = {{Irvine California USA}},
  doi = {10.1145/2999572.2999582},
  urldate = {2023-07-12},
  abstract = {There has been a great interest in recognizing gestures using wireless communication signals. We are motivated in detecting extremely fine, subtle finger gestures with WiFi signals. We envision this technology to find applications in finger-gesture control, disabled-friendly devices, physical therapy etc. The requirements of mm-level sensitivity and user-friendly feature using existing WiFi signals pose great challenges. Here, we present Mudra, a fine-grained finger gesture recognition system which leverages WiFi signals to enable a near-human-to-machine interaction with finger motion. Mudra uses a two-antenna receiver to detect and recognize finger gesture. It uses the signals received from one antenna to cancel the signal from the other. This "cancellation" is extremely sensitive to and enables us detect small variation in channel due to finger movements. Since Mudra decodes gestures with existing WiFi transmissions, Mudra enables gesture recognition without sacrificing WiFi transmission opportunities. Besides, Mudra is user-friendly with no need of user training. To demonstrate Mudra, we implement prototype on the NI-based SDR platform and use COTS WiFi adapter. We evaluate Mudra in a typical office environment. The results show that our system can achieve 96\% accuracy.},
  isbn = {9781450342926},
  langid = {english},
  keywords = {app:activity-inference,based-on:wifi,classes:9,tech:wifi,type:paper},
  annotation = {56 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{https://dl.acm.org/doi/10.1145/2999572.2999582}}
}
% == BibTeX quality report for zhangMudraUserfriendlyFinegrained2016:
% ? unused Conference name ("CoNEXT '16: The 12th International Conference on emerging Networking EXperiments and Technologies")
% ? unused Library catalog ("Semantic Scholar")

@article{zhangRealTimeSurfaceEMG2019,
  title = {Real-{{Time Surface EMG Pattern Recognition}} for {{Hand Gestures Based}} on an {{Artificial Neural Network}}},
  author = {{Zhang} and {Yang} and {Qian} and {Zhang}},
  year = {2019},
  month = jul,
  journal = {Sensors},
  volume = {19},
  number = {14},
  pages = {3170},
  issn = {1424-8220},
  doi = {10.3390/s19143170},
  urldate = {2023-06-22},
  abstract = {In recent years, surface electromyography (sEMG) signals have been increasingly used in pattern recognition and rehabilitation. In this paper, a real-time hand gesture recognition model using sEMG is proposed. We use an armband to acquire sEMG signals and apply a sliding window approach to segment the data in extracting features. A feedforward artificial neural network (ANN) is founded and trained by the training dataset. A test method is used in which the gesture will be recognized when recognized label times reach the threshold of activation times by the ANN classifier. In the experiment, we collected real sEMG data from twelve subjects and used a set of five gestures from each subject to evaluate our model, with an average recognition rate of 98.7\% and an average response time of 227.76 ms, which is only one-third of the gesture time. Therefore, the pattern recognition system might be able to recognize a gesture before the gesture is completed.},
  langid = {english},
  keywords = {app:ambiguous,based-on:gloves,classes:5,fidelity:hand,hardware:myo-armband,have-read,model:ffnn,model:nn,movement:dynamic,participants:12,reference:doi.org/10.1007/s10846-017-0725-0,reference:doi.org/10.1007/s11554-013-0333-6,reference:doi.org/10.1016/j.bspc.2016.01.011,reference:doi.org/10.1016/j.bspc.2018.07.010,reference:doi.org/10.1016/j.eswa.2012.01.102,reference:doi.org/10.1016/j.imavis.2016.06.001,reference:doi.org/10.1016/j.jocs.2018.04.019,reference:doi.org/10.1039/C8TA08276F,reference:doi.org/10.1109/CVPRW.2015.7301342,reference:doi.org/10.1109/ETCM.2017.8247458,reference:doi.org/10.1109/ICICIP.2013.6568070,reference:doi.org/10.1109/ICOM.2011.5937135,reference:doi.org/10.1109/IEMBS.2011.6091100,reference:doi.org/10.1109/THMS.2016.2537747,reference:doi.org/10.1109/TIM.2011.2161140,reference:doi.org/10.1109/TMM.2018.2808769,reference:doi.org/10.1109/TNSRE.2019.2896269,reference:doi.org/10.1142/S0129065717500095,reference:doi.org/10.1145/2814940.2814997,reference:doi.org/10.1166/SL.2018.3926,reference:doi.org/10.23919/EUSIPCO.2017.8081366,reference:doi.org/10.3390/s19020371,repetitions:30,segmentation:implicit,tech:emg,type:paper},
  annotation = {89 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1039/C8TA08276F","10.1109/TMM.2018.2808769","10.1109/ETCM.2017.8247458","10.23919/EUSIPCO.2017.8081366","10.1109/THMS.2016.2537747","10.1109/TIM.2011.2161140","10.1109/ICOM.2011.5937135","10.1109/CVPRW.2015.7301342","10.1016/j.imavis.2016.06.001","10.1007/s11554-013-0333-6","10.1109/IEMBS.2011.6091100","10.1109/EMBC.2018.8512820"] ---},
  note = {\url{https://www.mdpi.com/1424-8220/19/14/3170}},
  file = {/Users/brk/Zotero/storage/NV2YLHJB/Zhang et al. - 2019 - Real-Time Surface EMG Pattern Recognition for Hand.pdf}
}
% == BibTeX quality report for zhangRealTimeSurfaceEMG2019:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{zhangStackedLSTMBasedDynamic2021,
  title = {Stacked {{LSTM-Based Dynamic Hand Gesture Recognition}} with {{Six-Axis Motion Sensors}}},
  author = {Zhang, Yidi and Ran, Mengyuan and Liao, Jun and Su, Guoxin and Liu, Ming and Liu, Li},
  year = {2021},
  month = oct,
  journal = {2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {2568--2575},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/SMC52423.2021.9658659},
  urldate = {2023-07-11},
  abstract = {Hand gesture recognition can be exploited to benefit ubiquitous applications using sensors. Currently, the inherent complexity of human physical activities makes it difficult to accurately recognize gestures with wearable sensors, especially in real time. To this end, a real-time hand gesture recognition system is presented in this paper. In particular, sliding window technology and y-axis threshold are used to detect intended gestures from a continuous data stream and then the segmented data are classified by applying a stacked Long Short-Term Memory (LSTM) model. After noise is removed, six-axis sensor data from wrist-worn devices are fed into the model without requiring feature engineering. We use twelve common hand gestures to evaluate the performance of our model. The experimental results demonstrate the feasibility of our proposed system with an accuracy of 99.8\% on average. Our approach allows for an accurate and nonindividual hand gesture recognition. It holds potential to be integrated into a smart watch or other wearable devices for intuitive human computer interaction.},
  isbn = {9781665442077},
  keywords = {/unread,based-on:gloves,classes:12,model:lstm,model:nn,pdf:paywalled,tech:accelerometer,type:paper},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-07-11]},
  note = {\url{https://ieeexplore.ieee.org/document/9658659/}}
}
% == BibTeX quality report for zhangStackedLSTMBasedDynamic2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{zhangVisionbasedSignLanguage2004,
  title = {A Vision-Based Sign Language Recognition System Using Tied-Mixture Density {{HMM}}},
  booktitle = {Proceedings of the 6th International Conference on {{Multimodal}} Interfaces},
  author = {Zhang, Liang-Guo and Chen, Yiqiang and Fang, Gaolin and Chen, Xilin and Gao, Wen},
  year = {2004},
  month = oct,
  pages = {198--204},
  publisher = {{ACM}},
  address = {{State College PA USA}},
  doi = {10.1145/1027933.1027967},
  urldate = {2023-07-02},
  abstract = {In this paper, a vision-based medium vocabulary Chinese sign language recognition (SLR) system is presented. The proposed recognition system consists of two modules. In the first module, techniques of robust hands detection, background subtraction and pupils detection are efficiently combined to precisely extract the feature information with the aid of simple colored gloves in the unconstrained environment. Meanwhile, an effective and efficient hierarchical feature description scheme with different scale features to characterize sign language is proposed, where principal component analysis (PCA) is employed to characterize the finger features more elaborately. In the second part, a Tied-Mixture Density Hidden Markov Models (TMDHMM) framework for SLR is proposed, which can speed up the recognition without the significant loss of recognition accuracy compared with the continuous hidden Markov models (CHMM). Experimental results based on 439 frequently used Chinese sign language (CSL) words show that the proposed methods can work well for the medium vocabulary SLR in the environment without special constraints and the recognition accuracy is up to 92.5\%.},
  isbn = {978-1-58113-995-2},
  langid = {english},
  keywords = {app:chinese-sl,app:sign-language,based-on:vision,classes:439,fidelity:finger,have-read,model:hmm,model:pca,tech:rgb,type:paper},
  annotation = {69 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://dl.acm.org/doi/10.1145/1027933.1027967}},
  file = {/Users/brk/Zotero/storage/JGGD4TLX/Zhang et al. - 2004 - A vision-based sign language recognition system us.pdf}
}
% == BibTeX quality report for zhangVisionbasedSignLanguage2004:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("ICMI04: Sixth International Conference on Multimodal Interfaces 2004")
% ? unused Library catalog ("Semantic Scholar")

@article{zhangWidar3ZeroEffortCrossDomain2021,
  title = {Widar3.0: {{Zero-Effort Cross-Domain Gesture Recognition}} with {{Wi-Fi}}},
  shorttitle = {Widar3.0},
  author = {Zhang, Yi and Zheng, Yue and Qian, Kun and Zhang, Guidong and Liu, Yunhao and Wu, Chenshu and Yang, Zheng},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3105387},
  urldate = {2023-03-07},
  abstract = {With the development of signal processing technology, the ubiquitous Wi-Fi devices open an unprecedented opportunity to solve the challenging human gesture recognition problem by learning motion representations from wireless signals. Wi-Fi-based gesture recognition systems, although yield good performance on specific data domains, are still practically difficult to be used without explicit adaptation efforts to new domains. Various pioneering approaches have been proposed to resolve this contradiction but extra training efforts are still necessary for either data collection or model re-training when new data domains appear. To advance cross-domain recognition and achieve fully zero-effort recognition, we propose Widar3.0, a Wi-Fi-based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and extract domain-independent features of human gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all general model that requires only one-time training but can adapt to different data domains. Experiments on various domain factors (i.e. environments, locations, and orientations of persons) demonstrate the accuracy of 92.7\% for in-domain recognition and 82.6\%-92.4\% for cross-domain recognition without model re-training, outperforming the state-of-the-art solutions.},
  keywords = {based-on:wifi,classes:10,classes:6,model:cnn,model:nn,model:rnn,model:rnn-gru,tech:wifi,type:paper},
  annotation = {256 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{https://ieeexplore.ieee.org/document/9516988/}},
  file = {/Users/brk/Zotero/storage/9FJSTD3F/Zhang et al. - 2021 - Widar3.0 Zero-Effort Cross-Domain Gesture Recogni.pdf}
}
% == BibTeX quality report for zhangWidar3ZeroEffortCrossDomain2021:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Journal abbreviation ("IEEE Trans. Pattern Anal. Mach. Intell.")
% ? unused Library catalog ("Semantic Scholar")

@article{zhangWiFiIDHumanIdentification2016,
  title = {{{WiFi-ID}}: {{Human Identification Using WiFi Signal}}},
  shorttitle = {{{WiFi-ID}}},
  author = {Zhang, Jin and Wei, Bo and Hu, Wen and Kanhere, Salil S.},
  year = {2016},
  month = may,
  journal = {2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)},
  pages = {75--82},
  publisher = {{IEEE}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/DCOSS.2016.30},
  urldate = {2023-07-12},
  abstract = {Prior research has shown the potential of device-free WiFi sensing for human activity recognition. In this paper, we show for the first time WiFi signals can also be used to uniquely identify people. There is strong evidence that suggests that all humans have a unique gait. An individual's gait will thus create unique perturbations in the WiFi spectrum. We propose a system called WiFi-ID that analyses the channel state information to extract unique features that are representative of the walking style of that individual and thus allow us to uniquely identify that person. We implement WiFi-ID on commercial off-the-shelf devices. We conduct extensive experiments to demonstrate that our system can uniquely identify people with average accuracy of 93\% to 77\% from a group of 2 to 6 people, respectively. We envisage that this technology can find many applications in small office or smart home settings.},
  isbn = {9781509014606},
  keywords = {app:activity-inference,app:human-identification,based-on:wifi,participants:2,participants:6,tech:wifi,type:paper},
  annotation = {181 citations (Semantic Scholar/DOI) [2023-07-12]},
  note = {\url{http://ieeexplore.ieee.org/document/7536315/}},
  file = {/Users/brk/Zotero/storage/PZ2KLPXF/Zhang et al. - 2016 - WiFi-ID Human Identification Using WiFi Signal.pdf}
}
% == BibTeX quality report for zhangWiFiIDHumanIdentification2016:
% ? Title looks like it was stored in title-case in Zotero
% ? unused Library catalog ("Semantic Scholar")

@article{zhaoMultifeatureGestureRecognition2016,
  title = {Multi-Feature Gesture Recognition Based on {{Kinect}}},
  author = {Zhao, Yue and Liu, Yunda and Dong, Min and Bi, Sheng},
  year = {2016},
  month = jun,
  journal = {2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)},
  pages = {392--396},
  publisher = {{IEEE}},
  address = {{Chengdu, China}},
  doi = {10.1109/CYBER.2016.7574856},
  urldate = {2023-06-22},
  abstract = {Human Computer Interaction (HCI) has been a popular research area during the last few years. Compared with the tradition HCI methods such as using a keyboard or mouse, people prefer to have their tasks done in a more natural way. As an essential form of non-verbal communication in daily life, gesture is a good choice to turn the ideas into reality. Although various recognition methods are proposed to solve the problem, these methods are time-tensed, space-tensed or miscellaneous. This paper introduced a new method to recognize the hand gesture correctly and efficiently. The recognition is done through two phases: the skeleton phase concerning capturing and processing skeleton feature of the hand gesture, and the hand phase focusing on extracting hand contour feature of the hand gesture. Experimental results confirm an overall 94\% accuracy in recognizing and matching the pre-defined templates and robustness to backgrounds.},
  isbn = {9781509027330},
  keywords = {based-on:vision,hardware:kinect,pdf:paywalled,tech:rgbd,type:paper},
  annotation = {6 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://ieeexplore.ieee.org/document/7574856/}}
}
% == BibTeX quality report for zhaoMultifeatureGestureRecognition2016:
% ? unused Library catalog ("Semantic Scholar")

@article{zhaoRealtimeHeadGesture2017,
  title = {Real-Time Head Gesture Recognition on Head-Mounted Displays Using Cascaded Hidden {{Markov}} Models},
  author = {Zhao, Jingbo and Allison, Robert S.},
  year = {2017},
  month = oct,
  journal = {2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages = {2361--2366},
  publisher = {{IEEE}},
  address = {{Banff, AB}},
  doi = {10.1109/SMC.2017.8122975},
  urldate = {2023-03-07},
  abstract = {Head gesture is a natural means of face-to-face communication between people but the recognition of head gestures in the context of virtual reality and use of head gesture as an interface for interacting with virtual avatars and virtual environments have been rarely investigated. In the current study, we present an approach for real-time head gesture recognition on head-mounted displays using Cascaded Hidden Markov Models. We conducted two experiments to evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden Markov Models and assessed the offline classification performance using collected head motion data. In experiment 2, we characterized the real-time performance of the approach by estimating the latency to recognize a head gesture with recorded real-time classification data. Our results show that the proposed approach is effective in recognizing head gestures. The method can be integrated into a virtual reality system as a head gesture interface for interacting with virtual worlds.},
  isbn = {9781538616451},
  keywords = {based-on:vision,classes:9,exclude:wrong-topic,hardware:oculus-rift,model:hmm,tech:rgb,type:paper},
  annotation = {11 citations (Semantic Scholar/DOI) [2023-07-02] --- DOIs\_of\_references: ["10.1109/ETFA.2009.5347014","10.1109/TSMCC.2008.2001716","10.1109/ICCVW.2013.85","10.1145/166117.166134","10.1007/S12206-009-1178-6","10.1109/AFGR.1996.557258","10.1109/CW.2008.30","10.1109/5.18626","10.1109/ICPR.2006.109","10.1016/j.jvlc.2011.02.005","10.1109/ICAT.2013.6728902","10.1109/ICRA.2014.6906608","10.1207/S15327582IJPR1204\_03","10.1145/2557500.2557527","10.1109/ICPR.1996.546990","10.1145/1476589.1476686"] ---},
  note = {\url{http://ieeexplore.ieee.org/document/8122975/}},
  file = {/Users/brk/Zotero/storage/JUTLYHW4/Zhao and Allison - 2017 - Real-time head gesture recognition on head-mounted.pdf}
}
% == BibTeX quality report for zhaoRealtimeHeadGesture2017:
% ? unused Library catalog ("Semantic Scholar")

@inproceedings{zimmermanHandGestureInterface1987,
  title = {A Hand Gesture Interface Device},
  booktitle = {Proceedings of the {{SIGCHI}}/{{GI}} Conference on {{Human}} Factors in Computing Systems and Graphics Interface  - {{CHI}} '87},
  author = {Zimmerman, Thomas G. and Lanier, Jaron and Blanchard, Chuck and Bryson, Steve and Harvill, Young},
  year = {1987},
  pages = {189--192},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/29933.275628},
  urldate = {2023-03-07},
  abstract = {This paper reports on the development of a hand to machine interface device that provides real-time gesture, position and orientation information. The key element is a glove and the device as a whole incorporates a collection of technologies. Analog flex sensors on the glove measure finger bending. Hand position and orientation are measured either by ultrasonics, providing five degrees of freedom, or magnetic flux sensors, which provide six degrees of freedom. Piezoceramic benders provide the wearer of the glove with tactile feedback. These sensors are mounted on the light-weight glove and connected to the driving hardware via a small cable. Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
  isbn = {978-0-89791-213-6},
  langid = {english},
  keywords = {based-on:gloves,have-read,model:none,tech:flex,type:paper},
  annotation = {501 citations (Semantic Scholar/DOI) [2023-07-02]},
  note = {\url{http://portal.acm.org/citation.cfm?doid=29933.275628}},
  file = {/Users/brk/Zotero/storage/CTAKLA8C/Zimmerman et al. - 1987 - A hand gesture interface device.pdf}
}
% == BibTeX quality report for zimmermanHandGestureInterface1987:
% ? Unsure about the formatting of the booktitle
% ? unused Conference name ("the SIGCHI/GI conference")
% ? unused Library catalog ("Semantic Scholar")


% Required packages:
% * textcomp
